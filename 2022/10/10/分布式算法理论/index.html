<!DOCTYPE html><html lang="zh-CN" data-default-color-scheme="auto"><head><meta charset="UTF-8"><link rel="stylesheet" href="https://npm.elemecdn.com/lxgw-wenkai-screen-webfont/style.css" media="print" onload='this.media="all"'><link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png"><link rel="icon" href="/img/fluid.png"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=5,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta name="theme-color" content="#2f4154"><meta name="author" content="wugengfeng"><meta name="keywords" content="wugengfeng"><meta name="description" content="分布式理论  CAP定理 不可能三角 CAP不可能同时得到满足  CAP定理，也称为布鲁尔定理（Brewer’s Theorem），是由加州大学伯克利分校的计算机科学家Eric Brewer在2000年提出的。这个定理是分布式计算领域的一个基本原则，它描述了分布式系统在设计时需要在一致性（Consistency）、可用性（Availability）和分区容忍性（Partition toleran"><meta property="og:type" content="article"><meta property="og:title" content="分布式算法理论"><meta property="og:url" content="https://wugengfeng.cn/2022/10/10/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95%E7%90%86%E8%AE%BA/index.html"><meta property="og:site_name" content="技术博客"><meta property="og:description" content="分布式理论  CAP定理 不可能三角 CAP不可能同时得到满足  CAP定理，也称为布鲁尔定理（Brewer’s Theorem），是由加州大学伯克利分校的计算机科学家Eric Brewer在2000年提出的。这个定理是分布式计算领域的一个基本原则，它描述了分布式系统在设计时需要在一致性（Consistency）、可用性（Availability）和分区容忍性（Partition toleran"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://wugengfeng.cn/img/banner/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95.png"><meta property="article:published_time" content="2022-10-10T09:27:36.000Z"><meta property="article:modified_time" content="2023-11-14T02:41:32.818Z"><meta property="article:author" content="wugengfeng"><meta property="article:tag" content="wugengfeng"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:image" content="https://wugengfeng.cn/img/banner/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95.png"><meta name="baidu-site-verification" content="codeva-uJ0fkFPmHB"><title>分布式算法理论 - 技术博客</title><link rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css"><link rel="stylesheet" href="/css/main.css"><link id="highlight-css" rel="stylesheet" href="/css/highlight.css"><link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css"><link rel="stylesheet" href="/css/custom.css"><script id="fluid-configs">var Fluid=window.Fluid||{};Fluid.ctx=Object.assign({},Fluid.ctx);var dntVal,CONFIG={hostname:"wugengfeng.cn",root:"/",version:"1.9.3",typing:{enable:!0,typeSpeed:70,cursorChar:"_",loop:!1,scope:[]},anchorjs:{enable:!1,element:"h9",placement:"left",visible:"hover",icon:""},progressbar:{enable:!0,height_px:3,color:"#29d",options:{showSpinner:!1,trickleSpeed:100}},code_language:{enable:!0,default:"TEXT"},copy_btn:!0,image_caption:{enable:!0},image_zoom:{enable:!0,img_url_replace:["",""]},toc:{enable:!0,placement:"right",headingSelector:"h1,h2,h3,h4,h5,h6",collapseDepth:1},lazyload:{enable:!0,loading_img:"/img/loading.gif",onlypost:!1,offset_factor:2},web_analytics:{enable:!1,follow_dnt:!0,baidu:null,google:null,gtag:null,tencent:{sid:null,cid:null},woyaola:null,cnzz:null,leancloud:{app_id:null,app_key:null,server_url:null,path:"window.location.pathname",ignore_local:!1}},search_path:"/local-search.xml"};CONFIG.web_analytics.follow_dnt&&(dntVal=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,Fluid.ctx.dnt=dntVal&&(dntVal.startsWith("1")||dntVal.startsWith("yes")||dntVal.startsWith("on")))</script><script src="/js/utils.js"></script><script src="/js/color-schema.js"></script><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="技术博客" type="application/atom+xml"></head><body><header><div class="header-inner" style="height:70vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/"><strong>吴耿锋</strong> </a><button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/"><i class="iconfont icon-home-fill"></i> <span>首页</span></a></li><li class="nav-item"><a class="nav-link" href="/archives/"><i class="iconfont icon-archive-fill"></i> <span>归档</span></a></li><li class="nav-item"><a class="nav-link" href="/categories/"><i class="iconfont icon-category-fill"></i> <span>分类</span></a></li><li class="nav-item"><a class="nav-link" href="/tags/"><i class="iconfont icon-tags-fill"></i> <span>标签</span></a></li><li class="nav-item"><a class="nav-link" href="/about/"><i class="iconfont icon-user-fill"></i> <span>关于</span></a></li><li class="nav-item" id="search-btn"><a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search"><i class="iconfont icon-search"></i></a></li><li class="nav-item" id="color-toggle-btn"><a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle"><i class="iconfont icon-dark" id="color-toggle-icon"></i></a></li></ul></div></div></nav><div id="banner" class="banner" parallax="true" style="background:url(/img/default.png) no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.3)"><div class="banner-text text-center fade-in-up"><div class="h2"><span id="subtitle" data-typed-text="分布式算法理论"></span></div><div class="mt-3"><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2022-10-10 17:27" pubdate>2022年10月10日 下午</time></span></div><div class="mt-1"><span class="post-meta mr-2"><i class="iconfont icon-chart"></i> 26k 字 </span><span class="post-meta mr-2"><i class="iconfont icon-clock-fill"></i> 213 分钟</span></div></div></div></div></div></div></header><main><div class="container-fluid nopadding-x"><div class="row nomargin-x"><div class="side-col d-none d-lg-block col-lg-2"></div><div class="col-lg-8 nopadding-x-md"><div class="container nopadding-x-md" id="board-ctn"><div id="board"><article class="post-content mx-auto"><h1 style="display:none">分布式算法理论</h1><div class="markdown-body"><h2 id="分布式理论"><a class="markdownIt-Anchor" href="#分布式理论"></a> 分布式理论</h2><h3 id="cap定理"><a class="markdownIt-Anchor" href="#cap定理"></a> CAP定理</h3><p><code>不可能三角</code> <code>CAP不可能同时得到满足</code></p><p><img src="/2022/10/10/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95%E7%90%86%E8%AE%BA/cap.png" srcset="/img/loading.gif" lazyload alt></p><p>CAP定理，也称为布鲁尔定理（Brewer’s Theorem），是由加州大学伯克利分校的计算机科学家Eric Brewer在2000年提出的。这个定理是分布式计算领域的一个基本原则，它描述了分布式系统在设计时需要在一致性（Consistency）、可用性（Availability）和分区容忍性（Partition tolerance）三个方面做出权衡。</p><div class="note note-warning"><p><code>C</code>：Consistency（一致性）</p><p><code>A</code>：Availability（可用性）</p><p><code>P</code>：Partition tolerance（分区容忍性）</p></div><p>CAP定理的核心观点是，在任何给定的时刻，一个分布式系统只能同时满足上述三个要素中的两个。换句话说，如果系统保证了分区容忍性，它就必须在一致性和可用性之间做出选择。这个权衡的存在是因为在发生网络分区时，系统不可能同时保证数据的绝对一致性和高可用性。</p><p class="note note-primary">Partition Tolerance 分区容忍性</p><p><code>网络波动</code> <code>通讯故障</code></p><p><img src="/2022/10/10/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95%E7%90%86%E8%AE%BA/%E7%BD%91%E7%BB%9C%E5%88%86%E5%8C%BA.png" srcset="/img/loading.gif" lazyload alt></p><p>分区容忍性是指系统能够持续提供服务，即使出现了网络分区，也就是说，一些节点之间的通信可能因为网络故障而中断。<span class="green-line">在现实世界中，网络分区是不可避免的，因此分区容忍性在分布式系统中是必须要考虑的。</span></p><p><code>解决方案</code>：增强分区容忍性，通常会在多个节点间复制数据，以确保即使在某些节点之间的网络连接失败时，系统的其他部分仍然可以继续运行。这种策略的关键在于，数据的多个副本分布在不同的网络分区中，可以独立地接受和处理请求。</p><p><code>AC悖论</code></p><ul><li>如果选择一致性和分区容忍性（CP），系统在网络分区发生时会牺牲可用性来保证数据的一致性（强一致性，任何节点宕机都不允许进行写操作）。这意味着某些操作可能会因为无法保证数据的一致性而不被处理。</li><li>如果选择可用性和分区容忍性（AP），系统在网络分区发生时会牺牲一致性来保证服务的可用性。在这种情况下，用户可能会读到过时的数据。</li></ul><p class="note note-primary">Consistency 一致性</p><p><img src="/2022/10/10/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95%E7%90%86%E8%AE%BA/%E4%B8%80%E8%87%B4%E6%80%A7.png" srcset="/img/loading.gif" lazyload alt></p><p>一致性意味着所有节点在同一时间看到的数据是一样的。换句话说，如果一个数据项在分布式系统的一个节点上被更新，那么所有的其他节点都应该立即知道这个更新。这是一个强一致性模型，类似于关系数据库中的事务，它保证了数据的一致性视图。</p><p class="note note-primary">Availability 可用性</p><p><img src="/2022/10/10/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95%E7%90%86%E8%AE%BA/%E5%8F%AF%E7%94%A8%E6%80%A7.png" srcset="/img/loading.gif" lazyload alt></p><p>可用性是指系统的每个请求都能在有限的时间内收到一个响应，不管是成功还是失败的响应。在分布式系统中，即使某些节点宕机，系统仍然需要对外提供服务。</p><div class="note note-warning"><p>由于实现强一致性的代价很高，可能会显著影响写入性能，因此许多开源的分布式系统选择在CAP定理的CP模型中实现最终一致性或线性一致性，这些是相对较弱的一致性模型。这种在可用性（Availability）和一致性（Consistency）之间的权衡催生了BASE理论，它强调在分布式系统中基本可用性、软状态和最终一致性的重要性。</p></div><h3 id="base理论"><a class="markdownIt-Anchor" href="#base理论"></a> BASE理论</h3><p>BASE理论是在CAP定理的背景下发展起来的，它是对一致性（Consistency）和可用性（Availability）之间权衡的结果。该理论的核心思想是，<span class="green-line">虽然在分布式系统中难以实现强一致性，但是每个应用都可以根据自己的业务需求，采用适当的策略来确保系统最终达到一致性（Eventual Consistency）。</span></p><p>在CAP定理指出一致性（C）和可用性（A）难以同时满足的情况下，BASE理论提出了一种替代方案。这种方案通过放宽对强一致性的要求，来提高系统的可用性和容错性。这样的设计允许在分布式系统中实现更高的性能和可伸缩性。</p><div class="note note-primary"><p><strong>Basically Available（基本可用）</strong></p><p>基本可用指的是分布式系统在遇到故障时，保证核心功能可用。这意味着在发生故障时，系统可能会提供服务降级，但核心功能仍然是可用的。例如，一个在线商店在高流量期间可能会暂时关闭评论功能，以确保核心的购物车和结账功能仍然可以使用。</p></div><div class="note note-primary"><p><strong>Soft state（软状态）</strong></p><p>软状态意味着系统的状态不需要时刻保持一致，而是可以有一段时间的不一致。这允许系统在不同步的情况下继续运行，而不是强制每次操作都立即同步。</p></div><div class="note note-primary"><p><strong>Eventually consistent（最终一致性）</strong></p><p>最终一致性是指，系统中的所有数据副本，在经过一段时间的同步之后，最终会达到一个一致的状态。这是对强一致性要求的放松，允许系统在一定时间内存在不一致的状态。</p></div><div class="note note-warning"><p><strong>案例说明</strong></p><p>假设有一个大型在线商店，它需要处理成千上万的用户请求，包括浏览商品、添加到购物车、结账等操作。在这种高流量的环境中，采用BASE理论可以帮助商店在保持高可用性的同时，处理数据一致性问题。</p><ul><li><p><strong>Basically Available（基本可用）</strong>：</p><p>在促销或高流量期间，为了保证核心交易系统的可用性，商店可能会暂时降级某些非关键功能，比如推荐算法或用户评论。这意味着即使在高负载下，用户仍然可以浏览商品、添加到购物车并完成购买，尽管某些附加功能可能暂时不可用。</p></li><li><p><strong>Soft state（软状态）</strong>：</p><p>商品的库存信息可能不会实时更新。例如，当一个用户将商品添加到购物车时，系统不会立即从库存中扣除。相反，库存的准确计算可能会延迟进行，以减少对数据库的即时写入压力。这意味着在高并发情况下，系统的状态（如库存量）是“软的”，可能不会立即反映最新情况。</p></li><li><p><strong>Eventually consistent（最终一致性）</strong>：</p><p>用户完成订单支付后，用户积分不会马上赠送，但在经过一段时间后，最终会将积分回馈给用户。</p></li></ul><p>通过这种方式，在线商店能够在保持高可用性和良好用户体验的同时，处理大规模数据和请求。虽然这可能导致短暂的数据不一致，但这种不一致是可控的，并且不会严重影响用户的购物体验。</p></div><h3 id="akf拆分原则"><a class="markdownIt-Anchor" href="#akf拆分原则"></a> AKF拆分原则</h3><p>随着项目规模的不断扩大，传统的单体架构往往无法有效地满足日益增长的性能和可伸缩性需求。在这种背景下，转向分布式系统成为了一种必要的选择。分布式系统的设计和管理带来了一系列挑战，尤其是在可伸缩性（Scalability）方面。为了应对这些挑战，需要一种系统化的方法论来指导分布式系统的设计和扩展。AKF立方体模型正是为此而提出的，它提供了一种系统化的框架来指导技术架构和组织结构的扩展。</p><p>AKF 把系统扩展分为以下三个维度</p><p><img src="/2022/10/10/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95%E7%90%86%E8%AE%BA/AKF%E5%8E%9F%E5%88%99.png" srcset="/img/loading.gif" lazyload alt></p><div class="note note-primary"><p><strong>X轴 水平扩展</strong></p><p><img src="/2022/10/10/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95%E7%90%86%E8%AE%BA/X%E8%BD%B4%E6%89%A9%E5%B1%95.png" srcset="/img/loading.gif" lazyload alt></p><p>X轴扩展涉及在水平方向上增加相同类型的资源（如服务器、数据库实例）来处理更多的工作负载。这种扩展方式通常用于处理增加的用户数量或请求量。例如，一个Web应用可以通过增加更多的Web服务器来分散流量，从而提高整体的处理能力。</p></div><div class="note note-primary"><p><strong>Y轴 服务分解</strong></p><p><img src="/2022/10/10/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95%E7%90%86%E8%AE%BA/Y%E8%BD%B4%E6%89%A9%E5%B1%95.png" srcset="/img/loading.gif" lazyload alt></p><p>Y轴扩展涉及将应用程序或服务分解为更小、更专注的部分，这通常是通过微服务架构来实现的。每个微服务负责应用程序的一个特定功能或业务领域。这种方法不仅有助于提高可伸缩性，还有助于提高团队的敏捷性，因为每个团队可以独立开发和部署其负责的服务。</p></div><div class="note note-primary"><p><strong>Z轴 数据分片</strong></p><p><img src="/2022/10/10/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95%E7%90%86%E8%AE%BA/Z%E8%BD%B4%E6%89%A9%E5%B1%95.png" srcset="/img/loading.gif" lazyload alt></p><p>Z轴扩展涉及将数据分割成多个分片，每个分片可以独立存储和处理。这种方式常用于数据库，可以通过数据的关键属性（如用户ID）来分割数据。每个分片可以部署在不同的服务器上，从而分散负载和减少单点故障的风险。</p></div><p>假设对一个单体应用的电子商务平台进行AKF立体模型扩展。主要解决原有架构的可伸缩性和性能问题。水平扩展提高了处理高流量的能力，服务分解提高了开发效率和系统的灵活性，数据分片则解决了数据库的性能瓶颈。这使得平台能够有效地支持用户增长和业务扩展，同时保持高效的运营和快速的市场响应。</p><h2 id="分布式算法"><a class="markdownIt-Anchor" href="#分布式算法"></a> 分布式算法</h2><h3 id="一致性"><a class="markdownIt-Anchor" href="#一致性"></a> 一致性</h3><blockquote><p><strong>什么是一致性</strong></p><p>在分布式系统中，一致性（Consistency）是指多副本（Replications）问题中的数据一致性。可以分为<code>强一致性</code>、<code>顺序一致性</code>与<code>弱一致性</code></p><hr><p><strong>一致性的种类</strong></p><ul><li>事务一致性</li><li>数据一致性</li></ul><hr><p><strong>强一致性（Strict Consistency）</strong></p><p>也称为：</p><ul><li><strong>原子一致性（Atomic Consistency）</strong></li><li><strong>线性一致性（Linearizable Consistency）</strong></li></ul><p>两个要求：</p><ul><li>任何一次读都能读到某个数据的最近一次写的数据。</li><li>系统中的所有进程，看到的操作顺序，都和全局时钟下的顺序一致。</li></ul><p><span style="border-bottom:2px dashed green">简言之，在任意时刻，所有节点中的数据是一样的。</span></p><p>例如，对于关系型数据库，要求更新过的数据能被后续的访问都能看到（整个集群的更改过程是同步的），这是强一致性。</p><hr><p><strong>顺序一致性（Sequential Consistency）</strong></p><p>任何执行的结果都是一样的，就好像所有处理器的操作都按某种顺序执行一样，并且每个单独处理器的操作按其程序指定的顺序出现在这个顺序中</p><p>两个要求：</p><ul><li>任何一次读都能读到某个数据的最近一次写的数据。</li><li><span style="border-bottom:2px dashed green">系统的所有进程的顺序一致，而且是合理的</span>。即不需要和全局时钟下的顺序一致，错的话一起错，对的话一起对。</li></ul><hr><p><strong>弱一致性</strong></p><p><span style="border-bottom:2px dashed green">数据更新后，如果能容忍后续的访问只能访问到部分或者全部访问不到，则是弱一致性</span></p><hr><p><strong>最终一致性</strong></p><p>最终一致性属于弱一致性</p><p><span style="border-bottom:2px dashed green">不保证在任意时刻任意节点上的同一份数据都是相同的，但是在一段时间后，节点间的数据会最终达到一致状态</span></p></blockquote><h3 id="一致性hash算法"><a class="markdownIt-Anchor" href="#一致性hash算法"></a> 一致性hash算法</h3><h4 id="hash-算法"><a class="markdownIt-Anchor" href="#hash-算法"></a> hash 算法</h4><blockquote><p>MD系列(MD5)、SHA系列(SHA-1)、CRC，甚至JDK hashCode()也是哈希算法的一种。可以将他们分成三代：</p><p><code>第一代</code>：SHA-1（1993），MD5（1992），CRC（1975），Lookup3（2006）<br><code>第二代</code>：MurmurHash（2008）<br><code>第三代</code>：CityHash， SpookyHash（2011）</p><p>分类可分为加密型、非加密型：</p><p><code>加密型</code>：MD系列(MD5)、SHA系列(SHA-1)</p><p><code>非加密型</code>：CRC、MurmurHash</p><hr><p><strong>使用场景</strong>：</p><p><code>MD5</code>：消息摘要算法，常见的场景就是下载文件时进行MD5值验证</p><p><code>SHA</code>：安全散列算法，该算法的思想是接收一段明文，然后以一种不可逆的方式将它转换成一段密文，常用于明文密码加密</p><p><code>CRC</code>：循环冗余校验码，简称循环码，是一种常用的、具有检错、纠错能力的校验码，在早期的通信中运用广泛</p><p><code>murmurhash</code>：MurmurHash 是一种非加密型哈希函数，<span style="border-bottom:2px dashed green">适用于一般的哈希检索操作</span></p><hr><p><strong>murmurhash</strong></p><ul><li>高运算性能</li><li>低碰撞率</li></ul><p>Redis，Memcached，Cassandra，Hadoop，HBase，Lucene，spark，nginx，常见的大数据库底层，都使用了这个算法作为底层的存储算法</p></blockquote><h4 id="hash环"><a class="markdownIt-Anchor" href="#hash环"></a> hash环</h4><blockquote><p><code>分布式缓存</code> <code>负载均衡算法</code></p><p>​	一致性Hash是一种特殊的Hash算法，由于其均衡性、持久性的映射特点，被广泛的应用于负载均衡领域，如nginx和memcached都采用了一致性Hash来作为集群负载均衡的方案</p><hr><p><strong>传统hash</strong></p><p>​	<span style="border-bottom:2px dashed green">要了解一致性哈希，首先我们必须了解传统的哈希及其在大规模分布式系统中的局限性</span>。简单地说，哈希就是一个键值对存储，在给定键的情况下，可以非常高效地找到所关联的值。假设我们要根据其邮政编码查找城市中的街道名称。一种最简单的实现方式是将此信息以哈希字典的形式进行存储 <code>&lt;Zip Code，Street Name&gt;</code></p><p>​	当数据太大而无法存储在一个节点或机器上时，问题变得更加有趣，系统中需要多个这样的节点或机器来存储它。比如，<a href="/2022/05/01/elasticsearch/#%E8%B7%AF%E7%94%B1%E8%AE%A1%E7%AE%97">Elasticsearch的分片路由方案</a>。<span style="border-bottom:2px dashed green">那如何确定哪个 key 存储在哪个节点上？针对该问题，最简单的解决方案是使用哈希取模来确定。</span> 给定一个 key，先对 key 进行哈希运算，将其除以系统中的节点数，然后将该 key 放入该节点。同样，在获取 key 时，对 key 进行哈希运算，再除以节点数，然后转到该节点并获取值。上述过程对应的哈希算法定义如下</p><p><code>routing = hash(key) % len # 其中 len 为节点数</code></p><p><img src="/2022/10/10/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95%E7%90%86%E8%AE%BA/%E4%BC%A0%E7%BB%9Fhash.png" srcset="/img/loading.gif" lazyload alt></p><p><strong>缺点</strong></p><p>​	其实通过<a href="/2022/05/01/elasticsearch/#%E5%90%88%E7%90%86%E8%AE%BE%E7%BD%AE%E5%88%86%E7%89%87%E6%95%B0">Elasticsearch的分片策略</a>就可以得出传统hash的局限性，当Elasticsearch的主分片数量设置后，是不能修改的，因为数据通过 <code>hash(key) % len</code> 计算后就散列在某个主分片上了，如果修改主分片数量（修改 len）则key的路由会失败。所以传统hash是不具备动态<code>扩容</code> <code>缩容</code>能力的</p><hr><p><strong>一致性哈希算法</strong></p><p>​	为了解决传统hash上的缺陷，一致性哈希算法应运而生</p><p>​	一致性哈希算法在 1997 年由麻省理工学院提出，是一种特殊的哈希算法，在移除或者添加一个服务器时，能够尽可能小地改变已存在的服务请求与处理请求服务器之间的映射关系。一致性哈希解决了简单哈希算法在分布式哈希表（Distributed Hash Table，DHT）中存在的动态伸缩等问题</p><hr><p><strong>一致性hash算法的特性</strong></p><ul><li><p><code>平衡性</code>：尽可能让数据尽可能分散到所有节点上，避免造成极其不均匀</p></li><li><p><code>单调性</code>：要求在新增或者减少节点的时候，原有的结果绝大部分不受影响，而新增的数据尽可能分配到新加的节点</p></li><li><p><code>分散性</code>：好的算法在不同终端，针对相同的数据的计算，得到的结果应该是一样的，一致性要很强</p></li><li><p><code>负载</code>：针对相同的节点，避免被不同终端映射不同的内容</p></li><li><p><code>平滑性</code>：对于增加节点或者减少节点，应该能够平滑过渡</p></li></ul><hr><p><strong>一致性哈希算法原理</strong></p><p><span style="border-bottom:2px dashed green">一致性哈希算法通过一个叫作一致性哈希环的数据结构实现。这个环的起点是 0，终点是 2^32 - 1，并且起点与终点连接，故这个环的整数分布范围是 [0, 2^32-1]</span></p><p><img src="/2022/10/10/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95%E7%90%86%E8%AE%BA/hash%E7%8E%AF.png" srcset="/img/loading.gif" lazyload alt></p><hr><p><strong>算法原理</strong></p><ul><li>虚拟一个环的概念，在环上构造一个0～2^32-1个点</li><li>将N台服务器节点计算Hash值，映射到这个环上</li></ul><p>可以用节点的IP或者主机名来计算hash值，得到一个Hash环</p><p><img src="/2022/10/10/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95%E7%90%86%E8%AE%BA/hash%E7%8E%AF%E6%9C%8D%E5%8A%A1%E6%98%A0%E5%B0%84.png" srcset="/img/loading.gif" lazyload alt></p><p>将数据用相同的Hash算法计算的值，映射到这个环上</p><p><img src="/2022/10/10/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95%E7%90%86%E8%AE%BA/hash%E7%8E%AFKey%E6%98%A0%E5%B0%84.png" srcset="/img/loading.gif" lazyload alt></p><p>然后<span style="border-bottom:2px dashed green">顺时针寻找，找到的第一个服务器节点就是目标要保存的节点，如果超过2^32-1，就放到第一个节点</span></p><p><img src="/2022/10/10/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95%E7%90%86%E8%AE%BA/hash%E7%8E%AF%E6%98%A0%E5%B0%84.png" srcset="/img/loading.gif" lazyload alt></p><ul><li>key1、key2 被映射到NodeB</li><li>key3、key4、key5 被映射到NodeB</li><li>key6、key7、key8 被映射到NodeA</li></ul><hr><p><strong>动态扩容</strong></p><p>​	假设随着业务的扩大，整个集群需要扩展一个服务节点Node D,经过同样的 hash 运算，该服务器最终落于 Node A 和 Node C 服务器之间，具体如下图所示：</p><p><img src="/2022/10/10/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95%E7%90%86%E8%AE%BA/hash%E7%8E%AF%E6%96%B0%E5%A2%9E%E8%8A%82%E7%82%B9.png" srcset="/img/loading.gif" lazyload alt></p><p>​	对于上述的情况，只有 Node A 和 Node C之间的key需要重新分配。在以上示例中key6、key7 需要重新分配，即它被重新到 Node D 。在前面我们已经分析过，如果使用简单的取模方法，当新添加服务器时可能会导致大部分缓存失效，而使用一致性哈希算法后，这种情况得到了较大的改善，因为只有少部分对象需要重新分配</p><p><img src="/2022/10/10/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95%E7%90%86%E8%AE%BA/hash%E7%8E%AF%E9%87%8D%E6%96%B0%E5%88%86%E9%85%8D.png" srcset="/img/loading.gif" lazyload alt></p><hr><p><strong>故障转移（动态缩容）</strong></p><p>​	假设某个服务节点宕机，只要hash环上还存在可用节点，那么服务整体还是可用的</p><p><img src="/2022/10/10/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95%E7%90%86%E8%AE%BA/hash%E7%8E%AF%E8%8A%82%E7%82%B9%E5%AE%95%E6%9C%BA.png" srcset="/img/loading.gif" lazyload alt></p><p>​	假设Node B宕机，那么key1、key2将跳过故障节点Node B，继续顺时针找到第一个可用节点Node C，由Node C继续提供服务，实现故障转移</p><hr><p><strong>数据倾斜（虚拟节点）</strong></p><p><img src="/2022/10/10/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95%E7%90%86%E8%AE%BA/hash%E7%8E%AF%E5%88%86%E5%B8%83%E4%B8%8D%E5%9D%87.png" srcset="/img/loading.gif" lazyload alt></p><p>​	假设根据hash计算后，服务节点在hash环上散列不均匀。那么就会导致数据倾斜，某个服务节点就会承受较高的服务压力，整个集群变得不稳定。解决方法就是在环上增加虚拟节点，尽量让服务节点在环上分布均匀，从而实现负载均衡</p><p><img src="/2022/10/10/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95%E7%90%86%E8%AE%BA/hash%E7%8E%AF%E8%99%9A%E6%8B%9F%E8%8A%82%E7%82%B9.png" srcset="/img/loading.gif" lazyload alt></p><hr><p><strong>不带虚拟节点的hash一致性算法</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">import</span> java.util.SortedMap;<br><span class="hljs-keyword">import</span> java.util.TreeMap;<br><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">ConsistentHashingWithoutVirtualNode</span> &#123;<br>    <span class="hljs-comment">//待添加入Hash环的服务器列表</span><br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">static</span> String[] servers = &#123;<span class="hljs-string">&quot;192.168.0.1:8888&quot;</span>, <span class="hljs-string">&quot;192.168.0.2:8888&quot;</span>, <br>      <span class="hljs-string">&quot;192.168.0.3:8888&quot;</span>&#125;;<br><br>    <span class="hljs-comment">//key表示服务器的hash值，value表示服务器</span><br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">static</span> SortedMap&lt;Integer, String&gt; sortedMap = <span class="hljs-keyword">new</span> <span class="hljs-title class_">TreeMap</span>&lt;Integer, String&gt;();<br><br>    <span class="hljs-comment">//程序初始化，将所有的服务器放入sortedMap中</span><br>    <span class="hljs-keyword">static</span> &#123;<br>        <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> <span class="hljs-variable">i</span> <span class="hljs-operator">=</span> <span class="hljs-number">0</span>; i &lt; servers.length; i++) &#123;<br>            <span class="hljs-type">int</span> <span class="hljs-variable">hash</span> <span class="hljs-operator">=</span> getHash(servers[i]);<br>            System.out.println(<span class="hljs-string">&quot;[&quot;</span> + servers[i] + <span class="hljs-string">&quot;]加入集合中, 其Hash值为&quot;</span> + hash);<br>            sortedMap.put(hash, servers[i]);<br>        &#125;<br>    &#125;<br><br>    <span class="hljs-comment">//得到应当路由到的结点</span><br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">static</span> String <span class="hljs-title function_">getServer</span><span class="hljs-params">(String key)</span> &#123;<br>        <span class="hljs-comment">//得到该key的hash值</span><br>        <span class="hljs-type">int</span> <span class="hljs-variable">hash</span> <span class="hljs-operator">=</span> getHash(key);<br>        <span class="hljs-comment">//得到大于该Hash值的所有Map</span><br>        SortedMap&lt;Integer, String&gt; subMap = sortedMap.tailMap(hash);<br>        <span class="hljs-keyword">if</span> (subMap.isEmpty()) &#123;<br>            <span class="hljs-comment">//如果没有比该key的hash值大的，则从第一个node开始</span><br>            <span class="hljs-type">Integer</span> <span class="hljs-variable">i</span> <span class="hljs-operator">=</span> sortedMap.firstKey();<br>            <span class="hljs-comment">//返回对应的服务器</span><br>            <span class="hljs-keyword">return</span> sortedMap.get(i);<br>        &#125; <span class="hljs-keyword">else</span> &#123;<br>            <span class="hljs-comment">//第一个Key就是顺时针过去离node最近的那个结点</span><br>            <span class="hljs-type">Integer</span> <span class="hljs-variable">i</span> <span class="hljs-operator">=</span> subMap.firstKey();<br>            <span class="hljs-comment">//返回对应的服务器</span><br>            <span class="hljs-keyword">return</span> subMap.get(i);<br>        &#125;<br>    &#125;<br><br>    <span class="hljs-comment">//使用FNV1_32_HASH算法计算服务器的Hash值</span><br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">static</span> <span class="hljs-type">int</span> <span class="hljs-title function_">getHash</span><span class="hljs-params">(String str)</span> &#123;<br>        <span class="hljs-keyword">final</span> <span class="hljs-type">int</span> <span class="hljs-variable">p</span> <span class="hljs-operator">=</span> <span class="hljs-number">16777619</span>;<br>        <span class="hljs-type">int</span> <span class="hljs-variable">hash</span> <span class="hljs-operator">=</span> (<span class="hljs-type">int</span>) <span class="hljs-number">2166136261L</span>;<br>        <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> <span class="hljs-variable">i</span> <span class="hljs-operator">=</span> <span class="hljs-number">0</span>; i &lt; str.length(); i++)<br>            hash = (hash ^ str.charAt(i)) * p;<br>        hash += hash &lt;&lt; <span class="hljs-number">13</span>;<br>        hash ^= hash &gt;&gt; <span class="hljs-number">7</span>;<br>        hash += hash &lt;&lt; <span class="hljs-number">3</span>;<br>        hash ^= hash &gt;&gt; <span class="hljs-number">17</span>;<br>        hash += hash &lt;&lt; <span class="hljs-number">5</span>;<br><br>        <span class="hljs-comment">// 如果算出来的值为负数则取其绝对值</span><br>        <span class="hljs-keyword">if</span> (hash &lt; <span class="hljs-number">0</span>)<br>            hash = Math.abs(hash);<br>        <span class="hljs-keyword">return</span> hash;<br>    &#125;<br><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">main</span><span class="hljs-params">(String[] args)</span> &#123;<br>        String[] keys = &#123;<span class="hljs-string">&quot;semlinker&quot;</span>, <span class="hljs-string">&quot;kakuqo&quot;</span>, <span class="hljs-string">&quot;fer&quot;</span>&#125;;<br>        <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> <span class="hljs-variable">i</span> <span class="hljs-operator">=</span> <span class="hljs-number">0</span>; i &lt; keys.length; i++)<br>            System.out.println(<span class="hljs-string">&quot;[&quot;</span> + keys[i] + <span class="hljs-string">&quot;]的hash值为&quot;</span> + getHash(keys[i])<br>                    + <span class="hljs-string">&quot;, 被路由到结点[&quot;</span> + getServer(keys[i]) + <span class="hljs-string">&quot;]&quot;</span>);<br>    &#125;<br><br>&#125;<br></code></pre></td></tr></table></figure><p>参考文章：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/146011745">https://zhuanlan.zhihu.com/p/146011745</a></p></blockquote><h4 id="hash槽"><a class="markdownIt-Anchor" href="#hash槽"></a> hash槽</h4><h3 id="共识算法"><a class="markdownIt-Anchor" href="#共识算法"></a> 共识算法</h3><h4 id="拜占庭将军问题"><a class="markdownIt-Anchor" href="#拜占庭将军问题"></a> 拜占庭将军问题</h4><p><img src="/2022/10/10/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95%E7%90%86%E8%AE%BA/%E6%8B%9C%E5%8D%A0%E5%BA%AD%E5%B0%86%E5%86%9B.png" srcset="/img/loading.gif" lazyload alt></p><div class="note note-warning"><p>想象拜占庭帝国的军队在攻击一个城市。军队被分散到城市周围的几个营地中，每个营地由一位将军指挥。为了攻占城市，将军们必须达成共识决定他们是同时进攻还是撤退。将军们通过信使来发送和接收消息，因为他们相隔很远，不能直接通讯。</p><p>问题在于，<span class="green-line">一些将军可能是叛徒，他们可能会故意发送错误的攻击计划</span>。例如，忠诚的将军可能决定明天早上进攻，但是叛徒将军可能告诉一些将军明天进攻，告诉其他将军撤退。由于信使可能也是叛徒，<span class="green-line">消息在传递过程中可能被篡改。</span></p></div><p><strong><code>问题场景</code></strong>：</p><ul><li>有若干个将军，他们需要就是否进攻或撤退达成一致决策。</li><li>这些将军分散在不同位置，仅能通过传递消息进行通信。</li><li>至少一个将军可能是叛徒，他可能会发送错误的消息或篡改消息。</li></ul><p><strong><code>问题复杂性</code></strong>：</p><ul><li>叛徒可能不仅仅是发送错误的信息，他们可能会根据其他将军的行为来决定自己的行为，以最大化混乱和误导。</li><li>叛徒的行为模式可能非常复杂，包括但不限于假装忠诚、完全沉默或不一致的消息传递。</li></ul><p><strong><code>问题本质</code></strong>：</p><p>​ 分布式系统如何在部分组件可能发生故障或表现出恶意行为的情况下，仍然能够达成一致的决策。</p><p><strong><code>解决方案要求</code></strong>：</p><ul><li>所有忠诚的将军必须达成相同的决策。</li><li>如果所有忠诚的将军都决定进攻，那么他们应该进攻；如果一个忠诚的将军认为应该撤退，那么他们都应该撤退（以避免部分军队进攻导致的灾难性后果）。</li></ul><p><strong><code>解决方案挑战</code></strong>：</p><ul><li>忠诚将军之间的通信可能被叛徒所干扰。</li><li>忠诚将军需要一种方法来确定哪些消息是可信的。</li></ul><p><strong><code>实际应用</code></strong>：</p><p>在实际的计算机网络和分布式系统中，拜占庭将军问题等同于如何在可能存在恶意节点的系统中达成共识。这个问题的解决方案形成了拜占庭容错（Byzantine Fault Tolerance, BFT）算法的基础，这类算法能够确保系统即使在一些节点表现出任意或恶意行为时仍然能够正常工作。</p><p>拜占庭容错性在现代分布式系统中非常重要，特别是在需要高安全性的系统中，如金融服务、航空交通控制系统和区块链技术。例如，比特币的区块链网络使用了工作量证明（Proof of Work）机制来实现拜占庭容错，并确保整个网络达成共识。</p><h4 id="paxos共识算法"><a class="markdownIt-Anchor" href="#paxos共识算法"></a> Paxos共识算法</h4><p>具有高度容错的分布式共识算法`</p><p><code>解决分布式系统就某个值（提议）达成一致</code></p><p><a target="_blank" rel="noopener" href="http://harry.me/blog/2014/12/27/neat-algorithms-paxos">动态演示地址</a></p><p class="note note-primary">背景</p><p>Paxos算法由Leslie Lamport于1989年提出，并在1998年详细描述和发表。Lamport是分布式系统理论的先驱，他通过Paxos算法提供了一个基础解决方案，该方案能够保证即使在某些节点失效的情况下，系统也能达成一致的决策。Paxos算法以古希腊岛屿Paxos命名，Lamport用它来描述一个虚构的议会系统，以帮助理解这一抽象的问题。</p><p><code>前提条件</code>：非拜占庭将军问题，保证军队没有叛徒（即通信是保证可靠的不会被传改的），但是可以存在丢失延迟等问题</p><p class="note note-primary">角色介绍</p><ol><li><code>提议者（Proposer）</code>：<ul><li>负责发起提案（proposal），每个提案包含一个唯一的编号（proposal number）和一个建议值（value）。</li><li>提议者需要确保其提案编号是唯一的，并且在其生命周期中递增。</li><li>提议者的目标是让其提案获得足够多的接受者（Acceptor）的接受。</li></ul></li><li><code>接受者（Acceptor）</code>：<ul><li>在Paxos算法中起决定性作用，它们对提议者的提案给予响应。</li><li>接受者可以决定是否对收到的提案做出承诺（promise）不接受任何较低编号的提案，以及是否接受（accept）提议者的提案。</li><li>一个提案如果得到了集群中超过半数的接受者的接受，就被认为是被“选择”（chosen）了。</li></ul></li><li><code>学习者（Learner）</code>：<ul><li>学习者的作用是了解被多数接受者接受的提案的值。</li><li>学习者不参与提案的接受或拒绝过程，但它们需要知道哪个提案被最终确认，以便系统可以按照这个值进行操作。</li></ul></li></ol><ul><li><strong>Proposal</strong> : 提议 提议编号n和内容value</li></ul><p><code>算法最终目标</code>：每个Proposer, Acceptor, Learner都认为同一个Proposal中的value被选中。</p><p>Paxos共识算法追求的是线性一致性。</p><p>参考文章：<a target="_blank" rel="noopener" href="https://juejin.cn/post/6844903817297788942">https://juejin.cn/post/6844903817297788942</a></p><p>参考B站视频：<a target="_blank" rel="noopener" href="https://www.bilibili.com/medialist/play/ml1801382893/BV1kA411G7cK?oid=333101519&amp;otype=2">https://www.bilibili.com/medialist/play/ml1801382893/BV1kA411G7cK?oid=333101519&amp;otype=2</a></p><h5 id="basic-paxos"><a class="markdownIt-Anchor" href="#basic-paxos"></a> Basic Paxos</h5><p><code>单提议的分布式共识算法,只是理论,没有被实现</code></p><p>一个或多个提议进程 (Proposer) 可以发起提案 (Proposal)，Paxos算法使所有提案中的某一个提案，在所有进程中达成一致。系统中的多数派同时认可该提案，即达成了一致。<span style="border-bottom:2px dashed green">最多只针对一个确定的提案达成一致</span>。</p><p><img src="/2022/10/10/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95%E7%90%86%E8%AE%BA/BasicPaxos%E6%B5%81%E7%A8%8B.png" srcset="/img/loading.gif" lazyload alt></p><p class="note note-primary">两阶段式提交</p><p><code>Prepare阶段</code></p><ul><li>提议者（Proposer）生成一个提案编号，并向所有接受者（Acceptors）发送一个包含该编号的Prepare请求。</li><li>接受者（Acceptor）收到Prepare请求后，如果提案编号大于它之前承诺过的任何编号，它会向提议者发出承诺，不再接受编号小于该提案编号的任何提案，并且回复它所接受的最大编号的提案（如果有的话）。</li></ul><p><code>Accept阶段</code></p><ul><li>如果提议者从多数接受者处收到了承诺回应，它将向所有接受者发送Accept请求，该请求包含提案编号和提案值。</li><li>当接受者收到Accept请求时，如果它没有对更高编号的提案做出过承诺，则接受该提案。</li></ul><p class="note note-primary">活锁</p><p>Basic Paxos中产生活锁（Livelock）的原因主要是因为它允许多个提议者（Proposers）并发地尝试推动他们的提案，而这些提案可能会相互冲突。活锁的情况通常出现在一个高度竞争的环境中，其中每个提议者都试图将其提案编号（Proposal Number）设置为最高，以便它们的提案被接受者（Acceptors）接受。</p><p>以下是产生活锁的几个具体原因：</p><ol><li><code>竞争条件</code>：<ul><li>多个提议者可能同时发起提案，每个提议者都试图使用比其他人更高的提案编号。</li><li>由于网络延迟，不同提议者的提案可能几乎同时到达接受者。</li></ul></li><li><code>多数接受者承诺</code>：<ul><li>接受者承诺接受最高编号的提案，如果不同提议者的提案编号不断增加，接受者可能不断改变其承诺。</li></ul></li><li><code>不断的提案编号增加</code>：<ul><li>提议者收到拒绝其提案的消息时，通常会选择一个更高的编号重试。</li><li>如果提议者不断地观察到其他提议者使用更高的编号，它们也会相应地增加自己的编号，并重新发起提案。</li></ul></li></ol><h5 id="multi-paxos"><a class="markdownIt-Anchor" href="#multi-paxos"></a> Multi Paxos</h5><p class="note note-primary">Multi Paxos 做出的优化</p><ul><li><code>Leader 选举</code>：在Multi Paxos中，系统会选举出一个稳定的 Leader，这个 Leader 负责协调提案的过程。这减少了因提议者间的竞争而引起的活锁问题（只有Leader 能发起提案）。</li><li><code>减少通信轮次</code>：Leader 在它的任期内，只需完成一次Prepare阶段，之后可以直接发起Accept请求，无需每次都进行Prepare请求。</li><li><code>减少状态转换</code>：由于有了固定的 Leader，接受者的状态转换次数减少，因为它们只需要响应来自该领导者的消息。</li></ul><p class="note note-primary">Multi Paxos 流程</p><p><strong>前提条件</strong>：从<code>Proposers</code>中选出一个<code>Leader</code>，选取Leader过程可以是</p><ol><li><code>领导者选举</code>：<ul><li>在开始之前，集群通过某种机制选举出一个领导者。</li></ul></li><li><code>领导者的准备阶段</code>（Prepare ）：<ul><li>一旦选举出领导者，它会开始一轮新的准备阶段，发送Prepare消息给所有接受者，消息中包含了一个新的提案编号。</li><li>接受者收到Prepare消息后，如果提案编号高于它们之前承诺过的任何编号，则会向领导者发出承诺，不再接受低于该编号的提案，并告知领导者它们已接受的最高编号提案的详情。</li></ul></li><li><code>领导者的接受阶段</code>（Accept）：<ul><li>领导者收到多数接受者的承诺后，会开始接受阶段，发送带有提案编号和值的Accept消息给所有接受者。</li><li>如果接受者没有对更高编号的提案做出承诺，它们将接受该提案。</li></ul></li><li><code>提案被接受</code>：<ul><li>一旦提案被多数接受者接受，该提案即被认为选定。</li><li>学习者（Learners）被告知哪个提案被选定，并据此更新其状态。</li></ul></li><li><code>连续提案</code>：<ul><li><span class="green-line">一旦领导者完成了初次的准备阶段并得到了多数接受者的承诺，它就可以连续地发起多个提案，而无需每次都发送Prepare消息。</span></li><li>领导者可以利用之前获得的授权来发起新的提案，每个提案都有唯一递增的编号。</li></ul></li><li><code>客户端响应</code>：<ul><li>领导者也负责将决策结果通知给客户端，客户端随后可以基于这些结果执行相应的操作。</li></ul></li></ol><p><img src="/2022/10/10/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95%E7%90%86%E8%AE%BA/multi-paxos.png" srcset="/img/loading.gif" lazyload alt></p><h4 id="raft算法"><a class="markdownIt-Anchor" href="#raft算法"></a> Raft算法</h4><blockquote><p><a target="_blank" rel="noopener" href="http://www.kailing.pub/raft/index.html">动画演示</a></p><p><code>consul</code> <code>Nacos</code></p><p>Raft是一种用于替代Paxos的共识算法。相比于Paxos，Raft的目标是提供更清晰的逻辑分工使得算法本身能被更好地理解，同时它安全性更高，并能提供一些额外的特性</p><p>分布式存储系统通常通过维护多个副本来进行容错，提高系统的可用性。要实现此目标，就必须要解决分布式存储系统的最核心问题：<span style="border-bottom:2px dashed green">维护多个副本的一致性</span></p><p>Raft 是一种更为简单方便易于理解的分布式算法，<span style="border-bottom:2px dashed green">主要解决了分布式中的一致性问题</span>。相比传统的 Paxos 算法，Raft 将大量的计算问题分解成为了一些简单的相对独立的子问题，并有着和 Multi-Paxos 同样的性能，下面我们通过动图，以后还原 Raft 内部原理</p><hr><p><strong>角色（状态）</strong></p><p>Raft协议的每个副本都会处于三种状态之一：<code>Leader</code>、<code>Follower</code>、<code>Candidate</code></p><ul><li><strong>Leader</strong>（领导）：所有请求的处理者，Leader副本接受client的更新请求，本地处理后再同步至多个其他副本；</li><li><strong>Follower</strong>（跟随者）：请求的被动更新者，从Leader接受更新请求，然后写入本地日志文件</li><li><strong>Candidate</strong>（候选人）：如果Follower副本在一段时间内没有收到Leader副本的心跳，则判断Leader可能已经故障，此时启动选主过程，此时副本会变成Candidate状态，直到选主结束</li></ul><hr><p><strong>Leader选举 （Leader election）</strong></p><p><strong>Term</strong>（任期）：它其实是个单独递增的连续数字，每一次任期就会重新发起一次领导人选举</p><p><strong>Election Timeout</strong>（选举超时）：就是一个超时时间，Follower众超时未收到Leader的心跳时，会重新进行选举</p><p><strong>选举流程</strong></p><ol><li><p>集群启动时，刚开始所有节点身份都是<code>Follower</code></p><p><img src="/2022/10/10/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95%E7%90%86%E8%AE%BA/raft-init.png" srcset="/img/loading.gif" lazyload alt></p></li><li><p>成为候选人：每个节点都有自己的<code>超时时间</code>，因为是随机的，区间值为150~300ms，所以出现相同随机时间的概率比较小，当某个<code>Flollower</code>超过超时时间没收到<code>Leader</code>的心跳，它自己可以成为候选人，首选为自己投票（假设a最先超时）</p><p><img src="/2022/10/10/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95%E7%90%86%E8%AE%BA/raft-1.gif" srcset="/img/loading.gif" lazyload alt></p></li><li><p>候选人a向其他节点发送 Request Vote请求投票信息；如果接收节点再这个选举任期中还没有投票，那就将票投给候选人（Term=1）</p><p><img src="/2022/10/10/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95%E7%90%86%E8%AE%BA/raft-2.gif" srcset="/img/loading.gif" lazyload alt></p></li><li><p>如果候选人a获得超过半数以上节点则当选新的<code>Leader</code>；<code>Leader</code>开始向其他<code>Follower</code>发送Append Entries（追加条目或日志），这些消息以<code>heartbeat timeout</code>心跳超时指定的时间间隔发送，<code>Follower</code>响应每个Append Entries 追加条目消息</p><p><img src="/2022/10/10/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95%E7%90%86%E8%AE%BA/raft-3.gif" srcset="/img/loading.gif" lazyload alt></p></li><li><p>如果<code>Leader</code>宕机，超过<code>heartbeat timeout</code> 时长 <code>Follower</code>没有接收来自<code>Leader</code>的心跳则重新触发选举流程，选取新的<code>Leader</code>（假设节点B当选）</p><p><img src="/2022/10/10/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95%E7%90%86%E8%AE%BA/raft-4.gif" srcset="/img/loading.gif" lazyload alt></p></li></ol><hr><p><strong>网络分区（脑裂）修复</strong></p><p><img src="/2022/10/10/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95%E7%90%86%E8%AE%BA/%E5%88%86%E5%8C%BA%E8%84%91%E8%A3%82%E4%BF%AE%E5%A4%8D.gif" srcset="/img/loading.gif" lazyload alt></p><ol><li>假设<code>AB</code>和<code>CDE</code>发生了网络分区，那么<code>CDE</code>由于不存在<code>Leader</code>则会重新选举一个新Leader（节点E，任期=3）</li><li>现在整个集群就产生了脑裂问题，有两个<code>Leader</code>存在</li><li>客户端就会将更改信息发送到两个<code>Leader</code>中处理<ol><li>发送到节点B，由于更改只收到节点A的响应，没有集群节点过半的响应，更改不能提交</li><li>发送到节点E，它能够得到集群过半节点的响应，此分区集群达成共识，更改提交</li></ol></li><li>网络分区恢复</li><li>节点B通过心跳发现节点E的任期代号比自己高，则节点B退化为<code>Follower</code></li><li>节点A和B都将回滚其未提交的日志并重新同步新<code>Leader</code>的日志</li><li>现在，所有日志在集群中是一致的，分区数据完成修复</li></ol><hr><p><strong>复制状态机</strong></p><p><img src="/2022/10/10/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95%E7%90%86%E8%AE%BA/%E5%A4%8D%E5%88%B6%E7%8A%B6%E6%80%81%E6%9C%BA.png" srcset="/img/loading.gif" lazyload alt></p><p>状态机基础上增加复制多个服务来实现来实现分布式系统的容错。通常而言，一个支持F个故障的系统，必须至少包含2F+1个副本。复制状态机通常都是基于复制日志实现的。每一个服务器存储一个包含一系列指令的日志，并且按照日志的顺序进行执行。每一个日志都按照相同的顺序包含相同的指令，所以每一个服务器都执行相同的指令序列。因为每个状态机都是确定的，每一次执行操作都产生相同的状态和同样的序列。保证复制日志相同就是一致性算法的工作了。服务器集群看起来形成一个高可靠的状态机。</p><hr><p><strong>日志复制过程</strong></p><p><span style="border-bottom:2px dashed green">经过<code>Leader</code>选举后，所有的写操作都交给<code>Leader</code>负责</span></p><ol><li><p>分布式系统现在要对某个值进行更新，只能通过<code>Leader</code>操作</p><p><img src="/2022/10/10/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95%E7%90%86%E8%AE%BA/raft-5.gif" srcset="/img/loading.gif" lazyload alt></p></li><li><p><code>Leader</code>会把客户端发起的更改请求记录到日志中，并未提交</p></li><li><p>然后将待提交的日志和日志的<code>nxetIndex</code>（下一条日志写入点，保证各节点日志的顺序性）到所有的<code>Follower</code>中，<code>Follower</code>接受到Rpc请求后将请求写入各自的日志后响应<code>Leader</code></p></li><li><p><code>Leader</code>接收到过半节点的响应后，就将自身的值修改</p></li><li><p>然后<code>Leader</code>再发出一个<code>commit</code>请求让<code>Follower</code>提交刚刚写入的日志。现在，整个集群系统就达成了共识，修改生效</p></li><li><p>响应客户端</p><p><img src="/2022/10/10/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95%E7%90%86%E8%AE%BA/raft-6.gif" srcset="/img/loading.gif" lazyload alt></p></li></ol><hr><p><strong>日志复制原理</strong></p><p>日志格式</p><p>在 Raft 算法中，需要实现分布式一致性的数据被称作日志，我们 Java 后端绝大部分人谈到日志，一般会联想到项目通过 log4j 等日志框架输出的信息，而 Raft 算法中的数据提交记录，他们会按照时间顺序进行追加，Raft 也是严格按照时间顺序并以一定的格式写入日志文件中：</p><p><img src="/2022/10/10/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95%E7%90%86%E8%AE%BA/raft%E6%97%A5%E5%BF%97%E6%A0%BC%E5%BC%8F.png" srcset="/img/loading.gif" lazyload alt></p><p>如上图所示，Raft 的日志以日志项（LogEntry）的形式来组织，每个日志项包含一条命令、任期信息、日志项在日志中的位置信息（索引值 LogIndex）。</p><ul><li>指令：由客户端请求发送的执行指令，有点绕口，我觉得理解成客户端需要存储的<a target="_blank" rel="noopener" href="https://cloud.tencent.com/solution/cloudlog?from=10680">日志数据</a>即可。</li><li>索引值：日志项在日志中的位置，需要注意索引值是一个连续并且单调递增的整数。</li><li>任期编号：创建这条日志项的领导者的任期编号。</li></ul><p><strong>如何保证日志的一致性？</strong></p><p><code>Follower获取上一个日志项做比对</code></p><p><code>Leader递减日志索引值重新复制日志给落后的Follower,直到完成一致</code></p><p>假设原来的<code>Leader</code>宕机，选取新的<code>Leader</code>如何保证日志的一致性</p><ul><li>领导者会通过强制覆盖的方式让跟随者复制自己的日志来解决日志不一致的问题</li><li>领导者在追加 RPC 请求过程中会<span style="border-bottom:2px dashed green">附带需要复制的日志以及前一个日志项相关信息，如果跟随者匹配不到前一个日志项，那么他就会拒绝接收新的日志条目</span></li><li>接着领导者会继续递减要复制的日志项索引值，直至找到相同索引和任期号的日志项，最后就直接覆盖跟随者之后的日志项</li><li>可认为两个条目拥有相同的索引和任期号，那么他们之前的所有日志条目也全部相同</li></ul></blockquote><h2 id="分布式协议"><a class="markdownIt-Anchor" href="#分布式协议"></a> 分布式协议</h2><h3 id="gossip协议"><a class="markdownIt-Anchor" href="#gossip协议"></a> Gossip协议</h3><blockquote><p><code>带冗余容错的最终一致性协议</code> <code>去中心化分布式协议</code> <code>要求非拜占庭将军问题</code></p><p>​	Gossip protocol 也叫 Epidemic Protocol <code>（流行病协议）</code>，是基于流行病传播方式的节点或者进程之间信息交换的协议。Gossip protocol在1987年8月由施乐公司帕洛阿尔托研究中心研究员艾伦·德默斯（Alan Demers）发表在ACM上的论文《Epidemic Algorithms for Replicated Database Maintenance》中被提出</p><p>​	Gossip协议在计算机系统通常以随机的<code>对等选择</code>形式实现：以给定的频率，每台计算机随机选择另一台计算机，并共享任何消息。定义十分简单，所以实现方式非常多，可能有几百种Gossip协议变种。因为每个使用场景都可能根据组织的特定需求进行定制</p><hr><p><strong>Gossip协议执行过程</strong></p><ul><li><code>种子节点</code>周期性的散播消息 （假定把周期限定为 1 秒）</li><li>被感染节点随机选择N个邻接节点散播消息（假定fan-out(扇出)设置为6，每次最多往6个节点散播）</li><li>节点只接收消息不反馈结果</li><li>每次散播消息都选择<code>尚未发送过的节点</code>进行散播</li><li>收到消息的节点<code>不回传散播</code>：A -&gt; B，那么B进行散播的时候，不再发给 A</li></ul><p><img src="/2022/10/10/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95%E7%90%86%E8%AE%BA/gossip%E6%B5%81%E7%A8%8B.gif" srcset="/img/loading.gif" lazyload alt></p><p><span style="border-bottom:2px dashed green">Goosip 协议的信息传播和扩散通常需要由<code>种子节点</code>发起。整个传播过程可能需要一定的时间，由于不能保证某个时刻所有节点都收到消息，但是理论上最终所有节点都会收到消息，因此它是一个最终一致性协议</span></p><p>​	<span style="border-bottom:2px dashed green">Gossip协议是一个多主协议，所有写操作可以由不同节点发起，并且同步给其他副本。</span>Gossip内组成的网络节点都是对等节点，是非结构化网</p><hr><p><strong>-传播方式-</strong></p><p><strong>反熵传播</strong></p><p>使用<code>simple epidemics(SI model)</code>（简单流行病）的方式：以固定的概率传播所有的数据。所有参与节点只有两种状态</p><ul><li><code>Suspective(病原)</code>：处于 susceptible 状态的节点代表其并没有收到来自其他节点的更新</li><li><code>Infective(感染)</code>：处于 infective 状态的节点代表其有数据更新，并且会将这个数据分享给其他节点</li></ul><p>反熵传播过程是每个节点周期性地随机选择其他节点，然后通过互相交换自己的所有数据来消除两者之间的差异</p><p><span style="border-bottom:2px dashed green">反熵传播方法每次节点两两交换自己的所有数据会带来非常大的通信负担，因此不会频繁使用，通常只用于新加入节点的数据初始化</span></p><p><strong>谣言传播</strong></p><p><code>complex epidemics(SIR model)</code>（复杂流行病）的方式:以固定的概率仅传播新到达的数据。所有参与节点有三种状态：<code>Suspective(病原)</code>、<code>Infective(感染)</code>、<code>Removed(愈除)</code></p><ul><li><code>Suspective(病原)</code>：处于 susceptible 状态的节点代表其并没有收到来自其他节点的更新</li><li><code>Infective(感染)</code>：处于 infective 状态的节点代表其有数据更新，并且会将这个数据分享给其他节点</li><li><code>Removed(愈除)</code>：其已经接收到来自其他节点的更新，但是其并不会将这个更新分享给其他节点</li></ul><p><span style="border-bottom:2px dashed green">谣言传播过程是消息只包含最新 update，谣言消息在某个时间点之后会被标记为removed，并且不再被传播。缺点是系统有一定的概率会不一致，通常用于节点间数据增量同步</span></p><p>一般来说，为了在通信代价和可靠性之间取得折中，需要将这两种方法结合使用</p><hr><p><strong>通信方式</strong></p><p><span style="border-bottom:2px dashed green">Gossip 协议最终目的是将数据分发到网络中的每一个节点</span>。根据不同的具体应用场景，网络中两个节点之间存在三种通信方式</p><ul><li><code>Push（推送模式）</code>: 节点 A 将数据 (key,value,version) 及对应的版本号推送给 节点B，节点B 更新 节点A 中比自己新的数据</li><li><code>Pull（拉取模式）</code>：节点A 仅将数据 key, version 推送给节点 B，节点B 将本地比 节点A 新的数据（Key, value, version）推送给节点A，节点A 更新本地</li><li><code>Push&amp;Pull（推送&amp;拉取模式）</code>：发起信息交换的 节点A 向选择的 节点B 发送信息，同时从对方获取数据，用于更新自己的本地数据</li></ul><p>如果把两个节点数据同步一次定义为一个周期，则在一个周期内，<span style="border-bottom:2px dashed green">Push 需通信 1 次，Pull 需 2 次，Push&amp;Pull 则需 3 次。虽然消息数增加了，但从效果上来讲，Push&amp;Pull 最好，理论上一个周期内可以使两个节点完全一致</span>。直观上，Push&amp;Pull 的收敛速度也是最快的</p><hr><p><strong>总结</strong></p><p><span style="border-bottom:2px dashed green">Gossip是一种去中心化的分布式协议，数据通过节点像病毒一样逐个传播</span>。因为是指数级传播，整体传播速度非常快，很像现在流感病毒一样。它具备以下优势</p><ul><li><strong>可扩展性</strong>（Scalable）：允许节点的任意增加和减少，新增节点的状态最终会与其他节点一致。</li><li><strong>容错</strong>（Fault-tolerance）：网络中任何节点的重启或者宕机都不会影响 gossip 协议的运行，具有天然的分布式系统容错特性。</li><li><strong>健壮性</strong>（Robust）：gossip 协议是去中心化的协议，所以集群中的所有节点都是对等的，没有特殊的节点，所以任何节点出现问题都不会阻止其他节点继续发送消息。任何节点都可以随时加入或离开，而不会影响系统的整体服务质量。</li><li><strong>最终一致性</strong>（Convergent consistency）：谣言传播可以是指数级的快速传播，因此新信息传播时，消息可以快速地发送到全局节点，在有限的时间内能够做到所有节点都拥有最新的数据。</li><li><strong>简单</strong></li></ul><p>同样也存在以下缺点：</p><ul><li><strong>消息延迟</strong>：节点随机向少数几个节点发送消息，消息最终是通过多个轮次的散播而到达全网，不可避免的造成消息延迟。</li><li><strong>消息冗余</strong>：节点定期随机选择周围节点发送消息，而收到消息的节点也会重复该步骤，因此不可避免地引起同一节点多次接收同一消息，增加消息处理的压力。一次通信会对网路带宽、CUP资源造成很大的负载，而这些负载又受限于 通信频率，该频率又影响着算法收敛的速度。</li><li><strong>拜占庭问题</strong>：如果有一个恶意传播消息的节点，Gossip协议的分布式系统就会出问题。</li></ul><p>上述优缺点的本质是因为<span style="border-bottom:2px dashed green">Gossip是一个带冗余的容错算法，是一个最终一致性算法，虽然无法保证在某个时刻所有节点状态一致，但可以保证在<code>最终所有节点一致</code>，<code>最终</code>的时间是一个理论无法明确的时间点</span>。所以适合于<code>AP</code>场景的数据一致性处理，常见应用有：<code>Apache Cassandra</code>、<code>Redis Cluster</code> ，还有<code>Apache Gossip</code>框架的开源实现供Gossip协议的学习</p><p>文章出处：<a target="_blank" rel="noopener" href="https://cloud.tencent.com/developer/article/1662426">https://cloud.tencent.com/developer/article/1662426</a></p></blockquote><h3 id="zab协议"><a class="markdownIt-Anchor" href="#zab协议"></a> Zab协议</h3><h4 id="zab简介"><a class="markdownIt-Anchor" href="#zab简介"></a> Zab简介</h4><p>ZAB（ZooKeeper Atomic Broadcast）协议是专为ZooKeeper设计的共识协议，确保了分布式系统在面对节点故障时的数据一致性和系统的可靠恢复。ZAB协议在概念上受到了 <code>Multi-Paxos</code> 算法的启发，但针对ZooKeeper的特定场景进行了优化和调整。</p><p>ZAB协议的核心特点包括：</p><ol><li><code>原子广播</code>：ZAB确保所有的更新（如配置更改或状态变更）以相同的顺序被所有活跃的集群成员处理，这是通过原子广播机制实现的。</li><li><code>崩溃恢复</code>：当系统检测到Leader节点故障时，ZAB协议会触发恢复模式，这包括了一个新的Leader选举过程。在新的Leader被选举出来之前，系统不会对外提供写服务，以避免可能的不一致状态。</li><li><code>数据同步</code>：新选举出的Leader节点负责将其状态与其他Follower节点同步，确保整个集群达到一致的状态，从而实现所谓的“线性一致性”。这保证了任何已经被ZooKeeper确认的更新都不会丢失，并且客户端总是看到最新的状态。</li><li><code>领导者选举</code>：ZAB通过一个快速的领导者选举机制，最小化了由于Leader崩溃导致的系统不可用时间。</li><li><code>消息广播效率</code>：在Leader稳定之后，ZAB通过减少消息的往返次数优化了消息广播的效率，从而提高了整个系统的性能。</li></ol><p class="note note-primary">ZAB协议中节点存在四种状态</p><ol><li><code>Leading</code><ul><li>节点作为<strong>领导者</strong>（Leader）运行，负责处理所有的事务请求，并协调整个集群的事务顺序。领导者节点确保所有的更新以相同的顺序被所有活跃的集群成员处理。</li></ul></li><li><code>Following</code><ul><li>节点作为<strong>跟随者</strong>（Follower）运行，遵循领导者的指导并参与事务的执行。跟随者节点接受领导者的提案，并在本地复制状态变更。</li></ul></li><li><code>Looking</code><ul><li>节点处于<strong>锁定</strong>状态，表明集群当前没有活跃的领导者，节点正在参与选举过程，以选出新的领导者。在这个状态下，集群不会对外提供写服务。</li></ul></li><li><code>Observing</code><ul><li>节点作为<strong>观察者</strong>（Observer）运行，它与跟随者类似，保持与集群状态的同步，并提供读服务，但不参与领导者的选举过程也不参与到事务的投票中。观察者的主要作用是提高读取性能并扩展集群的规模，而不会影响写操作的一致性和性能。</li></ul></li></ol><p class="note note-primary">ZAB协议中的两种工作模式</p><ul><li><code>原子广播</code>：在集群稳定运行时，领导者（Leader）使用原子广播机制来确保所有跟随者（Followers）节点的状态保持一致。</li><li><code>崩溃恢复</code>：集群启动或 Leader 崩溃时系统进入恢复模式，选举 Leader 并将集群中各节点的数据同步到最新状态。</li></ul><h4 id="zxid"><a class="markdownIt-Anchor" href="#zxid"></a> ZXID</h4><p><img src="/2022/10/10/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95%E7%90%86%E8%AE%BA/zxid.png" srcset="/img/loading.gif" lazyload alt></p><p class="note note-primary">组成</p><ul><li><p><code>高32位</code>：代表领导者（Leader）的纪元（epoch），每当一个新的领导者被选举出来时，这个数字会递增。纪元用于区分不同领导者的领导期，确保即使事务编号从0重新开始，事务ID也仍然是全局唯一和递增的。</p></li><li><p><code>低32位</code>：在特定领导者的领导期内，每发生一个新事务，这个数字就递增。这确保了在同一个领导期内，所有的事务都能被正确排序。</p></li></ul><p class="note note-primary">作用</p><ul><li>ZXID是ZooKeeper集群中用于确保所有事务全局有序执行的唯一标识符。每个ZXID包含一个纪元号和一个事务计数器，纪元号代表领导者的任期，事务计数器则是在该任期内事务的递增序号。</li><li>ZXID中的纪元号帮助集群在领导者崩溃和随后的选举过程中维护一致性。新的领导者开始一个新纪元，确保其领导期内的事务与之前的任期明确区分，从而避免执行无效或过时的事务。</li><li>跟随者使用ZXID中的纪元号来验证自己的数据是否最新，或者是否需要通过与领导者同步日志来更新自己的状态。</li></ul><h4 id="原子广播"><a class="markdownIt-Anchor" href="#原子广播"></a> 原子广播</h4><p><code>类似分布式事务2PC提交</code></p><p><code>顺序一致性保证</code></p><p><img src="/2022/10/10/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95%E7%90%86%E8%AE%BA/%E4%BA%8B%E5%8A%A1%E6%8F%90%E4%BA%A4.png" srcset="/img/loading.gif" lazyload alt></p><p>原子广播是协议中确保所有更新按相同顺序应用到所有服务器上的机制。以下是ZAB协议原子广播的详细流程：</p><ol><li><p><code>事务请求</code></p><ul><li>客户端可以将事务请求（如创建、更新或删除ZNode）发送给任何一个ZooKeeper服务器节点，无论是领导者还是跟随者。</li></ul></li><li><p><code>请求转发</code></p></li></ol><ul><li>如果跟随者接收到了事务请求，它会将该请求转发给领导者。这是因为只有领导者才能决定事务的顺序。</li></ul><ol start="3"><li><p><code>领导者提案</code></p><ul><li>领导者收到事务请求后，创建一个提案（Proposal），其中包括了ZXID（事务ID），并将此提案发送给所有跟随者。</li></ul></li><li><p><code>提案广播</code></p><ul><li>领导者将提案广播给所有跟随者。每个提案都包括一个全局唯一的递增ZXID，用于保证顺序一致性。</li></ul></li><li><p><code>跟随者确认提案</code></p><ul><li><p>跟随者（Followers）接收到来自领导者（Leader）的提案广播时，它们首先将这些提案按照接收顺序放入预处理队列。这个队列的作用是保证事务能够根据其ZXID（事务ID）的顺序进行处理，即使在网络传输过程中事务的顺序发生了变化。</p></li><li><p>跟随者接着将这些提案写入它们的事务日志，但尚未进行实际的提交。</p></li></ul></li><li><p><code>提案提交</code></p><ul><li>一旦领导者从大多数跟随者那里得到了足够的投票，它将决定提交提案，并向所有跟随者发送提交（Commit）消息。</li></ul></li><li><p><code>应用提案</code></p><ul><li>接收到提交消息的跟随者将提案应用到它们的状态机，并向客户端发送响应，表明事务已经被处理。</li><li>如果Commit 的事务ID 和本地未提交的事务ID不一致，放弃请求。重新向 Leader 同步数据，保证数据线性一致。</li></ul></li><li><p><code>连续提案处理</code></p><ul><li>在一个稳定的领导者任期内，领导者可以连续地处理多个事务请求，而不需要为每个提案都进行新的选举或准备（Prepare）阶段。</li><li>领导者根据请求的到达顺序分配ZXID，并连续地广播提案。这样做提高了效率并减少了延迟。</li></ul></li><li><p><code>状态同步</code></p><ul><li>如果某个跟随者落后或者有新跟随者加入集群，领导者会与这些跟随者进行状态同步，确保它们的数据状态是最新的。</li></ul></li></ol><div class="note note-warning"><p>为什么是半数ACK则commit</p><ul><li><code>容错性</code>：ZAB协议旨在确保即使在发生节点故障的情况下，集群仍能继续正常工作。通过只要求多数节点（超过半数）的确认，协议可以容忍少数节点的失败，而不影响整个集群的可用性和一致性。</li><li><code>写性能优化</code>：等待所有节点的确认可能会导致系统性能显著下降，特别是在分布式环境中节点数量较多或节点之间网络延迟较大的情况下。通过只需要多数节点的确认，ZAB协议提高了事务的处理速度和系统的吞吐量。</li><li><code>避免不必要的等待</code>：如果集群中某个节点因为网络问题或其他原因响应缓慢，等待它的确认会延迟整个集群的事务处理。多数投票机制确保了不会因为单个或少数节点的问题而影响整个集群的操作。</li></ul></div><h4 id="崩溃恢复"><a class="markdownIt-Anchor" href="#崩溃恢复"></a> 崩溃恢复</h4><p>在ZooKeeper集群中，若现任领导者（Leader）遇到故障，或在集群启动时，集群会进入恢复模式。这个模式首先触发新的领导者选举，并同步所有节点以确保整个集群反映了最新且正确的状态。领导者崩溃的判定基于其与大多数跟随者（Followers）失去通信的能力。</p><ul><li>超过半数Follower丢失和Leader心跳，判断Leader宕机。</li><li>Leader丢失了超过半数的Follower心跳，自动退出Leader。</li></ul><p>为了确保一致性，在崩溃恢复的过程中，集群必须坚持以下原则：</p><ul><li><code>事务持久性</code>：任何由先前领导者提交并广播的事务必须在所有活跃的跟随者节点上得到持久化，确保这些事务不会因领导者的崩溃而丢失。</li><li><code>清理未提交事务</code>：在领导者故障时还未提交的事务应被新的领导者视为无效，丢弃这个未提交事务。</li></ul><h5 id="集群初始化选举"><a class="markdownIt-Anchor" href="#集群初始化选举"></a> 集群初始化选举</h5><p><code>遇强选强，myid大有投票优势</code></p><p><img src="/2022/10/10/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95%E7%90%86%E8%AE%BA/%E5%88%9D%E5%A7%8B%E9%80%89%E7%A5%A8.png" srcset="/img/loading.gif" lazyload alt></p><p>集群初始化选举时，选票内容为（myid，zXid）,由于集群初始化的zXid均为0，下文则忽略zXid</p><p><img src="/2022/10/10/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95%E7%90%86%E8%AE%BA/%E7%AC%AC%E4%B8%80%E6%AC%A1%E5%90%AF%E5%8A%A8%E9%80%89%E4%B8%BE%E6%B5%81%E7%A8%8B.png" srcset="/img/loading.gif" lazyload alt></p><p>集群初始化选举流程：</p><ol><li><code>启动</code><ul><li>集群中的每个节点在启动时都会进入“查找”（LOOKING）状态，表明它们正在寻找一个领导者。</li></ul></li><li><code>投票</code><ul><li>每个节点都会投出一票，起初，每个节点都会为自己投票。</li><li>投票包含两个信息：所选节点的标识符（通常是服务器ID）和该节点已知的最高事务ID（ZXID）。ZXID越高，代表该节点的数据越新。</li></ul></li><li><code>收集选票</code><ul><li>每个节点将自己的选票发送给其他所有节点，并同时收集来自其他节点的选票。</li></ul></li><li><code>更新投票</code><ul><li>当一个节点收到来自其他节点的选票时，如果发现另一个节点有更高的ZXID，或者在ZXID相同的情况下服务器ID更高，它会更新自己的投票并将此新投票广播出去。</li></ul></li><li><code>确定领导者</code><ul><li>一旦一个节点收到超过半数的相同投票，它会宣布那个节点为领导者。</li><li>确定领导者后，该节点将自己的状态更改为“领导”（LEADING），其他所有节点将自己的状态更改为“跟随”（FOLLOWING）或“观察”（OBSERVING），观察者节点是一种特殊的节点，它接收更新但不参与选举过程。</li></ul></li><li><code>同步状态</code><ul><li>新选出的领导者会与所有的跟随者进行状态同步，确保所有节点的数据都是最新的。</li></ul></li><li><code>开始服务</code><ul><li>一旦状态同步完成，领导者就开始处理客户端的请求，并将更新广播给所有跟随者。</li></ul></li></ol><div class="note note-warning"><p><code>数据同步过程</code></p><ol><li>Follower 发送一个同步请求，该请求包含了 Follower 最后已知的数据状态（最大ZXID）。</li><li>Leader 收到 Follower 的同步请求后，通过事务日志比对ZXID，确定同步起点。</li><li>开始同步<ul><li>如果数据差距不大，直接进行事务日志同步。</li><li>数据差距过大，直接发送数据快照，然后再同步后面的事务日志。</li></ul></li><li>Follower完成同步，向外提供读取功能。</li></ol></div><h5 id="运行期间选举"><a class="markdownIt-Anchor" href="#运行期间选举"></a> 运行期间选举</h5><p>在ZooKeeper集群的运行期间，领导者（Leader）的宕机将触发一个选举新领导者的流程，以保证集群的高可用性和一致性。</p><p><img src="/2022/10/10/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95%E7%90%86%E8%AE%BA/%E8%BF%90%E8%A1%8C%E6%97%B6%E9%80%89%E7%A5%A8.png" srcset="/img/loading.gif" lazyload alt></p><div class="note note-warning"><ul><li><p><code>Epoch</code>（选举周期）</p><p>Epoch是每次领导者选举时的逻辑时钟值，用于标识Leader的任期。在没有领导者的情况下，同一轮选举中所有节点的Epoch值是相同的。每完成一次投票，节点的Epoch值就会递增，这样可以防止选举过程中接收到陈旧的投票信息。</p></li><li><p><code>ZXID</code>（事务ID）</p><p>ZXID是一个全局单调递增的序列号，用于标识集群状态变更的每一次事务。不同节点上的ZXID值可能不同，这是因为ZooKeeper集群中的每个服务器可能以不同的顺序处理来自客户端的更新请求。ZXID的不一致性不会影响系统的最终一致性，因为选举机制会确保选出的新领导者具有最新的数据状态。</p></li><li><p><code>SID</code>（服务器ID）</p><p>SID是用来唯一标识ZooKeeper集群中的每台机器的标识符。这个ID在集群中每台机器上的配置文件中被设置为<code>myid</code>，并且每台机器的<code>myid</code>都是唯一的。在选举过程中，如果Epoch和ZXID都相同，那么SID（<code>myid</code>）将作为最后的决策因素。</p></li></ul></div><p class="note note-primary">选举过程</p><ol><li><code>Leader 检测</code><ul><li>领导者会周期性地向跟随者（Followers）发送心跳信号。</li><li>如果跟随者在预定的时间间隔内没有收到心跳，它会认定领导者已宕机，关闭当前的会话，并转入Looking状态。</li></ul></li><li><code>触发选举</code><ul><li>当过半数的节点进入Looking状态时，选举过程被触发，因为这意味着领导者与过半数的跟随者失去了联系。</li></ul></li><li><code>发起投票</code><ul><li>每个Looking状态的节点将发起一轮投票，每轮投票都有一个唯一的Epoch值，该值随着每次投票而递增，以防止来自之前选举轮次的过期投票干扰。</li></ul></li><li><code>投票过程</code><ul><li>在发起投票时，每个节点会向其他所有节点发送包含自己Epoch、ZXID和SID的投票。</li></ul></li><li><code>Epoch优先</code><ul><li>节点首先比较Epoch值，因为一个更高的Epoch表示节点在更近的领导者任期内活跃，优先选择Epoch较大的节点作为领导者是为了保证信息的时效性和减少可能的数据回滚。</li></ul></li><li><code>ZXID比较</code><ul><li>如果Epoch相同，则比较ZXID，因为更大的ZXID表示节点拥有最新的数据变更。</li></ul></li><li><code>SID决断</code><ul><li>如果Epoch和ZXID都相同，则使用SID，即myid来决定。这通常意味着节点启动的先后顺序，但在实践中几乎不会依赖于此因素来决定领导者。</li></ul></li><li><code>宣布新领导者</code><ul><li>当一个节点接收到超过半数的节点对同一个候选者的支持时，它将宣布该候选者为新领导者，并通知所有节点。</li></ul></li><li><code>状态同步</code><ul><li>新领导者接管后，会与每个跟随者进行状态同步，以确保所有节点反映了最新且一致的集群状态。</li></ul></li><li><code>集群恢复</code><ul><li>一旦同步完成，新的领导者开始处理客户端请求，集群继续其正常操作。</li></ul></li></ol><p><img src="/2022/10/10/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95%E7%90%86%E8%AE%BA/%E8%BF%90%E8%A1%8C%E6%9C%9F%E9%97%B4%E7%9A%84%E9%80%89%E4%B8%BE.png" srcset="/img/loading.gif" lazyload alt></p><h2 id="分布式事务解决方案"><a class="markdownIt-Anchor" href="#分布式事务解决方案"></a> 分布式事务解决方案</h2><blockquote><p>参考文章</p><p>分布式事务最经典的七种解决方案：<a target="_blank" rel="noopener" href="https://segmentfault.com/a/1190000040321750">https://segmentfault.com/a/1190000040321750</a></p><p>七种分布式事务的解决方案：<a target="_blank" rel="noopener" href="https://cloud.tencent.com/developer/article/1806989">https://cloud.tencent.com/developer/article/1806989</a></p><p>柔性事务：<a target="_blank" rel="noopener" href="https://www.modb.pro/db/69276">https://www.modb.pro/db/69276</a> <code>Saga</code></p><p>seata模式详解：<a target="_blank" rel="noopener" href="https://seata.io/zh-cn/blog/seata-at-tcc-saga.html">https://seata.io/zh-cn/blog/seata-at-tcc-saga.html</a> <code>AT</code> <code>Saga</code></p><p><img src="/2022/10/10/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95%E7%90%86%E8%AE%BA/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1%E5%A4%A7%E5%85%A8.png" srcset="/img/loading.gif" lazyload alt></p></blockquote><h3 id="dtp模型"><a class="markdownIt-Anchor" href="#dtp模型"></a> DTP模型</h3><blockquote><p><img src="/2022/10/10/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95%E7%90%86%E8%AE%BA/tdp%E6%A8%A1%E5%9E%8B.png" srcset="/img/loading.gif" lazyload alt></p><p>DTP（Distributed Transaction Processing）分布式事务模型</p><ul><li><p><code>AP</code>：应用程序，例如订单服务、库存服务</p></li><li><p><code>RM</code>：资源管理器可以把理解为一个数据库（mysql）AP通过RM对资源进行控制。资源必须实现XA定义的接口。</p></li><li><p><code>TM</code>：事务管理器负责分配事务唯一标识，监控事务的执行进度，并负责事务的提交，回滚等。</p></li></ul></blockquote><h3 id="两阶段提交xa"><a class="markdownIt-Anchor" href="#两阶段提交xa"></a> 两阶段提交（XA）</h3><blockquote><p><code>2PC多用于数据库层面</code></p><p>​	熟悉mysql底层对两阶段提交应该颇为熟悉，mysql的事务就是通过<code>日志系统</code>来完成<a href="/2022/02/26/Mysql/#%E5%86%99%E6%B5%81%E7%A8%8B">两阶段提交</a>的</p><p>​	二阶段提交（2PC）是XA分布式事务协议的一种实现。其实在XA协议定义的函数中，通过<code>xa_prepare</code>，<code>xa_commit</code>已经能发现XA完整提交分准备和提交两个阶段</p><p>​	XA是由X/Open组织提出的分布式事务的规范，XA规范主要定义了(全局)事务管理器<code>TM</code>和(局部)资源管理器<code>RM</code>之间的接口。本地的数据库如mysql在XA中扮演的是<code>RM</code>角色</p><p>​	XA规范一共分为两阶段</p><ul><li><strong>第一阶段（prepare）</strong>：即所有的参与者<code>RM</code>准备执行事务并锁住需要的资源。参与者ready时，向TM报告已准备就绪</li><li><strong>第二阶段 (commit/rollback)</strong>：当事务管理者™确认所有参与者(RM)都ready后，向所有参与者发送commit命令</li></ul><p>目前主流的数据库基本都支持XA事务，包括<code>mysql</code>、<code>oracle</code>、<code>sqlserve</code>r、<code>postgreSql</code></p><p>XA 事务由一个或多个资源管理器（RM）、一个事务管理器（TM）和一个应用程序（ApplicationProgram）组成</p><p><img src="/2022/10/10/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95%E7%90%86%E8%AE%BA/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1%E4%BA%8C%E9%98%B6%E6%AE%B5%E6%8F%90%E4%BA%A4.png" srcset="/img/loading.gif" lazyload alt></p><ul><li><strong>一阶段</strong>：<code>PrePare阶段</code>此时协调者会向所有的参与者发送<code>Prepare</code>请求，参与者收到后开始执行事务操作，并将Undo和Redo信息记录到事务日志中（此时属于未提交事务的状态，向协调者节点反馈<code>Ack</code>消息</li><li><strong>二阶段</strong>：<ul><li><code>commit阶段</code>：在阶段二中如果所有的参与者节点都可以进行<code>PrePare</code>提交，那么协调者就会从<code>预提交状态</code>转变为<code>提交状态</code>。然后向所有的参与者节点发送<code>Commit</code>请求，参与者节点在收到提交请求后就会各自执行事务提交操作</li><li><code>rollback阶段</code>：如果有一个参与者节点未完成<code>PrePrepare</code>的反馈或者反馈超时，那么协调者都会向所有的参与者节点发送<code>rollback</code>请求，从而中断事务</li></ul></li></ul><hr><p><strong>缺点</strong></p><ul><li>单点故障：一旦事务管理器出现故障，整个系统不可用</li><li>数据不一致：如果事务管理器只发送了部分 commit 消息，此时网络发生异常，那么只有部分参与者接收到 commit 消息，也就是说只有部分参与者提交了事务，使得系统数据不一致</li><li>整个消息链路是串行的，要等待响应结果，不适合高并发的场景</li></ul></blockquote><h3 id="三阶段提交3pc"><a class="markdownIt-Anchor" href="#三阶段提交3pc"></a> 三阶段提交（3PC）</h3><blockquote><p><code>CanCommit阶段保证RM有提交事务能力</code> <code>二阶段超时自动Commit</code></p><p>​	<span style="border-bottom:2px dashed green">三阶段提交又称3PC，相对于2PC来说增加了<code>CanCommit</code>阶段和超时机制。如果短时间内没有收到协调者的commit请求，那么就会自动进行commit，解决了2PC单点故障的问题</span></p><ul><li>第一阶段：<code>CanCommit阶段</code>这个阶段所做的事很简单，就是协调者询问事务参与者，你是否有能力完成此次事务（提前预防某个RM故障而无法完成全局事务场景）<ul><li>如果都返回yes，则进入第二阶段</li><li>有一个返回no或等待响应超时，则中断事务，并向所有参与者发送事务中断</li></ul></li><li>第二阶段：<code>PreCommit阶段</code>此时协调者会向所有的参与者发送PreCommit请求，参与者收到后开始执行事务操作（或者超时自动commit），并将Undo和Redo信息记录到事务日志中。参与者执行完事务操作后（此时属于未提交事务的状态），就会向协调者反馈<code>Ack</code>表示我已经准备好提交了，并等待协调者的下一步指令</li><li>第三阶段：<code>DoCommit阶段</code>在阶段二中如果所有的参与者节点都可以进行PreCommit提交，那么协调者就会从<code>预提交状态</code>转变为<code>提交状态</code>。然后向所有的参与者节点发送<code>doCommit</code>请求，参与者节点在收到提交请求后就会各自执行事务提交操作，并向协调者节点反馈<code>Ack</code>消息，协调者收到所有参与者的Ack消息后完成事务。相反，如果有一个参与者节点未完成<code>PreCommit</code>的反馈或者反馈超时，那么协调者都会向所有的参与者节点发送abort请求，从而中断事务</li></ul></blockquote><h3 id="补偿事务tcc"><a class="markdownIt-Anchor" href="#补偿事务tcc"></a> 补偿事务（TCC）</h3><blockquote><p><img src="/2022/10/10/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95%E7%90%86%E8%AE%BA/TCC.png" srcset="/img/loading.gif" lazyload alt="TCC.png"></p><p>关于 TCC（Try-Confirm-Cancel）的概念，最早是由 Pat Helland 于 2007 年发表的一篇名为《Life beyond Distributed Transactions:an Apostate’s Opinion》的论文提出。</p><p>TCC分为3个阶段</p><ul><li><code>Try 阶段</code>：尝试执行，完成所有业务检查（一致性）, 预留必须业务资源（准隔离性）</li><li><code>Confirm 阶段</code>：确认执行真正执行业务，不作任何业务检查，只使用 Try 阶段预留的业务资源，Confirm 操作要求具备幂等设计，Confirm 失败后需要进行重试。</li><li><code>Cancel 阶段</code>：取消执行，释放 Try 阶段预留的业务资源。Cancel 阶段的异常和 Confirm 阶段异常处理方案基本上一致，<span style="border-bottom:2px dashed green">要求满足幂等设计</span></li></ul><hr><p><strong>两阶段模式和TCC模式的区别</strong></p><ul><li>2PC是偏数据库层面的 ，是<code>XA</code>分布式事务协议的一种实现</li><li>而TCC是纯业务层面 ，要求<code>RM</code>必须提供<code>Try</code> <code>Confirm</code> <code>Cancel</code>三个接口</li><li>资源锁定粒度不同，TCC根据具体业务来实现控制资源锁的粒度变小，不会锁定整个资源</li></ul></blockquote><h3 id="本地消息表"><a class="markdownIt-Anchor" href="#本地消息表"></a> 本地消息表</h3><blockquote><p><code>本地事务保证消息表原子性</code> <code>通过消息表和定时任务保证消息送达，失败重试</code></p><p>​	本地消息表这个方案最初是 ebay 架构师 Dan Pritchett 在 2008 年发表给 ACM 的文章。<span style="border-bottom:2px dashed green">设计核心是将需要分布式处理的任务通过消息的方式来异步确保执行。</span></p><p><img src="/2022/10/10/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95%E7%90%86%E8%AE%BA/%E6%9C%AC%E5%9C%B0%E6%B6%88%E6%81%AF%E8%A1%A8.png" srcset="/img/loading.gif" lazyload alt></p><hr><p><strong>执行流程</strong></p><ul><li><span style="border-bottom:2px dashed green">消息生产方，需要额外建一个消息表，并记录消息发送状态。消息表和业务数据要在一个事务里提交，也就是说他们要在一个数据库里面。然后消息会经过MQ发送到消息的消费方。<ul><li>如果消息发送失败，会进行重试发送。</li></ul></span></li><li>消息消费方，需要<code>处理</code>这个<code>消息</code>，并完成自己的业务逻辑<ul><li>如果是<code>业务上面的失败</code>，可以给生产方<code>发送一个业务补偿消息</code>，通知生产方进行回滚等操作</li><li>此时如果本地事务处理成功，表明已经处理成功了，修改消息表状态（或者删除消息表数据）</li><li>如果处理失败，那么就会重试执行</li></ul></li><li>生产方和消费方定时扫描本地消息表，把还没处理完成的消息或者失败的消息再发送一遍</li></ul></blockquote><h3 id="事务消息"><a class="markdownIt-Anchor" href="#事务消息"></a> 事务消息</h3><blockquote><p><code>RocketMQ 事务消息</code> <code>适用于可异步执行的业务，且后续操作无需回滚的业务</code></p><p><span style="border-bottom:2px dashed green">在上述的本地消息表方案中，生产者需要额外创建消息表，还需要对本地消息表进行轮询，业务负担较重。阿里开源的<code>RocketMQ 4.3</code>之后的版本正式支持事务消息，该事务消息本质上是把本地消息表放到RocketMQ上，解决生产端的消息发送与本地事务执行的原子性问题</span></p><p><img src="/2022/10/10/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95%E7%90%86%E8%AE%BA/%E4%BA%8B%E5%8A%A1%E6%B6%88%E6%81%AF.png" srcset="/img/loading.gif" lazyload alt></p><hr><p><strong>执行流程</strong></p><p>正常流程</p><ol><li><code>RocketMQ</code>发送半消息到<code>Broker</code></li><li><code>Borker</code> Ack,半消息发送成功</li><li>执行本地事务</li><li>本地事务<code>Commit</code>或<code>Rollback</code>通知<code>Broker</code></li><li><code>Broker</code>定时回查本地事务状态</li><li><code>Broker</code>确定本地事务提交，将消息投递给消费者</li></ol><p>补偿流程</p><ol><li>对没有Commit/Rollback的事务消息（pending状态的消息），从服务端发起一次<code>回查</code></li><li>Producer收到回查消息，返回消息对应的本地事务的状态，为<code>Commit</code>或者<code>Rollback</code></li><li>如果本地事务回滚，则不需要处理补偿业务，半消息定期自动清除</li></ol></blockquote><h3 id="最大努力通知"><a class="markdownIt-Anchor" href="#最大努力通知"></a> 最大努力通知</h3><blockquote><p>​	<span style="border-bottom:2px dashed green">最大努力通知型( Best-effort delivery)是最简单的一种柔性事务，适用于一些最终一致性时间敏感度低的业务，且被动方处理结果不影响主动方的处理结果</span>。典型的使用场景：如银行通知、商户通知等。最大努力通知型的实现方案，一般符合以下特点：</p><ul><li><code>不可靠消息</code>：业务活动主动方，在完成业务处理之后，向业务活动的被动方发送消息，直到通知N次后不再通知，允许消息丢失(不可靠消息)</li><li><code>定期校对</code>：业务活动的被动方，根据定时策略，向业务活动主动方查询(主动方提供查询接口)，恢复丢失的业务消息</li></ul><p><img src="/2022/10/10/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95%E7%90%86%E8%AE%BA/%E6%9C%80%E5%A4%A7%E5%8A%AA%E5%8A%9B%E9%80%9A%E7%9F%A5.png" srcset="/img/loading.gif" lazyload alt></p><p>最大努力通知适用于业务通知类型，例如支付宝交易的结果，就是通过最大努力通知方式通知各个商户，既有多次支付回调通知，也有交易查询接口</p></blockquote><h3 id="saga"><a class="markdownIt-Anchor" href="#saga"></a> Saga</h3><blockquote><p>​	1987年普林斯顿大学的Hector Garcia-Molina和Kenneth Salem发表了一篇Paper Sagas，讲述的是如何处理long lived transaction（长活事务）。Saga是一个长活事务可被分解成可以交错运行的子事务集合。其中每个子事务都是一个保持数据库一致性的真实事务。</p><p>​	<span style="border-bottom:2px dashed green">Saga模型是把一个分布式事务拆分为多个本地事务，每个本地事务都有相应的执行模块和补偿模块（对应TCC中的Confirm和Cancel），当Saga事务中任意一个本地事务出错时，可以通过调用相关的补偿方法恢复之前的事务，达到事务最终一致性。</span></p><hr><p><strong>Saga 模型由三部分组成</strong></p><ul><li>LLT（Long Live Transaction）：由一个个本地事务组成的事务链。</li><li>本地事务：事务链由一个个子事务（本地事务）组成，LLT = T1+T2+T3+…+Ti。</li><li>补偿：每个本地事务 Ti 有对应的补偿 Ci。</li></ul><hr><p><strong>Saga 的执行顺序</strong></p><ul><li>正常情况：T1,T2,T3,…,Ti</li><li>异常情况：T1,T2,T3,…Ti,Ci,…C3,C2,C1</li></ul><hr><p><strong>Saga 两种恢复策略</strong></p><ul><li>向后恢复（Backward Recovery）：撤销掉之前所有成功子事务。如果任意本地子事务失败，则补偿已完成的事务。如异常情况的执行顺序T1,T2,T3,…Ti,Ci,…C3,C2,C1。</li><li>向前恢复（Forward Recovery）：即重试失败的事务，适用于必须要成功的场景，该情况下不需要Ci。执行顺序：T1,T2,…,Tj（失败）,Tj（重试）,…,Ti。</li></ul><hr><p><strong>Saga 模型可以满足事务的三个特性ACD</strong></p><ul><li><p>原子性：Saga 协调器协调事务链中的本地事务要么全部提交，要么全部回滚。</p></li><li><p>一致性：Saga 事务可以实现最终一致性。</p></li><li><p>持久性：基于本地事务，所以这个特性可以很好实现。</p><p><span style="border-bottom:2px dashed green">Saga缺乏隔离性会带来脏读，幻读，不可重复读的问题。</span>由于Saga 事务和 TCC 事务一样，都是强依靠业务改造，因此需要在业务设计上去解决这个问题：</p></li><li><p>在应⽤层⾯加⼊逻辑锁的逻辑。</p></li><li><p>Session 层⾯隔离来保证串⾏化操作。</p></li><li><p>业务层⾯采⽤预先冻结数据的方式隔离此部分数据。</p></li><li><p>业务操作过程中通过及时读取当前状态的⽅式获取更新。</p></li></ul><hr><p><strong>实现Saga的注意事项</strong></p><ol><li>Ti和Ci必须是幂等的。如向后恢复和向前恢复时候如果不是幂等操作会导致数据不一致。</li><li>Ci必须是能够成功的，如果无法成功则需要人工介入。</li><li>Ti-&gt;Ci和Ci-&gt;Ti的执行结果必须是一样的。</li></ol><hr><p><strong>Saga对比TCC</strong></p><p>Saga和TCC都是补偿型事务，他们的区别为：</p><p><strong>劣势</strong></p><ul><li>无法保证隔离性；</li></ul><p><strong>优势</strong></p><ul><li>一阶段提交本地事务，无锁，高性能；</li><li>事件驱动模式，参与者可异步执行，高吞吐；</li><li>Saga 对业务侵入较小，只需要提供一个逆向操作的Cancel即可；而TCC需要对业务进行全局性的流程改造；</li></ul></blockquote><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css"></div><hr><div><div class="post-metas my-3"></div><div class="license-box my-3"><div class="license-title"><div>分布式算法理论</div><div>https://wugengfeng.cn/2022/10/10/分布式算法理论/</div></div><div class="license-meta"><div class="license-meta-item"><div>作者</div><div>wugengfeng</div></div><div class="license-meta-item license-meta-date"><div>发布于</div><div>2022年10月10日</div></div><div class="license-meta-item"><div>许可协议</div><div><a target="_blank" href="https://creativecommons.org/licenses/by/4.0/"><span class="hint--top hint--rounded" aria-label="BY - 署名"><i class="iconfont icon-by"></i></span></a></div></div></div><div class="license-icon iconfont"></div></div><div class="post-prevnext my-3"><article class="post-prev col-6"><a href="/2022/11/12/%E5%93%8D%E5%BA%94%E5%BC%8F%E7%BC%96%E7%A8%8B/" title="响应式编程"><i class="iconfont icon-arrowleft"></i> <span class="hidden-mobile">响应式编程</span> <span class="visible-mobile">上一篇</span></a></article><article class="post-next col-6"><a href="/2022/09/26/MAT%E4%B8%AD%E6%96%87%E6%96%87%E6%A1%A3/" title="MAT中文文档"><span class="hidden-mobile">MAT中文文档</span> <span class="visible-mobile">下一篇</span> <i class="iconfont icon-arrowright"></i></a></article></div></div></article></div></div></div><div class="side-col d-none d-lg-block col-lg-2"><aside class="sidebar" style="margin-left:-1rem"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i> <span>目录</span></p><div class="toc-body" id="toc-body"></div></div></aside></div></div></div><a id="scroll-top-button" aria-label="TOP" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a><div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable modal-lg" role="document"><div class="modal-content"><div class="modal-header text-center"><h4 class="modal-title w-100 font-weight-bold">搜索</h4><button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button></div><div class="modal-body mx-3"><div class="md-form mb-5"><input type="text" id="local-search-input" class="form-control validate"> <label data-error="x" data-success="v" for="local-search-input">关键词</label></div><div class="list-group" id="local-search-result"></div></div></div></div></div></main><footer><div class="footer-inner"><div class="footer-content"><a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a></div></div></footer><script src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js"></script><link rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css"><script>NProgress.configure({showSpinner:!1,trickleSpeed:100}),NProgress.start(),window.addEventListener("load",function(){NProgress.done()})</script><script src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js"></script><script src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js"></script><script src="/js/events.js"></script><script src="/js/plugins.js"></script><script src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js"></script><script>!function(t){var e=Fluid.plugins.typing,i=t.getElementById("subtitle");i&&e&&e(i.getAttribute("data-typed-text"))}((window,document))</script><script src="/js/img-lazyload.js"></script><script>Fluid.utils.createScript("https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js",function(){var t,o=jQuery("#toc");0!==o.length&&window.tocbot&&(t=jQuery("#board-ctn").offset().top,window.tocbot.init(Object.assign({tocSelector:"#toc-body",contentSelector:".markdown-body",linkClass:"tocbot-link",activeLinkClass:"tocbot-active-link",listClass:"tocbot-list",isCollapsedClass:"tocbot-is-collapsed",collapsibleClass:"tocbot-is-collapsible",scrollSmooth:!0,includeTitleTags:!0,headingsOffset:-t},CONFIG.toc)),0<o.find(".toc-list-item").length&&o.css("visibility","visible"),Fluid.events.registerRefreshCallback(function(){if("tocbot"in window){tocbot.refresh();var t=jQuery("#toc");if(0===t.length||!tocbot)return;0<t.find(".toc-list-item").length&&t.css("visibility","visible")}}))})</script><script src="https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js"></script><script>Fluid.plugins.codeWidget()</script><script>Fluid.utils.createScript("https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js",function(){Fluid.plugins.fancyBox()})</script><script>Fluid.plugins.imageCaption()</script><script src="/js/local-search.js"></script><script src="/js/boot.js"></script><noscript><div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div></noscript></body></html>