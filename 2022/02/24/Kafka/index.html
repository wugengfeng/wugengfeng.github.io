<!DOCTYPE html><html lang="zh-CN" data-default-color-scheme="auto"><head><meta charset="UTF-8"><link rel="stylesheet" href="https://npm.elemecdn.com/lxgw-wenkai-screen-webfont/style.css" media="print" onload='this.media="all"'><link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png"><link rel="icon" href="/img/fluid.png"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=5,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta name="theme-color" content="#2f4154"><meta name="author" content="wugengfeng"><meta name="keywords" content="wugengfeng"><meta name="description" content="思维脑图 项目涉及源码  消息队列概述  两种工作模式 点对点模式  一对一，消费者主动拉取数据，消息收到后消息清除 在点对点消息队列（Point-to-Point, P2P）模式中，消息生产者将消息发送至特定的队列（Queue），而消息消费者则从该队列中主动拉取并处理消息。每个消息只能被一个消费者接收和处理。一旦消费者成功处理了消息，该消息就会从队列中被移除，确保不会被其他消费者再次处理。虽然一"><meta property="og:type" content="article"><meta property="og:title" content="Kafka"><meta property="og:url" content="https://wugengfeng.cn/2022/02/24/Kafka/index.html"><meta property="og:site_name" content="技术博客"><meta property="og:description" content="思维脑图 项目涉及源码  消息队列概述  两种工作模式 点对点模式  一对一，消费者主动拉取数据，消息收到后消息清除 在点对点消息队列（Point-to-Point, P2P）模式中，消息生产者将消息发送至特定的队列（Queue），而消息消费者则从该队列中主动拉取并处理消息。每个消息只能被一个消费者接收和处理。一旦消费者成功处理了消息，该消息就会从队列中被移除，确保不会被其他消费者再次处理。虽然一"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://wugengfeng.cn/img/banner/kafka.png"><meta property="article:published_time" content="2022-02-24T08:52:56.000Z"><meta property="article:modified_time" content="2023-10-27T06:11:47.632Z"><meta property="article:author" content="wugengfeng"><meta property="article:tag" content="wugengfeng"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:image" content="https://wugengfeng.cn/img/banner/kafka.png"><meta name="baidu-site-verification" content="codeva-uJ0fkFPmHB"><title>Kafka - 技术博客</title><link rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css"><link rel="stylesheet" href="/css/main.css"><link id="highlight-css" rel="stylesheet" href="/css/highlight.css"><link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css"><link rel="stylesheet" href="/css/custom.css"><script id="fluid-configs">var Fluid=window.Fluid||{};Fluid.ctx=Object.assign({},Fluid.ctx);var dntVal,CONFIG={hostname:"wugengfeng.cn",root:"/",version:"1.9.3",typing:{enable:!0,typeSpeed:70,cursorChar:"_",loop:!1,scope:[]},anchorjs:{enable:!1,element:"h9",placement:"left",visible:"hover",icon:""},progressbar:{enable:!0,height_px:3,color:"#29d",options:{showSpinner:!1,trickleSpeed:100}},code_language:{enable:!0,default:"TEXT"},copy_btn:!0,image_caption:{enable:!0},image_zoom:{enable:!0,img_url_replace:["",""]},toc:{enable:!0,placement:"right",headingSelector:"h1,h2,h3,h4,h5,h6",collapseDepth:1},lazyload:{enable:!0,loading_img:"/img/loading.gif",onlypost:!1,offset_factor:2},web_analytics:{enable:!1,follow_dnt:!0,baidu:null,google:null,gtag:null,tencent:{sid:null,cid:null},woyaola:null,cnzz:null,leancloud:{app_id:null,app_key:null,server_url:null,path:"window.location.pathname",ignore_local:!1}},search_path:"/local-search.xml"};CONFIG.web_analytics.follow_dnt&&(dntVal=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,Fluid.ctx.dnt=dntVal&&(dntVal.startsWith("1")||dntVal.startsWith("yes")||dntVal.startsWith("on")))</script><script src="/js/utils.js"></script><script src="/js/color-schema.js"></script><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="技术博客" type="application/atom+xml"></head><body><header><div class="header-inner" style="height:70vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/"><strong>吴耿锋</strong> </a><button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/"><i class="iconfont icon-home-fill"></i> <span>首页</span></a></li><li class="nav-item"><a class="nav-link" href="/archives/"><i class="iconfont icon-archive-fill"></i> <span>归档</span></a></li><li class="nav-item"><a class="nav-link" href="/categories/"><i class="iconfont icon-category-fill"></i> <span>分类</span></a></li><li class="nav-item"><a class="nav-link" href="/tags/"><i class="iconfont icon-tags-fill"></i> <span>标签</span></a></li><li class="nav-item"><a class="nav-link" href="/about/"><i class="iconfont icon-user-fill"></i> <span>关于</span></a></li><li class="nav-item" id="search-btn"><a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search"><i class="iconfont icon-search"></i></a></li><li class="nav-item" id="color-toggle-btn"><a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle"><i class="iconfont icon-dark" id="color-toggle-icon"></i></a></li></ul></div></div></nav><div id="banner" class="banner" parallax="true" style="background:url(/img/default.png) no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.3)"><div class="banner-text text-center fade-in-up"><div class="h2"><span id="subtitle" data-typed-text="Kafka"></span></div><div class="mt-3"><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2022-02-24 16:52" pubdate>2022年2月24日 下午</time></span></div><div class="mt-1"><span class="post-meta mr-2"><i class="iconfont icon-chart"></i> 83k 字 </span><span class="post-meta mr-2"><i class="iconfont icon-clock-fill"></i> 691 分钟</span></div></div></div></div></div></div></header><main><div class="container-fluid nopadding-x"><div class="row nomargin-x"><div class="side-col d-none d-lg-block col-lg-2"></div><div class="col-lg-8 nopadding-x-md"><div class="container nopadding-x-md" id="board-ctn"><div id="board"><article class="post-content mx-auto"><h1 style="display:none">Kafka</h1><div class="markdown-body"><p><a target="_blank" rel="noopener" href="https://www.mubu.com/doc/4Z1-84Fd4i">思维脑图</a></p><p><a target="_blank" rel="noopener" href="https://gitee.com/kenful/spring-boot-kafka">项目涉及源码</a></p><h2 id="消息队列概述"><a class="markdownIt-Anchor" href="#消息队列概述"></a> 消息队列概述</h2><h3 id="两种工作模式"><a class="markdownIt-Anchor" href="#两种工作模式"></a> 两种工作模式</h3><p class="note note-primary">点对点模式</p><p><img src="/2022/02/24/Kafka/%E7%82%B9%E5%AF%B9%E7%82%B9%E6%A8%A1%E5%BC%8F.png" srcset="/img/loading.gif" lazyload alt></p><p>一对一，消费者主动拉取数据，消息收到后消息清除</p><p>在点对点消息队列（Point-to-Point, P2P）模式中，消息生产者将消息发送至特定的队列（Queue），而消息消费者则从该队列中主动拉取并处理消息。每个消息只能被一个消费者接收和处理。一旦消费者成功处理了消息，该消息就会从队列中被移除，确保不会被其他消费者再次处理。虽然一个队列可以拥有多个消费者，每条消息只能被其中一个消费者接收。此模式确保每条消息都会被精确地处理一次，避免重复或遗漏的情况发生。</p><p class="note note-primary">发布/订阅模式</p><p><img src="/2022/02/24/Kafka/%E5%8F%91%E5%B8%83%E8%AE%A2%E9%98%85%E6%A8%A1%E5%BC%8F.png" srcset="/img/loading.gif" lazyload alt></p><p>一对多，消费者消费数据之后不会清理消息，有时间限制</p><p>在消息队列的发布-订阅（Publish-Subscribe）模式中，消息生产者（或称发布者）将消息发送到一个称作“主题”（Topic）的通道中。与点对点模式不同的是，所有订阅了该主题的消费者（或称订阅者）都有能力接收到这些消息。重要的是，一条发布到主题的消息会被所有活跃的订阅者消费，实现了一条消息能够同时被多个消费者接收的效果。此模式支持一条消息被多次处理，由不同的消费者分别进行处理和响应。发布到主题的消息通常会在所有订阅者消费完成后保留一段预定的时间或直至满足某些条件后才被清理。</p><div class="note note-warning"><p><code>kafka</code> 是发布订阅模式，只有一个消费者组就类似点对点模式，多个消费者组就是发布/订阅模式，但是不会马上删除数据，需要配合配置文件</p></div><h3 id="两种获取消息的方式"><a class="markdownIt-Anchor" href="#两种获取消息的方式"></a> 两种获取消息的方式</h3><p class="note note-primary">推模式</p><p>在推模式中，当 <code>Broker</code> 接收到 <code>Producer</code> 的消息后，它会主动地将这些消息推送给 <code>Consumer</code>。具体来说，客户端（Consumer）与服务端（Broker）之间会建立一个持久化的连接。当Broker有新的消息时，它会直接通过这个已建立的连接推送消息到客户端，从而确保客户端可以实时接收到最新的消息。</p><p><code>优点</code></p><ul><li><strong><code>实时性</code></strong>：由于消息是被主动推送的，客户端能够快速地接收到最新的消息，确保了消息的实时传递。</li><li><strong><code>客户端实现简单</code></strong>：只需要监听服务端的推送消息</li></ul><p><code>缺点</code></p><ul><li><strong><code>无法应对不同能力的消费者</code></strong>：每个客户端的消费能力是不同的，如果简单粗暴进行消息推送就会出现 <code>消息堆积</code> 而引发宕机</li></ul><p class="note note-primary">拉模式</p><p>在拉模式中，<code>Consumer</code> 会主动向 <code>Broker</code> 请求消息，而不是靠Broker将消息推送给它。这意味着Consumer决定何时接收消息，并控制获取消息的频率。</p><p><span class="green-line"><code>长轮询</code>：客户端向服务端发起请求，如果此时有数据就直接返回，如果没有数据就保持连接，等到有数据时就直接返回。如果一直没有数据，超时后客户端再次发起请求，保持连接</span></p><p><code>优点</code></p><ul><li><strong><code>避免消息堆积</code></strong>：由于Consumer控制消息的拉取时机，它可以确保在上一批消息处理完成后再拉取新的消息，从而避免在客户端出现消息堆积。</li><li>长轮询实现的拉模式实时性也能够保证消息时效性</li></ul><p><code>缺点</code></p><ul><li><strong><code>实现复杂性</code></strong>：客户端需要维护拉取消息的逻辑和处理潜在的超时/重试情况，又要考虑消息的时效性和避免忙等</li></ul><p><span style="border-bottom:2px dashed green"><code>kafka</code> <code>consumer</code>从<code>broker</code>拉取消息</span></p><h2 id="kafka概念"><a class="markdownIt-Anchor" href="#kafka概念"></a> Kafka概念</h2><h3 id="定义"><a class="markdownIt-Anchor" href="#定义"></a> 定义</h3><p><code>Kafka传统定义</code>： Kafka 通常被定义为一个分布式的、基于发布/订阅模式的消息队列系统，它在大数据和实时分析领域具有广泛的应用。</p><p><code>发布/订阅</code>：消息发布者（Producer）不会直接将消息发送到特定的接收者（Consumer）。相反，发布者将消息发送到一个中间层，通常是一个“主题”（Topic），而订阅者则从这个主题订阅它们感兴趣的消息。这种解耦的方式使得生产者和消费者可以独立地扩展和演变。</p><p><code>Kafka最新定义</code>：如今通常被认为是一个开源的分布式事件流平台，它不仅仅能处理高吞吐量的事件流数据，也能支持高性能的数据管道、数据集成、实时分析和关键任务应用。其灵活的架构使其在全球范围内的数千家公司中被用于构建多种应用场景的事件驱动解决方案。</p><h3 id="优缺点"><a class="markdownIt-Anchor" href="#优缺点"></a> 优缺点</h3><p><code>优点</code>：</p><ul><li><strong><code>高性能与高吞吐量</code></strong>：Kafka展现了卓越的性能表现和高吞吐量，单机事务吞吐量可达到百万条/秒级别。</li><li><strong><code>高可用性</code></strong>：作为一个分布式系统，Kafka通过数据副本保证了高可用性和数据的持久性。即使部分节点宕机，由于多副本的存在，数据仍不会丢失，系统依然可用。</li><li><strong><code>主动拉取消息</code></strong>：消费者采用Pull（拉取）方式获取消息，支持消息有序消费，并能通过控制策略确保消息被消费一次且仅被消费一次。</li><li><strong><code>生态支持</code></strong>：Kafka拥有稳定的社区支持，以及丰富的第三方工具，例如Kafka Web管理界面EFAK。在日志处理和大数据实时处理领域（如Flink、CDC、ETL等）具有广泛的应用。</li><li><strong><code>功能定位</code></strong>：虽然功能相对简洁，主要专注于消息队列（MQ）功能，但在大数据领域，这种定位使其在实时计算和日志采集等应用场景下表现卓越。</li></ul><p><code>缺点</code>：</p><ul><li><strong><code>再平衡延迟</code></strong>：消费者组成员的变化可能导致再平衡，这会引入额外的延迟。</li><li><strong><code>实时性与轮询策略</code></strong>：消费者使用轮询方式获取消息，因此消息的实时性会受轮询间隔时间的影响。</li><li><strong><code>不支持自动重试</code></strong>：在消费消息失败的场景下，Kafka本身不提供自动重试的机制。</li><li><strong><code>消息顺序问题</code></strong>：保证单分区内消息有序，但是不保证多分区间的消息有序。</li></ul><h3 id="应用场景"><a class="markdownIt-Anchor" href="#应用场景"></a> 应用场景</h3><p class="note note-primary">缓存/消峰</p><p><img src="/2022/02/24/Kafka/%E7%BC%93%E5%AD%98%E6%B6%88%E5%B3%B0.png" srcset="/img/loading.gif" lazyload alt></p><p>Kafka 能够作为一个高效的缓冲层，协助控制和优化数据在系统间的流动，<span class="green-line">尤其适用于生产消息的速度和消费消息的处理速度不匹配的场景</span>。以秒杀功能为例：在高流量场景下，如用户请求量超过服务器的处理能力，可以利用Kafka进行秒杀请求的缓存。服务器根据其处理能力，稳定地从队列中拉取数据进行处理，从而有效地实现流量的削峰。</p><p class="note note-primary">解耦</p><p><img src="/2022/02/24/Kafka/%E8%A7%A3%E8%80%A6.png" srcset="/img/loading.gif" lazyload alt></p><p>Kafka 在系统解耦方面表现卓越，特别是在需要异构数据同步的上下文中。例如，在一个需要从多个服务系统中收集日志数据、并将数据分发到不同数据库进行各异分析的日志收集系统中。通过Kafka进行数据交流和传递，不同的服务和处理模块可以解耦合，保持独立运行和演进，只需确保数据的消费处理逻辑保持一致。</p><p class="note note-primary">异步通信</p><p><img src="/2022/02/24/Kafka/%E5%BC%82%E6%AD%A5%E9%80%9A%E4%BF%A1.png" srcset="/img/loading.gif" lazyload alt></p><p>Kafka 支持异步通信模型，允许生产者将消息发布到队列中而无需立即进行处理。消费者可以根据自身的处理能力，在适当的时候从队列中拉取消息进行处理。一个常见的应用场景是电商平台的订单通知系统：当用户下单成功时，下单信息被发送到Kafka，短信通知服务作为消费者异步从Kafka中获取这些信息，进而在可控的速度和时间内，发送订单确认短信给用户。</p><p class="note note-primary">数据聚合</p><p>Kafka 可以用于收集来自多个源的数据，将其聚合在一处，方便进行数据分析和信息提取。比如多个服务的操作日志可以汇集至Kafka，再由专门的日志分析服务消费，实现集中式的日志管理和分析。</p><p class="note note-primary">事件驱动架构</p><p>在事件驱动架构中，Kafka 常作为事件传递的中介，各个服务发布和订阅事件，从而实现低耦合的交互和协作。通过精细定义事件的种类和格式，服务可以轻松响应其他服务的状态变化或请求，而无需直接交互。</p><h3 id="kafka架构"><a class="markdownIt-Anchor" href="#kafka架构"></a> Kafka架构</h3><p><a target="_blank" rel="noopener" href="https://jiamaoxiang.top/2020/10/24/Kafka-producer%E7%9A%84%E5%87%A0%E4%B8%AA%E9%87%8D%E8%A6%81%E9%85%8D%E7%BD%AE%E5%8F%82%E6%95%B0/">几个重要配置参数</a></p><p><img src="/2022/02/24/Kafka/kafka%E6%9E%B6%E6%9E%84.png" srcset="/img/loading.gif" lazyload alt></p><h4 id="broker"><a class="markdownIt-Anchor" href="#broker"></a> Broker</h4><p><code>定义</code></p><ul><li><strong><code>消息处理中心</code></strong>：Kafka Broker（服务实例） 接收来自生产者的消息，存储这些消息，并处理消费者的读取请求。</li><li><strong><code>分布式节点</code></strong>：在Kafka的分布式环境中，Broker是作为独立节点运行的服务器或一组服务器。</li></ul><p><code>核心功能</code></p><ul><li><strong><code>数据存储</code></strong>：Broker负责将生产者发送的消息持久化到磁盘上，确保数据的安全性。</li><li><strong><code>数据提取</code></strong>：Broker允许消费者读取存储的数据，支持并行读取。</li><li><strong><code>分区管理</code></strong>：Kafka的数据存储在不同的Topic中，每个Topic可以被分成多个分区。Broker管理这些分区的数据，每个分区可以在不同的Broker上。</li><li><strong><code>副本管理</code></strong>：为了提高数据的可用性和耐用性，Kafka的数据通常会在多个Broker上进行复制。Broker负责管理这些副本，并在必要时进行故障转移。</li><li><strong><code>协调和同步</code></strong>：在多Broker环境下，Broker间会协调并同步消息和事务状态。</li></ul><h4 id="topic"><a class="markdownIt-Anchor" href="#topic"></a> Topic</h4><p>在Kafka中，消息被归类到不同的 <code>topic</code>。每条发送到Kafka集群的消息都应指定一个 <code>topic</code>。默认情况下，topic中的数据保留7天（168小时），但这是可以配置的。</p><p><span class="green-line"><code>Topic </code> 在Kafka中是逻辑上的分区，而其下的 <code>partition</code> 则代表物理上的分区。</span></p><p class="note note-primary">特殊Topic</p><p><a target="_blank" rel="noopener" href="https://www.cxyzjd.com/article/yan3013216087/104245609">特殊Topic</a></p><p>从Kafka 0.10.x版本开始，消费者的 <code>offset</code> 信息默认存储在内置的 <code>__consumer_offsets</code> topic中。当消费者首次尝试消费Kafka数据时，这个特殊的topic会自动被创建。需要注意的是，该topic的副本数不受集群的默认topic副本数配置的影响，而它的默认分区数为50，但这也是可以调整的。</p><h4 id="partition"><a class="markdownIt-Anchor" href="#partition"></a> Partition</h4><p><code>Topic -&gt; Partition -&gt; 副本</code></p><p><img src="/2022/02/24/Kafka/partation.png" srcset="/img/loading.gif" lazyload alt></p><p><span class="green-line">Partition（分区）是用于实现消息在处理时的并行化的基本单位（物理分区）</span>。每个Topic可以被分成一个或多个Partition，每个Partition可以存在于多个节点上以提供数据冗余，以确保数据在某些节点失败的情况下仍然可用。这样，Partition既能提供系统的横向扩展性，也提供了数据的高可用性。</p><p><code>核心功能</code>：</p><ul><li><strong><code>并行处理</code></strong>：通过将Topic分区，Kafka可以在多个broker（节点）上并行处理数据，也允许在多个消费者之间并行处理数据。</li><li><strong><code>数据持久化</code></strong>：每个Partition都是一个有序的、不可变的记录集，并且可以持久地存储到磁盘上。每个在Partition中的消息都会被分配一个唯一的、递增的ID号，称为“offset”。</li><li><strong><code>水平扩展</code></strong>：通过增加Partition数量，Kafka能够水平扩展处理更多的消息。</li><li><strong><code>复制</code></strong>：Partition也支持复制，以防止数据丢失。<span class="green-line">每个Partition会有一个Leader和零个或多个Follower。所有的读写操作都由Leader处理，而Follower用来在Leader失败时提供冗余备份。</span></li><li><strong><code>消费者组内的并行消费</code></strong>：消费者组中的每个消费者实例都会消费Topic的一个或多个Partition的数据。一个Partition在一个消费者组内只会被一个消费者实例消费，从而实现了在消费者组内的消息处理的负载均衡。</li></ul><p><span class="green-line">Partition数量只能增加不能减少</span></p><h4 id="replica"><a class="markdownIt-Anchor" href="#replica"></a> Replica</h4><p>Kafka中的每个分区（Partition）可以有一个或多个副本，这些副本分散在集群中的不同集群节点上，以实现数据的持久化和高可用。</p><p class="note note-primary">Leader</p><p>在Kafka中，每个分区都有一个并且只有一个 Leader 副本，它负责处理该分区的所有读写操作。虽然一个Topic可能包含多个分区，但每个分区的Leader副本都是独立且唯一的。</p><p class="note note-primary">Follower</p><p>Follower 副本主要用于备份和同步Leader副本中的数据。在Leader副本出现故障时，Kafka会从Follower副本中选举一个新的Leader来保证高可用性。</p><p class="note note-primary">ISR</p><p>表示和 Leader 保持同步的副本集合，主节点宕机可作为备选节点。</p><p class="note note-primary">OSR</p><p>表示 Follower 与 Leader 副本同步时，延迟过多的副本集合。</p><h4 id="producer"><a class="markdownIt-Anchor" href="#producer"></a> Producer</h4><p>生产者（Producer）是消息生产和发布的实体。它负责将应用程序的消息发布到指定的 Kafka 主题。</p><ul><li><p><strong><code>main线程</code></strong></p><p>主线程主要负责将消息（Records）封装到一个 ProducerRecord 对象中，并发送给内部的 <code>RecordAccumulator</code>。</p><ul><li><p><strong>序列化：</strong> 将 Key 和 Value 对象序列化为字节数组。</p></li><li><p><strong>分区：</strong> 如果 ProducerRecord 中没有指定 Partition，Sender 线程需要通过 Partitioner 来决定消息将发送到 Topic 的哪个 Partition 上。</p></li></ul></li><li><p><strong><code>RecordAccumulator</code>（内存缓冲区）</strong></p><p>RecordAccumulator 是一个消息的内存缓冲区。当主线程调用 Producer 的 <code>send()</code> 方法发送消息时，消息实际上被先存储在此缓冲区中。它会尽可能地批量处理这些消息（即尽可能地将多个消息打包在一个 batch 中）以提高效率。当 batch 满了或经过一定时间（<code>linger.ms</code> 属性设定的时间）后，消息会被发送到目标分区（Partition）。</p></li><li><p><strong><code>send线程</code></strong></p><p>Sender 线程是 Kafka 生产者的核心，它从 RecordAccumulator 中取出消息并负责将消息发送到 Kafka Broker 上对应的 Partition 中。</p><ul><li><strong>压缩：</strong> 根据配置，Sender 线程可能会将几个消息压缩到一起发送以节省带宽。</li><li><strong>发送：</strong> Sender 线程将消息发送到 Broker 并等待 Broker 的确认。</li><li><strong>重试：</strong> 如果消息发送失败，根据生产者的配置（例如 <code>retries</code> 和 <code>retry.backoff.ms</code> 等参数），Sender 线程可能会进行一定次数的重试。</li><li><strong>回调：</strong> 如果消息发送成功或者最终失败，Sender 线程将调用回调方法（如果在发送消息时提供了回调函数）。</li></ul></li></ul><h4 id="consumergroup"><a class="markdownIt-Anchor" href="#consumergroup"></a> ConsumerGroup</h4><p><img src="/2022/02/24/Kafka/%E6%B6%88%E8%B4%B9%E8%80%85%E7%BB%84.png" srcset="/img/loading.gif" lazyload alt></p><p>在Kafka中，消费者组是一种机制，<span class="green-line">使多个消费者可以协作处理同一个主题的消息</span>。消费者组内的每个消费者负责读取该主题的不同分区，确保每条消息只被消费者组中的一个消费者消费一次。</p><ul><li><strong><code>消费者组内的再分配</code></strong>：当消费者组中的消费者数量发生变化（例如新的消费者加入或现有的消费者离开）时，Kafka会重新分配主题分区给消费者。这种再分配确实会对系统造成一些开销，但它允许Kafka的消费模式具有弹性和容错性。</li><li><strong><code>分区和消费者的数量关系</code></strong>：为了确保每个消费者都有数据可以读取，一个主题的分区数量应当大于或等于消费者组中消费者的数量。</li><li><strong><code>消费者和消费者组</code></strong>：每个消费者都属于一个消费者组，并可以为每个消费者指定一个组名。如果不指定组名，消费者会属于默认组。<span class="green-line">一条消息可以被多个消费者组中的消费者读取，但在一个消费者组内，一条消息只会被一个消费者读取。</span></li></ul><h4 id="consumer"><a class="markdownIt-Anchor" href="#consumer"></a> Consumer</h4><p>消费者在Kafka中负责从Broker读取消息。Kafka采用的是发布-订阅模式，但与传统的发布-订阅系统不同的是，Kafka中的订阅者是<strong>消费者组</strong>而非单一的消费者实例。<span class="green-line">在消费者组内，每条消息只会被一个消费者实例处理。但值得注意的是，不同的消费者组可以独立地并行消费同一条消息。</span></p><h2 id="kafka集群搭建"><a class="markdownIt-Anchor" href="#kafka集群搭建"></a> Kafka集群搭建</h2><p class="note note-primary">java8环境安装</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">sudo apt-get update<br>sudo apt-get install openjdk-8-jdk<br></code></pre></td></tr></table></figure><p class="note note-primary">下载并解压</p><p><a target="_blank" rel="noopener" href="https://kafka.apache.org/downloads">kafka最新版本下载</a></p><p>新版本Kafka不需要额外安装zookeeper，使用内置zookeeper搭建集群。目前还支持kraft方式部署，不过目前版本还未稳定故不选择</p><p>下载kafka_2.12-3.2.0.tgz（或目前最新版本）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">创建目录</span><br>mkdir /data/kafka/k1<br>mkdir /data/kafka/k2<br>mkdir /data/kafka/k3<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">解压到创建目录</span><br>tar -zvxf kafka_2.12-3.2.0.tgz -C /data/kafka/k1/<br>tar -zvxf kafka_2.12-3.2.0.tgz -C /data/kafka/k2/<br>tar -zvxf kafka_2.12-3.2.0.tgz -C /data/kafka/k3/<br></code></pre></td></tr></table></figure><p class="note note-primary">zookeeper存储位置配置</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">创建zookeeper数据存储目录</span><br>mkdir /data/kafka/k1/kafka_2.12-3.2.0/zookeeper<br>mkdir /data/kafka/k1/kafka_2.12-3.2.0/zookeeper/data<br>mkdir /data/kafka/k1/kafka_2.12-3.2.0/zookeeper/log<br><br>mkdir /data/kafka/k2/kafka_2.12-3.2.0/zookeeper<br>mkdir /data/kafka/k2/kafka_2.12-3.2.0/zookeeper/data<br>mkdir /data/kafka/k2/kafka_2.12-3.2.0/zookeeper/log<br><br>mkdir /data/kafka/k3/kafka_2.12-3.2.0/zookeeper<br>mkdir /data/kafka/k3/kafka_2.12-3.2.0/zookeeper/data<br>mkdir /data/kafka/k3/kafka_2.12-3.2.0/zookeeper/log<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">创建myid文件</span><br>echo 1 &gt; /data/kafka/k1/kafka_2.12-3.2.0/zookeeper/data/myid<br>echo 2 &gt; /data/kafka/k2/kafka_2.12-3.2.0/zookeeper/data/myid<br>echo 3 &gt; /data/kafka/k3/kafka_2.12-3.2.0/zookeeper/data/myid<br></code></pre></td></tr></table></figure><p class="note note-primary">zookeeper配置修改</p><p>将三个目录下的zookeeper配置文件分别修改下文三个配置</p><ul><li>/data/kafka/k1/kafka_2.12-3.2.0/config/zookeeper.properties</li><li>/data/kafka/k2/kafka_2.12-3.2.0/config/zookeeper.properties</li><li>/data/kafka/k3/kafka_2.12-3.2.0/config/zookeeper.properties</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash">心跳时间2秒</span> <br>tickTime=2000<br><span class="hljs-meta prompt_">#</span><span class="language-bash">Follower跟随者服务器与Leader领导者服务器之间初始化连接时能容忍的最多心跳数10*tickTime</span> <br>initLimit=10<br><span class="hljs-meta prompt_">#</span><span class="language-bash">集群中Leader与Follower之间的最大响应时间单位5*tickTime</span> <br>syncLimit=5<br><span class="hljs-meta prompt_">#</span><span class="language-bash">存储快照文件 snapshot 的目录。默认情况下，事务日志也会存储在这里。建议同时配置参数dataLogDir, 事务日志的写性能直接影响zk性能</span><br>dataDir=/data/kafka/k1/kafka_2.12-3.2.0/zookeeper/data<br><span class="hljs-meta prompt_">#</span><span class="language-bash">事务日志输出目录。尽量给事务日志的输出配置单独的磁盘或是挂载点，这将极大的提升ZK性能</span><br>dataLogDir=/data/kafka/k1/kafka_2.12-3.2.0/zookeeper/log<br><span class="hljs-meta prompt_">#</span><span class="language-bash">zookeeper端口</span>  <br>clientPort=2181<br><span class="hljs-meta prompt_">#</span><span class="language-bash">单个客户端与单台服务器之间的连接数的限制，是ip级别的，默认是60，如果设置为0，那么表明不作任何限制</span><br>maxClientCnxns=60<br><span class="hljs-meta prompt_">#</span><span class="language-bash">server.1代表一台服务器的编号，第一个端口为集群通讯端口，第二个端口代表Leader选举的端口</span> <br>server.1=127.0.0.1:2881:3881<br>server.2=127.0.0.1:2882:3882<br>server.3=127.0.0.1:2883:3883<br></code></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash">心跳时间2秒</span> <br>tickTime=2000<br><span class="hljs-meta prompt_">#</span><span class="language-bash">Follower跟随者服务器与Leader领导者服务器之间初始化连接时能容忍的最多心跳数10*tickTime</span> <br>initLimit=10<br><span class="hljs-meta prompt_">#</span><span class="language-bash">集群中Leader与Follower之间的最大响应时间单位5*tickTime</span> <br>syncLimit=5<br><span class="hljs-meta prompt_">#</span><span class="language-bash">存储快照文件 snapshot 的目录。默认情况下，事务日志也会存储在这里。建议同时配置参数dataLogDir, 事务日志的写性能直接影响zk性能</span><br>dataDir=/data/kafka/k2/kafka_2.12-3.2.0/zookeeper/data<br><span class="hljs-meta prompt_">#</span><span class="language-bash">事务日志输出目录。尽量给事务日志的输出配置单独的磁盘或是挂载点，这将极大的提升ZK性能</span><br>dataLogDir=/data/kafka/k2/kafka_2.12-3.2.0/zookeeper/log<br><span class="hljs-meta prompt_">#</span><span class="language-bash">zookeeper端口</span>  <br>clientPort=2182<br><span class="hljs-meta prompt_">#</span><span class="language-bash">单个客户端与单台服务器之间的连接数的限制，是ip级别的，默认是60，如果设置为0，那么表明不作任何限制</span><br>maxClientCnxns=60<br><span class="hljs-meta prompt_">#</span><span class="language-bash">server.1代表一台服务器的编号，第一个端口为集群通讯端口，第二个端口代表Leader选举的端口</span> <br>server.1=127.0.0.1:2881:3881<br>server.2=127.0.0.1:2882:3882<br>server.3=127.0.0.1:2883:3883<br></code></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash">心跳时间2秒</span> <br>tickTime=2000<br><span class="hljs-meta prompt_">#</span><span class="language-bash">Follower跟随者服务器与Leader领导者服务器之间初始化连接时能容忍的最多心跳数10*tickTime</span> <br>initLimit=10<br><span class="hljs-meta prompt_">#</span><span class="language-bash">集群中Leader与Follower之间的最大响应时间单位5*tickTime</span> <br>syncLimit=5<br><span class="hljs-meta prompt_">#</span><span class="language-bash">存储快照文件 snapshot 的目录。默认情况下，事务日志也会存储在这里。建议同时配置参数dataLogDir, 事务日志的写性能直接影响zk性能</span><br>dataDir=/data/kafka/k3/kafka_2.12-3.2.0/zookeeper/data<br><span class="hljs-meta prompt_">#</span><span class="language-bash">事务日志输出目录。尽量给事务日志的输出配置单独的磁盘或是挂载点，这将极大的提升ZK性能</span><br>dataLogDir=/data/kafka/k3/kafka_2.12-3.2.0/zookeeper/log<br><span class="hljs-meta prompt_">#</span><span class="language-bash">zookeeper端口</span>  <br>clientPort=2183<br><span class="hljs-meta prompt_">#</span><span class="language-bash">单个客户端与单台服务器之间的连接数的限制，是ip级别的，默认是60，如果设置为0，那么表明不作任何限制</span><br>maxClientCnxns=60<br><span class="hljs-meta prompt_">#</span><span class="language-bash">server.1代表一台服务器的编号，第一个端口为集群通讯端口，第二个端口代表Leader选举的端口</span> <br>server.1=127.0.0.1:2881:3881<br>server.2=127.0.0.1:2882:3882<br>server.3=127.0.0.1:2883:3883<br></code></pre></td></tr></table></figure><p class="note note-primary">编写启动脚本启动</p><ul><li>/data/kafka/k1/kafka_2.12-3.2.0</li><li>/data/kafka/k1/kafka_2.12-3.2.0</li><li>/data/kafka/k1/kafka_2.12-3.2.0</li></ul><p><code>zk-start.sh</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash">!/bin/bash</span><br>nohup bin/zookeeper-server-start.sh config/zookeeper.properties &gt; logs/zookeeper/zookeeper.log 2&gt;1 &amp;<br></code></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">创建<span class="hljs-built_in">log</span>目录</span><br>mkdir /data/kafka/k1/kafka_2.12-3.2.0/logs<br>mkdir /data/kafka/k1/kafka_2.12-3.2.0/logs/zookeeper<br>mkdir /data/kafka/k2/kafka_2.12-3.2.0/logs<br>mkdir /data/kafka/k2/kafka_2.12-3.2.0/logs/zookeeper<br>mkdir /data/kafka/k3/kafka_2.12-3.2.0/logs<br>mkdir /data/kafka/k3/kafka_2.12-3.2.0/logs/zookeeper<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">分别启动zk</span><br>. zk-start.sh<br></code></pre></td></tr></table></figure><p class="note note-primary">创建kafka数据目录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">mkdir /data/kafka/k1/kafka_2.12-3.2.0/data<br>mkdir /data/kafka/k2/kafka_2.12-3.2.0/data<br>mkdir /data/kafka/k3/kafka_2.12-3.2.0/data<br></code></pre></td></tr></table></figure><p class="note note-primary">kafka配置修改</p><p>将三个目录下的kafka配置文件分别修改下文三个配置</p><ul><li>/data/kafka/k1/kafka_2.12-3.2.0/config/server.properties</li><li>/data/kafka/k2/kafka_2.12-3.2.0/config/server.properties</li><li>/data/kafka/k3/kafka_2.12-3.2.0/config/server.properties</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">broker的全局唯一编号，不能重复</span><br>broker.id=1<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">提供给客户端响应的地址和端口</span><br>listeners = PLAINTEXT://0.0.0.0:9091<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">提供给客户端响应的地址和端口（允许外网访问）</span><br>advertised.listeners=PLAINTEXT://127.0.0.1:9091<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">处理网络请求的线程数量，也就是接收消息的线程数。</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">接收线程会将接收到的消息放到内存中，然后再从内存中写入磁盘。</span><br>num.network.threads=3<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">消息从内存中写入磁盘是时候使用的线程数量。</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">用来处理磁盘IO的线程数量</span><br>num.io.threads=8<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">发送套接字的缓冲区大小</span><br>socket.send.buffer.bytes=102400<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">接受套接字的缓冲区大小</span><br>socket.receive.buffer.bytes=102400<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">请求套接字的缓冲区大小</span><br>socket.request.max.bytes=104857600<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">kafka运行日志存放的路径</span><br>log.dirs=/data/kafka/k1/kafka_2.12-3.2.0/data<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">topic在当前broker上的分片个数</span><br>num.partitions=3<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">我们知道segment文件默认会被保留7天的时间，超时的话就</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">会被清理，那么清理这件事情就需要有一些线程来做。这里就是</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">用来设置恢复和清理data下数据的线程数量</span><br>num.recovery.threads.per.data.dir=1<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">副本因子</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">实际使用过程中很多用户都会将offsets.topic.replication.factor设置成大于1的数以增加可靠性，这是推荐的做法</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">存储的消费者客户端offsets偏移量</span><br>offsets.topic.replication.factor=3<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">副本因子事物状态日志数量，存储事务明细</span><br>transaction.state.log.replication.factor=3<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">事物状态日志最小数量</span><br>transaction.state.log.min.isr=3<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">segment文件保留的最长时间，默认保留7天（168小时），</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">超时将被删除，也就是说7天之前的数据将被清理掉。</span><br>log.retention.hours=168<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">日志文件中每个segment的大小，默认为1G</span><br>log.segment.bytes=1073741824<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">上面的参数设置了每一个segment文件的大小是1G，那么</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">就需要有一个东西去定期检查segment文件有没有达到1G，</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">多长时间去检查一次，就需要设置一个周期性检查文件大小</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">的时间（单位是毫秒）。</span><br>log.retention.check.interval.ms=300000<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">日志清理是否打开</span><br>log.cleaner.enable=true<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">broker需要使用zookeeper保存meta数据</span><br>zookeeper.connect=127.0.0.1:2181,127.0.0.1:2182,127.0.0.1:2183<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">zookeeper链接超时时间</span><br>zookeeper.connection.timeout.ms=18000<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">上面我们说过接收线程会将接收到的消息放到内存中，然后再从内存</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">写到磁盘上，那么什么时候将消息从内存中写入磁盘，就有一个</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">时间限制（时间阈值）和一个数量限制（数量阈值），这里设置的是</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">数量阈值，下一个参数设置的则是时间阈值。</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">partion buffer中，消息的条数达到阈值，将触发flush到磁盘。</span><br>log.flush.interval.messages=10000<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">消息buffer的时间，达到阈值，将触发将消息从内存flush到磁盘，</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">单位是毫秒。</span><br>log.flush.interval.ms=3000<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">删除topic需要server.properties中设置delete.topic.enable=<span class="hljs-literal">true</span>否则只是标记删除</span><br>delete.topic.enable=true<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">防止成员加入请求后本应当即开启的rebalance</span><br>group.initial.rebalance.delay.ms=0<br></code></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">broker的全局唯一编号，不能重复</span><br>broker.id=2<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">提供给客户端响应的地址和端口</span><br>listeners = PLAINTEXT://0.0.0.0:9092<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">提供给客户端响应的地址和端口（允许外网访问）</span><br>advertised.listeners=PLAINTEXT://127.0.0.1:9092<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">处理网络请求的线程数量，也就是接收消息的线程数。</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">接收线程会将接收到的消息放到内存中，然后再从内存中写入磁盘。</span><br>num.network.threads=3<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">消息从内存中写入磁盘是时候使用的线程数量。</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">用来处理磁盘IO的线程数量</span><br>num.io.threads=8<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">发送套接字的缓冲区大小</span><br>socket.send.buffer.bytes=102400<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">接受套接字的缓冲区大小</span><br>socket.receive.buffer.bytes=102400<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">请求套接字的缓冲区大小</span><br>socket.request.max.bytes=104857600<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">kafka运行日志存放的路径</span><br>log.dirs=/data/kafka/k2/kafka_2.12-3.2.0/data<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">topic在当前broker上的分片个数</span><br>num.partitions=3<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">我们知道segment文件默认会被保留7天的时间，超时的话就</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">会被清理，那么清理这件事情就需要有一些线程来做。这里就是</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">用来设置恢复和清理data下数据的线程数量</span><br>num.recovery.threads.per.data.dir=1<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">副本因子</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">实际使用过程中很多用户都会将offsets.topic.replication.factor设置成大于1的数以增加可靠性，这是推荐的做法</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">存储的消费者客户端offsets偏移量</span><br>offsets.topic.replication.factor=3<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">副本因子事物状态日志数量，存储事务明细</span><br>transaction.state.log.replication.factor=3<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">事物状态日志最小数量</span><br>transaction.state.log.min.isr=3<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">segment文件保留的最长时间，默认保留7天（168小时），</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">超时将被删除，也就是说7天之前的数据将被清理掉。</span><br>log.retention.hours=168<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">日志文件中每个segment的大小，默认为1G</span><br>log.segment.bytes=1073741824<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">上面的参数设置了每一个segment文件的大小是1G，那么</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">就需要有一个东西去定期检查segment文件有没有达到1G，</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">多长时间去检查一次，就需要设置一个周期性检查文件大小</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">的时间（单位是毫秒）。</span><br>log.retention.check.interval.ms=300000<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">日志清理是否打开</span><br>log.cleaner.enable=true<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">broker需要使用zookeeper保存meta数据</span><br>zookeeper.connect=127.0.0.1:2181,127.0.0.1:2182,127.0.0.1:2183<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">zookeeper链接超时时间</span><br>zookeeper.connection.timeout.ms=18000<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">上面我们说过接收线程会将接收到的消息放到内存中，然后再从内存</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">写到磁盘上，那么什么时候将消息从内存中写入磁盘，就有一个</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">时间限制（时间阈值）和一个数量限制（数量阈值），这里设置的是</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">数量阈值，下一个参数设置的则是时间阈值。</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">partion buffer中，消息的条数达到阈值，将触发flush到磁盘。</span><br>log.flush.interval.messages=10000<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">消息buffer的时间，达到阈值，将触发将消息从内存flush到磁盘，</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">单位是毫秒。</span><br>log.flush.interval.ms=3000<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">删除topic需要server.properties中设置delete.topic.enable=<span class="hljs-literal">true</span>否则只是标记删除</span><br>delete.topic.enable=true<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">防止成员加入请求后本应当即开启的rebalance</span><br>group.initial.rebalance.delay.ms=0<br></code></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">broker的全局唯一编号，不能重复</span><br>broker.id=3<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">提供给客户端响应的地址和端口</span><br>listeners = PLAINTEXT://0.0.0.0:9093<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">提供给客户端响应的地址和端口（允许外网访问）</span><br>advertised.listeners=PLAINTEXT://127.0.0.1:9093<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">处理网络请求的线程数量，也就是接收消息的线程数。</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">接收线程会将接收到的消息放到内存中，然后再从内存中写入磁盘。</span><br>num.network.threads=3<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">消息从内存中写入磁盘是时候使用的线程数量。</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">用来处理磁盘IO的线程数量</span><br>num.io.threads=8<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">发送套接字的缓冲区大小</span><br>socket.send.buffer.bytes=102400<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">接受套接字的缓冲区大小</span><br>socket.receive.buffer.bytes=102400<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">请求套接字的缓冲区大小</span><br>socket.request.max.bytes=104857600<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">kafka运行日志存放的路径</span><br>log.dirs=/data/kafka/k3/kafka_2.12-3.2.0/data<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">topic在当前broker上的分片个数</span><br>num.partitions=3<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">我们知道segment文件默认会被保留7天的时间，超时的话就</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">会被清理，那么清理这件事情就需要有一些线程来做。这里就是</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">用来设置恢复和清理data下数据的线程数量</span><br>num.recovery.threads.per.data.dir=1<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">副本因子</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">实际使用过程中很多用户都会将offsets.topic.replication.factor设置成大于1的数以增加可靠性，这是推荐的做法</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">存储的消费者客户端offsets偏移量</span><br>offsets.topic.replication.factor=3<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">副本因子事物状态日志数量，存储事务明细</span><br>transaction.state.log.replication.factor=3<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">事物状态日志最小数量</span><br>transaction.state.log.min.isr=3<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">segment文件保留的最长时间，默认保留7天（168小时），</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">超时将被删除，也就是说7天之前的数据将被清理掉。</span><br>log.retention.hours=168<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">日志文件中每个segment的大小，默认为1G</span><br>log.segment.bytes=1073741824<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">上面的参数设置了每一个segment文件的大小是1G，那么</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">就需要有一个东西去定期检查segment文件有没有达到1G，</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">多长时间去检查一次，就需要设置一个周期性检查文件大小</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">的时间（单位是毫秒）。</span><br>log.retention.check.interval.ms=300000<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">日志清理是否打开</span><br>log.cleaner.enable=true<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">broker需要使用zookeeper保存meta数据</span><br>zookeeper.connect=127.0.0.1:2181,127.0.0.1:2182,127.0.0.1:2183<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">zookeeper链接超时时间</span><br>zookeeper.connection.timeout.ms=18000<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">上面我们说过接收线程会将接收到的消息放到内存中，然后再从内存</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">写到磁盘上，那么什么时候将消息从内存中写入磁盘，就有一个</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">时间限制（时间阈值）和一个数量限制（数量阈值），这里设置的是</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">数量阈值，下一个参数设置的则是时间阈值。</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">partion buffer中，消息的条数达到阈值，将触发flush到磁盘。</span><br>log.flush.interval.messages=10000<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">消息buffer的时间，达到阈值，将触发将消息从内存flush到磁盘，</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">单位是毫秒。</span><br>log.flush.interval.ms=3000<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">删除topic需要server.properties中设置delete.topic.enable=<span class="hljs-literal">true</span>否则只是标记删除</span><br>delete.topic.enable=true<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">防止成员加入请求后本应当即开启的rebalance</span><br>group.initial.rebalance.delay.ms=0<br></code></pre></td></tr></table></figure><p class="note note-primary">编写启动脚本启动</p><ul><li>/data/kafka/k1/kafka_2.12-3.2.0</li><li>/data/kafka/k2/kafka_2.12-3.2.0</li><li>/data/kafka/k3/kafka_2.12-3.2.0</li></ul><p>在上面三个目录分别创建 <a target="_blank" rel="noopener" href="http://kafka-start.sh">kafka-start.sh</a> 启动脚本</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash">!/bin/bash</span><br>nohup ./bin/kafka-server-start.sh config/server.properties &gt; logs/kafka/kafka.log 2&gt;1 &amp;<br></code></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">创建日志目录</span><br>mkdir /data/kafka/k1/kafka_2.12-3.2.0/logs/kafka<br>mkdir /data/kafka/k2/kafka_2.12-3.2.0/logs/kafka<br>mkdir /data/kafka/k3/kafka_2.12-3.2.0/logs/kafka<br></code></pre></td></tr></table></figure><p>启动</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">. kafka-start.sh<br></code></pre></td></tr></table></figure><p class="note note-primary">安装kafka-manager</p><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_46903045/article/details/115181392">kafka-manager 安装</a></p><h2 id="kafka命令"><a class="markdownIt-Anchor" href="#kafka命令"></a> Kafka命令</h2><h3 id="topic-2"><a class="markdownIt-Anchor" href="#topic-2"></a> Topic</h3><table><thead><tr><th>参数</th><th>描述</th></tr></thead><tbody><tr><td><code>--bootstrap-server &lt;String: server toconnect to&gt;</code></td><td>连接的 Kafka Broker 主机名称和端口号</td></tr><tr><td><code>--topic &lt;String: topic&gt;</code></td><td>操作的 topic 名称</td></tr><tr><td><code>--create</code></td><td>创建主题</td></tr><tr><td><code>--delete</code></td><td>删除主题</td></tr><tr><td><code>--alter</code></td><td>修改主题</td></tr><tr><td><code>--list</code></td><td>查看所有主题</td></tr><tr><td><code>--describe</code></td><td>查看主题详细描述</td></tr><tr><td><code>--partitions &lt;Integer: # of partitions&gt;</code></td><td>设置分区数</td></tr><tr><td><code>--replication-factor&lt;Integer: replication factor&gt;</code></td><td>设置分区副本</td></tr><tr><td><code>--config &lt;String: name=value&gt;</code></td><td>更新系统默认的配置</td></tr></tbody></table><p class="note note-primary">列出所有topic</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">sh kafka-topics.sh --list --bootstrap-server localhost:9092<br></code></pre></td></tr></table></figure><p class="note note-primary">创建topic</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">sh kafka-topics.sh --bootstrap-server localhost:9092 --create --topic test --partitions 3 --replication-factor 3<br>参数 --topic 指定 Topic 名，--partitions 指定分区数，--replication-factor 指定备份数<br></code></pre></td></tr></table></figure><p class="note note-primary">查看topic</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">sh kafka-topics.sh --describe --bootstrap-server localhost:9092 --topic test<br>可以查看每个broker上的topic分区副本信息<br></code></pre></td></tr></table></figure><p class="note note-primary">增加topic的partition数量</p><p>Partition数量只能增加不能减少</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">sh kafka-topics.sh --bootstrap-server localhost:9092 --alter --topic test --partitions 2<br></code></pre></td></tr></table></figure><p class="note note-primary">删除topic</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">sh kafka-topics.sh --bootstrap-server localhost:9092 --topic test3 --delete<br></code></pre></td></tr></table></figure><h3 id="生产消息"><a class="markdownIt-Anchor" href="#生产消息"></a> 生产消息</h3><table><thead><tr><th>参数</th><th>描述</th></tr></thead><tbody><tr><td><code>--bootstrap-server &lt;String: server toconnect to&gt;</code></td><td>连接的 Kafka Broker 主机名称和端口号</td></tr><tr><td><code>--topic &lt;String: topic&gt;</code></td><td>操作的 topic 名称</td></tr></tbody></table><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">sh kafka-console-producer.sh --bootstrap-server localhost:9092 --topic test<br>此时控制台会出现光标，这时候就可以发送消息<br></code></pre></td></tr></table></figure><h3 id="消费消息"><a class="markdownIt-Anchor" href="#消费消息"></a> 消费消息</h3><table><thead><tr><th>参数</th><th>描述</th></tr></thead><tbody><tr><td><code>--bootstrap-server &lt;String: server toconnect to&gt;</code></td><td>连接的 Kafka Broker 主机名称和端口号</td></tr><tr><td><code>--topic &lt;String: topic&gt;</code></td><td>操作的 topic 名称</td></tr><tr><td><code>--from-beginning</code></td><td>从头开始消费</td></tr><tr><td><code>--group &lt;String: consumer group id&gt;</code></td><td>指定消费者组名称</td></tr></tbody></table><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">sh kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic first<br></code></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">sh kafka-console-consumer.sh --bootstrap-server localhost:9092 --from-beginning --topic first<br></code></pre></td></tr></table></figure><h2 id="kafka生产者"><a class="markdownIt-Anchor" href="#kafka生产者"></a> Kafka生产者</h2><h3 id="消息发送过程"><a class="markdownIt-Anchor" href="#消息发送过程"></a> 消息发送过程</h3><p><img src="/2022/02/24/Kafka/%E6%B6%88%E6%81%AF%E5%8F%91%E9%80%81%E6%B5%81%E7%A8%8B.png" srcset="/img/loading.gif" lazyload alt></p><ol><li><strong><code>创建Producer实例</code></strong>：当我们创建一个Kafka Producer实例，后台会启动几个线程，其中最重要的是 <code>main</code> 和 <code>Sender</code> 线程。</li><li><strong><code>序列化</code></strong>：使用配置的序列化器对消息的 key 和 value 进行序列化。</li><li><strong><code>分区</code></strong>：消息在被添加到 <code>RecordAccumulator</code> 之前需要确定目标分区。这是通过Partitioner完成的，它可以基于消息键、消息内容或自定义策略来决定消息应该进入哪个分区。</li><li><strong><code>添加消息到缓冲区</code></strong>：在应用程序的 <code>main</code> 线程中，当调用Producer的 <code>send</code> 方法时，它实际上并不直接发送消息。<span class="green-line">消息首先会被添加到一个批次中（称为Batch），然后存储在一个名为 <code>RecordAccumulator</code> 的内部结构中。这个 <code>RecordAccumulator</code> 实际上是由多个双端队列组成的，每个队列对应一个Kafka的分区</span>。</li><li><strong><code>消息批处理</code></strong>：Kafka Producer将消息分组成批次，这样可以一次性发送多个消息，从而提高吞吐量。每个批次都有一个大小上限（由配置参数 <code>batch.size</code> 确定），当批次满了，或达到一定的延迟时（由 <code>linger.ms</code> 确定），Sender线程就会发送这些消息。</li><li><strong><code>Sender线程</code></strong>：这是一个后台线程，不断地从 <code>RecordAccumulator</code> 中拉取待发送的批次，然后将它们发送到相应的Kafka Broker。发送完成后，它也会处理来自Broker的响应，例如确认消息是否已经成功写入。</li><li><strong><code>消息确认</code></strong>：Kafka支持几种确认模式，这是由 <code>acks</code> 参数确定的。<ul><li><p><strong><code>acks=0</code></strong>：不进行ack确认。</p></li><li><p><strong><code>acks=1</code></strong>：只要Leader收到消息，Producer就认为消息已成功发送。</p></li><li><p><strong><code>acks=all</code> 或 <code>acks=-1</code></strong>：Leader + ISR列表里面的同步副本收到消息，Producer才会认为消息发送成功。</p></li></ul></li><li><strong><code>重试策略</code></strong>：如果消息发送失败，Kafka Producer会尝试重新发送。重试的次数和间隔是可以配置的。</li><li><strong><code>关闭Producer</code></strong>：当你完成所有的消息发送并调用 <code>close</code> 方法时，它会确保所有待发送的消息都被发送出去，并且所有已发送的消息都得到了确认。</li></ol><p class="note note-primary">Send线程拉取参数</p><ul><li><code>batch.size</code>：只有数据积累到batch.size之后，sender才会发送数据。默认16k</li><li><code>linger.ms</code>：如果数据迟迟未达到batch.size，sender等待linger.ms设置的时间到了之后就会发送数据。单位ms，默认值是0ms，表示没有延迟</li></ul><h3 id="生产者重要参数"><a class="markdownIt-Anchor" href="#生产者重要参数"></a> 生产者重要参数</h3><table><thead><tr><th>参数名称</th><th>描述</th></tr></thead><tbody><tr><td><code>bootstrap.servers</code></td><td>生产者连接集群所需的 broker 地址清单，多个逗号隔开。注意这里并非需要所有的 broker 地址，因为生产者从给定的 broker 里查找到其他 broker 信息</td></tr><tr><td><code>key.serializer</code> 和 <code>value.serializer</code></td><td>指定发送消息的 key 和 value 的序列化类型。一定要写全类名</td></tr><tr><td><code>buffer.memory</code></td><td>RecordAccumulator 缓冲区总大小，默认 32m</td></tr><tr><td><strong><code>batch.size</code></strong></td><td>缓冲区一批数据最大值，默认 16k。适当增加该值，可以提高吞吐量，但是如果该值设置太大，会导致数据传输延迟增加</td></tr><tr><td><strong><code>linger.ms</code></strong></td><td>如果数据迟迟未达到 batch.size，sender 等待 linger.time 之后就会发送数据。单位 ms，默认值是 0ms，表示没有延迟。生产环境建议该值大小为 5-100ms 之间</td></tr><tr><td><code>acks</code></td><td>0：生产者发送过来的数据，不需要等数据落盘应答。<br>1：生产者发送过来的数据，Leader 收到数据后应答。<br>-1（all）：生产者发送过来的数据，Leader+和 isr 队列 里面的所有节点收齐数据后应答。<br>默认值是-1，-1 和 all 是等价的。</td></tr><tr><td><code>max.in.flight.requests.per.connection</code></td><td>允许最多没有返回 ack 的次数，默认为 5，开启幂等性要保证该值是 1-5 的数字</td></tr><tr><td><code>retries</code></td><td>当消息发送出现错误的时候，系统会重发消息。retries 表示重试次数。默认是 int 最大值，2147483647。 如果设置了重试，还想保证消息的有序性，需要设置 MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION=1 否则在重试此失败消息的时候，其他的消息可能发送成功了</td></tr><tr><td><code>retry.backoff.ms</code></td><td>两次重试之间的时间间隔，默认是 100ms</td></tr><tr><td><code>enable.idempotence</code></td><td>是否开启幂等性，默认 true，开启幂等性</td></tr><tr><td><code>compression.type</code></td><td>生产者发送的所有数据的压缩方式。默认是 none，也就是不压缩。 支持压缩类型：none、gzip、snappy、lz4 和 zstd</td></tr></tbody></table><h3 id="异步发送"><a class="markdownIt-Anchor" href="#异步发送"></a> 异步发送</h3><p><img src="/2022/02/24/Kafka/%E5%BC%82%E6%AD%A5%E5%8F%91%E9%80%81.png" srcset="/img/loading.gif" lazyload alt></p><ol><li><strong><code>异步发送概述</code></strong>：Kafka提供异步方式发送消息，<span class="green-line">主要特点是消息发送操作不会因等待Broker的确认而阻塞。</span></li><li><strong><code>消息异步提交到 RecordAccumulator</code></strong> ：在异步模式下，Main线程调用 send 方法将消息提交消息到 <code>RecordAccumulator</code> 时是非阻塞的。这意味着Main线程可以迅速地继续其它任务，而不需等待消息实际被发送到Broker。</li><li><strong><code>Sender线程</code></strong>：Sender线程在后台工作，负责从 <code>RecordAccumulator</code> 取出消息并发送至Kafka Broker。</li><li><strong><code>非阻塞的优势</code></strong>：这种异步方式的主要优势是它提高了生产者的吞吐量和效率，因为Main线程不会因消息发送而被频繁阻塞。</li></ol><p>项目添加依赖</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">dependency</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">groupId</span>&gt;</span>org.apache.kafka<span class="hljs-tag">&lt;/<span class="hljs-name">groupId</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">artifactId</span>&gt;</span>kafka-clients<span class="hljs-tag">&lt;/<span class="hljs-name">artifactId</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">version</span>&gt;</span>3.2.0<span class="hljs-tag">&lt;/<span class="hljs-name">version</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">dependency</span>&gt;</span><br></code></pre></td></tr></table></figure><p class="note note-primary">kafka-client</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-meta">@Slf4j</span><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">ProductTest</span> &#123;<br><br>    <span class="hljs-keyword">private</span> KafkaProducer&lt;String, String&gt; kafkaProducer;<br><br>    <span class="hljs-meta">@BeforeEach</span><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">init</span><span class="hljs-params">()</span> &#123;<br>        <span class="hljs-comment">// 1. 创建 kafka 生产者的配置对象</span><br>        <span class="hljs-type">Properties</span> <span class="hljs-variable">properties</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">Properties</span>();<br>        <span class="hljs-comment">// 2. 给 kafka 配置对象添加配置信息：bootstrap.servers</span><br>        properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="hljs-string">&quot;127.0.0.1:9092&quot;</span>);<br>        <span class="hljs-comment">// key,value 序列化（必须）：key.serializer，value.serializer</span><br>        properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG,<br>                <span class="hljs-string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);<br><br>        properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG,<br>                <span class="hljs-string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);<br><br>        <span class="hljs-comment">// 3. 创建 kafka 生产者对象</span><br>        kafkaProducer = <span class="hljs-keyword">new</span> <span class="hljs-title class_">KafkaProducer</span>&lt;&gt;(properties);<br>    &#125;<br><br>    <span class="hljs-comment">/**</span><br><span class="hljs-comment">     * 异步发送</span><br><span class="hljs-comment">     */</span><br>    <span class="hljs-meta">@Test</span><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">send</span><span class="hljs-params">()</span> &#123;<br>        <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> <span class="hljs-variable">i</span> <span class="hljs-operator">=</span> <span class="hljs-number">0</span>; i &lt; <span class="hljs-number">5</span>; i++) &#123;<br>            kafkaProducer.send(<span class="hljs-keyword">new</span> <span class="hljs-title class_">ProducerRecord</span>&lt;&gt;(<span class="hljs-string">&quot;first&quot;</span>, <span class="hljs-string">&quot;hello &quot;</span> + i));<br>        &#125;<br><br>        kafkaProducer.close();<br>    &#125;<br>&#125;<br><br></code></pre></td></tr></table></figure><p>带回调函数的异步发送</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-comment">/**</span><br><span class="hljs-comment"> * 带回调函数的异步发送</span><br><span class="hljs-comment"> */</span><br><span class="hljs-meta">@Test</span><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">callBack</span><span class="hljs-params">()</span> &#123;<br>    <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> <span class="hljs-variable">i</span> <span class="hljs-operator">=</span> <span class="hljs-number">0</span>; i &lt; <span class="hljs-number">5</span>; i++) &#123;<br>        kafkaProducer.send(<span class="hljs-keyword">new</span> <span class="hljs-title class_">ProducerRecord</span>&lt;&gt;(<span class="hljs-string">&quot;first&quot;</span>, <span class="hljs-string">&quot;hello &quot;</span> + i), <span class="hljs-keyword">new</span> <span class="hljs-title class_">Callback</span>() &#123;<br>            <span class="hljs-meta">@Override</span><br>            <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">onCompletion</span><span class="hljs-params">(RecordMetadata recordMetadata, Exception e)</span> &#123;<br>                <span class="hljs-keyword">if</span> (Objects.isNull(e)) &#123;<br>                    log.info(<span class="hljs-string">&quot;主题：&#123;&#125;,分区：&#123;&#125;&quot;</span>, recordMetadata.topic(), recordMetadata.partition());<br>                &#125;<br>            &#125;<br>        &#125;);<br>    &#125;<br><br>    kafkaProducer.close();<br>&#125;<br></code></pre></td></tr></table></figure><p class="note note-primary">Spring Boot</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">dependency</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">groupId</span>&gt;</span>org.springframework.kafka<span class="hljs-tag">&lt;/<span class="hljs-name">groupId</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">artifactId</span>&gt;</span>spring-kafka<span class="hljs-tag">&lt;/<span class="hljs-name">artifactId</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">dependency</span>&gt;</span><br></code></pre></td></tr></table></figure><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">spring:</span><br>  <span class="hljs-attr">kafka:</span><br>    <span class="hljs-attr">bootstrap-servers:</span> <span class="hljs-string">localhost:9091</span>           <span class="hljs-comment"># 连接kafka的地址，多个地址用逗号分隔</span><br>    <span class="hljs-attr">producer:</span><br>      <span class="hljs-attr">batch-size:</span> <span class="hljs-number">16384</span>                                                         <span class="hljs-comment"># batch.size 批次大小，默认16k</span><br>      <span class="hljs-attr">buffer-memory:</span> <span class="hljs-number">33554432</span>                                                   <span class="hljs-comment"># RecordAccumulator 大小,默认32M</span><br>      <span class="hljs-attr">key-serializer:</span> <span class="hljs-string">org.apache.kafka.common.serialization.StringSerializer</span>    <span class="hljs-comment"># 关键字的序列化类</span><br>      <span class="hljs-attr">value-serializer:</span> <span class="hljs-string">org.apache.kafka.common.serialization.StringSerializer</span>  <span class="hljs-comment"># 值的序列化类</span><br></code></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-meta">@Slf4j</span><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">ProductTest</span> &#123;<br><br>    <span class="hljs-keyword">private</span> KafkaProducer&lt;String, String&gt; kafkaProducer;<br><br>    <span class="hljs-meta">@BeforeEach</span><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">init</span><span class="hljs-params">()</span> &#123;<br>        <span class="hljs-comment">// 1. 创建 kafka 生产者的配置对象</span><br>        <span class="hljs-type">Properties</span> <span class="hljs-variable">properties</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">Properties</span>();<br>        <span class="hljs-comment">// 2. 给 kafka 配置对象添加配置信息：bootstrap.servers</span><br>        properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="hljs-string">&quot;121.37.23.172:9092&quot;</span>);<br>        <span class="hljs-comment">// key,value 序列化（必须）：key.serializer，value.serializer</span><br>        properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG,<br>                <span class="hljs-string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);<br><br>        properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG,<br>                <span class="hljs-string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);<br><br>        <span class="hljs-comment">// 3. 创建 kafka 生产者对象</span><br>        kafkaProducer = <span class="hljs-keyword">new</span> <span class="hljs-title class_">KafkaProducer</span>&lt;&gt;(properties);<br>    &#125;<br><br>    <span class="hljs-comment">/**</span><br><span class="hljs-comment">     * 异步发送</span><br><span class="hljs-comment">     */</span><br>    <span class="hljs-meta">@Test</span><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">send</span><span class="hljs-params">()</span> &#123;<br>        <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> <span class="hljs-variable">i</span> <span class="hljs-operator">=</span> <span class="hljs-number">0</span>; i &lt; <span class="hljs-number">5</span>; i++) &#123;<br>            kafkaProducer.send(<span class="hljs-keyword">new</span> <span class="hljs-title class_">ProducerRecord</span>&lt;&gt;(<span class="hljs-string">&quot;first&quot;</span>, i + <span class="hljs-string">&quot;&quot;</span>));<br>        &#125;<br><br>        kafkaProducer.close();<br>    &#125;<br><br><br>    <span class="hljs-comment">/**</span><br><span class="hljs-comment">     * 带回调函数的异步发送</span><br><span class="hljs-comment">     */</span><br>    <span class="hljs-meta">@Test</span><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">callBack</span><span class="hljs-params">()</span> &#123;<br>        <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> <span class="hljs-variable">i</span> <span class="hljs-operator">=</span> <span class="hljs-number">0</span>; i &lt; <span class="hljs-number">500</span>; i++) &#123;<br>            kafkaProducer.send(<span class="hljs-keyword">new</span> <span class="hljs-title class_">ProducerRecord</span>&lt;&gt;(<span class="hljs-string">&quot;first&quot;</span>,  i+<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;hello &quot;</span> + i), <span class="hljs-keyword">new</span> <span class="hljs-title class_">Callback</span>() &#123;<br>                <span class="hljs-meta">@Override</span><br>                <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">onCompletion</span><span class="hljs-params">(RecordMetadata recordMetadata, Exception e)</span> &#123;<br>                    <span class="hljs-keyword">if</span> (Objects.isNull(e)) &#123;<br>                        log.info(<span class="hljs-string">&quot;主题：&#123;&#125;,分区：&#123;&#125;&quot;</span>, recordMetadata.topic(), recordMetadata.partition());<br>                    &#125;<br>                &#125;<br>            &#125;);<br>        &#125;<br><br>        kafkaProducer.close();<br>    &#125;<br><br><br>    <span class="hljs-comment">/**</span><br><span class="hljs-comment">     * 同步发送</span><br><span class="hljs-comment">     */</span><br>    <span class="hljs-meta">@Test</span><br>    <span class="hljs-meta">@SneakyThrows</span><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">syncSend</span><span class="hljs-params">()</span> &#123;<br>        <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> <span class="hljs-variable">i</span> <span class="hljs-operator">=</span> <span class="hljs-number">0</span>; i &lt; <span class="hljs-number">5</span>; i++) &#123;<br>            kafkaProducer.send(<span class="hljs-keyword">new</span> <span class="hljs-title class_">ProducerRecord</span>&lt;&gt;(<span class="hljs-string">&quot;first&quot;</span>, <span class="hljs-string">&quot;hello &quot;</span> + i)).get();<br>        &#125;<br><br>        kafkaProducer.close();<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="同步发送"><a class="markdownIt-Anchor" href="#同步发送"></a> 同步发送</h3><ol><li><strong><code>同步发送概述</code></strong>：在Kafka的同步发送模式中，Main线程会在每次发送消息后阻塞，直到从Broker收到确认。</li><li><strong><code>消息提交到 RecordAccumulator</code></strong>：Main线程首先将消息提交到 <code>RecordAccumulator</code>，这是一个内部缓冲区，用于存储待发送的消息。</li><li><strong><code>Sender线程</code></strong>：Sender线程负责从 <code>RecordAccumulator</code> 中提取消息并发送到Kafka Broker。</li><li><strong><code>阻塞等待确认</code></strong>：不像异步发送，同步发送会导致Main线程在消息被Sender线程发送并收到Broker的ACK之前一直阻塞。只有在收到ACK后，Main线程才会进行下一次发送或其他操作。</li></ol><p>只需在异步发送的基础上，再调用一下 get()方法即可</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-meta">@Test</span><br><span class="hljs-meta">@SneakyThrows</span><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">syncSend</span><span class="hljs-params">()</span> &#123;<br>    <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> <span class="hljs-variable">i</span> <span class="hljs-operator">=</span> <span class="hljs-number">0</span>; i &lt; <span class="hljs-number">5</span>; i++) &#123;<br>        kafkaProducer.send(<span class="hljs-keyword">new</span> <span class="hljs-title class_">ProducerRecord</span>&lt;&gt;(<span class="hljs-string">&quot;first&quot;</span>, <span class="hljs-string">&quot;hello &quot;</span> + i)).get();<br>    &#125;<br><br>    kafkaProducer.close();<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="自定义拦截器"><a class="markdownIt-Anchor" href="#自定义拦截器"></a> 自定义拦截器</h3><ol><li><strong><code>拦截器目的</code></strong>：Kafka的Producer拦截器提供了一种机制，允许用户在关键的消息发送阶段介入，以实现特定的自定义需求。</li><li><strong><code>主要功能</code></strong>：<ul><li><strong>消息发送前处理</strong>：在消息实际被发送到Broker之前，拦截器的 <code>onSend</code> 方法会被调用。</li><li><strong>ACK接收后处理</strong>：当Producer从Broker接收到一个ACK或消息发送失败时，<code>onAcknowledgement</code> 方法会被触发。</li></ul></li><li><strong><code>拦截链</code></strong>：Producer支持多个拦截器的配置。这些拦截器会按照配置的顺序依次处理消息，形成所谓的“拦截链”。</li><li><strong><code>如何实现</code></strong>：<ul><li>要创建自己的拦截器，用户需实现 <code>ProducerInterceptor</code> 接口。</li><li>此接口中有两个主要方法，分别是 <code>onSend</code>（在消息发送前调用）和 <code>onAcknowledgement</code>（在从Broker接收到响应前调用）。</li></ul></li><li><strong><code>如何配置</code></strong>：通过Producer的配置属性 <code>interceptor.classes</code>，用户可以指定一个或多个拦截器的完整类名。Producer在初始化时会按照配置的顺序加载并激活这些拦截器。</li></ol><p class="note note-primary">拦截器接口</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-keyword">interface</span> <span class="hljs-title class_">ProducerInterceptor</span>&lt;K, V&gt; <span class="hljs-keyword">extends</span> <span class="hljs-title class_">Configurable</span> &#123;<br> <br> <span class="hljs-comment">/**</span><br><span class="hljs-comment">     * 获取配置信息或初始化数据时调用</span><br><span class="hljs-comment">     */</span><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">configure</span><span class="hljs-params">(Map&lt;String, ?&gt; configs)</span><br><br>    <span class="hljs-comment">/**</span><br><span class="hljs-comment">     * 该方法封装进KafkaProducer.send方法中，即它运行在用户主线程中。Producer确保在消息被序列化以及计算分区前调用该方法。用户可以在该方法中对消息做任何操作，但最好保证不要修改消息所属的topic和分区，否则会影响目标分区计算</span><br><span class="hljs-comment">     */</span><br>    <span class="hljs-keyword">public</span> ProducerRecord&lt;K, V&gt; <span class="hljs-title function_">onSend</span><span class="hljs-params">(ProducerRecord&lt;K, V&gt; record)</span>;<br>    <br>    <span class="hljs-comment">/**</span><br><span class="hljs-comment">     * 会在消息从RecordAccumulator成功发送到kafka broker之后，或者再发送过程中失败调用，并且通常都是在producer回调逻辑触发之前。它运行在producer的IO线程，因此不要放入很重的逻辑，否则会影响消息发送效率</span><br><span class="hljs-comment">     */</span><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">onAcknowledgement</span><span class="hljs-params">(RecordMetadata metadata, Exception exception)</span>;<br>    <br>    <span class="hljs-comment">/**</span><br><span class="hljs-comment">     * 关闭interceptor,主要用于执行一些资源释放</span><br><span class="hljs-comment">     * interceptor 可能被运行在多个线程中，因此需要注意线程安全问题。如果指定多个interceptor,则producer将按照指定顺序调用它们，并仅仅是补货每个interceptor可能抛出的一场记录到错误日志中而非向上传递</span><br><span class="hljs-comment">     */</span><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">close</span><span class="hljs-params">()</span>;<br>&#125;<br></code></pre></td></tr></table></figure><p class="note note-primary">定义拦截器</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><code class="hljs java">**<br> * 消息添加时间戳<br> */<br><span class="hljs-meta">@Slf4j</span><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">TimeInterceptor</span> <span class="hljs-keyword">implements</span> <span class="hljs-title class_">ProducerInterceptor</span>&lt;String, String&gt; &#123;<br><br>    <span class="hljs-meta">@Override</span><br>    <span class="hljs-keyword">public</span> ProducerRecord&lt;String, String&gt; <span class="hljs-title function_">onSend</span><span class="hljs-params">(ProducerRecord&lt;String, String&gt; record)</span> &#123;<br>        <span class="hljs-type">String</span> <span class="hljs-variable">value</span> <span class="hljs-operator">=</span> record.value();<br>        value = String.format(<span class="hljs-string">&quot;%s_%s&quot;</span>, value, System.currentTimeMillis());<br>        <span class="hljs-keyword">return</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">ProducerRecord</span>&lt;String, String&gt;(record.topic(), value);<br>    &#125;<br><br>    <span class="hljs-meta">@Override</span><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">onAcknowledgement</span><span class="hljs-params">(RecordMetadata metadata, Exception exception)</span> &#123;<br>        log.info(<span class="hljs-string">&quot;metadata: &#123;&#125;&quot;</span>, JSON.toJSONString(metadata));<br>        log.info(<span class="hljs-string">&quot;exception: &#123;&#125;&quot;</span>, JSON.toJSONString(exception));<br>    &#125;<br><br>    <span class="hljs-meta">@Override</span><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">close</span><span class="hljs-params">()</span> &#123;<br><br>    &#125;<br><br>    <span class="hljs-meta">@Override</span><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">configure</span><span class="hljs-params">(Map&lt;String, ?&gt; configs)</span> &#123;<br>        log.info(JSON.toJSONString(configs));<br>    &#125;<br>&#125;<br><br><span class="hljs-comment">/**</span><br><span class="hljs-comment"> * 计数拦截器</span><br><span class="hljs-comment"> */</span><br><span class="hljs-meta">@Slf4j</span><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">CountInterceptor</span> <span class="hljs-keyword">implements</span> <span class="hljs-title class_">ProducerInterceptor</span>&lt;String, String&gt; &#123;<br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">final</span> <span class="hljs-type">AtomicInteger</span> <span class="hljs-variable">sendCount</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">AtomicInteger</span>(<span class="hljs-number">0</span>);<br><br>    <span class="hljs-meta">@Override</span><br>    <span class="hljs-keyword">public</span> ProducerRecord&lt;String, String&gt; <span class="hljs-title function_">onSend</span><span class="hljs-params">(ProducerRecord&lt;String, String&gt; record)</span> &#123;<br>        <span class="hljs-keyword">return</span> record;<br>    &#125;<br><br>    <span class="hljs-meta">@Override</span><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">onAcknowledgement</span><span class="hljs-params">(RecordMetadata metadata, Exception exception)</span> &#123;<br>        <span class="hljs-keyword">if</span> (Objects.isNull(exception)) &#123;<br>            log.info(<span class="hljs-string">&quot;发送成功：&#123;&#125;&quot;</span>, sendCount.incrementAndGet());<br>        &#125;<br>    &#125;<br><br>    <span class="hljs-meta">@Override</span><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">close</span><span class="hljs-params">()</span> &#123;<br><br>    &#125;<br><br>    <span class="hljs-meta">@Override</span><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">configure</span><span class="hljs-params">(Map&lt;String, ?&gt; configs)</span> &#123;<br><br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><p class="note note-primary">SpringBoot方式集成</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">server:</span><br>  <span class="hljs-attr">port:</span> <span class="hljs-number">9000</span><br><br><span class="hljs-attr">spring:</span><br>  <span class="hljs-attr">kafka:</span><br>    <span class="hljs-attr">bootstrap-servers:</span> <span class="hljs-number">127.0</span><span class="hljs-number">.0</span><span class="hljs-number">.1</span><span class="hljs-string">:9092</span>                                     <span class="hljs-comment"># 连接kafka的地址，多个地址用逗号分隔</span><br>    <span class="hljs-attr">producer:</span><br>      <span class="hljs-attr">retries:</span> <span class="hljs-number">0</span>                                                                <span class="hljs-comment"># 若设置大于0的值，客户端会将发送失败的记录重新发送</span><br>      <span class="hljs-attr">batch-size:</span> <span class="hljs-number">16384</span>                                                         <span class="hljs-comment"># 当将多个记录被发送到同一个分区时， Producer 将尝试将记录组合到更少的请求中。这有助于提升客户端和服务器端的性能。这个配置控制一个批次的默认大小（以字节为单位）。16384是缺省的配置</span><br>      <span class="hljs-attr">buffer-memory:</span> <span class="hljs-number">33554432</span>                                                   <span class="hljs-comment"># #Producer 用来缓冲等待被发送到服务器的记录的总字节数，33554432是缺省配置</span><br>      <span class="hljs-attr">key-serializer:</span> <span class="hljs-string">org.apache.kafka.common.serialization.StringSerializer</span>    <span class="hljs-comment"># 关键字的序列化类</span><br>      <span class="hljs-attr">value-serializer:</span> <span class="hljs-string">org.apache.kafka.common.serialization.StringSerializer</span>  <span class="hljs-comment"># 值的序列化类</span><br>      <span class="hljs-attr">properties:</span><br>        <span class="hljs-attr">interceptor.classes:</span> <span class="hljs-string">com.wgf.interceptor.TimeInterceptor,com.wgf.interceptor.CountInterceptor</span>             <span class="hljs-comment"># 支持多个，逗号隔开</span><br><br></code></pre></td></tr></table></figure><p class="note note-primary">Kafka-client方式集成</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">InterceptorTest</span> &#123;<br>    <span class="hljs-keyword">private</span> KafkaProducer&lt;String, String&gt; kafkaProducer;<br><br>    <span class="hljs-meta">@BeforeEach</span><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">init</span><span class="hljs-params">()</span> &#123;<br>        <span class="hljs-comment">// 1. 创建 kafka 生产者的配置对象</span><br>        <span class="hljs-type">Properties</span> <span class="hljs-variable">properties</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">Properties</span>();<br>        <span class="hljs-comment">// 2. 给 kafka 配置对象添加配置信息：bootstrap.servers</span><br>        properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="hljs-string">&quot;127.0.0.1:9092&quot;</span>);<br>        <span class="hljs-comment">// key,value 序列化（必须）：key.serializer，value.serializer</span><br>        properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG,<br>                <span class="hljs-string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);<br><br>        properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG,<br>                <span class="hljs-string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);<br><br>        <span class="hljs-comment">// 添加拦截器</span><br>        List&lt;String&gt; interceptors = <span class="hljs-keyword">new</span> <span class="hljs-title class_">ArrayList</span>&lt;&gt;();<br>        interceptors.add(<span class="hljs-string">&quot;com.wgf.interceptor.CountInterceptor&quot;</span>);<br>        interceptors.add(<span class="hljs-string">&quot;com.wgf.interceptor.TimeInterceptor&quot;</span>);<br>        properties.put(ProducerConfig.INTERCEPTOR_CLASSES_CONFIG, interceptors);<br><br>        <span class="hljs-comment">// 3. 创建 kafka 生产者对象</span><br>        kafkaProducer = <span class="hljs-keyword">new</span> <span class="hljs-title class_">KafkaProducer</span>&lt;&gt;(properties);<br>    &#125;<br><br>    <span class="hljs-meta">@Test</span><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">sendTest</span><span class="hljs-params">()</span> &#123;<br>        kafkaProducer.send(<span class="hljs-keyword">new</span> <span class="hljs-title class_">ProducerRecord</span>&lt;&gt;(<span class="hljs-string">&quot;first&quot;</span>, <span class="hljs-string">&quot;interceptor&quot;</span>));<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="自定义序列化"><a class="markdownIt-Anchor" href="#自定义序列化"></a> 自定义序列化</h3><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/shirukai/article/details/82152172">Kafka 自定义消息序列化和反序列化方式</a></p><p class="note note-primary">生产者</p><p>需要实现序列化接口<code>Serializer</code>，序列化包含key和value的序列化，两者的序列化方式可不同，分别对应<code>key.serializer</code>和<code>value.serializer</code></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-keyword">interface</span> <span class="hljs-title class_">Serializer</span>&lt;T&gt; <span class="hljs-keyword">extends</span> <span class="hljs-title class_">Closeable</span> &#123;<br>    <span class="hljs-keyword">default</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">configure</span><span class="hljs-params">(Map&lt;String, ?&gt; configs, <span class="hljs-type">boolean</span> isKey)</span> &#123;<br>    &#125;<br><br>    <span class="hljs-type">byte</span>[] serialize(String var1, T var2);<br><br>    <span class="hljs-keyword">default</span> <span class="hljs-type">byte</span>[] serialize(String topic, Headers headers, T data) &#123;<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">this</span>.serialize(topic, data);<br>    &#125;<br><br>    <span class="hljs-keyword">default</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">close</span><span class="hljs-params">()</span> &#123;<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><p class="note note-primary">消费者</p><p>需要实现反序列化接口<code>Deserializer</code>，反序列化包含key和value的反序列化，两者的反序列化方式可不同，分别对应<code>key.deserializer</code>和<code>value.deserializer</code></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-keyword">interface</span> <span class="hljs-title class_">Deserializer</span>&lt;T&gt; <span class="hljs-keyword">extends</span> <span class="hljs-title class_">Closeable</span> &#123;<br>    <span class="hljs-keyword">default</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">configure</span><span class="hljs-params">(Map&lt;String, ?&gt; configs, <span class="hljs-type">boolean</span> isKey)</span> &#123;<br>    &#125;<br><br>    T <span class="hljs-title function_">deserialize</span><span class="hljs-params">(String var1, <span class="hljs-type">byte</span>[] var2)</span>;<br><br>    <span class="hljs-keyword">default</span> T <span class="hljs-title function_">deserialize</span><span class="hljs-params">(String topic, Headers headers, <span class="hljs-type">byte</span>[] data)</span> &#123;<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">this</span>.deserialize(topic, data);<br>    &#125;<br><br>    <span class="hljs-keyword">default</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">close</span><span class="hljs-params">()</span> &#123;<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="生产者分区"><a class="markdownIt-Anchor" href="#生产者分区"></a> 生产者分区</h3><p><img src="/2022/02/24/Kafka/%E5%88%86%E5%8C%BA%E5%A5%BD%E5%A4%84.png" srcset="/img/loading.gif" lazyload alt></p><ol><li><strong><code>分区的目的</code></strong>：Kafka使用分区实现数据的水平扩展性、并行处理和容错能力。</li><li><strong><code>存储和分发</code></strong>：<ul><li><strong>多Broker存储</strong>：分散存储和IO负载，Kafka将每个Topic的数据划分为多个分区，并将这些分区分布在多个Broker上。</li><li><strong>负载均衡</strong>：通过合理地分配分区到各个Broker，可以有效地实现负载均衡，确保每个Broker承担相似的数据和流量。</li></ul></li><li><strong><code>提高并行度</code></strong>：<ul><li><strong>生产者并行发送</strong>：生产者可以同时向多个分区发送数据，从而提高消息的生产速率。</li><li><strong>消费者并行消费</strong>：消费者组中的不同消费者实例可以并行地从不同分区消费数据，从而提高整体的数据消费速率。</li></ul></li></ol><h4 id="分区策略"><a class="markdownIt-Anchor" href="#分区策略"></a> 分区策略</h4><p class="note note-primary">ProducerRecord</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">ProducerRecord</span>&lt;K, V&gt; &#123;<br>    <span class="hljs-comment">// 该消息需要发往的主题</span><br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">final</span> String topic;<br>    <span class="hljs-comment">// 该消息需要发往的主题中的某个分区，如果该字段有值，则分区器不起作用，直接发往指定的分区</span><br>    <span class="hljs-comment">// 如果该值为null，则利用分区器进行分区的选择 </span><br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">final</span> Integer partition;<br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">final</span> Headers headers;<br>    <span class="hljs-comment">// 如果partition字段不为null，则使用分区器进行分区选择时会用到该key字段，该值可为空 </span><br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">final</span> K key;<br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">final</span> V value;<br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">final</span> Long timestamp;<br></code></pre></td></tr></table></figure><p class="note note-primary">指定Partition</p><p>在发送消息时，如果明确指定了分区，则消息会直接发送到该指定分区。如果同时提供了 <code>partition</code> 和 <code>key</code>，则 <code>partition</code> 优先级高于 <code>key</code>。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs java">    <span class="hljs-keyword">private</span> <span class="hljs-type">Callback</span> <span class="hljs-variable">callback</span> <span class="hljs-operator">=</span> (metadata, exception) -&gt; log.info(<span class="hljs-string">&quot;发送的分区是：&#123;&#125;&quot;</span>, metadata.partition());   <br>	<span class="hljs-comment">/**</span><br><span class="hljs-comment">     * 指定分区</span><br><span class="hljs-comment">     */</span><br>    <span class="hljs-meta">@Test</span><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">partitionTest</span><span class="hljs-params">()</span> &#123;<br>        <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> <span class="hljs-variable">i</span> <span class="hljs-operator">=</span> <span class="hljs-number">0</span>; i &lt; <span class="hljs-number">3</span>; i++) &#123;<br>            kafkaProducer.send(<span class="hljs-keyword">new</span> <span class="hljs-title class_">ProducerRecord</span>&lt;&gt;(<span class="hljs-string">&quot;first&quot;</span>, <span class="hljs-number">0</span>, <span class="hljs-literal">null</span>, <span class="hljs-string">&quot;hello&quot;</span>), callback);<br>        &#125;<br>    &#125;<br><span class="hljs-comment">//14:44:05.375 [kafka-producer-network-thread | producer-1] INFO com.wgf.kafka.PartitionTest - 发送的分区是：0</span><br><span class="hljs-comment">//14:44:05.375 [kafka-producer-network-thread | producer-1] INFO com.wgf.kafka.PartitionTest - 发送的分区是：0</span><br><span class="hljs-comment">//14:44:05.375 [kafka-producer-network-thread | producer-1] INFO com.wgf.kafka.PartitionTest - 发送的分区是：0</span><br></code></pre></td></tr></table></figure><p class="note note-primary">指定key</p><p>当发送消息时未显式指定 <code>Partition</code>，但提供了<code>key</code>，Kafka会使用 <code>key</code> 的MurmurHash值与Topic的Partition数量进行取模运算，以确定消息的目标Partition。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs java">    <span class="hljs-comment">/**</span><br><span class="hljs-comment">     * 指定key</span><br><span class="hljs-comment">     */</span><br>    <span class="hljs-meta">@Test</span><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">keyTest</span><span class="hljs-params">()</span> &#123;<br>        <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> <span class="hljs-variable">i</span> <span class="hljs-operator">=</span> <span class="hljs-number">0</span>; i &lt; <span class="hljs-number">3</span>; i++) &#123;<br>            kafkaProducer.send(<span class="hljs-keyword">new</span> <span class="hljs-title class_">ProducerRecord</span>&lt;&gt;(<span class="hljs-string">&quot;first&quot;</span>,  <span class="hljs-string">&quot;aaa&quot;</span>, <span class="hljs-string">&quot;hello&quot;</span>), callback);<br>        &#125;<br>    &#125;<br><span class="hljs-comment">//14:44:46.084 [kafka-producer-network-thread | producer-1] INFO com.wgf.kafka.PartitionTest - 发送的分区是：1</span><br><span class="hljs-comment">//14:44:46.085 [kafka-producer-network-thread | producer-1] INFO com.wgf.kafka.PartitionTest - 发送的分区是：1</span><br><span class="hljs-comment">//14:44:46.085 [kafka-producer-network-thread | producer-1] INFO com.wgf.kafka.PartitionTest - 发送的分区是：1</span><br></code></pre></td></tr></table></figure><p class="note note-primary">都不指定</p><p>若既未指定<code>partition</code>值，又未提供<code>key</code>值，Kafka将使用Sticky Partition策略。初始时，它随机选择一个整数作为起始点。对于随后的每次调用，此整数值会递增。为确定消息的目标Partition，该整数值会与Topic的可用Partition总数进行取模运算，这一策略类似于轮询（round-robin）算法。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs java">    <span class="hljs-comment">/**</span><br><span class="hljs-comment">     * Partition和Key不指定则使用轮询</span><br><span class="hljs-comment">     */</span><br>    <span class="hljs-meta">@Test</span><br>    <span class="hljs-meta">@SneakyThrows</span><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">roundRobinTest</span><span class="hljs-params">()</span> &#123;<br>        <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> <span class="hljs-variable">i</span> <span class="hljs-operator">=</span> <span class="hljs-number">0</span>; i &lt; <span class="hljs-number">3</span>; i++) &#123;<br>            TimeUnit.SECONDS.sleep(<span class="hljs-number">1</span>);<br>            kafkaProducer.send(<span class="hljs-keyword">new</span> <span class="hljs-title class_">ProducerRecord</span>&lt;&gt;(<span class="hljs-string">&quot;first&quot;</span>,  <span class="hljs-string">&quot;hello&quot;</span>), callback);<br>        &#125;<br>    &#125;<br><span class="hljs-comment">//14:47:26.704 [kafka-producer-network-thread | producer-1] INFO com.wgf.kafka.PartitionTest - 发送的分区是：0</span><br><span class="hljs-comment">//14:47:27.700 [kafka-producer-network-thread | producer-1] INFO com.wgf.kafka.PartitionTest - 发送的分区是：1</span><br><span class="hljs-comment">//14:47:28.685 [kafka-producer-network-thread | producer-1] INFO com.wgf.kafka.PartitionTest - 发送的分区是：0</span><br></code></pre></td></tr></table></figure><h4 id="自定义分区策略"><a class="markdownIt-Anchor" href="#自定义分区策略"></a> 自定义分区策略</h4><p>在Kafka中，为满足特定的业务需求或数据路由逻辑，可以自定义分区策略。这可以通过实现 <code>Partitioner</code> 接口并覆写其方法来完成。虽然Kafka提供了 <code>DefaultPartitioner</code> 作为默认的分区策略，但有时候，为了实现更优的负载均衡或根据特定的数据属性将消息路由到不同的分区，自定义的分区策略是必要的。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">MyPartitioner</span> <span class="hljs-keyword">implements</span> <span class="hljs-title class_">Partitioner</span> &#123;<br><br> <span class="hljs-comment">// 返回的是第几个分区</span><br> <span class="hljs-meta">@Override</span><br> <span class="hljs-keyword">public</span> <span class="hljs-type">int</span> <span class="hljs-title function_">partition</span><span class="hljs-params">(String topic, Object key, <span class="hljs-type">byte</span>[] keyBytes, Object value, <span class="hljs-type">byte</span>[] valueBytes, Cluster cluster)</span> &#123;<br>     <span class="hljs-comment">// 获取topic的partitions</span><br>     List&lt;PartitionInfo&gt; partitionInfoList = cluster.partitionsForTopic(topic);<br>     <span class="hljs-type">int</span> <span class="hljs-variable">numPartitions</span> <span class="hljs-operator">=</span> partitionInfoList.size();<br><br>     <span class="hljs-comment">// 根据key进行hash计算</span><br>     <span class="hljs-type">int</span> <span class="hljs-variable">hashCode</span> <span class="hljs-operator">=</span> key.hashCode();<br>     <span class="hljs-type">int</span> <span class="hljs-variable">partition</span> <span class="hljs-operator">=</span> Math.abs(hashCode % numPartitions);<br>     <span class="hljs-keyword">return</span> partition;<br> &#125;<br><br> <span class="hljs-meta">@Override</span><br> <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">close</span><span class="hljs-params">()</span> &#123;<br><br> &#125;<br><br> <span class="hljs-meta">@Override</span><br> <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">configure</span><span class="hljs-params">(Map&lt;String, ?&gt; configs)</span> &#123;<br><br> &#125;<br>&#125;<br></code></pre></td></tr></table></figure><p class="note note-primary">SpringBoot配置</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">spring:</span><br><span class="hljs-attr">kafka:</span><br> <span class="hljs-attr">bootstrap-servers:</span> <span class="hljs-number">127.0</span><span class="hljs-number">.0</span><span class="hljs-number">.1</span><span class="hljs-string">:9092</span>                                     <span class="hljs-comment"># 连接kafka的地址，多个地址用逗号分隔</span><br> <span class="hljs-attr">producer:</span><br>   <span class="hljs-attr">transaction-id-prefix:</span> <span class="hljs-string">kafka-tx-</span><br>   <span class="hljs-attr">retries:</span> <span class="hljs-number">1</span>                                                                <span class="hljs-comment"># 若设置大于0的值，客户端会将发送失败的记录重新发送, 若开启事务则需要大于0</span><br>   <span class="hljs-attr">acks:</span> <span class="hljs-number">-1</span>                                                                  <span class="hljs-comment"># 若开启事务，则必须设置为-1</span><br>   <span class="hljs-attr">batch-size:</span> <span class="hljs-number">16384</span>                                                         <span class="hljs-comment"># 当将多个记录被发送到同一个分区时， Producer 将尝试将记录组合到更少的请求中。这有助于提升客户端和服务器端的性能。这个配置控制一个批次的默认大小（以字节为单位）。16384是缺省的配置</span><br>   <span class="hljs-attr">buffer-memory:</span> <span class="hljs-number">33554432</span>                                                   <span class="hljs-comment"># #Producer 用来缓冲等待被发送到服务器的记录的总字节数，33554432是缺省配置</span><br>   <span class="hljs-attr">key-serializer:</span> <span class="hljs-string">org.apache.kafka.common.serialization.StringSerializer</span>    <span class="hljs-comment"># 关键字的序列化类</span><br>   <span class="hljs-attr">value-serializer:</span> <span class="hljs-string">org.apache.kafka.common.serialization.StringSerializer</span>  <span class="hljs-comment"># 值的序列化类</span><br>   <span class="hljs-attr">properties:</span><br>     <span class="hljs-attr">enable.idempotence:</span> <span class="hljs-literal">true</span>                                                <span class="hljs-comment"># 开启消息幂等</span><br>     <span class="hljs-attr">partitioner.class:</span> <span class="hljs-string">com.wgf.partition.MyPartitioner</span>                      <span class="hljs-comment"># 自定义分区策略</span><br></code></pre></td></tr></table></figure><p class="note note-primary">kafka-client配置</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">init</span><span class="hljs-params">()</span> &#123;<br>    <span class="hljs-comment">// 1. 创建 kafka 生产者的配置对象</span><br>    <span class="hljs-type">Properties</span> <span class="hljs-variable">properties</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">Properties</span>();<br>    <span class="hljs-comment">// 2. 给 kafka 配置对象添加配置信息：bootstrap.servers</span><br>    properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="hljs-string">&quot;localhost:9092&quot;</span>);<br>    <span class="hljs-comment">// key,value 序列化（必须）：key.serializer，value.serializer</span><br>    properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG,<br>            <span class="hljs-string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);<br><br>    properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG,<br>            <span class="hljs-string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);<br><br>    <span class="hljs-comment">// 自定义分区器</span><br>    properties.put(ProducerConfig.PARTITIONER_CLASS_CONFIG, <span class="hljs-string">&quot;com.wgf.partition.MyPartitioner&quot;</span>);<br><br>    <span class="hljs-comment">// 3. 创建 kafka 生产者对象</span><br>    kafkaProducer = <span class="hljs-keyword">new</span> <span class="hljs-title class_">KafkaProducer</span>&lt;&gt;(properties);<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="生产者提高吞吐量"><a class="markdownIt-Anchor" href="#生产者提高吞吐量"></a> 生产者提高吞吐量</h3><p><img src="/2022/02/24/Kafka/%E6%B6%88%E6%81%AF%E5%8F%91%E9%80%81%E6%B5%81%E7%A8%8B.png" srcset="/img/loading.gif" lazyload alt></p><p>由上图我们可知，batch.size，RecordAccumulator，<a target="_blank" rel="noopener" href="http://linger.ms">linger.ms</a>，配置的大小都会影响吞吐量</p><ul><li><code>batch.size</code>：控制批次大小，默认为16K。当批次大小增加时，每次发送的数据量也增加，从而提高吞吐量。</li><li><code>linger.ms</code>：延迟等待时间，默认为0ms。适当增加这个值（例如，设置在5-100ms范围内）可以使批次中累积更多的消息，从而提高吞吐量。但是，这也意味着消息的延迟会增加。</li><li><code>RecordAccumulator</code>：生产者用于缓存数据的总内存大小，默认为32M。在有大量的生产者分区或多个Topic的情况下，适当增大此值可以提高吞吐量。</li><li><code>compression.type</code>：设置消息的压缩算法，可以提高网络吞吐量，但会增加CPU的使用，常用的压缩类型为 <code>snappy</code>。</li></ul><p class="note note-primary">SpringBoot配置</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs yml"><span class="hljs-attr">spring:</span><br>  <span class="hljs-attr">kafka:</span><br>    <span class="hljs-attr">bootstrap-servers:</span> <span class="hljs-string">localhost:9092,localhost:9093,localhost:9094</span>             <span class="hljs-comment"># 连接kafka的地址，多个地址用逗号分隔</span><br>    <span class="hljs-attr">producer:</span><br>      <span class="hljs-attr">acks:</span> <span class="hljs-number">-1</span>                                                                  <span class="hljs-comment"># 若开启事务，则必须设置为-1</span><br>      <span class="hljs-attr">batch-size:</span> <span class="hljs-number">16384</span>                                                         <span class="hljs-comment"># batch.size 批次大小，默认16k</span><br>      <span class="hljs-attr">buffer-memory:</span> <span class="hljs-number">33554432</span>                                                   <span class="hljs-comment"># RecordAccumulator 大小,默认32M</span><br>      <span class="hljs-attr">compression-type:</span> <span class="hljs-string">snappy</span>                                                  <span class="hljs-comment"># 开启压缩类型</span><br>      <span class="hljs-attr">key-serializer:</span> <span class="hljs-string">org.apache.kafka.common.serialization.StringSerializer</span>    <span class="hljs-comment"># 关键字的序列化类</span><br>      <span class="hljs-attr">value-serializer:</span> <span class="hljs-string">org.apache.kafka.common.serialization.StringSerializer</span>  <span class="hljs-comment"># 值的序列化类</span><br>      <span class="hljs-attr">properties:</span><br>        <span class="hljs-attr">linger.ms:</span> <span class="hljs-number">1</span><br>        <span class="hljs-attr">enable.idempotence:</span> <span class="hljs-literal">true</span>                                                <span class="hljs-comment"># 开启消息幂等</span><br></code></pre></td></tr></table></figure><p class="note note-primary">kafka-client配置</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">ProducerThroughputTest</span> &#123;<br>    <span class="hljs-keyword">private</span> KafkaProducer&lt;String, String&gt; kafkaProducer;<br><br>    <span class="hljs-meta">@BeforeEach</span><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">init</span><span class="hljs-params">()</span> &#123;<br>        <span class="hljs-comment">// 1. 创建 kafka 生产者的配置对象</span><br>        <span class="hljs-type">Properties</span> <span class="hljs-variable">properties</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">Properties</span>();<br>        <span class="hljs-comment">// 2. 给 kafka 配置对象添加配置信息：bootstrap.servers</span><br>        properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="hljs-string">&quot;localhost:9092&quot;</span>);<br>        <span class="hljs-comment">// key,value 序列化（必须）：key.serializer，value.serializer</span><br>        properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG,<br>                <span class="hljs-string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);<br><br>        properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG,<br>                <span class="hljs-string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);<br><br>        <span class="hljs-comment">// batch.size：批次大小，默认 16K</span><br>        properties.put(ProducerConfig.BATCH_SIZE_CONFIG, <span class="hljs-number">16384</span>);<br><br>        <span class="hljs-comment">// linger.ms：等待时间，默认 0</span><br>        properties.put(ProducerConfig.LINGER_MS_CONFIG, <span class="hljs-number">10</span>);<br><br>        <span class="hljs-comment">// RecordAccumulator：缓冲区大小，默认 32M：buffer.memory</span><br>        properties.put(ProducerConfig.BUFFER_MEMORY_CONFIG, <span class="hljs-number">33554432</span>);<br><br>        <span class="hljs-comment">// compression.type：压缩，默认 none，可配置值 gzip、snappy、lz4 和 zstd</span><br>        properties.put(ProducerConfig.COMPRESSION_TYPE_CONFIG,<span class="hljs-string">&quot;snappy&quot;</span>);<br><br>        <span class="hljs-comment">// 3. 创建 kafka 生产者对象</span><br>        kafkaProducer = <span class="hljs-keyword">new</span> <span class="hljs-title class_">KafkaProducer</span>&lt;&gt;(properties);<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="生产者消息可靠性保证"><a class="markdownIt-Anchor" href="#生产者消息可靠性保证"></a> <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/301757690">生产者消息可靠性保证</a></h3><p>为了确保Producer发送的数据被可靠投递指定的Topic，Kafka引入了一个 <code>acks</code> 机制。当Topic的某个partition收到Producer发送的数据后，会根据 <code>acks</code> 的配置决定是否向Producer发送ACK响应。如果Producer收到 <code>ACK</code>，则认为数据已经被成功发送，否则会尝试重新发送。</p><p class="note note-primary">生产者的ack机制</p><table><thead><tr><th>acks</th><th>描述</th><th>优点</th><th>缺点</th></tr></thead><tbody><tr><td>0</td><td>不进行ack确认, 一旦Broker接收到消息就立即返回确认</td><td>数据吞吐量最大</td><td>当broker发生故障时，消息有可能会丢失</td></tr><tr><td>1</td><td>仅leader ack，当leader落盘成功后返回ack（默认配置）</td><td>平衡了性能和数据安全性</td><td>如果follower未完成同步时leader发生故障，消息可能丢失</td></tr><tr><td>-1(all)</td><td>所有的ISR (In-Sync Replicas) 成员都必须ack</td><td>数据安全性最高，只有当所有ISR成员都确认后才认为消息已写入</td><td>如果leader发生故障或网络问题，可能造成数据重复，原因是消息已写入，acks无法及时应答，Producer进行重试</td></tr></tbody></table><p class="note note-primary">SpringBoot 生产者ACK配置</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">spring:</span><br>  <span class="hljs-attr">kafka:</span><br>    <span class="hljs-attr">bootstrap-servers:</span> <span class="hljs-number">127.0</span><span class="hljs-number">.0</span><span class="hljs-number">.1</span><span class="hljs-string">:9092</span>                                 <span class="hljs-comment"># 连接kafka的地址，多个地址用逗号分隔</span><br>    <span class="hljs-attr">producer:</span><br>      <span class="hljs-attr">acks:</span> <span class="hljs-number">-1</span>     <br>      <span class="hljs-attr">retries:</span> <span class="hljs-number">3</span>                                                      <span class="hljs-comment"># 开启ack一般配合重试次数使用,默认Integer最大值</span><br></code></pre></td></tr></table></figure><p class="note note-primary">kafka-client配置</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">AckTest</span> &#123;<br>    <span class="hljs-keyword">private</span> KafkaProducer&lt;String, String&gt; kafkaProducer;<br><br>    <span class="hljs-meta">@BeforeEach</span><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">init</span><span class="hljs-params">()</span> &#123;<br>        <span class="hljs-comment">// 1. 创建 kafka 生产者的配置对象</span><br>        <span class="hljs-type">Properties</span> <span class="hljs-variable">properties</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">Properties</span>();<br>        <span class="hljs-comment">// 2. 给 kafka 配置对象添加配置信息：bootstrap.servers</span><br>        properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="hljs-string">&quot;localhost:9092&quot;</span>);<br>        <span class="hljs-comment">// key,value 序列化（必须）：key.serializer，value.serializer</span><br>        properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG,<br>                <span class="hljs-string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);<br><br>        properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG,<br>                <span class="hljs-string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);<br><br>        <span class="hljs-comment">// 设置ack</span><br>        properties.put(ProducerConfig.ACKS_CONFIG, <span class="hljs-string">&quot;-1&quot;</span>);<br><br>        <span class="hljs-comment">// 设置重试次数，默认是 int 最大值，2147483647</span><br>        properties.put(ProducerConfig.RETRIES_CONFIG, <span class="hljs-number">3</span>);<br><br>        <span class="hljs-comment">// 3. 创建 kafka 生产者对象</span><br>        kafkaProducer = <span class="hljs-keyword">new</span> <span class="hljs-title class_">KafkaProducer</span>&lt;&gt;(properties);<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="数据去重"><a class="markdownIt-Anchor" href="#数据去重"></a> 数据去重</h3><p>由上文的ACK应答机制可知，当消息发送失败会进行重试，那么重试就有可能导致数据重复</p><p class="note note-primary">数据传递语义</p><ul><li><strong>At most once（最多一次）</strong>:<ul><li>消息可能会丢失，但绝不会被传递多次。</li><li>生产者设置：<ul><li><code>acks=0</code>: 不等待任何确认。</li><li><code>retries=0</code>: 不进行重新发送。</li></ul></li></ul></li><li><strong>At least once（最少一次）</strong>:<ul><li>消息不会丢失，但可能会被传递多次。</li><li>要实现这种语义，可以考虑以下配置：<ul><li>生产者设置：<ul><li><code>acks=1</code> 或 <code>acks=all</code>: 确保至少leader副本或所有in-sync副本都已确认。</li><li><code>retries</code> 设置为一个较大的值（如 <code>Integer.MAX_VALUE</code>）：确保消息在失败时会被重新发送。</li></ul></li><li>消费者：<ul><li>设置 <code>enable.auto.commit=false</code> 并手动提交offset，这样只有在消息被成功处理之后才提交offset。</li></ul></li></ul></li></ul></li><li><strong>Exactly once（仅一次）</strong>:<ul><li>消息既不会丢失，也不会被重复传递。这是最强的传输保证，但也是最复杂的。</li><li>要实现这种语义，可以考虑以下配置：<ul><li>生产者设置：<ul><li><code>acks=all</code>: 确保所有in-sync副本都已确认。</li><li><code>enable.idempotence=true</code>: 确保消息不会在网络故障等情况下被重复发送。</li><li>使用事务发送消息</li></ul></li><li>消费者：<ul><li>使用事务消费消息，并确保消费和生产是在同一事务中。</li><li>设置 <code>enable.auto.commit=false</code> 并手动提交offset。</li></ul></li></ul></li></ul></li></ul><h4 id="消息幂等"><a class="markdownIt-Anchor" href="#消息幂等"></a> <a target="_blank" rel="noopener" href="https://www.cnblogs.com/smartloli/p/11922639.html">消息幂等</a></h4><p><img src="/2022/02/24/Kafka/kafka%E5%B9%82%E7%AD%89.png" srcset="/img/loading.gif" lazyload alt></p><p>幂等性在 Kafka 中确保了，无论生产者向 broker 发送多少次重复的消息，broker 端都只会持久化一条相同的消息，从而避免了数据的重复。</p><p>重复数据的判断基于以下标准：消息具有相同的 <code>&lt;Producer ID, Partition, SeqNumber&gt;</code> 组合将被视为重复的。其中：</p><ul><li><strong><code>Producer ID</code></strong>：是由 Kafka 分配的唯一标识符。每次 Kafka 重启时都会为生产者分配一个新的 Producer ID。</li><li><strong><code>Partition</code></strong>：表示该消息所在的分区号。</li><li><strong><code>Sequence Number</code></strong>：是在每个分区中单调自增的数字，由生产者为每条消息分配。</li></ul><p>只有当上述三个属性完全相同时，消息才被视为重复，Broker 在这种情况下只会持久化一条数据。</p><p class="note note-primary">局限性</p><p>生产者重启时，<code>ProducerID</code> 会改变，而不同的Partition对应不同的 <code>SequenceNumber</code>。因此，Kafka的幂等性保证仅限于单个生产者会话。一旦生产者重启，之前的幂等性保证将不再有效，即Kafka只能保证会话内的幂等，而无法保证跨会话的幂等。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs java">server:<br>  port: <span class="hljs-number">9000</span><br><br>spring:<br>  kafka:<br>    bootstrap-servers: <span class="hljs-number">127.0</span><span class="hljs-number">.0</span><span class="hljs-number">.1</span>:<span class="hljs-number">9092</span>                                     # 连接kafka的地址，多个地址用逗号分隔<br>    producer:<br>      retries: <span class="hljs-number">1</span>                                                                # 若设置大于<span class="hljs-number">0</span>的值，客户端会将发送失败的记录重新发送，如果开启事务或者幂等，则必须大于<span class="hljs-number">0</span><br>      acks: -<span class="hljs-number">1</span>                                                                  # 若开启事务，则必须设置为-<span class="hljs-number">1</span>    <br>      batch-size: <span class="hljs-number">16384</span>                                                         # 当将多个记录被发送到同一个分区时， Producer 将尝试将记录组合到更少的请求中。这有助于提升客户端和服务器端的性能。这个配置控制一个批次的默认大小（以字节为单位）。<span class="hljs-number">16384</span>是缺省的配置<br>      buffer-memory: <span class="hljs-number">33554432</span>                                                   # #Producer 用来缓冲等待被发送到服务器的记录的总字节数，<span class="hljs-number">33554432</span>是缺省配置<br>      key-serializer: org.apache.kafka.common.serialization.StringSerializer    # 关键字的序列化类<br>      value-serializer: org.apache.kafka.common.serialization.StringSerializer  # 值的序列化类<br>      properties:<br>        enable.idempotence: <span class="hljs-literal">true</span>                                                # 开启消息幂等<br></code></pre></td></tr></table></figure><h4 id="生产者事务"><a class="markdownIt-Anchor" href="#生产者事务"></a> <a target="_blank" rel="noopener" href="https://www.modb.pro/db/111263">生产者事务</a></h4><ul><li><p>当进行批量消息发送时，所有消息要么全部成功发布，要么全部不发布，确保数据的一致性。</p></li><li><p>开启事务，必须开启幂等性</p></li><li><p>在事务启用的情况下，Kafka 将 <code>TransactionID</code> 与 <code>ProducerID</code> 关联，从而确保在生产者重启的情况下也不会丢失 <code>ProducerID</code>，进一步强化了幂等性的持久保障。</p></li></ul><p class="note note-primary">跨分区跨会话事务</p><p><img src="/2022/02/24/Kafka/kafka%E4%BA%8B%E5%8A%A1.png" srcset="/img/loading.gif" lazyload alt></p><ul><li><p>为了支持跨分区和跨会话的事务，Kafka 引入了一个全局唯一的 <code>TransactionID</code>。每当一个 Producer 启动事务时，它会获得一个特定的 <code>ProducerID</code> ，并将这个 <code>ProducerID</code> 与 <code>TransactionID</code> 进行绑定。这种设计确保了，即使 Producer 重启，也能够通过 <code>TransactionID</code> 重新获取原始的 <code>ProducerID</code> ，从而保持事务的连续性。</p></li><li><p>Kafka 为了管理和维护事务状态，引入了一个专门的组件：Transaction Coordinator（事务协调器）。Producer 与 Transaction Coordinator 交互以获得 <code>TransactionID</code> 和管理事务的生命周期。为了持久化和跟踪事务的状态，Transaction Coordinator 将事务的元数据写入 Kafka 的一个专用的内部 Topic。这样，即使 Kafka 集群中的节点重启，由于事务的状态被持久化，任何进行中的事务都可以从上次的状态恢复，保证事务的完整性。</p></li></ul><p class="note note-primary">事务API</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-comment">// 1 初始化事务</span><br><span class="hljs-keyword">void</span> <span class="hljs-title function_">initTransactions</span><span class="hljs-params">()</span>;<br><span class="hljs-comment">// 2 开启事务</span><br><span class="hljs-keyword">void</span> <span class="hljs-title function_">beginTransaction</span><span class="hljs-params">()</span> <span class="hljs-keyword">throws</span> ProducerFencedException;<br><span class="hljs-comment">// 3 在事务内提交已经消费的偏移量（主要用于消费者）</span><br><span class="hljs-keyword">void</span> <span class="hljs-title function_">sendOffsetsToTransaction</span><span class="hljs-params">(Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets,</span><br><span class="hljs-params"> String consumerGroupId)</span> <span class="hljs-keyword">throws</span> <br>ProducerFencedException;<br><span class="hljs-comment">// 4 提交事务</span><br><span class="hljs-keyword">void</span> <span class="hljs-title function_">commitTransaction</span><span class="hljs-params">()</span> <span class="hljs-keyword">throws</span> ProducerFencedException;<br><span class="hljs-comment">// 5 放弃事务（类似于回滚事务的操作）</span><br><span class="hljs-keyword">void</span> <span class="hljs-title function_">abortTransaction</span><span class="hljs-params">()</span> <span class="hljs-keyword">throws</span> ProducerFencedException;<br></code></pre></td></tr></table></figure><p class="note note-primary">前置</p><ul><li><code>transaction.state.log.replication.factor</code> 的值必须大于等于 <code>transaction.state.log.min.isr</code> 的值：事务状态日志的副本数量必须大于等于最小副本同步因子。这是为了确保事务状态日志的可靠性和一致性。</li><li><code>transaction.state.log.replication.factor</code> 的值必须小于等于 Kafka 集群中的可用 broker 数量：事务状态日志的副本数量不能超过 Kafka 集群中可用的 broker 数量。这是为了确保事务状态日志的复制和分布在 Kafka 集群中的可行性。</li></ul><p class="note note-primary">SpringBoot使用事务</p><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/H900302/article/details/110200128">springboot 事务配置</a></p><p>producer配置</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">spring:</span><br>  <span class="hljs-attr">kafka:</span><br>    <span class="hljs-attr">bootstrap-servers:</span> <span class="hljs-number">127.0</span><span class="hljs-number">.0</span><span class="hljs-number">.1</span><span class="hljs-string">:9092</span>                                     <span class="hljs-comment"># 连接kafka的地址，多个地址用逗号分隔</span><br>    <span class="hljs-attr">producer:</span><br>      <span class="hljs-attr">transaction-id:</span> <span class="hljs-string">my-transaction-id</span>											<span class="hljs-comment"># 开启事务，设置事务id</span><br>      <span class="hljs-attr">retries:</span> <span class="hljs-number">1</span>                                                                <span class="hljs-comment"># 若设置大于0的值，客户端会将发送失败的记录重新发送, 若开启事务则需要大于0</span><br>      <span class="hljs-attr">acks:</span> <span class="hljs-number">-1</span>                                                                  <span class="hljs-comment"># 若开启事务，则必须设置为-1</span><br>      <span class="hljs-attr">batch-size:</span> <span class="hljs-number">16384</span>                                                         <span class="hljs-comment"># 当将多个记录被发送到同一个分区时， Producer 将尝试将记录组合到更少的请求中。这有助于提升客户端和服务器端的性能。这个配置控制一个批次的默认大小（以字节为单位）。16384是缺省的配置</span><br>      <span class="hljs-attr">buffer-memory:</span> <span class="hljs-number">33554432</span>                                                   <span class="hljs-comment"># #Producer 用来缓冲等待被发送到服务器的记录的总字节数，33554432是缺省配置</span><br>      <span class="hljs-attr">key-serializer:</span> <span class="hljs-string">org.apache.kafka.common.serialization.StringSerializer</span>    <span class="hljs-comment"># 关键字的序列化类</span><br>      <span class="hljs-attr">value-serializer:</span> <span class="hljs-string">org.apache.kafka.common.serialization.StringSerializer</span>  <span class="hljs-comment"># 值的序列化类</span><br>      <span class="hljs-attr">properties:</span><br>        <span class="hljs-attr">enable.idempotence:</span> <span class="hljs-literal">true</span>                                                <span class="hljs-comment"># 开启消息幂等</span><br></code></pre></td></tr></table></figure><p>consumer配置</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">spring:</span><br>  <span class="hljs-attr">kafka:</span><br>    <span class="hljs-attr">bootstrap-servers:</span> <span class="hljs-number">127.0</span><span class="hljs-number">.0</span><span class="hljs-number">.1</span><span class="hljs-string">:9092</span>                                     <span class="hljs-comment"># 连接kafka的地址，多个地址用逗号分隔</span><br>    <span class="hljs-attr">consumer:</span><br>      <span class="hljs-attr">enable-auto-commit:</span> <span class="hljs-literal">false</span>                                                 <span class="hljs-comment"># 参数设置成true。那么offset交给kafka来管理，offset进行默认的提交模式,置成false。那么就是Spring来替为我们做人工提交，从而简化了人工提交的方式.需要手动提交还需要指定ack-model</span><br>      <span class="hljs-attr">properties:</span><br>        <span class="hljs-attr">session.timeout.ms:</span> <span class="hljs-number">15000</span><br>      <span class="hljs-attr">key-deserializer:</span> <span class="hljs-string">org.apache.kafka.common.serialization.StringDeserializer</span><br>      <span class="hljs-attr">value-deserializer:</span> <span class="hljs-string">org.apache.kafka.common.serialization.StringDeserializer</span><br>      <span class="hljs-attr">auto-offset-reset:</span> <span class="hljs-string">earliest</span><br>    <span class="hljs-attr">listener:</span><br>      <span class="hljs-attr">ack-mode:</span> <span class="hljs-string">manual</span>                                                          <span class="hljs-comment"># 设置手动ack</span><br>    <span class="hljs-attr">properties:</span><br>      <span class="hljs-attr">isolation.level:</span> <span class="hljs-string">read_committed</span>                                           <span class="hljs-comment"># 置为 read_committed ，Consumer 仅读取已提交的消息， 否则不生效</span><br></code></pre></td></tr></table></figure><p>使用事务</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-comment">// 第一种用法，注解</span><br><span class="hljs-meta">@Transactional</span><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">send</span><span class="hljs-params">(<span class="hljs-type">boolean</span> flag)</span> &#123;<br>    <span class="hljs-built_in">this</span>.kafkaTemplate.send(<span class="hljs-string">&quot;testTopic&quot;</span>, <span class="hljs-string">&quot;before&quot;</span>);<br><br>    <span class="hljs-keyword">if</span> (flag) &#123;<br>        <span class="hljs-keyword">throw</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">RuntimeException</span>();<br>    &#125;<br><br>    <span class="hljs-built_in">this</span>.kafkaTemplate.send(<span class="hljs-string">&quot;testTopic&quot;</span>, <span class="hljs-string">&quot;after&quot;</span>);<br>&#125;<br><br><br><span class="hljs-comment">// 第二种用法，手动操作</span><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">send</span><span class="hljs-params">(<span class="hljs-type">boolean</span> flag)</span> &#123;<br>        kafkaTemplate.executeInTransaction(operations -&gt; &#123;<br>            <span class="hljs-built_in">this</span>.kafkaTemplate.send(<span class="hljs-string">&quot;testTopic&quot;</span>, <span class="hljs-string">&quot;before&quot;</span>);<br><br>            <span class="hljs-keyword">if</span> (flag) &#123;<br>                <span class="hljs-keyword">throw</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">RuntimeException</span>();<br>            &#125;<br><br>            <span class="hljs-built_in">this</span>.kafkaTemplate.send(<span class="hljs-string">&quot;testTopic&quot;</span>, <span class="hljs-string">&quot;after&quot;</span>);<br>            <span class="hljs-keyword">return</span> <span class="hljs-literal">true</span>;<br>        &#125;);<br>    &#125;<br></code></pre></td></tr></table></figure><p class="note note-primary">kafka-client使用事务</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">TransactionTest</span> &#123;<br>    <span class="hljs-keyword">private</span> KafkaProducer&lt;String, String&gt; kafkaProducer;<br><br>    <span class="hljs-meta">@BeforeEach</span><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">init</span><span class="hljs-params">()</span> &#123;<br>        <span class="hljs-comment">// 1. 创建 kafka 生产者的配置对象</span><br>        <span class="hljs-type">Properties</span> <span class="hljs-variable">properties</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">Properties</span>();<br>        <span class="hljs-comment">// 2. 给 kafka 配置对象添加配置信息：bootstrap.servers</span><br>        properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="hljs-string">&quot;localhost:9092&quot;</span>);<br>        <span class="hljs-comment">// key,value 序列化（必须）：key.serializer，value.serializer</span><br>        properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG,<br>                <span class="hljs-string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);<br><br>        properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG,<br>                <span class="hljs-string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);<br><br>        <span class="hljs-comment">// 设置事务 id（必须），事务 id 任意起名</span><br>        properties.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, <span class="hljs-string">&quot;test&quot;</span>);<br><br>        <span class="hljs-comment">// 3. 创建 kafka 生产者对象</span><br>        kafkaProducer = <span class="hljs-keyword">new</span> <span class="hljs-title class_">KafkaProducer</span>&lt;&gt;(properties);<br>    &#125;<br><br>    <span class="hljs-meta">@Test</span><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">test</span><span class="hljs-params">()</span> &#123;<br>        <span class="hljs-comment">// 初始化事务</span><br>        kafkaProducer.initTransactions();<br>        <span class="hljs-comment">// 开启事务</span><br>        kafkaProducer.beginTransaction();<br><br>        <span class="hljs-keyword">try</span> &#123;<br>            <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> <span class="hljs-variable">i</span> <span class="hljs-operator">=</span> <span class="hljs-number">0</span>; i &lt; <span class="hljs-number">6</span>; i++) &#123;<br>                <span class="hljs-comment">// 发送消息</span><br>                kafkaProducer.send(<span class="hljs-keyword">new</span> <span class="hljs-title class_">ProducerRecord</span>&lt;&gt;(<span class="hljs-string">&quot;first&quot;</span>, <span class="hljs-string">&quot;test &quot;</span> + i));<br>            &#125;<br><br>            <span class="hljs-comment">// int i = 1 / 0;</span><br><br>            <span class="hljs-comment">// 提交事务</span><br>            kafkaProducer.commitTransaction();<br>        &#125; <span class="hljs-keyword">catch</span> (Exception e) &#123;<br>            <span class="hljs-comment">// 终止事务</span><br>            kafkaProducer.abortTransaction();<br>        &#125; <span class="hljs-keyword">finally</span> &#123;<br>            <span class="hljs-comment">// 5. 关闭资源</span><br>            kafkaProducer.close();<br>        &#125;<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="数据有序"><a class="markdownIt-Anchor" href="#数据有序"></a> 数据有序</h3><p><img src="/2022/02/24/Kafka/%E6%95%B0%E6%8D%AE%E6%9C%89%E5%BA%8F.png" srcset="/img/loading.gif" lazyload alt></p><p>由于 Topic 可以分多个 Partition，Kafka为了保证其高吞吐量，<span class="green-line">只能保证单个 Partition 数据有序，不能保证多个 Partition 之间数据的有序性</span>，如果要保证 Topic 数据的有序性有以下几种做法：</p><ul><li><code>单分区顺序保证</code>：当一个主题只有一个分区时，可以保证消息的全局有序性。因为只有一个分区，所有消息都按照发送顺序写入该分区，消费者可以按照相同的顺序读取消息，实现全局有序性。</li><li><code>有序分区键</code>：当一个主题有多个分区时，可以使用有序分区键来实现相对有序性。有序分区键是根据业务需求生成的键，不同的业务数据类型（如订单类型）或业务相关键将路由到不同的分区。这样，同一类型的消息将被发送到同一分区，消费者可以按照分区顺序消费消息，实现相对有序性。</li></ul><h3 id="数据乱序"><a class="markdownIt-Anchor" href="#数据乱序"></a> 数据乱序</h3><p><img src="/2022/02/24/Kafka/%E6%B6%88%E6%81%AF%E4%B9%B1%E5%BA%8F.png" srcset="/img/loading.gif" lazyload alt></p><p><strong>Kafka 数据分区有序性：</strong></p><p>Kafka 通过分区内的偏移量来保证单个分区内的消息有序。每个分区的消息都具有唯一的偏移量，它们按照偏移量的顺序被消费者读取。这确保了分区内的消息是有序的，不论是 Kafka 1.x 之前还是之后的版本。</p><p><strong>max.in.flight.requests.per.connection 配置：</strong></p><ul><li><p>它控制了生产者在与 Kafka 服务器的连接上可以同时发送的未确认请求的最大数量。这个参数影响了生产者的性能、吞吐量以及消息的顺序性。</p></li><li><p><strong>Kafka 1.x 之前版本：</strong> 在 Kafka 1.x 之前的版本中，为了保证数据的有序性，生产者通常需要将 <code>max.in.flight.requests.per.connection</code> 设置为 1，以确保每次只发送一个请求，避免乱序。</p></li><li><p><strong>Kafka 1.x 及以后版本：</strong> 在 Kafka 1.x 以及之后的版本，有两种情况：</p><ul><li>当未启用幂等性时，依然建议将 <code>max.in.flight.requests.per.connection</code> 设置为 1，以确保数据的有序性。</li><li>当启用幂等性时，可以将 <code>max.in.flight.requests.per.connection</code> 设置为小于或等于 5。这是因为 Kafka 服务端会缓存生产者发送的最近 5 个请求的元数据，无论如何，都可以保证这最近的 5 个请求的数据是有序的。</li></ul></li></ul><p>生产者可以根据需要对 <code>max.in.flight.requests.per.connection</code> 进行配置，以在吞吐量和有序性之间找到适当的平衡。</p><p><img src="/2022/02/24/Kafka/conn.png" srcset="/img/loading.gif" lazyload alt></p><h2 id="kafka-broker"><a class="markdownIt-Anchor" href="#kafka-broker"></a> Kafka Broker</h2><h3 id="zk存储的kafka信息"><a class="markdownIt-Anchor" href="#zk存储的kafka信息"></a> ZK存储的Kafka信息</h3><p><img src="/2022/02/24/Kafka/zk%E5%AD%98%E5%82%A8%E7%9A%84kafka%E4%BF%A1%E6%81%AF.png" srcset="/img/loading.gif" lazyload alt></p><ul><li>controller：负责管理集群Broker的上下线，所有Topic的分区副本分配和Leader的选举工作等</li><li>broker：服务节点信息，包括上线的节点id和Topic信息</li></ul><h3 id="broker重要参数"><a class="markdownIt-Anchor" href="#broker重要参数"></a> Broker重要参数</h3><table><thead><tr><th>参数名称</th><th>描述</th></tr></thead><tbody><tr><td><code>replica.lag.time.max.ms</code></td><td>ISR 中，如果 Follower 长时间未向 Leader 发送通 信请求或同步数据，则该 Follower 将被踢出 ISR。 该时间阈值，默认 30s</td></tr><tr><td><code>auto.leader.rebalance.enable</code></td><td>默认是 true。 自动 Leader Partition 平衡</td></tr><tr><td><code>leader.imbalance.per.broker.percentage</code></td><td>默认是 10%。每个 broker 允许的不平衡的 leader 的比率。如果每个 broker 超过了这个值，控制器会触发 leader 的平衡</td></tr><tr><td><code>leader.imbalance.check.interval.seconds</code></td><td>默认值 300 秒。检查 leader 负载是否平衡的间隔时间</td></tr><tr><td><code>log.segment.bytes</code></td><td>Kafka 中 log 日志是分成一块块存储的，此配置是 指 log 日志划分成块的大小，默认值 1G</td></tr><tr><td><code>log.index.interval.bytes</code></td><td>默认 4kb，kafka 里面每当写入了 4kb 大小的日志 （.log），然后就往 index 文件里面记录一个索引</td></tr><tr><td><code>log.retention.hours</code></td><td>Kafka 中数据保存的时间，默认 7 天</td></tr><tr><td><code>log.retention.minutes</code></td><td>Kafka 中数据保存的时间，分钟级别，默认关闭</td></tr><tr><td><code>log.retention.ms</code></td><td>Kafka 中数据保存的时间，毫秒级别，默认关闭</td></tr><tr><td><code>log.retention.check.interval.ms</code></td><td>检查数据是否保存超时的间隔，默认是 5 分钟</td></tr><tr><td><code>log.retention.bytes</code></td><td>默认等于-1，表示无穷大。超过设置的所有日志总大小，删除最早的 segment</td></tr><tr><td><code>log.cleanup.policy</code></td><td>默认是 delete，表示所有数据启用删除策略； 如果设置值为 compact，表示所有数据启用压缩策略</td></tr><tr><td><code>num.io.threads</code></td><td>默认是 8。负责写磁盘的线程数。整个参数值要占总核数的 50%</td></tr><tr><td><code>num.replica.fetchers</code></td><td>副本拉取线程数，这个参数占总核数的 50%的 1/3</td></tr><tr><td><code>num.network.threads</code></td><td>默认是 3。数据传输线程数，这个参数占总核数的 50%的 2/3</td></tr><tr><td><code>log.flush.interval.messages</code></td><td>强制页缓存刷写到磁盘的条数，默认是 long 的最大值，9223372036854775807。一般不建议修改， 交给系统自己管理</td></tr><tr><td><code>log.flush.interval.ms</code></td><td>每隔多久，刷数据到磁盘，默认是 null。一般不建议修改，交给系统自己管理</td></tr></tbody></table><h3 id="服役新节点"><a class="markdownIt-Anchor" href="#服役新节点"></a> 服役新节点</h3><p>假设<code>first</code>Topic有三个分区和三个副本分别分布在三台<code>Broker</code>上（0，1，2），这时候新服役了一台id为3的节点，如何把现有的分区和副本重新负载到这四台节点上</p><p class="note note-primary">编写平衡主题脚本</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">vim topics-to-move.json<br></code></pre></td></tr></table></figure><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs json"><span class="hljs-punctuation">&#123;</span><br>    <span class="hljs-attr">&quot;topics&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-punctuation">[</span><br>        <span class="hljs-punctuation">&#123;</span><br>            <span class="hljs-attr">&quot;topic&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-string">&quot;first&quot;</span><br>        <span class="hljs-punctuation">&#125;</span><br>    <span class="hljs-punctuation">]</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">&quot;version&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-number">1</span><br><span class="hljs-punctuation">&#125;</span><br></code></pre></td></tr></table></figure><p class="note note-primary">生成一个负载均衡计划</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">bin/kafka-reassign-partitions.sh --bootstrap-server localhost:9092 --topics-to-move-json-file topics-to-move.json --broker-list &quot;0,1,2,3&quot; --generate<br></code></pre></td></tr></table></figure><p>这是控制台会输出均衡计划</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs json">Current partition replica assignment<br><span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;version&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-number">1</span><span class="hljs-punctuation">,</span><span class="hljs-attr">&quot;partitions&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-punctuation">[</span><span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;topic&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-string">&quot;first&quot;</span><span class="hljs-punctuation">,</span><span class="hljs-attr">&quot;partition&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-number">0</span><span class="hljs-punctuation">,</span><span class="hljs-attr">&quot;replicas&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-punctuation">[</span><span class="hljs-number">0</span><span class="hljs-punctuation">,</span><span class="hljs-number">2</span><span class="hljs-punctuation">,</span><span class="hljs-number">1</span><span class="hljs-punctuation">]</span><span class="hljs-punctuation">,</span><span class="hljs-attr">&quot;log_dirs&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-punctuation">[</span><span class="hljs-string">&quot;any&quot;</span><span class="hljs-punctuation">,</span><span class="hljs-string">&quot;any&quot;</span><span class="hljs-punctuation">,</span><span class="hljs-string">&quot;any&quot;</span><span class="hljs-punctuation">]</span><span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span><span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;topic&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-string">&quot;first&quot;</span><span class="hljs-punctuation">,</span><span class="hljs-attr">&quot;partition&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-number">1</span><span class="hljs-punctuation">,</span><span class="hljs-attr">&quot;replicas&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-punctuation">[</span><span class="hljs-number">2</span><span class="hljs-punctuation">,</span><span class="hljs-number">1</span><span class="hljs-punctuation">,</span><span class="hljs-number">0</span><span class="hljs-punctuation">]</span><span class="hljs-punctuation">,</span><span class="hljs-attr">&quot;log_dirs&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-punctuation">[</span><span class="hljs-string">&quot;any&quot;</span><span class="hljs-punctuation">,</span><span class="hljs-string">&quot;any&quot;</span><span class="hljs-punctuation">,</span><span class="hljs-string">&quot;any&quot;</span><span class="hljs-punctuation">]</span><span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span><span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;topic&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-string">&quot;first&quot;</span><span class="hljs-punctuation">,</span><span class="hljs-attr">&quot;partition&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-number">2</span><span class="hljs-punctuation">,</span><span class="hljs-attr">&quot;replicas&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-punctuation">[</span><span class="hljs-number">1</span><span class="hljs-punctuation">,</span><span class="hljs-number">0</span><span class="hljs-punctuation">,</span><span class="hljs-number">2</span><span class="hljs-punctuation">]</span><span class="hljs-punctuation">,</span><span class="hljs-attr">&quot;log_dirs&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-punctuation">[</span><span class="hljs-string">&quot;any&quot;</span><span class="hljs-punctuation">,</span><span class="hljs-string">&quot;any&quot;</span><span class="hljs-punctuation">,</span><span class="hljs-string">&quot;any&quot;</span><span class="hljs-punctuation">]</span><span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">]</span><span class="hljs-punctuation">&#125;</span><br><br>Proposed partition reassignment configuration<br><span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;version&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-number">1</span><span class="hljs-punctuation">,</span><span class="hljs-attr">&quot;partitions&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-punctuation">[</span><span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;topic&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-string">&quot;first&quot;</span><span class="hljs-punctuation">,</span><span class="hljs-attr">&quot;partition&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-number">0</span><span class="hljs-punctuation">,</span><span class="hljs-attr">&quot;replicas&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-punctuation">[</span><span class="hljs-number">2</span><span class="hljs-punctuation">,</span><span class="hljs-number">3</span><span class="hljs-punctuation">,</span><span class="hljs-number">0</span><span class="hljs-punctuation">]</span><span class="hljs-punctuation">,</span><span class="hljs-string">&quot;log_dirs&quot;</span><span class="hljs-punctuation">[</span><span class="hljs-string">&quot;any&quot;</span><span class="hljs-punctuation">,</span><span class="hljs-string">&quot;any&quot;</span><span class="hljs-punctuation">,</span><span class="hljs-string">&quot;any&quot;</span><span class="hljs-punctuation">]</span><span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span><span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;topic&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-string">&quot;first&quot;</span><span class="hljs-punctuation">,</span><span class="hljs-attr">&quot;partition&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-number">1</span><span class="hljs-punctuation">,</span><span class="hljs-attr">&quot;replicas&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-punctuation">[</span><span class="hljs-number">3</span><span class="hljs-punctuation">,</span><span class="hljs-number">0</span><span class="hljs-punctuation">,</span><span class="hljs-number">1</span><span class="hljs-punctuation">]</span><span class="hljs-punctuation">,</span><span class="hljs-attr">&quot;log_dirs&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-punctuation">[</span><span class="hljs-string">&quot;any&quot;</span><span class="hljs-punctuation">,</span><span class="hljs-string">&quot;any&quot;</span><span class="hljs-punctuation">,</span><span class="hljs-string">&quot;any&quot;</span><span class="hljs-punctuation">]</span><span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span><span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;topic&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-string">&quot;first&quot;</span><span class="hljs-punctuation">,</span><span class="hljs-attr">&quot;partition&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-number">2</span><span class="hljs-punctuation">,</span><span class="hljs-attr">&quot;replicas&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-punctuation">[</span><span class="hljs-number">0</span><span class="hljs-punctuation">,</span><span class="hljs-number">1</span><span class="hljs-punctuation">,</span><span class="hljs-number">2</span><span class="hljs-punctuation">]</span><span class="hljs-punctuation">,</span><span class="hljs-attr">&quot;log_dirs&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-punctuation">[</span><span class="hljs-string">&quot;any&quot;</span><span class="hljs-punctuation">,</span><span class="hljs-string">&quot;any&quot;</span><span class="hljs-punctuation">,</span><span class="hljs-string">&quot;any&quot;</span><span class="hljs-punctuation">]</span><span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">]</span><span class="hljs-punctuation">&#125;</span><br></code></pre></td></tr></table></figure><p class="note note-primary">创建副本存储计划</p><p>将合适的计划复制到一个文件中</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">vim increase-replication-factor.json<br></code></pre></td></tr></table></figure><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs json"><span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;version&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-number">1</span><span class="hljs-punctuation">,</span><span class="hljs-attr">&quot;partitions&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-punctuation">[</span><span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;topic&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-string">&quot;first&quot;</span><span class="hljs-punctuation">,</span><span class="hljs-attr">&quot;partition&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-number">0</span><span class="hljs-punctuation">,</span><span class="hljs-attr">&quot;replicas&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-punctuation">[</span><span class="hljs-number">2</span><span class="hljs-punctuation">,</span><span class="hljs-number">3</span><span class="hljs-punctuation">,</span><span class="hljs-number">0</span><span class="hljs-punctuation">]</span><span class="hljs-punctuation">,</span><span class="hljs-attr">&quot;log_dirs&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-punctuation">[</span><span class="hljs-string">&quot;any&quot;</span><span class="hljs-punctuation">,</span><span class="hljs-string">&quot;any&quot;</span><span class="hljs-punctuation">,</span><span class="hljs-string">&quot;any&quot;</span><span class="hljs-punctuation">]</span><span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span><span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;topic&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-string">&quot;first&quot;</span><span class="hljs-punctuation">,</span><span class="hljs-attr">&quot;partition&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-number">1</span><span class="hljs-punctuation">,</span><span class="hljs-attr">&quot;replicas&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-punctuation">[</span><span class="hljs-number">3</span><span class="hljs-punctuation">,</span><span class="hljs-number">0</span><span class="hljs-punctuation">,</span><span class="hljs-number">1</span><span class="hljs-punctuation">]</span><span class="hljs-punctuation">,</span><span class="hljs-attr">&quot;log_dirs&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-punctuation">[</span><span class="hljs-string">&quot;any&quot;</span><span class="hljs-punctuation">,</span><span class="hljs-string">&quot;any&quot;</span><span class="hljs-punctuation">,</span><span class="hljs-string">&quot;any&quot;</span><span class="hljs-punctuation">]</span><span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span><span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;topic&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-string">&quot;first&quot;</span><span class="hljs-punctuation">,</span><span class="hljs-attr">&quot;partition&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-number">2</span><span class="hljs-punctuation">,</span><span class="hljs-attr">&quot;replicas&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-punctuation">[</span><span class="hljs-number">0</span><span class="hljs-punctuation">,</span><span class="hljs-number">1</span><span class="hljs-punctuation">,</span><span class="hljs-number">2</span><span class="hljs-punctuation">]</span><span class="hljs-punctuation">,</span><span class="hljs-attr">&quot;log_dirs&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-punctuation">[</span><span class="hljs-string">&quot;any&quot;</span><span class="hljs-punctuation">,</span><span class="hljs-string">&quot;any&quot;</span><span class="hljs-punctuation">,</span><span class="hljs-string">&quot;any&quot;</span><span class="hljs-punctuation">]</span><span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">]</span><span class="hljs-punctuation">&#125;</span><br></code></pre></td></tr></table></figure><p class="note note-primary">执行副本存储计划</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">bin/kafka-reassign-partitions.sh --bootstrap-server localhost:9092 --reassignment-json-file increase-replication-factor.json --execute<br></code></pre></td></tr></table></figure><p class="note note-primary">验证副本存储计划</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">bin/kafka-reassign-partitions.sh --bootstrap-server localhost:9092 --reassignment-json-file increase-replication-factor.json --verify<br></code></pre></td></tr></table></figure><h3 id="退役旧节点"><a class="markdownIt-Anchor" href="#退役旧节点"></a> 退役旧节点</h3><p>上文服役了四台节点，现在假设要退役BrokerId为3的节点，操作和服役新节点流程差不多，只不过生成负载均衡计划的 <code>--broker-list</code>变成 <code>&quot;0,1,2&quot;</code>即可</p><h3 id="kafka副本"><a class="markdownIt-Anchor" href="#kafka副本"></a> Kafka副本</h3><ul><li><strong><code>数据可靠性和高可用性</code></strong>：Kafka 副本确保当 Leader 副本宕机时，Follower 可以被选举为新的 Leader，保证服务可用性。</li><li><strong><code>副本数量考量</code></strong>：<ul><li><strong>默认设置</strong>：Kafka 的默认副本数为 1。</li><li><strong>生产推荐</strong>：在生产环境中，建议将副本数配置为 2 或更多，以确保数据冗余和可靠性。</li><li><strong>权衡</strong>：尽管增加的副本数会导致更多的磁盘使用和网络传输，但选择合适的副本数量是在可靠性和资源使用之间的一个权衡。</li></ul></li><li><strong><code>副本角色</code></strong>：<ul><li><strong>Leader</strong>：对于每个分区，Leader 负责处理所有的读写请求。</li><li><strong>Follower</strong>：Follower 从 Leader 同步数据，并作为备份和故障转移角色。</li></ul></li></ul><p class="note note-primary">AR</p><p>Kafka 分区中的所有副本被统称为 AR（All Replicas）。</p><p>AR = ISR + OSR</p><p class="note note-primary">ISR</p><p><img src="/2022/02/24/Kafka/lsr.png" srcset="/img/loading.gif" lazyload alt></p><ul><li>ISR 是 Kafka 分区中与 Leader 副本保持同步的 Follower 副本集合。</li><li>如果 Follower 副本长时间未向 Leader 发送同步请求或同步数据，则它会从 ISR 中被移除。这个“长时间”是由 <code>replica.lag.time.max.ms</code> 参数控制的，默认值为 30 秒。</li><li>当 Leader 副本发生故障时，一个新的 Leader 将从 ISR 中被选举。</li></ul><p>被踢出ISR列表的副本一般有如下原因：</p><ul><li><code>慢副本</code>：<ul><li><code>描述</code>：Follower 副本在一定的时间段内无法追赶 Leader 的速度。</li><li><code>原因</code>：常见原因是 I/O 瓶颈，导致 Follower 向 Leader 追加复制的消息速度低于从 Leader 拉取的速度。</li></ul></li><li><code>卡住的副本</code>：<ul><li><code>描述</code>：Follower 副本在一定的时间段内停止从 Leader 拉取请求。</li><li><code>原因</code>：可能是由于 GC 暂停、Follower 故障或者 Follower 进程死亡。</li></ul></li><li><code>新启动副本</code>：<ul><li><code>描述</code>：当为主题增加副本时，新的 Follower 副本最初不会在 ISR 中。</li><li><code>原因</code>：一旦它们完全追赶了 Leader 的日志，将加入ISR</li></ul></li></ul><p class="note note-primary">OSR</p><p>OSR 代表与 Leader 副本同步时存在较大延迟的 Follower 副本。</p><ul><li><p><code>从 ISR 到 OSR</code>：</p><p>触发条件：在 <code>replica.lag.time.max.ms</code> 时间范围内，若 Follower 的 LEO (Log End Offset) 没有追赶上该 partition 的 HW (High Watermark)，则该 Follower 被移至 OSR 列表。</p></li><li><p><code>从 OSR 到 ISR</code>：</p><p>触发条件：当 OSR 列表中的 Follower 的 LEO 同步并追赶上该 partition 的 HW 时，该 Follower 重新被加入到 ISR 列表。</p></li></ul><h3 id="leader选举流程"><a class="markdownIt-Anchor" href="#leader选举流程"></a> Leader选举流程</h3><p><img src="/2022/02/24/Kafka/Broker%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B.png" srcset="/img/loading.gif" lazyload alt></p><ol><li><strong><code>Broker 注册</code></strong>:<ul><li>当 Kafka Broker 启动后，它向 ZooKeeper 的 <code>/brokers/ids</code> 路径注册其当前节点 ID。</li></ul></li><li><strong><code>Controller 节点选举</code></strong>：<ul><li>在所有的Broker中选举一个Controller节点。Controller节点负责管理所有Partition的Leader选举。</li><li>第一个在 Zookerper 成功创建 <code>/controller</code> node的 Broker 将成为 Controller。</li></ul></li><li><strong><code>Controller 监听节点上下线</code></strong>：<ul><li>选举出来的 Controller 会监听 <code>/brokers</code> 路径的变化。</li></ul></li><li><strong><code>Controller 选举 Leader</code></strong>：<ul><li>Controller 根据特定规则来决定 Partition 的 Leader。</li><li>选举规则是：在 ISR 列表中处于活跃状态，且按照 AR 列表的顺序排列。例如：如果 AR 是 [1,0,2] 且 ISR 是 [1,0,2]，那么 Leader 就会按照 1, 0, 2 的顺序来轮询选举。</li></ul></li><li><strong><code>选举结果上报 ZooKeeper</code></strong>：<ul><li>Controller 将选举的结果更新到 ZooKeeper 的 <code>/brokers/topics/xxx/partitions/xxx</code> 路径。</li></ul></li><li><strong><code>Broker 同步</code></strong>：<ul><li>其他 Broker 会从 ZooKeeper 的相应路径同步选举结果。</li></ul></li><li><strong><code>Leader 宕机处理</code></strong>：<ul><li>假设 Broker1 中的 Leader 宕机了。</li><li>Controller 会监听到这个节点的变化。</li><li>Controller 获取该 Partition 的 ISR 列表。</li><li>根据上述选举规则，选举新的 Leader。例如，如果 0 在 AR 列表中的位置较前，且在 ISR 中处于活跃状态，它就会被选为新的 Leader。</li><li>Controller 更新新的 Leader 和 ISR 列表。</li></ul></li></ol><p class="note note-primary">验证</p><p><img src="/2022/02/24/Kafka/%E9%AA%8C%E8%AF%811.png" srcset="/img/loading.gif" lazyload alt></p><p>此时如果停止Broker为3的节点，那么Partition2的Leader将替换为1</p><p><img src="/2022/02/24/Kafka/%E9%AA%8C%E8%AF%812.png" srcset="/img/loading.gif" lazyload alt></p><p>重新启动Broker为3的节点，经过一段时间的 <a href="#leader-partition%E8%87%AA%E5%8A%A8%E5%B9%B3%E8%A1%A1">自动平衡</a> 后，Leader将重新替换为3</p><p><img src="/2022/02/24/Kafka/%E9%AA%8C%E8%AF%813.png" srcset="/img/loading.gif" lazyload alt></p><h3 id="故障处理"><a class="markdownIt-Anchor" href="#故障处理"></a> 故障处理</h3><p><img src="/2022/02/24/Kafka/leo.png" srcset="/img/loading.gif" lazyload alt></p><ul><li><strong><code>HW</code></strong>: （High Watermark）<ul><li>俗称高水位，代表消费者可以安全读取的最后一条消息的位置。</li><li>因为HW之前的消息已被所有ISR节点确认过。</li><li><code>作用</code>：确保消费者读取的是持久化的消息，保证数据一致性。</li><li><code>移动</code>：当 ISR 副本都确认某个消息，HW往前移动。</li></ul></li><li><strong><code>LEO</code></strong>: （Log End Offset）<ul><li>代表下条消息写入的位置。</li><li><code>作用</code>：这个标识用于从 Leader 拉取要同步的数据。</li></ul></li></ul><p>如图所示，它代表一个日志文件，这个日志文件中有 8 条消息，第一条消息的offset（LogStartOffset）为0，最后一条消息的offset为7，offset为8代表下一条待写入的消息。分区的HW为5，表示消费者只能拉取到offset在0至4之间的消息，而offset为5的消息对消费者而言是不可见的，是为了保证数据存储的一致性。</p><p class="note note-primary">Follower 故障</p><ul><li>当 Follower 发生故障，它会被临时移出 ISR 列表。</li><li>在此期间，Leader 继续处理新的数据写入请求，而正常的 Follower 也继续从 Leader 同步数据。</li><li>一旦故障的 Follower 恢复，它首先会读取本地磁盘上记录的最近的 HW，并将 log 文件中高于此 HW 的记录截断。</li><li>该 Follower 从 HW 开始向 Leader 请求同步，此时它位于 OSR 列表中。</li><li><span class="green-line">一旦 Follower 的 LEO 达到或超过该 Partition 的 HW，意味着 Follower 已经追赶上 Leader，它将重新被加入 ISR 列表。</span></li></ul><p class="note note-primary">Leader 故障</p><ul><li>当 Leader 发生故障，新的 Leader 会从 ISR 中选举出来。选举基于 AR 列表的排序，并确保选举的是 ISR 中的成员。</li><li>为确保数据一致性，所有 Follower（包括新选举的 Leader）会截断其 log 文件中超出 HW 的部分。</li><li>随后，这些 Follower 会从新的 Leader 开始同步数据。</li></ul><p>只能保证副本之间数据的一致性，消费数据一致性，不能保证数据是否丢失或不重复</p><h3 id="分区副本分配"><a class="markdownIt-Anchor" href="#分区副本分配"></a> 分区副本分配</h3><p>如果 Kafka 服务器只有 4 个节点，那么设置 Kafka 的分区数大于服务器台数，在 kafka底层如何分配存储副本呢</p><p>创建 16 分区，3 个副本</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs shell">hadoop102Topic: second4 Partition: 0 Leader: 0 Replicas: 0,1,2 Isr: 0,1,2<br>Topic: second4 Partition: 1 Leader: 1 Replicas: 1,2,3 Isr: 1,2,3<br>Topic: second4 Partition: 2 Leader: 2 Replicas: 2,3,0 Isr: 2,3,0<br>Topic: second4 Partition: 3 Leader: 3 Replicas: 3,0,1 Isr: 3,0,1<br><br>Topic: second4 Partition: 4 Leader: 0 Replicas: 0,2,3 Isr: 0,2,3<br>Topic: second4 Partition: 5 Leader: 1 Replicas: 1,3,0 Isr: 1,3,0<br>Topic: second4 Partition: 6 Leader: 2 Replicas: 2,0,1 Isr: 2,0,1<br>Topic: second4 Partition: 7 Leader: 3 Replicas: 3,1,2 Isr: 3,1,2<br><br>Topic: second4 Partition: 8 Leader: 0 Replicas: 0,3,1 Isr: 0,3,1<br>Topic: second4 Partition: 9 Leader: 1 Replicas: 1,0,2 Isr: 1,0,2<br>Topic: second4 Partition: 10 Leader: 2 Replicas: 2,1,3 Isr: 2,1,3<br>Topic: second4 Partition: 11 Leader: 3 Replicas: 3,2,0 Isr: 3,2,0<br><br>Topic: second4 Partition: 12 Leader: 0 Replicas: 0,1,2 Isr: 0,1,2<br>Topic: second4 Partition: 13 Leader: 1 Replicas: 1,2,3 Isr: 1,2,3<br>Topic: second4 Partition: 14 Leader: 2 Replicas: 2,3,0 Isr: 2,3,0<br>Topic: second4 Partition: 15 Leader: 3 Replicas: 3,0,1 Isr: 3,0,1<br></code></pre></td></tr></table></figure><p><img src="/2022/02/24/Kafka/%E5%88%86%E5%8C%BA%E5%89%AF%E6%9C%AC%E5%88%86%E9%85%8D.png" srcset="/img/loading.gif" lazyload alt></p><ol><li><strong><code>均匀分布 Leader</code></strong>: Kafka 会尽量确保每个 broker 上的 leader partition 数量均匀分布，以便平衡 broker 之间的读写负载。</li><li><strong><code>错开 Leader</code></strong>: Kafka 在分配 leader 时，会尽量错开 broker，确保不同 partition 的 leader 分布在不同的 broker 上，以提升集群的容错性。</li><li><strong><code>分散副本</code></strong>: Kafka 会尽量确保一个 partition 的所有副本分散在不同的 broker 上，避免因单个 broker 宕机导致的数据不可用。</li></ol><h3 id="手动调整分区副本分配"><a class="markdownIt-Anchor" href="#手动调整分区副本分配"></a> 手动调整分区副本分配</h3><p>在生产环境中，每台服务器的配置和性能不一致，但是Kafka只会根据自己的代码规则创建对应的分区副本，就会导致个别服务器存储压力较大。所有需要手动调整分区副本的存储</p><p><img src="/2022/02/24/Kafka/%E5%89%AF%E6%9C%AC%E8%B0%83%E6%95%B4.png" srcset="/img/loading.gif" lazyload alt></p><p>创建一个新的topic，4个分区，两个副本，名称为three。将该topic的所有副本都存储到broker0和broker1两台服务器上</p><p class="note note-primary">创建一个新的Topic</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">bin/kafka-topics.sh --bootstrap-server localhost:9092 --create --partitions 4 --replication-factor 2 --topic three<br></code></pre></td></tr></table></figure><p class="note note-primary">查看分区副本存储情况</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">bin/kafka-topics.sh --bootstrap-server localhost:9092 --describe --topic three<br></code></pre></td></tr></table></figure><p class="note note-primary">创建副本存储计划</p><p>所有副本都指定存储在 broker0、broker1 中</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">vim increase-replication-factor.json<br></code></pre></td></tr></table></figure><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><code class="hljs json"><span class="hljs-punctuation">&#123;</span><br>    <span class="hljs-attr">&quot;version&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-number">1</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">&quot;partitions&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-punctuation">[</span><br>        <span class="hljs-punctuation">&#123;</span><br>            <span class="hljs-attr">&quot;topic&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-string">&quot;three&quot;</span><span class="hljs-punctuation">,</span><br>            <span class="hljs-attr">&quot;partition&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-number">0</span><span class="hljs-punctuation">,</span><br>            <span class="hljs-attr">&quot;replicas&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-punctuation">[</span><br>                <span class="hljs-number">0</span><span class="hljs-punctuation">,</span><br>                <span class="hljs-number">1</span><br>            <span class="hljs-punctuation">]</span><br>        <span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span><br>        <span class="hljs-punctuation">&#123;</span><br>            <span class="hljs-attr">&quot;topic&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-string">&quot;three&quot;</span><span class="hljs-punctuation">,</span><br>            <span class="hljs-attr">&quot;partition&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-number">1</span><span class="hljs-punctuation">,</span><br>            <span class="hljs-attr">&quot;replicas&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-punctuation">[</span><br>                <span class="hljs-number">0</span><span class="hljs-punctuation">,</span><br>                <span class="hljs-number">1</span><br>            <span class="hljs-punctuation">]</span><br>        <span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span><br>        <span class="hljs-punctuation">&#123;</span><br>            <span class="hljs-attr">&quot;topic&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-string">&quot;three&quot;</span><span class="hljs-punctuation">,</span><br>            <span class="hljs-attr">&quot;partition&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-number">2</span><span class="hljs-punctuation">,</span><br>            <span class="hljs-attr">&quot;replicas&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-punctuation">[</span><br>                <span class="hljs-number">1</span><span class="hljs-punctuation">,</span><br>                <span class="hljs-number">0</span><br>            <span class="hljs-punctuation">]</span><br>        <span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span><br>        <span class="hljs-punctuation">&#123;</span><br>            <span class="hljs-attr">&quot;topic&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-string">&quot;three&quot;</span><span class="hljs-punctuation">,</span><br>            <span class="hljs-attr">&quot;partition&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-number">3</span><span class="hljs-punctuation">,</span><br>            <span class="hljs-attr">&quot;replicas&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-punctuation">[</span><br>                <span class="hljs-number">1</span><span class="hljs-punctuation">,</span><br>                <span class="hljs-number">0</span><br>            <span class="hljs-punctuation">]</span><br>        <span class="hljs-punctuation">&#125;</span><br>    <span class="hljs-punctuation">]</span><br><span class="hljs-punctuation">&#125;</span><br></code></pre></td></tr></table></figure><p class="note note-primary">执行副本存储计划</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">bin/kafka-reassign-partitions.sh --bootstrap-server localhost:9092 --reassignment-json-file increase-replication-factor.json --execute<br></code></pre></td></tr></table></figure><p class="note note-primary">验证副本存储计划</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">bin/kafka-reassign-partitions.sh --bootstrap-server localhost:9092 --reassignment-json-file increase-replication-factor.json --verify<br></code></pre></td></tr></table></figure><h3 id="leader-partition自动平衡"><a class="markdownIt-Anchor" href="#leader-partition自动平衡"></a> Leader Partition自动平衡</h3><p>在正常情况下，Kafka 会自动将 leader partition 均匀分散在各个 broker 上，以保证每台机器的读写吞吐量均衡。然而，当一些 broker 宕机时，leader partition 可能会过度集中在少数活跃的 broker 上，导致这些 broker 承受较高的读写请求压力。与此同时，重启后的 broker 将作为 follower partition，它们主要负责与 leader 同步数据，处理的读写请求相对较少，从而导致集群的负载不均衡。</p><p class="note note-primary">自动平衡参数</p><table><thead><tr><th>参数名称</th><th>描述</th></tr></thead><tbody><tr><td><code>auto.leader.rebalance.enable</code></td><td>控制是否自动进行 Leader Partition 平衡。默认值为 <code>true</code>。尽管平衡可以增强集群的健壮性，但在生产环境中，频繁的 leader 重选举可能会带来性能影响。因此，建议根据实际情况考虑将其设置为 <code>false</code> 或调整其他相关参数来减少影响。</td></tr><tr><td><code>leader.imbalance.per.broker.percentage</code></td><td>定义每个 broker 上允许的不平衡 leader 比例。默认值为 10%。当任意 broker 的不平衡 leader 比例超过此值时，控制器会触发 leader 重新分配过程。</td></tr><tr><td><code>leader.imbalance.check.interval.seconds</code></td><td>控制检查 leader 分布是否平衡的时间间隔。默认值为 300 秒。</td></tr></tbody></table><p class="note note-primary">平衡算法</p><p><img src="/2022/02/24/Kafka/partition%E5%B9%B3%E8%A1%A1%E7%AE%97%E6%B3%95.png" srcset="/img/loading.gif" lazyload alt></p><p>分区2的AR优先副本是0节点，但是0节点却不是Leader节点，所以不平衡数加1，AR副本总数是4，所以broker0节点不平衡率为1/4&gt;10%，需要再平衡</p><h3 id="增加副本因子"><a class="markdownIt-Anchor" href="#增加副本因子"></a> 增加副本因子</h3><p>先创建一个三个分区一个副本的Topic</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">sh kafka-topics.sh --bootstrap-server localhost:9092 --create --topic test --partitions 3 --replication-factor 1<br></code></pre></td></tr></table></figure><p>查看Topic</p><p><img src="/2022/02/24/Kafka/%E5%89%AF%E6%9C%AC%E5%9B%A0%E5%AD%90.png" srcset="/img/loading.gif" lazyload alt></p><p class="note note-primary">创建存储计划</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">vim increase-replication-factor.json<br></code></pre></td></tr></table></figure><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs json"><span class="hljs-punctuation">&#123;</span><br>    <span class="hljs-attr">&quot;version&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-number">1</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">&quot;partitions&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-punctuation">[</span><br>        <span class="hljs-punctuation">&#123;</span><br>            <span class="hljs-attr">&quot;topic&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-string">&quot;test&quot;</span><span class="hljs-punctuation">,</span><br>            <span class="hljs-attr">&quot;partition&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-number">0</span><span class="hljs-punctuation">,</span><br>            <span class="hljs-attr">&quot;replicas&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-punctuation">[</span><br>                <span class="hljs-number">1</span><span class="hljs-punctuation">,</span><br>                <span class="hljs-number">2</span><span class="hljs-punctuation">,</span><br>                <span class="hljs-number">3</span><br>            <span class="hljs-punctuation">]</span><br>        <span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span><br>        <span class="hljs-punctuation">&#123;</span><br>            <span class="hljs-attr">&quot;topic&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-string">&quot;test&quot;</span><span class="hljs-punctuation">,</span><br>            <span class="hljs-attr">&quot;partition&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-number">1</span><span class="hljs-punctuation">,</span><br>            <span class="hljs-attr">&quot;replicas&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-punctuation">[</span><br>                <span class="hljs-number">1</span><span class="hljs-punctuation">,</span><br>                <span class="hljs-number">2</span><span class="hljs-punctuation">,</span><br>                <span class="hljs-number">3</span><br>            <span class="hljs-punctuation">]</span><br>        <span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span><br>        <span class="hljs-punctuation">&#123;</span><br>            <span class="hljs-attr">&quot;topic&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-string">&quot;test&quot;</span><span class="hljs-punctuation">,</span><br>            <span class="hljs-attr">&quot;partition&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-number">2</span><span class="hljs-punctuation">,</span><br>            <span class="hljs-attr">&quot;replicas&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-punctuation">[</span><br>                <span class="hljs-number">1</span><span class="hljs-punctuation">,</span><br>                <span class="hljs-number">2</span><span class="hljs-punctuation">,</span><br>                <span class="hljs-number">3</span><br>            <span class="hljs-punctuation">]</span><br>        <span class="hljs-punctuation">&#125;</span><br>    <span class="hljs-punctuation">]</span><br><span class="hljs-punctuation">&#125;</span><br></code></pre></td></tr></table></figure><p class="note note-primary">执行副本存储计划</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">sh kafka-reassign-partitions.sh --bootstrap-server localhost:9092 --reassignment-json-file increase-replication-factor.json --execute<br></code></pre></td></tr></table></figure><p><img src="/2022/02/24/Kafka/%E5%89%AF%E6%9C%AC%E5%9B%A0%E5%AD%902.png" srcset="/img/loading.gif" lazyload alt></p><h3 id="kafka文件存储机制"><a class="markdownIt-Anchor" href="#kafka文件存储机制"></a> Kafka文件存储机制</h3><p><img src="/2022/02/24/Kafka/%E5%AD%98%E5%82%A8%E7%BB%93%E6%9E%84.png" srcset="/img/loading.gif" lazyload alt></p><p><span class="green-line">每个Topic包含一个或多个Partition，每个Partition在物理存储层面对应一个文件夹。在这个文件夹下存储了Partition的 <code>数据</code> 和 <code>索引</code> 文件</span>。Partition内部的消息是有序的，但不保证多个Partition之间消息的顺序。</p><ul><li>Topic是一类消息的集合，每条消息都需要指定一个Topic。在物理层面上，一个Topic会被划分为一个或多个Partition，每个Partition会有多个副本分布在不同的Broker中。</li><li>Partition在存储层面是由一系列 <code>append log</code> 文件组成，发布到此Partition的消息会被追加到log文件的尾部。这种 <code>顺序写入</code> 磁盘的方式较随机写入效率更高。每条消息在log文件中的位置由一个长整数型的offset（偏移量）表示，该offset唯一标识了一条消息。</li><li>每个Partition的log文件会被分割成多个 <code>Segment</code>，每个Segment大小为1GB，并由 <code>log</code> 文件和 <code>index</code> 文件组成。</li><li>消费者保存的唯一元数据是 <code>offset</code> 值，该值由消费者完全控制，保存在一个特殊的Topic或Zookeeper中。维度是consumer-group.topic.partition。</li><li>不同于传统的消息队列，Kafka集群会保留所有消息，而不是在消费后立即删除，以优化IO操作。由于磁盘空间的限制，Kafka不可能永久保留所有消息，消息的保存期限可以通过配置来指定。</li></ul><p class="note note-primary">消息索引过程</p><p><img src="/2022/02/24/Kafka/%E5%88%86%E5%8C%BA%E7%9B%AE%E5%BD%95.png" srcset="/img/loading.gif" lazyload alt></p><p><img src="/2022/02/24/Kafka/%E5%AD%98%E5%82%A8%E6%95%B0%E6%8D%AE.png" srcset="/img/loading.gif" lazyload alt></p><p><code>log</code> 文件和 <code>index</code> 文件的命名规则都是基于该文件中第一条消息的 <code>offset</code>。具体地，文件名是这个起始 <code>offset</code> 的长整数形式。这种命名方式便于在多个 <code>log</code> 文件中快速定位到包含特定 <code>offset</code> 的消息的文件。</p><p>稀疏索引：每当 <code>log</code> 文件增加了约4KB的数据，<code>index</code> 文件就会为这部分数据生成一条索引记录，这是稀疏的方式，而不是为 <code>log</code> 文件中的每一条消息都生成索引。</p><p><img src="/2022/02/24/Kafka/%E6%95%B0%E6%8D%AE%E7%B4%A2%E5%BC%95%E8%BF%87%E7%A8%8B.png" srcset="/img/loading.gif" lazyload alt></p><p>索引过程：</p><ul><li>通过偏移量定位到Partition目录下对应的index文件（文件名也是索引）</li><li>通过index文件定位到指定offset消息的log文件偏移量（index是相对偏移量，可降低文件大小）</li><li>通过index文件提供的偏移量读取log文件进行遍历（最坏情况下至多遍历4KB）</li></ul><p class="note note-primary">通过工具查看文件内容</p><p><img src="/2022/02/24/Kafka/%E6%97%A5%E5%BF%97%E6%9F%A5%E7%9C%8B.png" srcset="/img/loading.gif" lazyload alt></p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh">sh kafka-run-class.sh kafka.tools.DumpLogSegments --files ./00000000000000000000.<span class="hljs-built_in">log</span><br></code></pre></td></tr></table></figure><h3 id="文件清理策略"><a class="markdownIt-Anchor" href="#文件清理策略"></a> 文件清理策略</h3><p>Kafka 中默认的日志保存时间为 7 天，可以通过调整如下参数修改保存时间</p><table><thead><tr><th>参数</th><th>说明</th></tr></thead><tbody><tr><td><code>log.retention.hours</code></td><td>最低优先级小时，默认 7 天(168)</td></tr><tr><td><code>log.retention.minutes</code></td><td>分钟级别</td></tr><tr><td><code>log.retention.ms</code></td><td>最高优先级毫秒</td></tr><tr><td><code>log.retention.check.interval.ms</code></td><td>负责设置检查周期，默认 5 分钟</td></tr><tr><td><code>log.retention.bytes</code></td><td>所有日志段的总大小上限。一旦达到这个大小，最旧的日志段将被删除</td></tr></tbody></table><p>Kafka 中提供的日志清理策略有 <code>delete(删除)</code> 和 <code>compact（压缩）</code> 两种</p><p class="note note-primary">delete</p><p><code>log.cleanup.policy = delete</code> 所有数据启用删除策略</p><ul><li><code>基于时间</code>：默认打开。使用每个 Segment 中最大时间戳作为该日志段的时间戳。当消息在Kafka中存在的时间超过一定阈值时，Kafka将删除旧的日志段以释放磁盘空间。</li><li><code>基于大小</code>：当所有日志段的总大小超过设置的限制时，Kafka删除最早的日志段，以控制磁盘使用。</li></ul><p><img src="/2022/02/24/Kafka/%E6%95%B0%E6%8D%AE%E8%BF%87%E6%9C%9F.png" srcset="/img/loading.gif" lazyload alt></p><p>如果一个 segment 中有一部分数据过期，一部分没有过期，怎么处理?</p><p>等待 timeindex 文件的所有数据过期直接删除整个文件</p><p class="note note-primary">compact</p><p>compact日志压缩：对于相同key的不同value值，只保留最后一个版本</p><p><code>log.cleanup.policy = compact</code> 所有数据启用压缩策略</p><p><img src="/2022/02/24/Kafka/%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9.png" srcset="/img/loading.gif" lazyload alt></p><p>压缩后的offset可能是不连续的，比如上图中没有6，当从这些offset消费消息时，将会拿到比这个offset大的offset对应的消息，实际上会拿到offset为7的消息，并从这个位置开始消费。</p><p>这种策略适合一些特殊的场景，比如数据同步，同一条数据只保留最新版本。</p><h3 id="数据高效读写"><a class="markdownIt-Anchor" href="#数据高效读写"></a> 数据高效读写</h3><p class="note note-primary">集群，分片技术</p><ul><li>Kafka本身就是一个支持分布式的集群系统。</li><li>Topic采用分区技术，将多个分区分布在不同的Broker上。</li></ul><p class="note note-primary">顺序写盘</p><p>Kafka的producer负责生产数据，并将其顺序地追加到log文件的尾部。这种顺序写的方式在性能上有明显优势。据官方数据显示，相同的磁盘在顺序写的情况下能达到600M/s的速度，而随机写的速度仅为100K/s。这种性能差异主要是因为顺序写减少了磁头寻址的时间，而这与磁盘的机械结构有直接关系。</p><p class="note note-primary">稀疏索引</p><p>在读取数据时利用稀疏索引，使得在大量的log文件中能迅速定位到数据对应的文件和该数据在文件中的偏移量位置。</p><p class="note note-primary">页缓存和零拷贝</p><p><img src="/2022/02/24/Kafka/%E9%9B%B6%E6%8B%B7%E8%B4%9D.png" srcset="/img/loading.gif" lazyload alt></p><ul><li><p><strong><code>PageCache（页缓存）</code></strong>：Kafka高度依赖操作系统提供的PageCache机制。当发生写操作时，数据首先被写入到PageCache中，而不是直接写入磁盘。读操作先从PageCache中搜索数据；如果PageCache中没有所需的数据，系统才会从磁盘读取。简言之，PageCache将大部分的空闲内存利用作为磁盘缓存。</p></li><li><p><strong><a href="/2022/05/29/Linux/#%E9%9B%B6%E6%8B%B7%E8%B4%9D">零拷贝</a></strong>：数据不会从内核空间拷贝到用户空间。实际上，在数据传输时，数据直接从内核空间复制到网卡，避免了不必要的数据复制操作。</p></li></ul><p class="note note-primary">Producer的零拷贝</p><p><img src="/2022/02/24/Kafka/%E9%9B%B6%E6%8B%B7%E8%B4%9D1.png" srcset="/img/loading.gif" lazyload alt></p><ol><li><p><strong><code>消息存储</code></strong>：当你使用Kafka Producer发送消息时，消息首先被放入 <code>RecordAccumulator</code>。这是一个内部缓冲区，它负责批处理和组织消息，以便于高效发送。</p></li><li><p><strong><code>准备发送</code></strong>：Kafka的 <code>Sender</code> 线程会从 <code>RecordAccumulator</code> 中提取批量消息。这些消息通常被组织为按目标分区和Broker的顺序。</p></li><li><p><strong><code>零拷贝技术的实现</code></strong>：在传统的数据发送方法中，数据需要从应用程序的用户空间被拷贝到操作系统的内核空间，然后再从内核空间被拷贝到 Socket。这涉及至少两次数据拷贝。但是，使用零拷贝技术，Kafka可以直接从 <code>RecordAccumulator</code>（用户空间）发送数据到 Socket（内核空间），避免中间的拷贝步骤。</p><p>这主要是通过Java的 <code>FileChannel.transferTo</code> 方法实现的。这允许数据从文件或缓冲区直接传输到网络套接字，绕过了额外的用户空间和内核空间之间的数据拷贝。</p></li></ol><p class="note note-primary">Broker的零拷贝</p><p><img src="/2022/02/24/Kafka/%E9%9B%B6%E6%8B%B7%E8%B4%9D2.png" srcset="/img/loading.gif" lazyload alt></p><ul><li><strong><code>PageCache与磁盘交互</code></strong>：当数据被写入Kafka Broker或从Broker读取时，它们首先写入操作系统的 PageCache。这是操作系统为文件系统提供的一个高速缓存。</li><li><strong><code>实现</code></strong>：Kafka使用Java的NIO（非阻塞I/O）来实现这种零拷贝技术。特别是，它使用 <code>FileChannel.transferTo</code> 方法来从 PageCache 中直接将数据拷贝到 Socket。</li></ul><h2 id="kafka消费者"><a class="markdownIt-Anchor" href="#kafka消费者"></a> Kafka消费者</h2><p><img src="/2022/02/24/Kafka/%E6%B6%88%E8%B4%B9%E8%80%85%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B.png" srcset="/img/loading.gif" lazyload alt></p><h3 id="消费者组重要参数"><a class="markdownIt-Anchor" href="#消费者组重要参数"></a> 消费者组重要参数</h3><table><thead><tr><th>参数名称</th><th>描述</th></tr></thead><tbody><tr><td><code>bootstrap.servers</code></td><td>向 Kafka 集群建立初始连接用到的 host/port 列表</td></tr><tr><td><code>key.deserializer</code>和<br><code>value.deserializer</code></td><td>指定接收消息的 key 和 value 的反序列化类型。一定要写全类名</td></tr><tr><td><code>group.id</code></td><td>标记消费者所属的消费者组</td></tr><tr><td><code>enable.auto.commit</code></td><td>默认值为 true，消费者会自动周期性地向服务器提交偏移量</td></tr><tr><td><code>auto.commit.interval.ms</code></td><td>如果设置了 enable.auto.commit 的值为 true， 则该值定义了消费者偏移量向 Kafka 提交的频率，默认 5s</td></tr><tr><td><code>auto.offset.reset</code></td><td>当 Kafka 中没有初始偏移量或当前偏移量在服务器中不存在 （如，数据被删除了），该如何处理？<br><code>earliest</code>：当各分区下有已提交的offset时，从提交的offset开始消费；无提交的offset时，从头开始消费<br><code>latest</code>：默认，当各分区下有已提交的offset时，从提交的offset开始消费；无提交的offset时，消费新产生的该分区下的数据<br><code>none</code>：如果消费组原来的（previous）偏移量不存在，则向消费者抛异常<br><code>anything</code>：向消费者抛异常</td></tr><tr><td><code>offsets.topic.num.partitions</code></td><td>__consumer_offsets 的分区数，默认是 50 个分区</td></tr><tr><td><code>heartbeat.interval.ms</code></td><td>Kafka 消费者和 coordinator 之间的心跳时间，默认 3s。 该条目的值必须小于 <a target="_blank" rel="noopener" href="http://session.timeout.ms">session.timeout.ms</a> ，也不应该高于 <a target="_blank" rel="noopener" href="http://session.timeout.ms">session.timeout.ms</a> 的 1/3</td></tr><tr><td><code>session.timeout.ms</code></td><td>Kafka 消费者和 coordinator 之间连接超时时间，默认 45s。 超过该值，该消费者被移除，消费者组执行再平衡</td></tr><tr><td><code>max.poll.interval.ms</code></td><td>消费者处理消息的最大时长，默认是 5 分钟。超过该值，该 消费者被移除，消费者组执行再平衡</td></tr><tr><td><code>fetch.min.bytes</code></td><td>默认 1 个字节。消费者获取服务器端一批消息最小的字节数</td></tr><tr><td><code>fetch.max.wait.ms</code></td><td>默认 500ms。如果没有从服务器端获取到一批数据的最小字节数。该时间到，仍然会返回数据</td></tr><tr><td><code>fetch.max.bytes</code></td><td>默认 Default: 52428800（50 m）。消费者获取服务器端一批 消息最大的字节数。如果服务器端一批次的数据大于该值 （50m）仍然可以拉取回来这批数据，因此，这不是一个绝 对最大值。一批次的大小受 message.max.bytes （broker config）or max.message.bytes （topic config）影响</td></tr><tr><td><code>max.poll.records</code></td><td>一次 poll 拉取数据返回消息的最大条数，默认是 500 条。</td></tr></tbody></table><h3 id="消费者组原理"><a class="markdownIt-Anchor" href="#消费者组原理"></a> 消费者组原理</h3><p><img src="/2022/02/24/Kafka/%E6%B6%88%E8%B4%B9%E8%80%85%E7%BB%84%E8%AF%A6%E8%A7%A3.png" srcset="/img/loading.gif" lazyload alt></p><p><strong><code>Consumer Group (CG)</code></strong>: 定义为一组具有相同 <code>groupId</code> 的消费者集合。</p><ul><li>在消费者组内，每个消费者专门负责消费某个或某些分区的数据。这确保了同一消息只会被组内的一个消费者处理。</li><li>不同的消费者组间是独立的，它们的消费活动不会相互干扰。</li><li>每个消费者必须关联到一个消费者组，可以将消费者组视作逻辑上的订阅者。</li><li>为了确保有效的消费，消费者组中的消费者数量不应该超过其所消费 Topic 的分区数量。超过时，部分消费者将会处于空闲状态。</li><li>具有相同 <code>groupId</code> 的消费者自动归入同一消费者组。</li></ul><h3 id="消费者组初始化流程"><a class="markdownIt-Anchor" href="#消费者组初始化流程"></a> 消费者组初始化流程</h3><p><img src="/2022/02/24/Kafka/%E6%B6%88%E8%B4%B9%E8%80%85%E7%BB%84%E5%88%9D%E5%A7%8B%E5%8C%96%E6%B5%81%E7%A8%8B.png" srcset="/img/loading.gif" lazyload alt></p><p class="note note-primary">coordinator 协调器</p><p><strong><code>作用</code></strong></p><ol><li><strong><code>消费者组管理</code></strong>:<ul><li><strong><code>再平衡</code></strong>: 如果消费者组内的成员关系发生变化（例如，新消费者加入、现有消费者离开或失败），coordinator 负责触发再均衡，重新分配分区给组内的消费者。</li><li><strong><code>offset管理</code></strong>: coordinator 负责记录和存储消费者组内消费者所消费的消息的偏移量，这样消费者在失败后重启时可以从上次的位置继续消费。</li><li><strong><code>心跳检测</code></strong>: 通过消费者发送的心跳，coordinator 能够监控消费者的健康状态。如果在指定时间内没有收到某消费者的心跳，coordinator 会认为该消费者已经死亡并触发再均衡。</li></ul></li><li><strong><code>事务管理</code></strong><ul><li><strong><code>事务状态维护</code></strong>: coordinator 负责维护与生产者事务相关的状态和元数据。</li><li><strong><code>幂等性</code></strong>: 为了确保生产者重试不会导致数据的重复写入，coordinator 与生产者协作确保消息的幂等性。</li><li><strong><code>跨分区的事务原子性</code></strong>: 在涉及多个分区的事务中，coordinator 确保这些分区中的所有写操作要么全部成功，要么全部失败。</li></ul></li><li><strong><code>协调器选择</code></strong>:<ul><li>每个消费者组都有一个与之关联的 coordinator，这个 coordinator 是 Kafka 集群中的某个 broker。Kafka 的设计确保了协调器的负载均衡，每个 broker 都可以成为 coordinator。</li></ul></li></ol><p><strong><code>Coordinator选择</code></strong>：<span class="green-line">coordinator节点选择 = groupId的hashCode值 % __consumer_offsets的分区数</span>。例如，若<code>__consumer_offsets</code>的分区数为50，且groupId的hashCode为1，则<code>1 % 50 = 1</code>。假设第1分区的Leader位于Broker1上，则Broker1将被选为该消费者组的coordinator节点。此消费者组内的所有消费者在提交offset时都会向此协调分区提交。</p><p class="note note-primary">初始化流程</p><ol><li><strong><code>寻找协调器 (Coordinator)</code></strong>:<ul><li>当一个消费者启动并尝试加入消费者组时，它首先需要找到负责其消费者组的协调器。消费者会向任意一个 Broker 发送 <code>FindCoordinator</code> 请求来找到负责其 <code>groupId</code> 的协调器。</li></ul></li><li><strong><code>发送 JoinGroup 请求</code></strong>:<ul><li>消费者找到协调器后，会发送一个 <code>JoinGroup</code> 请求来加入消费者组。</li><li>如果这是一个全新的 <code>groupId</code>（即组内没有任何消费者），则协调器会立即接受此消费者作为组的首个成员。如果消费者组已经存在，消费者需要等待协调器触发重新平衡来加入组。</li></ul></li><li><strong><code>消费者组 Leader 选举</code></strong>:<ul><li>对于消费者组中的所有成员，协调器会选择一个作为 leader。leader 的职责是根据消费者的数量和主题的分区来确定 <code>分区的分配策略</code>。</li><li>由Coordinator自动选举，第一个发出 <code>JoinGroup</code> 的 Broker 当选。</li></ul></li><li><strong><code>分区分配</code></strong>:<ul><li>协调器会向消费者组 Leader 提供组内所有消费者的信息。leader 根据指定的分区分配策略将分区分配给组内的消费者，并将分配的结果返回给协调器。</li></ul></li><li><strong><code>同步消费者</code></strong>:<ul><li>协调器将分区的分配结果发送给组内的所有消费者。这是通过 <code>SyncGroup</code> 请求完成的。</li></ul></li><li><strong><code>开始消费</code></strong>:<ul><li>一旦消费者收到其分配的分区，它会开始从指定的 offset 开始消费消息。</li></ul></li><li><strong><code>维持心跳</code></strong>:<ul><li>为了告诉协调器它仍然是活跃的，并且正在消费消息，消费者会定期发送心跳。如果在指定的会话超时时间内，协调器没有收到来自消费者的心跳，它会认为该消费者已经失败，并可能触发再平衡。</li></ul></li></ol><p class="note note-primary">再平衡</p><ul><li><p><strong><code>心跳与会话超时</code></strong>：消费者确实会向其coordinator发送心跳以表示它仍然活跃。<code>heartbeat.interval.ms</code> 定义了发送心跳的频率，而 <code>session.timeout.ms</code> 定义了协调器等待心跳的时间。如果在 <code>session.timeout.ms</code> （45s）时间内，coordinator没有收到心跳，消费者将被认为已经死亡。</p></li><li><p><strong><code>最大轮询间隔</code></strong>：<code>max.poll.interval.ms</code> (5分钟)定义了消费者可以在两次 <code>poll()</code> 调用之间的最长时间。如果超过此时间，消费者会被认为是不活跃的，并从组中移除，从而可能触发再平衡。</p></li><li><p><strong><code>消费者的加入和离开</code></strong>：消费者的加入和离开确实都可能触发再平衡。频繁的再平衡不仅可能影响Kafka集群的性能，而且可能导致消息处理的延迟。</p></li></ul><h3 id="消费者拉取流程"><a class="markdownIt-Anchor" href="#消费者拉取流程"></a> 消费者拉取流程</h3><p><img src="/2022/02/24/Kafka/%E6%B6%88%E8%B4%B9%E6%B5%81%E7%A8%8B.png" srcset="/img/loading.gif" lazyload alt></p><ol><li><strong><code>初始化</code></strong>：<ul><li>当消费者启动时，它会创建一个<code>NetworkClient</code>实例，消费者与Kafka建立网络连接。</li></ul></li><li><strong><code>查找Coordinator</code></strong>：<ul><li>在发送拉取请求之前，消费者首先需要找到Consumer Group的Coordinator，以便正确跟踪偏移量和管理组成员关系。</li></ul></li><li><strong><code>Partition分配</code></strong>：<ul><li>如果是新的消费者或者消费者组重新平衡后，消费者会通过Coordinator确定自己要消费哪些Topic的哪些分区。</li></ul></li><li><strong><code>发送拉取请求</code></strong>：<ul><li>一旦知道要从哪些分区拉取，消费者会使用<code>sendFetches</code>方法发送请求。请求中指定了Topic、分区和开始的偏移量。</li></ul></li><li><strong><code>Broker处理</code></strong>：<ul><li>当Broker接收到拉取请求后，它会开始检查自己是否有满足请求条件的数据。Broker会等待，直至数据量达到1KB至50MB（默认500条数据），或达到了预设的超时时间。</li></ul></li><li><strong><code>数据接收与缓存</code></strong>：<ul><li>消费者接收到数据后，会将其暂存到内部的队列中，此队列主要用于暂存数据，以便逐条处理。</li></ul></li><li><strong><code>数据处理</code></strong>：<ul><li>从内部队列中拉取数据，并开始处理。如果数据处理出现异常，消费者可以配置重试策略或直接跳过。</li></ul></li><li><strong><code>偏移量提交</code></strong>：<ul><li>为了避免重复消费或消息遗失，消费者在处理完消息后，会将当前的偏移量提交回Coordinator。这样，在下一次启动或者重新平衡后，消费者知道从哪里开始拉取数据。</li></ul></li></ol><h3 id="消费者api"><a class="markdownIt-Anchor" href="#消费者api"></a> 消费者API</h3><p class="note note-primary">kafka-client</p><p>监听Topic</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-meta">@Slf4j</span><br><span class="hljs-comment">// 订阅主题测试</span><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">TopicTest</span>&#123;<br><br>    <span class="hljs-keyword">protected</span> KafkaConsumer kafkaConsumer;<br><br>    <span class="hljs-meta">@BeforeEach</span><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">init</span><span class="hljs-params">()</span> &#123;<br>        <span class="hljs-comment">// 1. 创建 kafka 生产者的配置对象</span><br>        <span class="hljs-type">Properties</span> <span class="hljs-variable">properties</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">Properties</span>();<br>        <span class="hljs-comment">// 2. 给 kafka 配置对象添加配置信息：bootstrap.servers</span><br>        properties.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="hljs-string">&quot;localhost:9092&quot;</span>);<br>        <span class="hljs-comment">// key,value 反序列化（必须）</span><br>        properties.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG,<br>                StringDeserializer.class.getName());<br>        properties.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG,<br>                StringDeserializer.class.getName());<br><br>        <span class="hljs-comment">// 配置消费者组id</span><br>        properties.put(ConsumerConfig.GROUP_ID_CONFIG, <span class="hljs-string">&quot;test&quot;</span>);<br><br>        <span class="hljs-comment">// 创建消费者对象</span><br>        kafkaConsumer = <span class="hljs-keyword">new</span> <span class="hljs-title class_">KafkaConsumer</span>&lt;String, String&gt;(properties);<br>    &#125;<br><br>    <span class="hljs-meta">@Test</span><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">listenerTest</span><span class="hljs-params">()</span> &#123;<br>        <span class="hljs-comment">// 注册要消费的主题（可以消费多个主题）</span><br>        ArrayList&lt;String&gt; topics = <span class="hljs-keyword">new</span> <span class="hljs-title class_">ArrayList</span>&lt;&gt;();<br>        topics.add(<span class="hljs-string">&quot;first&quot;</span>);<br>        kafkaConsumer.subscribe(topics);<br><br>        <span class="hljs-comment">// 拉取数据打印</span><br>        <span class="hljs-keyword">while</span> (<span class="hljs-literal">true</span>) &#123;<br>            <span class="hljs-comment">// 设置 1s 中消费一批数据</span><br>            ConsumerRecords&lt;String, String&gt; consumerRecords = kafkaConsumer.poll(Duration.ofSeconds(<span class="hljs-number">1</span>));<br><br>            <span class="hljs-comment">// 打印数据</span><br>            <span class="hljs-keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : consumerRecords) &#123;<br>                log.info(record.toString());<br>            &#125;<br>        &#125;<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>监听指定分区</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-meta">@Slf4j</span><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">PartitionTest</span> &#123;<br><br>    <span class="hljs-keyword">protected</span> KafkaConsumer kafkaConsumer;<br><br>    <span class="hljs-meta">@BeforeEach</span><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">init</span><span class="hljs-params">()</span> &#123;<br>        <span class="hljs-comment">// 1. 创建 kafka 生产者的配置对象</span><br>        <span class="hljs-type">Properties</span> <span class="hljs-variable">properties</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">Properties</span>();<br>        <span class="hljs-comment">// 2. 给 kafka 配置对象添加配置信息：bootstrap.servers</span><br>        properties.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="hljs-string">&quot;localhost:9092&quot;</span>);<br>        <span class="hljs-comment">// key,value 反序列化（必须）</span><br>        properties.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG,<br>                StringDeserializer.class.getName());<br>        properties.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG,<br>                StringDeserializer.class.getName());<br><br>        <span class="hljs-comment">// 配置消费者组id</span><br>        properties.put(ConsumerConfig.GROUP_ID_CONFIG, <span class="hljs-string">&quot;test&quot;</span>);<br><br>        <span class="hljs-comment">// 创建消费者对象</span><br>        kafkaConsumer = <span class="hljs-keyword">new</span> <span class="hljs-title class_">KafkaConsumer</span>&lt;String, String&gt;(properties);<br>    &#125;<br><br>    <span class="hljs-meta">@Test</span><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">listenerPartitionTest</span><span class="hljs-params">()</span> &#123;<br>        <span class="hljs-comment">// 消费某个主题的某个分区数据</span><br>        ArrayList&lt;TopicPartition&gt; topicPartitions = <span class="hljs-keyword">new</span> <span class="hljs-title class_">ArrayList</span>&lt;&gt;();<br>        topicPartitions.add(<span class="hljs-keyword">new</span> <span class="hljs-title class_">TopicPartition</span>(<span class="hljs-string">&quot;first&quot;</span>, <span class="hljs-number">0</span>));<br>        kafkaConsumer.assign(topicPartitions);<br><br>        <span class="hljs-keyword">while</span> (<span class="hljs-literal">true</span>)&#123;<br>            ConsumerRecords&lt;String, String&gt; consumerRecords = kafkaConsumer.poll(Duration.ofSeconds(<span class="hljs-number">1</span>));<br>            <span class="hljs-keyword">for</span> (ConsumerRecord&lt;String, String&gt; consumerRecord : consumerRecords) &#123;<br>                log.info(consumerRecord.toString());<br>            &#125;<br>        &#125;<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><p class="note note-primary">Spring Boot</p><p>添加依赖</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">dependency</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">groupId</span>&gt;</span>org.springframework.kafka<span class="hljs-tag">&lt;/<span class="hljs-name">groupId</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">artifactId</span>&gt;</span>spring-kafka<span class="hljs-tag">&lt;/<span class="hljs-name">artifactId</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">dependency</span>&gt;</span><br></code></pre></td></tr></table></figure><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">server:</span><br>  <span class="hljs-attr">port:</span> <span class="hljs-number">9001</span><br><br><span class="hljs-attr">spring:</span><br>  <span class="hljs-attr">kafka:</span><br>    <span class="hljs-attr">bootstrap-servers:</span> <span class="hljs-string">localhost:9092</span>    <span class="hljs-comment"># 连接kafka的地址，多个地址用逗号分隔</span><br>    <span class="hljs-attr">consumer:</span><br>      <span class="hljs-attr">properties:</span><br>        <span class="hljs-attr">session.timeout.ms:</span> <span class="hljs-number">15000</span><br>      <span class="hljs-attr">key-deserializer:</span> <span class="hljs-string">org.apache.kafka.common.serialization.StringDeserializer</span><br>      <span class="hljs-attr">value-deserializer:</span> <span class="hljs-string">org.apache.kafka.common.serialization.StringDeserializer</span><br></code></pre></td></tr></table></figure><p>监听Topic</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-meta">@Slf4j</span><br><span class="hljs-meta">@Component</span><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">ListenerTopic</span> &#123;<br>    <span class="hljs-meta">@KafkaListener(groupId = &quot;test&quot;, topics = &quot;first&quot;)</span><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">listener</span><span class="hljs-params">(ConsumerRecord&lt;String, String&gt; record)</span> &#123;<br>        log.info(<span class="hljs-string">&quot;监听消息：&#123;&#125;&quot;</span>, record.topic(), record.value());<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>监听指定指定分区</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-meta">@Slf4j</span><br><span class="hljs-meta">@Component</span><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">ListenerPartition</span> &#123;<br><br>    <span class="hljs-meta">@KafkaListener(groupId = &quot;test&quot;, topicPartitions = @TopicPartition(topic = &quot;first&quot;, partitions = &quot;0&quot;))</span><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">listener</span><span class="hljs-params">(String msg)</span> &#123;<br>        log.info(<span class="hljs-string">&quot;监听消息：&#123;&#125;&quot;</span>, msg);<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="分区分配策略"><a class="markdownIt-Anchor" href="#分区分配策略"></a> 分区分配策略</h3><p>Topic 可以被划分为多个分区，一个消费组内可以有多个消费者，因此分区和消费者之间是多对多的关系。为了实现分区的合理分配，必然需要一种策略将 Topic 的分区分配给对应的消费者进行消费。</p><p>需要注意的是，消费者的上下线会触发分区分配的再平衡过程，该过程会根据所选的分区分配策略重新分配分区，以保证分区数据的正常消费。</p><h4 id="range默认范围"><a class="markdownIt-Anchor" href="#range默认范围"></a> Range（默认，范围）</h4><p><img src="/2022/02/24/Kafka/Range%E5%88%86%E5%8C%BA%E5%88%86%E9%85%8D.png" srcset="/img/loading.gif" lazyload alt></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs java">assign(topic, consumers) &#123;<br>  <span class="hljs-comment">// 对分区和Consumer进行排序</span><br>  List&lt;Partition&gt; partitions = topic.getPartitions();<br>  sort(partitions);<br>  sort(consumers);<br>  <span class="hljs-comment">// 计算每个Consumer分配的分区数</span><br>  <span class="hljs-type">int</span> <span class="hljs-variable">numPartitionsPerConsumer</span> <span class="hljs-operator">=</span> partition.size() / consumers.size();<br>  <span class="hljs-comment">// 额外有一些Consumer会多分配到分区</span><br>  <span class="hljs-type">int</span> <span class="hljs-variable">consumersWithExtraPartition</span> <span class="hljs-operator">=</span> partition.size() % consumers.size();<br>  <span class="hljs-comment">// 计算分配结果</span><br>  <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> <span class="hljs-variable">i</span> <span class="hljs-operator">=</span> <span class="hljs-number">0</span>, n = consumers.size(); i &lt; n; i++) &#123;<br>    <span class="hljs-comment">// 第i个Consumer分配到的分区的index</span><br>        <span class="hljs-type">int</span> <span class="hljs-variable">start</span> <span class="hljs-operator">=</span> numPartitionsPerConsumer * i + Math.min(i, consumersWithExtraPartition);<br>        <span class="hljs-comment">// 第i个Consumer分配到的分区数</span><br>        <span class="hljs-type">int</span> <span class="hljs-variable">length</span> <span class="hljs-operator">=</span> numPartitionsPerConsumer + (i + <span class="hljs-number">1</span> &gt; consumersWithExtraPartition ? <span class="hljs-number">0</span> : <span class="hljs-number">1</span>);<br>        <span class="hljs-comment">// 分装分配结果</span><br>        assignment.get(consumersForTopic.get(i)).addAll(partitions.subList(start, start + length));<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><p><span class="green-line">Kafka的 <code>Range</code> 分区分配策略是消费者 <code>默认</code> 的分区分配策略。它的工作原理是为每个消费者分配连续的分区范围。</span></p><p><strong><code>工作原理</code></strong>：</p><ol><li><code>排序</code>：首先，所有的Topic的分区和消费者都会被排序。<ul><li>分区是基于Topic名称和分区号进行排序的，通常是字典序。</li><li>消费者是基于它们的消费者ID进行排序的。</li></ul></li><li><code>分区分配</code>：每个Topic下的分区都会按照消费者数量进行划分。每个消费者首先会获得 <code>总分区数 ÷ 总消费者数</code> 的分区。然后，如果存在余数，会按照排序的顺序，将额外的分区分配给前面的消费者。</li></ol><p><strong><code>例子</code></strong>：</p><p>假设我们有一个Topic <code>A</code>，它有8个分区：<code>A-0, A-1, ... A-7</code>。现在有3个消费者 <code>C1, C2, C3</code>。</p><ol><li>按照 <code>Range</code> 策略，首先将分区和消费者进行排序。在这种情况下，分区和消费者已经是有序的。</li><li>接下来，8个分区被3个消费者平均分配。这意味着每个消费者应该处理2到3个分区。</li><li><code>8 ÷ 3 = 2</code> 与余数 <code>2</code>。因此，每个消费者最初获得2个分区，但由于有2个额外的分区，它们会被分配给<code>C1</code>和<code>C2</code>。</li><li>结果是：<code>C1</code> 负责处理 <code>A-0, A-1, A-2</code>；<code>C2</code> 负责处理 <code>A-3, A-4, A-5</code>；而 <code>C3</code> 只处理 <code>A-6, A-7</code>。</li></ol><p class="note note-primary">数据倾斜</p><p><img src="/2022/02/24/Kafka/%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C.png" srcset="/img/loading.gif" lazyload alt></p><p><code>按照Topic维度分配</code></p><p>在 <code>Range</code> 分配策略下，当处理单一Topic时，如果分区数不能被消费者数整除，额外的分区分配给前面的消费者，但这种差异相对较小。然而，当消费者组订阅了多个这样的Topic时，这种不均衡的情况会被放大。具体地说，对于每一个这样的Topic，排序靠前的消费者会多被分配一个分区。随着Topic数量的增加，这种不均衡会累积，导致排序靠前的消费者，被分配到的分区数远多于其他消费者，从而产生明显的数据倾斜。</p><p class="note note-primary">适用场景</p><ul><li><code>单 Topic 场景</code>：当只有一个 Topic 时，<code>Range</code> 分区策略可以保证分区间的负载均衡，每个消费者处理的分区数量相近。</li><li><code>Topic 分区数量都能整除消费者数量</code>：在多个 Topic 的情况下，如果 Topic 的数量和分区数量能整除消费者数量，<code>Range</code> 分区策略也能够保证负载均衡。</li></ul><h4 id="roundrobin轮询"><a class="markdownIt-Anchor" href="#roundrobin轮询"></a> RoundRobin（轮询）</h4><p><img src="/2022/02/24/Kafka/RoundRobin.png" srcset="/img/loading.gif" lazyload alt></p><p>RoundRobin 轮询分区策略，主要特点是以轮询的方式平均分配分区，确保每个消费者处理的分区数量大致相同，从而实现负载均衡。</p><p><strong><code>工作原理</code></strong>:</p><ol><li>首先，将所有的Topic分区和消费者分别进行排序。</li><li>然后，依次遍历每个Topic的分区，并按照轮询的方式分配给消费者。</li></ol><p><strong><code>例子</code></strong></p><p>假设有两个Topic：TopicA有3个分区(A0, A1, A2)，TopicB有2个分区(B0, B1)，同时有两个消费者C1和C2。根据RoundRobin策略，分区分配如下：</p><ul><li>C1分配到分区：A0, A2, B1</li><li>C2分配到分区：A1, B0</li></ul><p>轮询分区，为每个消费者分配</p><p class="note note-primary">数据倾斜</p><p><img src="/2022/02/24/Kafka/rd%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C.png" srcset="/img/loading.gif" lazyload alt></p><p>假设有三个Topic：T1（拥有三个分区）、T2（一个分区）和T3（一个分区）。一个消费者组内包含三个消费者：C0、C1和C2。其中，C0订阅了T1、T2和T3；C1和C2只订阅了T1。在这种情况下，由于RoundRobin分配策略的机制，C0会承担更多的分区消费任务，从而可能导致数据倾斜问题。</p><p class="note note-primary">适用场景</p><ul><li><code>多Topic</code>：适用于多个Topic的场景，可以更均匀地分配各个Topic的分区，减少数据倾斜的可能性。</li><li><code>负载均衡</code>：通过轮询的方式，确保每个消费者分配到的分区数量大致相同，从而实现负载均衡。</li></ul><p><span class="green-line">RoundRobin分配策略适合消费组内消费者订阅的Topic列表是相同的，在这种情况下，通过轮询分配可以确保各个消费者尽可能平均地分配到分区。</span></p><h4 id="sticky粘性"><a class="markdownIt-Anchor" href="#sticky粘性"></a> Sticky（粘性）</h4><p><img src="/2022/02/24/Kafka/sticky.png" srcset="/img/loading.gif" lazyload alt></p><p><span class="green-line">Sticky 分区分配策略是为了解决消费者上下线时引起的分区再平衡</span>。尽可能保持之前的分配关系避免再平衡带来的性能损耗。</p><ol><li><code>首次分配</code>：识别每个消费者可以接受的分区数量的上下限，然后尽量均衡分配</li><li><code>维持当前的分配</code>：当需要再平衡时，策略首先考虑维持当前的分区-消费者关系。</li><li><code>最小化重新分配</code>：如果消费者组发生变化，该策略会尽量只重新分配受影响的分区，而不是所有分区。</li></ol><p class="note note-primary">适用场景</p><p>消费者频繁上下线场景，可以通过参考上次分配的结果减少调整分配的变动</p><h3 id="分区策略配置"><a class="markdownIt-Anchor" href="#分区策略配置"></a> 分区策略配置</h3><p>Kafka提供的分区策略</p><ul><li><p>org.apache.kafka.clients.consumer.RangeAssignor</p></li><li><p>org.apache.kafka.clients.consumer.RoundRobinAssignor</p></li><li><p>org.apache.kafka.clients.consumer.StickyAssignor</p></li></ul><p class="note note-primary">kafka-client</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-meta">@Slf4j</span><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">PartitionAssignorTest</span> &#123;<br>    <span class="hljs-keyword">protected</span> KafkaConsumer kafkaConsumer;<br><br>    <span class="hljs-meta">@BeforeEach</span><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">init</span><span class="hljs-params">()</span> &#123;<br>        <span class="hljs-comment">// 1. 创建 kafka 生产者的配置对象</span><br>        <span class="hljs-type">Properties</span> <span class="hljs-variable">properties</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">Properties</span>();<br>        <span class="hljs-comment">// 2. 给 kafka 配置对象添加配置信息：bootstrap.servers</span><br>        properties.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="hljs-string">&quot;localhost:9091,localhost:9092,localhost:9093&quot;</span>);<br>        <span class="hljs-comment">// key,value 反序列化（必须）</span><br>        properties.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG,<br>                StringDeserializer.class.getName());<br>        properties.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG,<br>                StringDeserializer.class.getName());<br><br>        <span class="hljs-comment">// 配置消费者组id</span><br>        properties.put(ConsumerConfig.GROUP_ID_CONFIG, <span class="hljs-string">&quot;test&quot;</span>);<br><br>        <span class="hljs-comment">// 分区分配策略</span><br>        <span class="hljs-comment">//properties.put(ConsumerConfig.PARTITION_ASSIGNMENT_STRATEGY_CONFIG, RangeAssignor.class.getName());</span><br>        properties.put(ConsumerConfig.PARTITION_ASSIGNMENT_STRATEGY_CONFIG, RoundRobinAssignor.class.getName());<br>        <span class="hljs-comment">//properties.put(ConsumerConfig.PARTITION_ASSIGNMENT_STRATEGY_CONFIG, StickyAssignor.class.getName());</span><br><br>        <span class="hljs-comment">// 创建消费者对象</span><br>        kafkaConsumer = <span class="hljs-keyword">new</span> <span class="hljs-title class_">KafkaConsumer</span>&lt;String, String&gt;(properties);<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><p class="note note-primary">Spring Boot</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">spring:</span><br>  <span class="hljs-attr">kafka:</span><br>    <span class="hljs-attr">bootstrap-servers:</span> <span class="hljs-string">localhost:9092</span>                                     <span class="hljs-comment"># 连接kafka的地址，多个地址用逗号分隔</span><br>    <span class="hljs-attr">consumer:</span><br>      <span class="hljs-attr">properties:</span><br>        <span class="hljs-attr">partition.assignment.strategy:</span> <span class="hljs-string">org.apache.kafka.clients.consumer.RoundRobinAssignor</span>  <span class="hljs-comment"># 分区策略</span><br>        <span class="hljs-attr">session.timeout.ms:</span> <span class="hljs-number">15000</span><br>      <span class="hljs-attr">key-deserializer:</span> <span class="hljs-string">org.apache.kafka.common.serialization.StringDeserializer</span><br>      <span class="hljs-attr">value-deserializer:</span> <span class="hljs-string">org.apache.kafka.common.serialization.StringDeserializer</span><br></code></pre></td></tr></table></figure><h3 id="自动提交offset"><a class="markdownIt-Anchor" href="#自动提交offset"></a> 自动提交offset</h3><p>为了简化offset管理，从而让开发者更加专注于业务逻辑，Kafka提供了自动提交offset的功能。</p><p>关于自动提交offset的相关参数：</p><ul><li><code>enable.auto.commit</code>：决定是否启用自动提交offset功能，默认值为<code>true</code>。</li><li><code>auto.commit.interval.ms</code>：定义了两次自动提交之间的时间间隔，默认是5000毫秒（即5秒）。</li></ul><p class="note note-primary">自动提交存在的问题</p><ul><li><code>数据丢失</code>：当消息刚被消费，但还未在业务逻辑中完全处理结束时，如果到达自动提交周期，offset就会被自动提交。此时，如果消费者宕机，那么在重启并重新消费处理时，可能会导致数据丢失。</li><li><code>数据重复</code>：如果消息已经被成功消费并处理，但在自动提交offset的周期到达前发生宕机，那么offset将不会被自动提交。因此，在重启服务后，系统将根据上一次提交的offset重新消费消息，这就导致了消息的重复消费。</li></ul><p class="note note-primary">kafka-client</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-meta">@Slf4j</span><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">AutoCommitTest</span> &#123;<br>    <span class="hljs-keyword">protected</span> KafkaConsumer kafkaConsumer;<br><br>    <span class="hljs-meta">@BeforeEach</span><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">init</span><span class="hljs-params">()</span> &#123;<br>        <span class="hljs-comment">// 1. 创建 kafka 生产者的配置对象</span><br>        <span class="hljs-type">Properties</span> <span class="hljs-variable">properties</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">Properties</span>();<br>        <span class="hljs-comment">// 2. 给 kafka 配置对象添加配置信息：bootstrap.servers</span><br>        properties.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="hljs-string">&quot;localhost:9091,localhost:9092,localhost:9093&quot;</span>);<br>        <span class="hljs-comment">// key,value 反序列化（必须）</span><br>        properties.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG,<br>                StringDeserializer.class.getName());<br>        properties.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG,<br>                StringDeserializer.class.getName());<br><br>        <span class="hljs-comment">// 是否自动提交 offset</span><br>        properties.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, <span class="hljs-literal">true</span>);<br>        <span class="hljs-comment">// 提交 offset 的时间周期 1000ms，默认 5s</span><br>        properties.put(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG, <span class="hljs-number">1000</span>);<br><br>        <span class="hljs-comment">// 配置消费者组id</span><br>        properties.put(ConsumerConfig.GROUP_ID_CONFIG, <span class="hljs-string">&quot;test&quot;</span>);<br><br>        <span class="hljs-comment">// 创建消费者对象</span><br>        kafkaConsumer = <span class="hljs-keyword">new</span> <span class="hljs-title class_">KafkaConsumer</span>&lt;String, String&gt;(properties);<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><p class="note note-primary">Spring Boot</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">spring:</span><br>  <span class="hljs-attr">kafka:</span><br>    <span class="hljs-attr">bootstrap-servers:</span> <span class="hljs-string">localhost:9092</span>                                     <span class="hljs-comment"># 连接kafka的地址，多个地址用逗号分隔</span><br>    <span class="hljs-attr">consumer:</span><br>      <span class="hljs-attr">enable-auto-commit:</span> <span class="hljs-literal">true</span>                                                <span class="hljs-comment"># 开启自动提交（默认）</span><br>      <span class="hljs-attr">auto-commit-interval:</span> <span class="hljs-number">1000</span>                                              <span class="hljs-comment"># 自动提交间隔时间（默认5秒）</span><br>      <span class="hljs-attr">key-deserializer:</span> <span class="hljs-string">org.apache.kafka.common.serialization.StringDeserializer</span><br>      <span class="hljs-attr">value-deserializer:</span> <span class="hljs-string">org.apache.kafka.common.serialization.StringDeserializer</span><br></code></pre></td></tr></table></figure><h3 id="手动提交offset"><a class="markdownIt-Anchor" href="#手动提交offset"></a> 手动提交offset</h3><p>虽然自动提交offset十分简单便利，但由于其是基于时间周期性提交，开发人员难以精准把握offset提交的时机。因此，Kafka还提供了手动提交offset的API。</p><p>手动提交offset的方法主要有两种：分别是<code>commitSync</code>（同步提交）和<code>commitAsync</code>（异步提交）。两者的共同点在于，都会提交消费者成功处理的最后一条消息的偏移量 + 1。</p><ul><li><code>commitSync</code>（同步提交）：这种方式会阻塞当前线程，直到offset提交成功或失败。如果提交失败，可以选择重试。因为该方法会返回提交成功或失败的结果，可以在需要确保offset准确性的场景下使用。</li><li><code>commitAsync</code>（异步提交）：这种方式在提交offset时不会阻塞当前线程，同时可以提供一个回调函数来处理提交成功或失败的情况。由于是异步的，所以性能较好，但需要注意处理可能出现的提交失败的情况。</li></ul><p class="note note-primary">kafka-client</p><p>同步提交</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-meta">@Slf4j</span><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">CommitSyncTest</span> &#123;<br>    <span class="hljs-keyword">protected</span> KafkaConsumer kafkaConsumer;<br><br>    <span class="hljs-meta">@BeforeEach</span><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">init</span><span class="hljs-params">()</span> &#123;<br>        <span class="hljs-comment">// 1. 创建 kafka 生产者的配置对象</span><br>        <span class="hljs-type">Properties</span> <span class="hljs-variable">properties</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">Properties</span>();<br>        <span class="hljs-comment">// 2. 给 kafka 配置对象添加配置信息：bootstrap.servers</span><br>        properties.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="hljs-string">&quot;localhost:9091,localhost:9092,localhost:9093&quot;</span>);<br>        <span class="hljs-comment">// key,value 反序列化（必须）</span><br>        properties.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG,<br>                StringDeserializer.class.getName());<br>        properties.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG,<br>                StringDeserializer.class.getName());<br><br>        <span class="hljs-comment">// 取消自动提交</span><br>        properties.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, <span class="hljs-literal">false</span>);<br><br>        <span class="hljs-comment">// 配置消费者组id</span><br>        properties.put(ConsumerConfig.GROUP_ID_CONFIG, <span class="hljs-string">&quot;test&quot;</span>);<br><br>        <span class="hljs-comment">// 创建消费者对象</span><br>        kafkaConsumer = <span class="hljs-keyword">new</span> <span class="hljs-title class_">KafkaConsumer</span>&lt;String, String&gt;(properties);<br>    &#125;<br><br>    <span class="hljs-meta">@Test</span><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">listenerPartitionTest</span><span class="hljs-params">()</span> &#123;<br>        <span class="hljs-comment">// 消费某个主题的某个分区数据</span><br>        ArrayList&lt;TopicPartition&gt; topicPartitions = <span class="hljs-keyword">new</span> <span class="hljs-title class_">ArrayList</span>&lt;&gt;();<br>        topicPartitions.add(<span class="hljs-keyword">new</span> <span class="hljs-title class_">TopicPartition</span>(<span class="hljs-string">&quot;first&quot;</span>, <span class="hljs-number">0</span>));<br>        kafkaConsumer.assign(topicPartitions);<br><br>        <span class="hljs-keyword">while</span> (<span class="hljs-literal">true</span>)&#123;<br>            ConsumerRecords&lt;String, String&gt; consumerRecords = kafkaConsumer.poll(Duration.ofSeconds(<span class="hljs-number">1</span>));<br><br>            <span class="hljs-keyword">for</span> (ConsumerRecord&lt;String, String&gt; consumerRecord : consumerRecords) &#123;<br>                log.info(consumerRecord.toString());<br>            &#125;<br><br>            <span class="hljs-comment">// 手动同步提交</span><br>            kafkaConsumer.commitSync();<br>        &#125;<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>手动异步提交</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-meta">@Slf4j</span><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">CommitASyncTest</span> &#123;<br>    <span class="hljs-keyword">protected</span> KafkaConsumer kafkaConsumer;<br><br>    <span class="hljs-meta">@BeforeEach</span><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">init</span><span class="hljs-params">()</span> &#123;<br>        <span class="hljs-comment">// 1. 创建 kafka 生产者的配置对象</span><br>        <span class="hljs-type">Properties</span> <span class="hljs-variable">properties</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">Properties</span>();<br>        <span class="hljs-comment">// 2. 给 kafka 配置对象添加配置信息：bootstrap.servers</span><br>        properties.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="hljs-string">&quot;localhost:9091,localhost:9092,localhost:9093&quot;</span>);<br>        <span class="hljs-comment">// key,value 反序列化（必须）</span><br>        properties.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG,<br>                StringDeserializer.class.getName());<br>        properties.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG,<br>                StringDeserializer.class.getName());<br><br>        <span class="hljs-comment">// 取消自动提交</span><br>        properties.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, <span class="hljs-literal">false</span>);<br><br>        <span class="hljs-comment">// 配置消费者组id</span><br>        properties.put(ConsumerConfig.GROUP_ID_CONFIG, <span class="hljs-string">&quot;test&quot;</span>);<br><br>        <span class="hljs-comment">// 创建消费者对象</span><br>        kafkaConsumer = <span class="hljs-keyword">new</span> <span class="hljs-title class_">KafkaConsumer</span>&lt;String, String&gt;(properties);<br>    &#125;<br><br>    <span class="hljs-meta">@Test</span><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">listenerPartitionTest</span><span class="hljs-params">()</span> &#123;<br>        <span class="hljs-comment">// 消费某个主题的某个分区数据</span><br>        ArrayList&lt;TopicPartition&gt; topicPartitions = <span class="hljs-keyword">new</span> <span class="hljs-title class_">ArrayList</span>&lt;&gt;();<br>        topicPartitions.add(<span class="hljs-keyword">new</span> <span class="hljs-title class_">TopicPartition</span>(<span class="hljs-string">&quot;first&quot;</span>, <span class="hljs-number">0</span>));<br>        kafkaConsumer.assign(topicPartitions);<br><br>        <span class="hljs-keyword">while</span> (<span class="hljs-literal">true</span>)&#123;<br>            ConsumerRecords&lt;String, String&gt; consumerRecords = kafkaConsumer.poll(Duration.ofSeconds(<span class="hljs-number">1</span>));<br><br>            <span class="hljs-keyword">for</span> (ConsumerRecord&lt;String, String&gt; consumerRecord : consumerRecords) &#123;<br>                log.info(consumerRecord.toString());<br>            &#125;<br><br>            <span class="hljs-comment">// 手动异步提交</span><br>            kafkaConsumer.commitAsync();<br>        &#125;<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><p class="note note-primary">Spring Boot</p><ul><li>关闭手动提交：spring.kafka.consumer.enable-auto-commit: false</li><li>配置监听应答模式：spring.kafka.listener.ack-mode: manual</li></ul><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">spring:</span><br>  <span class="hljs-attr">kafka:</span><br>    <span class="hljs-attr">bootstrap-servers:</span> <span class="hljs-string">localhost:9092</span>                                     <span class="hljs-comment"># 连接kafka的地址，多个地址用逗号分隔</span><br>    <span class="hljs-attr">consumer:</span><br>      <span class="hljs-attr">enable-auto-commit:</span> <span class="hljs-literal">false</span>                                               <span class="hljs-comment"># 关闭自动提交</span><br>      <span class="hljs-attr">key-deserializer:</span> <span class="hljs-string">org.apache.kafka.common.serialization.StringDeserializer</span><br>      <span class="hljs-attr">value-deserializer:</span> <span class="hljs-string">org.apache.kafka.common.serialization.StringDeserializer</span><br>    <span class="hljs-attr">listener:</span><br>      <span class="hljs-attr">ack-mode:</span> <span class="hljs-string">manual</span><br></code></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-meta">@Slf4j</span><br><span class="hljs-meta">@Component</span><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">TestListener</span> &#123;<br><br>    <span class="hljs-meta">@KafkaListener(topics = &quot;first&quot;, groupId = &quot;test&quot;)</span><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">testTopic</span><span class="hljs-params">(String message, Acknowledgment acknowledgment, Consumer&lt;String, String&gt; consumer)</span> &#123;<br>        log.info(<span class="hljs-string">&quot;接收到消息：&#123;&#125;&quot;</span>, message);<br><br>        <span class="hljs-comment">// 手动同步提交</span><br>        <span class="hljs-comment">// consumer.commitSync();</span><br><br>        <span class="hljs-comment">// 手动异步提交</span><br>        <span class="hljs-comment">// consumer.commitAsync();</span><br><br>        <span class="hljs-comment">// ACK手动提交offset, 如果整合了spring boot 建议使用这种</span><br>        <span class="hljs-comment">// acknowledgment.acknowledge();</span><br><br>        <span class="hljs-comment">// 不进行ack确认,让监听器线程休眠指定毫秒后重新开始消费此条消息</span><br>        <span class="hljs-comment">// acknowledgment.nack(100);</span><br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="offset消费策略"><a class="markdownIt-Anchor" href="#offset消费策略"></a> offset消费策略</h3><p><code>auto.offset.reset</code> 可选值： <code>earliest</code>、 <code>latest</code>、<code>none</code>。默认设置为 <code>latest</code>。</p><table><thead><tr><th>策略</th><th>说明</th></tr></thead><tbody><tr><td><code>earliest</code></td><td>当指定分区有已提交的offset时，从提交的offset开始消费；如果没有提交的offset，则从分区起始位置开始消费。</td></tr><tr><td><code>latest</code></td><td>当指定分区有已提交的offset时，从提交的offset开始消费；如果没有提交的offset，则只消费该分区新产生的数据。</td></tr><tr><td><code>none</code></td><td>如果指定分区没有已提交的offset，则会抛出异常，不会从任何位置开始消费。</td></tr></tbody></table><p class="note note-primary">kafka-client</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-meta">@Slf4j</span><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">AutoOffsetResetTest</span> &#123;<br>    <span class="hljs-keyword">protected</span> KafkaConsumer kafkaConsumer;<br><br>    <span class="hljs-meta">@BeforeEach</span><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">init</span><span class="hljs-params">()</span> &#123;<br>        <span class="hljs-comment">// 1. 创建 kafka 生产者的配置对象</span><br>        <span class="hljs-type">Properties</span> <span class="hljs-variable">properties</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">Properties</span>();<br>        <span class="hljs-comment">// 2. 给 kafka 配置对象添加配置信息：bootstrap.servers</span><br>        properties.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="hljs-string">&quot;localhost:9091,localhost:9092,localhost:9093&quot;</span>);<br>        <span class="hljs-comment">// key,value 反序列化（必须）</span><br>        properties.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG,<br>                StringDeserializer.class.getName());<br>        properties.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG,<br>                StringDeserializer.class.getName());<br><br>        <span class="hljs-comment">// 指定offset策略</span><br>        properties.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, <span class="hljs-string">&quot;earliest&quot;</span>);<br><br>        <span class="hljs-comment">// 配置消费者组id</span><br>        properties.put(ConsumerConfig.GROUP_ID_CONFIG, <span class="hljs-string">&quot;test&quot;</span>);<br><br>        <span class="hljs-comment">// 创建消费者对象</span><br>        kafkaConsumer = <span class="hljs-keyword">new</span> <span class="hljs-title class_">KafkaConsumer</span>&lt;String, String&gt;(properties);<br>    &#125;<br><br>    <span class="hljs-meta">@Test</span><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">listenerTest</span><span class="hljs-params">()</span> &#123;<br>        <span class="hljs-comment">// 注册要消费的主题（可以消费多个主题）</span><br>        ArrayList&lt;String&gt; topics = <span class="hljs-keyword">new</span> <span class="hljs-title class_">ArrayList</span>&lt;&gt;();<br>        topics.add(<span class="hljs-string">&quot;first&quot;</span>);<br>        kafkaConsumer.subscribe(topics);<br><br>        <span class="hljs-comment">// 拉取数据打印</span><br>        <span class="hljs-keyword">while</span> (<span class="hljs-literal">true</span>) &#123;<br>            <span class="hljs-comment">// 设置 1s 中消费一批数据</span><br>            ConsumerRecords&lt;String, String&gt; consumerRecords = kafkaConsumer.poll(Duration.ofSeconds(<span class="hljs-number">1</span>));<br><br>            <span class="hljs-comment">// 打印数据</span><br>            <span class="hljs-keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : consumerRecords) &#123;<br>                log.info(record.toString());<br>            &#125;<br>        &#125;<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><p class="note note-primary">Spring Boot</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">spring:</span><br>  <span class="hljs-attr">kafka:</span><br>    <span class="hljs-attr">bootstrap-servers:</span> <span class="hljs-string">localhost:9092</span>                                     <span class="hljs-comment"># 连接kafka的地址，多个地址用逗号分隔</span><br>    <span class="hljs-attr">consumer:</span><br>      <span class="hljs-attr">auto-offset-reset:</span> <span class="hljs-string">earliest</span>                                             <span class="hljs-comment"># offset 消费策略</span><br>      <span class="hljs-attr">enable-auto-commit:</span> <span class="hljs-literal">false</span>                                               <span class="hljs-comment"># 开启自动提交（默认）</span><br>      <span class="hljs-attr">auto-commit-interval:</span> <span class="hljs-number">1000</span>                                              <span class="hljs-comment"># 自动提交间隔时间（默认5秒）</span><br>      <span class="hljs-attr">key-deserializer:</span> <span class="hljs-string">org.apache.kafka.common.serialization.StringDeserializer</span><br>      <span class="hljs-attr">value-deserializer:</span> <span class="hljs-string">org.apache.kafka.common.serialization.StringDeserializer</span><br>    <span class="hljs-attr">listener:</span><br>      <span class="hljs-attr">ack-mode:</span> <span class="hljs-string">manual</span><br></code></pre></td></tr></table></figure><p class="note note-primary">如何重新消费队列数据</p><ul><li>更换新的消费者组</li><li>设置 <code>auto.offset.reset</code> 为 <code>earliest</code>；重新从头消费</li></ul><h3 id="指定offset消费"><a class="markdownIt-Anchor" href="#指定offset消费"></a> 指定offset消费</h3><p><code>异常恢复</code></p><p>任意指定 offset 位移开始消费，需要对每个分区进行设置</p><p class="note note-primary">kafka-client</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-meta">@Slf4j</span><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">SpecifyOffsetTest</span> &#123;<br>    <span class="hljs-keyword">protected</span> KafkaConsumer kafkaConsumer;<br><br>    <span class="hljs-meta">@BeforeEach</span><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">init</span><span class="hljs-params">()</span> &#123;<br>        <span class="hljs-comment">// 1. 创建 kafka 生产者的配置对象</span><br>        <span class="hljs-type">Properties</span> <span class="hljs-variable">properties</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">Properties</span>();<br>        <span class="hljs-comment">// 2. 给 kafka 配置对象添加配置信息：bootstrap.servers</span><br>        properties.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="hljs-string">&quot;localhost:9091,localhost:9092,localhost:9093&quot;</span>);<br>        <span class="hljs-comment">// key,value 反序列化（必须）</span><br>        properties.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG,<br>                StringDeserializer.class.getName());<br>        properties.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG,<br>                StringDeserializer.class.getName());<br><br>        <span class="hljs-comment">// 配置消费者组id</span><br>        properties.put(ConsumerConfig.GROUP_ID_CONFIG, <span class="hljs-string">&quot;test2&quot;</span>);<br><br>        <span class="hljs-comment">// 创建消费者对象</span><br>        kafkaConsumer = <span class="hljs-keyword">new</span> <span class="hljs-title class_">KafkaConsumer</span>&lt;String, String&gt;(properties);<br><br>        <span class="hljs-comment">// 2 订阅一个主题</span><br>        ArrayList&lt;String&gt; topics = <span class="hljs-keyword">new</span> <span class="hljs-title class_">ArrayList</span>&lt;&gt;();<br>        topics.add(<span class="hljs-string">&quot;first&quot;</span>);<br>        kafkaConsumer.subscribe(topics);<br><br>        <span class="hljs-comment">// 获取分区信息</span><br>        Set&lt;TopicPartition&gt; assignment = <span class="hljs-keyword">new</span> <span class="hljs-title class_">HashSet</span>&lt;&gt;();<br>        <span class="hljs-keyword">while</span> (assignment.size() == <span class="hljs-number">0</span>) &#123;<br>            kafkaConsumer.poll(Duration.ofSeconds(<span class="hljs-number">1</span>));<br>            <span class="hljs-comment">// 获取消费者分区分配信息（有了分区分配信息才能开始消费）</span><br>            assignment = kafkaConsumer.assignment();<br>        &#125;<br><br>        <span class="hljs-comment">// 遍历所有分区，并指定 offset 从 100 的位置开始消费</span><br>        <span class="hljs-keyword">for</span> (TopicPartition tp : assignment) &#123;<br>            kafkaConsumer.seek(tp, <span class="hljs-number">100</span>);<br>        &#125;<br>    &#125;<br><br><br>    <span class="hljs-meta">@Test</span><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">listenerTest</span><span class="hljs-params">()</span> &#123;<br>        <span class="hljs-comment">// 注册要消费的主题（可以消费多个主题）</span><br>        ArrayList&lt;String&gt; topics = <span class="hljs-keyword">new</span> <span class="hljs-title class_">ArrayList</span>&lt;&gt;();<br>        topics.add(<span class="hljs-string">&quot;first&quot;</span>);<br>        kafkaConsumer.subscribe(topics);<br><br>        <span class="hljs-comment">// 拉取数据打印</span><br>        <span class="hljs-keyword">while</span> (<span class="hljs-literal">true</span>) &#123;<br>            <span class="hljs-comment">// 设置 1s 中消费一批数据</span><br>            ConsumerRecords&lt;String, String&gt; consumerRecords = kafkaConsumer.poll(Duration.ofSeconds(<span class="hljs-number">1</span>));<br><br>            <span class="hljs-comment">// 打印数据</span><br>            <span class="hljs-keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : consumerRecords) &#123;<br>                log.info(<span class="hljs-string">&quot;topic：&#123;&#125;，offset：&#123;&#125;，value：&#123;&#125;&quot;</span>, record.topic(), record.offset(), record.value());<br>            &#125;<br>        &#125;<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><p class="note note-primary">Spring Boot</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-meta">@Slf4j</span><br><span class="hljs-meta">@Component</span><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">SpecifyOffsetListener</span> &#123;<br><br>    <span class="hljs-meta">@KafkaListener(groupId = &quot;test3&quot;, topics = &quot;first&quot;, topicPartitions = &#123;</span><br><span class="hljs-meta">            @TopicPartition(topic = &quot;first&quot;, partitionOffsets = &#123;</span><br><span class="hljs-meta">                    @PartitionOffset(partition = &quot;0&quot;, initialOffset = &quot;500&quot;),</span><br><span class="hljs-meta">                    @PartitionOffset(partition = &quot;1&quot;, initialOffset = &quot;500&quot;),</span><br><span class="hljs-meta">                    @PartitionOffset(partition = &quot;2&quot;, initialOffset = &quot;500&quot;)</span><br><span class="hljs-meta">            &#125;)</span><br><span class="hljs-meta">    &#125;)</span><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">listener</span><span class="hljs-params">(ConsumerRecord&lt;String, String&gt; record)</span> &#123;<br>        log.info(<span class="hljs-string">&quot;topic：&#123;&#125;，offset：&#123;&#125;，value：&#123;&#125;&quot;</span>, record.topic(), record.offset(), record.value());<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="指定时间消费offset"><a class="markdownIt-Anchor" href="#指定时间消费offset"></a> 指定时间消费offset</h3><p><code>异常恢复</code></p><p>在生产环境中，会遇到最近消费的几个小时数据异常，想重新按照时间消费。例如要求按照时间消费前一天的数据，怎么处理？</p><p>从Kafka 0.10.0版本开始，每条消息（即每个record）都有一个关联的时间戳。时间戳可以是消息创建时间（默认）或消息追加到log时的时间。</p><ol><li><code>CreateTime</code>：这是默认值。时间戳代表消息创建的时间。通常，它是在生产者客户端设置的，代表消息被发送之前的时间。</li><li><code>LogAppendTime</code>：时间戳代表消息被追加到log的时间。这是在broker上设置的，代表消息被写入日志的时间。</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-meta">@Slf4j</span><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">TimeOffsetListener</span> &#123;<br>    <span class="hljs-keyword">protected</span> KafkaConsumer kafkaConsumer;<br><br>    <span class="hljs-meta">@BeforeEach</span><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">init</span><span class="hljs-params">()</span> &#123;<br>        <span class="hljs-comment">// 1. 创建 kafka 生产者的配置对象</span><br>        <span class="hljs-type">Properties</span> <span class="hljs-variable">properties</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">Properties</span>();<br>        <span class="hljs-comment">// 2. 给 kafka 配置对象添加配置信息：bootstrap.servers</span><br>        properties.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="hljs-string">&quot;localhost:9091,localhost:9092,localhost:9093&quot;</span>);<br>        <span class="hljs-comment">// key,value 反序列化（必须）</span><br>        properties.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG,<br>                StringDeserializer.class.getName());<br>        properties.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG,<br>                StringDeserializer.class.getName());<br><br>        <span class="hljs-comment">// 指定offset策略</span><br>        properties.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, <span class="hljs-string">&quot;earliest&quot;</span>);<br><br>        <span class="hljs-comment">// 配置消费者组id</span><br>        properties.put(ConsumerConfig.GROUP_ID_CONFIG, <span class="hljs-string">&quot;test&quot;</span>);<br><br>        <span class="hljs-comment">// 创建消费者对象</span><br>        kafkaConsumer = <span class="hljs-keyword">new</span> <span class="hljs-title class_">KafkaConsumer</span>&lt;String, String&gt;(properties);<br><br>        <span class="hljs-comment">// 注册要消费的主题（可以消费多个主题）</span><br>        ArrayList&lt;String&gt; topics = <span class="hljs-keyword">new</span> <span class="hljs-title class_">ArrayList</span>&lt;&gt;();<br>        topics.add(<span class="hljs-string">&quot;first&quot;</span>);<br>        kafkaConsumer.subscribe(topics);<br>    &#125;<br><br>    <span class="hljs-comment">/**</span><br><span class="hljs-comment">     * kafka 提供基于时间获取offset的API</span><br><span class="hljs-comment">     */</span><br>    <span class="hljs-meta">@Test</span><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">offsetForTimesTest</span><span class="hljs-params">()</span> &#123;<br><br>        <span class="hljs-comment">// 获取分区信息</span><br>        Set&lt;TopicPartition&gt; assignment = <span class="hljs-keyword">new</span> <span class="hljs-title class_">HashSet</span>&lt;&gt;();<br>        <span class="hljs-keyword">while</span> (assignment.size() == <span class="hljs-number">0</span>) &#123;<br>            kafkaConsumer.poll(Duration.ofSeconds(<span class="hljs-number">1</span>));<br>            <span class="hljs-comment">// 获取消费者分区分配信息（有了分区分配信息才能开始消费）</span><br>            assignment = kafkaConsumer.assignment();<br>        &#125;<br><br>        HashMap&lt;TopicPartition, Long&gt; timestampToSearch = <span class="hljs-keyword">new</span> <span class="hljs-title class_">HashMap</span>&lt;&gt;();<br><br>        <span class="hljs-comment">// 封装集合存储，每个分区对应一天前的数据</span><br>        <span class="hljs-keyword">for</span> (TopicPartition topicPartition : assignment) &#123;<br>            timestampToSearch.put(topicPartition, System.currentTimeMillis() - <span class="hljs-number">1</span> * <span class="hljs-number">24</span> * <span class="hljs-number">3600</span> * <span class="hljs-number">1000</span>);<br>        &#125;<br>        <span class="hljs-comment">// 获取从 1 天前开始消费的每个分区的 offset</span><br>        Map&lt;TopicPartition, OffsetAndTimestamp&gt; offsets = kafkaConsumer.offsetsForTimes(timestampToSearch);<br><br>        offsets.forEach((k ,v) -&gt; &#123;<br>            log.info(<span class="hljs-string">&quot;partition：&#123;&#125;&quot;</span>, k.partition());<br>            log.info(<span class="hljs-string">&quot;offset：&#123;&#125;&quot;</span>, v.offset());<br>        &#125;);<br>    &#125;<br>    <br>    <span class="hljs-comment">// 根据时间获取到每个分区对应的offset，再调用kafkaConsumer.seek(...)，设置每个分区的offset即可</span><br>&#125;<br></code></pre></td></tr></table></figure><p>SpringBoot</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-meta">@Service</span><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">KafkaTimeBasedConsumerService</span> &#123;<br><br>    <span class="hljs-meta">@Autowired</span><br>    <span class="hljs-keyword">private</span> ConsumerFactory&lt;String, String&gt; consumerFactory;<br><br>    <span class="hljs-comment">// 这里使用了Java 8的Instant类作为时间的表示</span><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">consumeFromTimestamp</span><span class="hljs-params">(String topic, Instant timestampToStartFrom)</span> &#123;<br>        <span class="hljs-keyword">try</span> (Consumer&lt;String, String&gt; consumer = consumerFactory.createConsumer()) &#123;<br>            List&lt;PartitionInfo&gt; partitionInfos = consumer.partitionsFor(topic);<br>            List&lt;TopicPartition&gt; topicPartitions = partitionInfos.stream()<br>                    .map(partitionInfo -&gt; <span class="hljs-keyword">new</span> <span class="hljs-title class_">TopicPartition</span>(partitionInfo.topic(), partitionInfo.partition()))<br>                    .collect(Collectors.toList());<br><br>            Map&lt;TopicPartition, Long&gt; timestampToSearch = topicPartitions.stream()<br>                    .collect(Collectors.toMap(Function.identity(), tp -&gt; timestampToStartFrom.toEpochMilli()));<br><br>            Map&lt;TopicPartition, OffsetAndTimestamp&gt; offsets = consumer.offsetsForTimes(timestampToSearch);<br><br>            <span class="hljs-keyword">if</span> (offsets != <span class="hljs-literal">null</span>) &#123;<br>                consumer.assign(offsets.keySet());<br>                offsets.forEach((tp, ot) -&gt; &#123;<br>                    <span class="hljs-keyword">if</span> (ot != <span class="hljs-literal">null</span>) &#123;<br>                        consumer.seek(tp, ot.offset());<br>                    &#125;<br>                &#125;);<br><br>                ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofMillis(<span class="hljs-number">1000</span>));<br>                <span class="hljs-keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records) &#123;<br>                    <span class="hljs-comment">// Process your record here</span><br>                    System.out.println(<span class="hljs-string">&quot;Received Message with timestamp: &quot;</span> + record.timestamp() + <span class="hljs-string">&quot; Value: &quot;</span> + record.value());<br>                &#125;<br>            &#125;<br>        &#125;<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="消费者事务"><a class="markdownIt-Anchor" href="#消费者事务"></a> <strong>消费者事务</strong></h3><p><img src="/2022/02/24/Kafka/%E6%B6%88%E8%B4%B9%E8%80%85%E4%BA%8B%E5%8A%A1.png" srcset="/img/loading.gif" lazyload alt></p><p>实现Kafka的精确一次（Exactly Once Semantics, EOS）消费需要综合考虑生产、消费、处理以及偏移量存储的方面。</p><ol><li><strong><code>生产者</code></strong>：<ul><li>开启幂等性，<code>enable.idempotence=true</code>。</li><li>使用事务，<code>transactional.id=&lt;some-unique-id&gt;</code>。</li></ul></li><li><strong><code>消费者</code></strong>：<ul><li>为了消费那些已提交的事务消息，消费者需要将 <code>isolation.level</code> 配置设置为 <code>read_committed</code>。这样，消费者只会读取已提交事务的消息，并忽略未提交的消息。</li><li>默认情况下，<code>isolation.level</code> 是 <code>read_uncommitted</code>，这意味着消费者会读取所有消息，无论它们是否在事务中。</li><li>手动提交 offset</li></ul></li><li><strong><code>消费者与Offset管理</code></strong>：<ul><li>当消费者处理事务消息时，为了确保EOS，它们需要在处理消息的同时，也在同一个事务中提交offset。</li><li>这确保了处理消息和提交offset是原子操作，从而实现了end-to-end的EOS。</li></ul></li><li><strong><code>同步处理和外部系统</code></strong>：<ul><li>如果消费者的处理逻辑涉及与外部系统的交互，那么需要确保这种交互是幂等的。</li><li>例如，如果消费者需要将消息数据写入一个数据库，那么这个写操作应该是可以安全地重复的，以确保在面对失败和重试时不会有不一致的情况。</li></ul></li></ol><h3 id="数据积压"><a class="markdownIt-Anchor" href="#数据积压"></a> 数据积压</h3><p class="note note-primary">消费能力不足</p><p><img src="/2022/02/24/Kafka/%E6%B6%88%E8%B4%B9%E8%83%BD%E5%8A%9B.png" srcset="/img/loading.gif" lazyload alt></p><p>当Kafka遭遇消费能力不足导致数据积压时，可以通过以下策略来解决：</p><ol><li>增加Topic的分区数以提高并行处理的能力。</li><li>增加消费组内的消费者数量。为了实现最佳的负载均衡，消费者的数量应该等于或小于分区数。</li></ol><p>注意，仅增加分区数而不增加消费者数量（或反之）可能不会显著提高消费速度。两者应协同工作以实现最佳效果。</p><p class="note note-primary">数据处理不及时</p><p><img src="/2022/02/24/Kafka/%E6%B6%88%E8%B4%B9%E8%83%BD%E5%8A%9B%E4%B8%8D%E8%B6%B3.png" srcset="/img/loading.gif" lazyload alt></p><table><thead><tr><th>参数</th><th>描述</th></tr></thead><tbody><tr><td><code>fetch.max.bytes</code></td><td><code>fetch.max.bytes</code> 是 Kafka 消费者的一个配置参数，其默认值为 <code>52428800</code>（约为 50MB）。这个参数定义了消费者在单次拉取操作中可以从Kafka服务器获取的消息的最大字节数。</td></tr><tr><td><code>max.poll.records</code></td><td>一次 poll 拉取数据返回消息的最大条数，默认是 500 条</td></tr></tbody></table><p>当Kafka的数据积压是由于下游处理不及时引起的，可以考虑以下策略来解决：</p><ol><li>增加每批次从Kafka拉取的消息数量，从而提高处理的吞吐量。</li><li>分析并优化数据处理的效率，确保处理速度至少与生产速度相匹配。</li></ol><h2 id="efak-安装"><a class="markdownIt-Anchor" href="#efak-安装"></a> EFAK 安装</h2><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/101920303">java 环境变量配置</a></p><p><a target="_blank" rel="noopener" href="https://bbs.huaweicloud.com/blogs/385802">EFAK 安装</a></p><h2 id="kafka调优"><a class="markdownIt-Anchor" href="#kafka调优"></a> Kafka调优</h2><p><a href="/2023/01/23/kafka%E8%B0%83%E4%BC%98">调优篇</a></p><h2 id="kraft模式"><a class="markdownIt-Anchor" href="#kraft模式"></a> Kraft模式</h2><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_47658874/article/details/122730520">Kraft模式搭建</a></p><p><img src="/2022/02/24/Kafka/KRaft.png" srcset="/img/loading.gif" lazyload alt></p><p>左图为 Kafka 现有架构，元数据在 zookeeper 中，运行时动态选举 controller，由 controller 进行 Kafka 集群管理。右图为 kraft 模式架构（实验性），不再依赖 zookeeper 集群， 而是用三台 controller 节点代替 zookeeper，元数据保存在 controller 中，由 controller 直接进 行 Kafka 集群管理</p><p>这样做的好处有以下几个</p><ul><li>Kafka 不再依赖外部框架，而是能够独立运行</li><li>controller 管理集群时，不再需要从 zookeeper 中先读取数据，集群性能上升</li><li>由于不依赖 zookeeper，集群扩展时不再受到 zookeeper 读写能力限制</li><li>controller 不再动态选举，而是由配置文件规定。这样我们可以有针对性的加强controller 节点的配置，而不是像以前一样对随机 controller 节点的高负载束手无策</li></ul><h2 id="接入外部系统"><a class="markdownIt-Anchor" href="#接入外部系统"></a> 接入外部系统</h2><h3 id="flume"><a class="markdownIt-Anchor" href="#flume"></a> Flume</h3><h3 id="flink"><a class="markdownIt-Anchor" href="#flink"></a> Flink</h3><h3 id="spark"><a class="markdownIt-Anchor" href="#spark"></a> Spark</h3><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css"></div><hr><div><div class="post-metas my-3"></div><div class="license-box my-3"><div class="license-title"><div>Kafka</div><div>https://wugengfeng.cn/2022/02/24/Kafka/</div></div><div class="license-meta"><div class="license-meta-item"><div>作者</div><div>wugengfeng</div></div><div class="license-meta-item license-meta-date"><div>发布于</div><div>2022年2月24日</div></div><div class="license-meta-item"><div>许可协议</div><div><a target="_blank" href="https://creativecommons.org/licenses/by/4.0/"><span class="hint--top hint--rounded" aria-label="BY - 署名"><i class="iconfont icon-by"></i></span></a></div></div></div><div class="license-icon iconfont"></div></div><div class="post-prevnext my-3"><article class="post-prev col-6"><a href="/2022/02/26/Mysql/" title="Mysql"><i class="iconfont icon-arrowleft"></i> <span class="hidden-mobile">Mysql</span> <span class="visible-mobile">上一篇</span></a></article><article class="post-next col-6"><a href="/2022/02/04/Redis/" title="Redis"><span class="hidden-mobile">Redis</span> <span class="visible-mobile">下一篇</span> <i class="iconfont icon-arrowright"></i></a></article></div></div></article></div></div></div><div class="side-col d-none d-lg-block col-lg-2"><aside class="sidebar" style="margin-left:-1rem"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i> <span>目录</span></p><div class="toc-body" id="toc-body"></div></div></aside></div></div></div><a id="scroll-top-button" aria-label="TOP" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a><div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable modal-lg" role="document"><div class="modal-content"><div class="modal-header text-center"><h4 class="modal-title w-100 font-weight-bold">搜索</h4><button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button></div><div class="modal-body mx-3"><div class="md-form mb-5"><input type="text" id="local-search-input" class="form-control validate"> <label data-error="x" data-success="v" for="local-search-input">关键词</label></div><div class="list-group" id="local-search-result"></div></div></div></div></div></main><footer><div class="footer-inner"><div class="footer-content"><a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a></div></div></footer><script src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js"></script><link rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css"><script>NProgress.configure({showSpinner:!1,trickleSpeed:100}),NProgress.start(),window.addEventListener("load",function(){NProgress.done()})</script><script src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js"></script><script src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js"></script><script src="/js/events.js"></script><script src="/js/plugins.js"></script><script src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js"></script><script>!function(t){var e=Fluid.plugins.typing,i=t.getElementById("subtitle");i&&e&&e(i.getAttribute("data-typed-text"))}((window,document))</script><script src="/js/img-lazyload.js"></script><script>Fluid.utils.createScript("https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js",function(){var t,o=jQuery("#toc");0!==o.length&&window.tocbot&&(t=jQuery("#board-ctn").offset().top,window.tocbot.init(Object.assign({tocSelector:"#toc-body",contentSelector:".markdown-body",linkClass:"tocbot-link",activeLinkClass:"tocbot-active-link",listClass:"tocbot-list",isCollapsedClass:"tocbot-is-collapsed",collapsibleClass:"tocbot-is-collapsible",scrollSmooth:!0,includeTitleTags:!0,headingsOffset:-t},CONFIG.toc)),0<o.find(".toc-list-item").length&&o.css("visibility","visible"),Fluid.events.registerRefreshCallback(function(){if("tocbot"in window){tocbot.refresh();var t=jQuery("#toc");if(0===t.length||!tocbot)return;0<t.find(".toc-list-item").length&&t.css("visibility","visible")}}))})</script><script src="https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js"></script><script>Fluid.plugins.codeWidget()</script><script>Fluid.utils.createScript("https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js",function(){Fluid.plugins.fancyBox()})</script><script>Fluid.plugins.imageCaption()</script><script src="/js/local-search.js"></script><script src="/js/boot.js"></script><noscript><div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div></noscript></body></html>