{"meta":{"title":"技术博客","subtitle":"吴耿锋的博客","description":"","author":"wugengfeng","url":"https://wugengfeng.cn","root":"/"},"pages":[],"posts":[{"title":"MongoDB","slug":"MongoDB","date":"2023-04-14T09:36:43.000Z","updated":"2023-08-24T10:48:19.791Z","comments":true,"path":"2023/04/14/MongoDB/","link":"","permalink":"https://wugengfeng.cn/2023/04/14/MongoDB/","excerpt":"","text":"知识体系 思维导图 MongoDB简介 MongoDB 是一款开源的分布式文档数据库，介于关系型数据库和非关系型数据库之间。它是功能最丰富、最像关系型数据库的非关系型数据库产品之一。 MongoDB 支持以 JSON（BSON 是一种类似于 JSON 的二进制格式） 格式存储和查询文档，其底层由 C++ 语言编写。 在 MongoDB 中，每个记录都是一个 JSON 文档，类似于 JSON 对象。字段的值可以包括其他文档、数组和对象数组，因此可以存储相对复杂的数据类型。而且，MongoDB 最大的特点是支持强大的查询语言，其语法类似于面向对象的查询语言，可以实现关系型数据库单表查询的绝大部分功能，并支持对数据建立索引。这使得 MongoDB 在处理大量非结构化数据时非常适合使用。 使用文档的优点：文档（即对象）对应于许多编程语言中的内置数据类型。嵌入式文档和数组减少了对昂贵连接（join）的需求。动态模式（不需要事先定义表结构或字段）支持流畅的多态性。 BSON：二进制的JSON，有专门的编解码技术，有额外的日期，二进制数据类型JSON和BSON的区别BSON 数据类型 与关系型数据库对比 MongoDB 关系型数据库 数据库 数据库 集合 表 文档 行 字段 列 索引 索引 _id 主键 视图 视图 聚合操作（$lookup） 表连接 数据模型 数据模型 定义 数据库 最外层的概念，可以理解为逻辑上的名称空间，一个数据库包含多个不同名 称的集合。 集合 相当于SQL中的表，一个集合可以存放多个不同的文档。 文档 一个文档相当于数据表中的一行，由多个不同的字段组成。 字段 文档中的一个属性，等同于列（column）。 索引 索引是一种辅助存储技术，用于加速查询。索引是一种特殊的文档，它包含一个或多个键值对，这些键值对与查询的键值对匹配。 _id _id 是 MongoDB 文档中的一个特殊字段，用于唯一标识每个文档。它是一个内置字段，类似RDBMS的主键。 视图 可以看作一种虚拟的（非真实存在的）集合，与SQL中的视图类似。从MongoDB 3.4版本开始提供了视图功能，其通过聚合管道技术实现 聚合 ($lookup) 用于在集合之间进行关联查询。它类似于 SQL 中的内连接，但可以支持更多的查询操作。 支持的数据类型 数据类型 类型描述 整数类型 (Integer) 支持正整数、负整数和零值。 浮点数类型 (Float) 支持单精度浮点数和双精度浮点数。 字符串类型 (String) 支持任意长度的字符串。 布尔类型 (Boolean) 支持 true 和 false 两个值。 数组类型 (Array) 支持任意长度的数组。 对象类型 (Object) 支持包含多个字段的对象。 嵌套类型 (Deep Object) 支持嵌套对象，即包含多个嵌套对象的类型。 日期类型 (Date) 支持 JavaScript 日期对象。 数字类型 (Number) 支持任意精度的数字。 二进制数据 (BinData) 支持二进制数据。 自定义类型 (Custom Type) 支持用户定义的数据类型。 主要特征 MongoDB基于灵活的JSON文档模型，非常适合 敏捷式的快速开发。与此同时，其与生俱来的高可用、 高水平扩展能力使得它在处理海量、高并发的数据应用时颇具优势 动态模式 反范式：相对RDBMS错中复杂的表关系，mongoDB的JSON模型允许数组、嵌套对象结构，在数据结构上更接近对象关系，开发代码量低 字段扩展：mongoDB支持动态新增字段，不需要修改表结构，开发灵活，快速响应业务变化 强大的查询语言 相比其他非关系型数据库，MongoDB支持丰富的查询语言，支持读和写操作(CRUD)，比如数据聚合、文本搜索和地理空间查询等。 高可用 自动故障转移 数据备份 副本集是一组维护相同数据集合的 mongod实例，提供了冗余和提高了数据可用性 水平扩展 分片将数据分布在一个集群的机器上，支持集群节点扩展 从3.4开始，MongoDB支持基于分片键创建数据区域。在平衡群集中，MongoDB仅将区域覆盖的读写定向到区域内的那些分片 支持多种存储引擎 存储引擎 WiredTiger存储引擎 内存存储引擎 应用场景 应用场景 MongoDB 优势 实时数据处理 MongoDB 支持实时数据处理，可以实时存储和查询数据 关系型数据库替代品 MongoDB 可以存储非结构化数据，可以轻松替代关系型数据库 大规模数据处理 MongoDB 支持高效的批量查询和数据批量插入、更新、删除 日志记录 MongoDB 支持高效的日志记录和查询，可以实时记录和查询日志数据 API 存储 MongoDB 可以存储 API 文档和数据，支持实时查询和更新 分布式文件系统 MongoDB 可以充当分布式文件系统的数据库，支持读写分离和数据备份 分布式消息队列 MongoDB 可以充当分布式消息队列的数据库，支持消息存储和查询 社交场景 存储存储用户信息，以及用户发表的朋友圈信息 物流场景 订单状态在运送过程中会不断更新，以 MongoDB 内嵌数组的形式来存储，一次查询就能将订单所有的变更读取出来 物联网场景 存储所有接入的智能设备信息，以及设备汇报的日志信息，并对这些信息进行多维度的分析 选型原因 不需要复杂join查询以及事务处理 需要更高的读写QPS(3000以上) 需要至少TB或PB级别的数据存储 业务发展迅速，需要快速水平扩展 要求应用高可用 需要地理位置查询，文本查询 基本命令 安装平台为Linux，安装版本为社区6.x版本 linux安装 TODO docker安装 创建配置和数据目录 123456789mkdir -p /data/mongodb/dbmkdir -p /data/mongodb/backupmkdir -p /data/mongodb/logmkdir -p /data/mongodb/configchmod -R 777 /data/mongodb/dbchmod -R 777 /data/mongodb/backupchmod -R 777 /data/mongodb/logchmod -R 777 /data/mongodb/config 在配置目录下创建 mongod.conf 配置文件 12345678910111213141516171819net: port: 27017 bindIp: 0.0.0.0storage: dbPath: /data/mongodb/db journal: enabled: truesystemLog: destination: file logAppend: true path: /data/mongodb/log/mongod.logauth: truestorage: wiredTiger: engineConfig: cacheSizeGB: 0.5 安装MongoDB 123456789docker run -d \\--name mongodb \\-p 27017:27017 \\-v /data/mongodb/db/data:/data/db \\-v /data/mongodb/backup:/data/backup \\-v /data/mongodb/log:/data/log \\-v /data/mongodb/config:/data/conf \\--privileged=true mongo \\--auth 添加账号 12345docker exec -it mongodb /bin/bashmongouse admindb.createUser(&#123;user:&quot;admin&quot;,pwd:&quot;123456&quot;,roles:[&#123;role:&#x27;root&#x27;,db:&#x27;admin&#x27;&#125;]&#125;)exit Mongo shell Mongo shell是一个命令行工具，用于连接和操作MongoDB数据库。并为开发人员提供了直接测试数据库查询和操作的方法 1mongo –port:指定端口，默认为27017 –host:连接的主机地址，默认127.0.0.1 Mongo shell 是基于 JavaScript 语法实现的，MongoDB使用了SpiderMonkey作为其内部的JavaScript解释器引擎，这是由Mozilla官方提供的JavaScript内核解释器，该解释器也被同样用于大名鼎鼎的Firefox浏览器产品之中。SpiderMonkey对ECMA Script标准兼容性非常好，可以支持ECMA Script 6。可以通过下面的命令检查JavaScript解释器的版本 12interpreterVersion()MozJS-60 命令行帮助 要查看选项列表和启动mongo shell相关的帮助，请从命令行使用--help选项 1mongo --help Shell帮助 要查看选项列表和启动mongo shell相关的帮助，请从命令行使用--help选项 1help 数据库帮助 当需要查看服务器上的数据库列表，请使用 show dbs 命令 当需要查看可在db对象上使用的方法的帮助列表，请调用db.help()方法 当需要查看在 shell中查看某些方法的具体实现，请键入不带括号()的db.&lt;method name&gt;，如以下示例所示，它将返回方法db.updateUser()的实现 1db.updateUser 表级别帮助 要查看当前数据库中的集合列表，请使用 show collections 命令 要查看收集对象上可用方法的帮助（例如db.&lt;collection&gt;），请使用db.&lt;collection&gt;.help()方法 1db.collection.help() 创建集合 db.createCollection(name, options) 游标相关帮助 在mongo shell中使用find()方法执行读取操作时，可以使用各种游标方法来修改find()行为，并可以使用各种JavaScript方法来处理从find()方法返回的游标 要列出可用的修饰符和游标处理方法，请使用db.collection.find().help()命令 要查看cursor方法的实现，请输入不带括号(())的db.&lt;collection&gt;.find().&lt;method&gt;名称，如以下示例所示，它将返回toArray()方法的实现 1db.collection.find().toArray 处理游标的一些有用方法是:处理游标的一些有用方法是: hasNext() 检查光标是否还有更多文档要返回 next()返 回下一个文档，并将光标位置向前移动一个 数据库操作 选择和创建数据库 1use 数据库名称 如果数据库不存在则自动创建，当刚开始创建一个数据库的时候信息存储在内存中，所以刚创建的数据库是查询不到的。当数据库创建一个集合后，才会把数据库信息持久化到磁盘，此时数据库才能被检索 查看数据库 1show dbs 或 show databases 查看当前数据库 1db 删除数据库 1db.dropDatabase() 用于删除已持久化的数据库 数据库命名规则 不能是空字符串（“”)。 不得含有’ '（空格)、.、$、/、\\和\\0 (空字符)。 应全部小写。 最多64字节。 默认数据库 admin：这个特殊的数据库主要用于管理 MongoDB 实例，可以创建、删除用户和管理角色。 local：用来存储本地数据的，比如保存自己的复制集状态、操作日志等信息，这个数据库数据不会被复制（可以存储不想被其他节点复制的数据）。 config：用来存储分片集群的配置信息的，如果正在使用 MongoDB 的分片集群功能，则该集群的配置信息将保存在此数据库中。 集合操作 特殊集合： Capped Collections 固定集合，固定大小的集合，超过大小可以覆盖历史文档 Clustered Collections 聚簇集合，使用 聚簇索引 clustered index 创建的集合称为聚集集合 操作 说明 db.createCollection(name, options[可选]) 创建集合 show collections 查看集合 db.test.drop() 删除集合 集合的创建分 显式 和 隐式 显式：db.createCollection(name, options[可选]) 隐式：当使用插入方法（insert）插入到一个不存在的集合时，会自动创建该集合 注意通常采用隐式创建集合，即当向一个集合中插入一个文档的时候，如果集合不存在，则会自动创建集合。 options说明 集合参数 描述 capped 固定集合（Capped Collections）是性能出色且有着固定大小的集合，对于大小固定，我们可以想象其就像一个环形队列，当集合空间用完后，再插入的元素就会覆盖最初始的头部的元素 timeseries 指定集合是否为时间序列集合。时间序列集合包含时间戳和数据值，用于记录时间序列数据。 expireAfterSeconds 指定集合中文档的过期时间，单位为秒。当文档的过期时间到达时，该文档将被删除。对于聚集集合，将根据聚集索引键 _id 自动删除文档，并且值必须是日期类型。请参阅 TTL Indexes. clusteredIndex 指定集合是否使用聚簇索引。集群索引是将集合中的文档按照某种规则进行排序的索引，可以提高查询效率。 changeStreamPreAndPostImages 指定集合是否使用 changeStream。changeStream 是一种用于监视集合中文档更改的 API，可以实时获取集合中文档的更改。 size 指定集合的大小，单位为字节。可以使用 size 命令查询集合的大小。 max 指定集合中文档的最大数量。如果集合中已经有很多文档，则创建集合时可能会失败。 storageEngine 指定集合使用的存储引擎。 validator 指定集合中的文档验证器。验证器用于检查文档是否符合特定的验证规则。 validationLevel 指定集合中的文档验证级别。验证级别越高，验证器的要求也越高，可以提高查询效率，但可能会导致文档验证时间较长。 validationAction 指定集合中的文档验证失败后的行为。可以设置为 drop 或 continue，分别表示验证失败时将文档删除或继续验证。 indexOptionDefaults 指定集合中索引默认的属性。 viewOn 指定集合中文档的视图名称。可以使用 viewOn 命令查询文档的视图名称。 pipeline 指定查询语句中的过滤条件。 collation 指定集合中文档的排序规则。 writeConcern 指定写入操作的可靠性。writeConcern 文档包含了有关写入操作可靠性的详细信息。 CURD操作 CURD操作指的是文档的创建、读、更新以及删除操作 SQL到mongo的操作映射 插入文档 创建 或 插入 操作将新文档添加到集合中。如果该集合当前不存在，则插入操作将创建该集合 MongoDB提供以下将文档插入集合的方法： db.collection.insertOne() 插入一个文档 db.collection.insertMany() 插入多个文档 db.collection.inser() 插入一个或多个文档 db.collection.save() 已废弃，插入或覆盖一个文档 MongoDB中的所有写入操作在单个文档的级别上都是原子的，参考原子性和事务 insertOne 将一个文档插入到一个集合中，支持 writeConcern 写关注 123456db.collection.insertOne( &lt;document&gt;, &#123; writeConcern: &lt;document&gt; &#125;) writeConcern 写关注 决定一个写操作落到多少个节点上才算成功。writeConcern w 属性的取值包括： 0: 发起写操作，不进行写操作确认 1: 请求确认写入操作已传播到复制集中的独立副本或主节点。是MongoDB的默认写关注点 n: 写操作需要被复制到指定节点数才算成功（默认为1，主分片写入成功） majority：写操作需要被复制到大多数节点上才算成功（半数以上） 1234567891011db.user.insertOne(&#123; &quot;userName&quot;: &quot;wgf&quot;, &quot;age&quot;: 20, &quot;sex&quot;: 1, &quot;address&quot;: &quot;shenzhen&quot; &#125;)&#123; &quot;acknowledged&quot; : true, &quot;insertedId&quot; : ObjectId(&quot;64414ce28dddba084284aa92&quot;)&#125; 指定插入1个副本后才算写入成功 12345678910db.user.insertOne(&#123; &quot;userName&quot;: &quot;test&quot;, &quot;age&quot;: 21, &quot;sex&quot;: 1, &quot;address&quot;: &quot;shanghai&quot; &#125;,&#123; writeConcern: &#123; w: 1 &#125; &#125;) insertMany 将多个文档插入到一个集合中 1234567db.collection.insertMany( [ &lt;document 1&gt; , &lt;document 2&gt;, ... ], &#123; writeConcern: &lt;document&gt;, ordered: &lt;boolean&gt; &#125;) writeConcern 写关注 ordered: 是否按照顺序写入，默认为true true：顺序插入，如果某个文档插入失败，那么后面的插入操作也会终止 false：无序插入，如果某个文档插入失败，MongoDB 会记录错误，但不会停止插入操作（提高插入性能） 12345678910111213141516171819202122232425262728db.user.insertMany( [ &#123; &quot;userName&quot; : &quot;zhangsan&quot;, &quot;age&quot; : 18, &quot;sex&quot; : 1, &quot;address&quot; : &quot;beijing&quot; &#125;, &#123; &quot;userName&quot; : &quot;lisi&quot;, &quot;age&quot; : 26, &quot;sex&quot; : 1, &quot;address&quot; : &quot;hainan&quot; &#125;, ], &#123; &quot;ordered&quot;: true &#125;)&#123; &quot;acknowledged&quot; : true, &quot;insertedIds&quot; : [ ObjectId(&quot;644169568dddba084284aaa5&quot;), ObjectId(&quot;644169568dddba084284aaa6&quot;) ]&#125; insert 将一个或多个文档插入集合 1234567db.collection.insert( &lt;document or array of documents&gt;, &#123; writeConcern: &lt;document&gt;, ordered: &lt;boolean&gt; &#125;) 1234567891011121314db.user.insert(&#123; &quot;userName&quot; : &quot;luliu&quot;, &quot;age&quot; : 28, &quot;sex&quot; : 0, &quot;address&quot; : &quot;tianjing&quot; &#125;, &#123; &quot;writeConcern&quot;: &#123; &quot;w&quot;: 1 &#125;&#125;)Inserted 1 record(s) in 115ms save 覆盖现有的文档或插入新文档，具体取决于其document参数 123456db.collection.save( &lt;document&gt;, &#123; writeConcern: &lt;document&gt; &#125;) 1db.products.save( &#123; _id: 100, item: &quot;water&quot;, qty: 30 &#125; ) 如果 _id=100 的记录存在则更新文档(全量覆盖)，否则新增文档 insert和save的区别 insert: 若新增数据的主键已经存在，则会抛 org.springframework.dao.DuplicateKeyException 异常提示主键重复，不保存当前数据 save: 若新增数据的主键已经存在，则会对当前已经存在的数据进行修改操作(文档全量覆盖) 查询文档 查询语法如下 1db.collection.find(query, projection, options) 参数说明： query ：可选，一个查询条件，用于指定要返回的文档 projection ：可选，使用投影操作符指定返回的字段（默认返回所有），指定返回 &lt;field&gt;: &lt;1 or true&gt;，不返回 &lt;field&gt;: &lt;0 or false&gt; options：可选，查询选项 如果查询返回的条目数量较多，mongo shell则会自动实现分批显示。默认情况下每次只显示20条，可以输入 it 命令读取下一批 测试数据 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192[ &#123; &quot;_id&quot; : ObjectId(&quot;5e7835466b6f69d428000001&quot;), &quot;name&quot; : &quot;Alice&quot;, &quot;age&quot; : 30.0, &quot;address&quot; : &#123; &quot;city&quot; : &quot;New York&quot;, &quot;state&quot; : &quot;NY&quot;, &quot;zip&quot; : &quot;10001&quot; &#125; &#125;, &#123; &quot;_id&quot; : ObjectId(&quot;5e7835476b6f69d428000002&quot;), &quot;name&quot; : &quot;Bob&quot;, &quot;age&quot; : 25.0, &quot;address&quot; : &#123; &quot;city&quot; : &quot;Chicago&quot;, &quot;state&quot; : &quot;IL&quot;, &quot;zip&quot; : &quot;60606&quot; &#125; &#125;, &#123; &quot;_id&quot; : ObjectId(&quot;5e7835486b6f69d428000003&quot;), &quot;name&quot; : &quot;Charlie&quot;, &quot;age&quot; : 28.0, &quot;address&quot; : &#123; &quot;city&quot; : &quot;Houston&quot;, &quot;state&quot; : &quot;Texas&quot;, &quot;zip&quot; : &quot;77002&quot; &#125; &#125;, &#123; &quot;_id&quot; : ObjectId(&quot;5e7835496b6f69d428000004&quot;), &quot;name&quot; : &quot;Dave&quot;, &quot;age&quot; : 35.0, &quot;address&quot; : &#123; &quot;city&quot; : &quot;San Francisco&quot;, &quot;state&quot; : &quot;CA&quot;, &quot;zip&quot; : &quot;94107&quot; &#125; &#125;, &#123; &quot;_id&quot; : ObjectId(&quot;5e7835496b6f69d428000005&quot;), &quot;name&quot; : &quot;Eve&quot;, &quot;age&quot; : 20.0, &quot;address&quot; : &#123; &quot;city&quot; : &quot;New York&quot;, &quot;state&quot; : &quot;NY&quot;, &quot;zip&quot; : &quot;10001&quot; &#125; &#125;, &#123; &quot;_id&quot; : ObjectId(&quot;5e7835496b6f69d428000006&quot;), &quot;name&quot; : &quot;Adam&quot;, &quot;age&quot; : 25.0, &quot;address&quot; : &#123; &quot;city&quot; : &quot;London&quot;, &quot;state&quot; : &quot;GB&quot;, &quot;zip&quot; : &quot;E1 4ST&quot; &#125; &#125;, &#123; &quot;_id&quot; : ObjectId(&quot;5e7835496b6f69d428000007&quot;), &quot;name&quot; : &quot;Greg&quot;, &quot;age&quot; : 30.0, &quot;address&quot; : &#123; &quot;city&quot; : &quot;New York&quot;, &quot;state&quot; : &quot;NY&quot;, &quot;zip&quot; : &quot;10001&quot; &#125; &#125;, &#123; &quot;_id&quot; : ObjectId(&quot;5e7835496b6f69d428000008&quot;), &quot;name&quot; : &quot;Jack&quot;, &quot;age&quot; : 25.0, &quot;address&quot; : &#123; &quot;city&quot; : &quot;London&quot;, &quot;state&quot; : &quot;GB&quot;, &quot;zip&quot; : &quot;E1 1AA&quot; &#125; &#125;, &#123; &quot;_id&quot; : ObjectId(&quot;5e7835496b6f69d428000009&quot;), &quot;name&quot; : &quot;Emily&quot;, &quot;age&quot; : 20.0, &quot;address&quot; : &#123; &quot;city&quot; : &quot;Paris&quot;, &quot;state&quot; : &quot;FR&quot;, &quot;zip&quot; : &quot;10003&quot; &#125; &#125;] 查询 12345678910db.getCollection(&#x27;user&#x27;).find(&#123; name: &quot;Alice&quot;&#125;,&#123; name: 1, age: 1&#125;,&#123; max: 5&#125;) 查询条件 比较运算符 SQL Mongo 运算符 说明 a = 1 {a: 1} 或 {a: {$eq: 1}} $eq 等于 a != 1 {a: {$ne: 1}} $ne 不等于 a &gt; 1 {a: {$gt: 1}} $gt 大于 a &gt;= 1 {a: {$gte: 1}} $gte 大于等于 a &lt; 1 {a: {$lt: 1}} $lt 小于 a &lt;= 1 {a: {$lte: 1}} $lte 小于等于 a IN (1, 2, 3) {a: {$in: [1, 2, 3]}} $in 多值查询 a NOT IN (1,2,3) {a: {$nin: [1,2,3]}} $nin 多值查询取反 逻辑运算符 SQL Mongo 运算符 说明 a = 1 AND b = 1 {a: 1, b: 1}或{$and: [{a: 1}, {b: 1}]} $and 并且 a = 1 OR b = 1 {$or: [{a: 1}, {b: 1}]} $or 或者 a != 1 {a: { $not: { $eq: 1}}} $not 逻辑否定 a!=1 AND b!=1 {$nor: [{a:1}, {b: 1}]} $nor 用于实现多个查询条件之间的逻辑否定 元素运算符 SQL Mongo 运算符 说明 a IS NULL {a: {$exists: false}} $exists 判断对应字段值是否存在 - {“title” : {$type : ‘string’}} BSON 数据类型 $type 获取某个字段为指定类型的文档 数组运算符 Mongo 运算符 说明 { tags: { $all: [ “ssl” , “security” ] } } $all 匹配包含查询中指定的所有元素的数组 { results: { $elemMatch: { product: “xyz”, score: { $gte: 8 } } } }results 是个对象数组 $elemMatch 匹配数组字段，该数组至少有一个元素满足所有查询条件 { tag: { $size: 1 } } $size 查询特定长度的数组 文档查询 find db.collection.find(query, projection, options) 查询一个或多个文档，默认返回前20个文档，输入 it 继续迭代 12345678910111213141516171819db.getCollection(&#x27;user&#x27;).find( &#123; age: &#123; $gte: 32 &#125; &#125;, &#123; name: true, age: true &#125;)&#123; &quot;_id&quot; : ObjectId(&quot;5e7835496b6f69d428000004&quot;), &quot;name&quot; : &quot;Dave&quot;, &quot;age&quot; : 35.0&#125; findOne db.collection.findOne(query, projection, options) 查询单个文档，返回一个满足集合或视图上指定查询条件的文档。如果有多个文档满足查询，则该方法按照反映文档在磁盘上的顺序的 自然顺序返回第一个文档 123456789101112131415db.getCollection(&#x27;user&#x27;).findOne( &#123; &#125;, &#123; name: true, age: true &#125;)&#123; &quot;_id&quot; : ObjectId(&quot;5e7835466b6f69d428000001&quot;), &quot;name&quot; : &quot;Alice&quot;, &quot;age&quot; : 30.0&#125; findAndModify db.collection.findAndModify(document) 以原子方式修改并返回单个文档。默认情况下，返回的文档不包括对更新所做的修改（旧文档）。要返回包含更新修改的文档，请使用 new 选项（新文档） 1234567891011121314db.collection.findAndModify(&#123; query: &lt;document&gt;, sort: &lt;document&gt;, remove: &lt;boolean&gt;, update: &lt;document or aggregation pipeline&gt;, // Changed in MongoDB 4.2 new: &lt;boolean&gt;, fields: &lt;document&gt;, upsert: &lt;boolean&gt;, bypassDocumentValidation: &lt;boolean&gt;, writeConcern: &lt;document&gt;, collation: &lt;document&gt;, arrayFilters: [ &lt;filterdocument1&gt;, ... ], let: &lt;document&gt; // Added in MongoDB 5.0&#125;); 比较和 findOneAndUpdate() 比较，findAndModify()方法则可以执行更复杂的操作，如删除、插入、修改等。该方法也可以返回更新前的文档或更新后的文档。 参数 类型 描述 query document 可选的。修改的选择标准。 query字段使用与db.collection.find()方法中使用的query selectors相同的query selectors。虽然查询可能匹配多个文档，但findAndModify() 只会选择一个文档来修改。 如果未指定，则默认为空文档。 从 MongoDB 3.6.14(和 3.4.23)开始，如果查询参数不是文档，则操作错误。 sort document 可选的。如果查询选择多个文档，则确定操作修改的文档。 findAndModify()修改此参数指定的 sort order 中的第一个文档。 从 MongoDB 3.6.14(和 3.4.23)开始，如果 sort 参数不是文档，则操作错误。 remove boolean 必须指定remove或update字段。删除query字段中指定的文档。将其设置为true以删除所选文档。默认值为false。 update document 必须指定remove或update字段。执行所选文档的更新。 update字段使用相同的更新 operators或field: value规范来修改所选文档。 new boolean 可选的。当true时，返回修改后的文档而不是原始文档。 findAndModify()方法忽略remove操作的new选项。默认值为false。 fields document 可选的。 return 的字段子集。 fields文档指定包含1的字段，如：fields: &#123; &lt;field1&gt;: 1, &lt;field2&gt;: 1, ... &#125;。见投影。 从 MongoDB 3.6.14(和 3.4.23)开始，如果 fields 参数不是文档，则操作错误。 upsert boolean 可选的。与update字段结合使用。 当true，findAndModify()时： 如果没有文件匹配query，则创建一个新文档。有关详细信息，请参阅upsert 行为。 更新与query匹配的单个文档。 要避免多次 upsert，请确保query字段为唯一索引。 默认为false。 bypassDocumentValidation boolean 可选的。允许db.collection.findAndModify在操作期间绕过文档验证。这使您可以更新不符合验证要求的文档。 version 3.2 中的新内容。 writeConcern document 可选的。表示写关注的文件。省略使用默认写入问题。 version 3.2 中的新内容。 maxTimeMS integer 可选的。指定处理操作的 time 限制(以毫秒为单位)。 collation document 可选的。 指定要用于操作的整理。 整理允许用户为 string 比较指定 language-specific 规则，例如字母和重音标记的规则。 排序规则选项具有以下语法： 排序规则：{ locale：， caseLevel：， caseFirst：， strength：， numericOrdering：， alternate：， maxVariable：， backwards ： } 指定排序规则时，locale字段是必填字段;所有其他校对字段都是可选的。有关字段的说明，请参阅整理文件。 如果未指定排序规则但集合具有默认排序规则(请参阅db.createCollection())，则操作将使用为集合指定的排序规则。 如果没有为集合或操作指定排序规则，MongoDB 使用先前版本中用于 string 比较的简单二进制比较。 您无法为操作指定多个排序规则。对于 example，您不能为每个字段指定不同的排序规则，或者如果使用排序执行查找，则不能对查找使用一个排序规则，而对排序使用另一个排序规则。 version 3.4 中的新内容。 arrayFilters array 可选的。过滤器文档的 array，用于确定要在 array 字段上为更新操作修改哪些 array 元素。 在更新文档中，使用$ []过滤后的位置 operator 来定义标识符，然后在 array 过滤器文档中进行 reference。如果标识符未包含在更新文档中，则不能为标识符提供 array 过滤器文档。 注意 &lt;identifier&gt;必须以小写字母开头，并且只包含字母数字字符。 您可以在更新文档中多次包含相同的标识符;但是，对于更新文档中的每个不同标识符($[identifier])，您必须指定恰好一个对应的 array 过滤器文档。也就是说，您不能为同一标识符指定多个 array 过滤器文档。对于 example，如果 update 语句包含标识符x(可能多次)，则不能为arrayFilters指定以下内容，其中包含 2 个单独的x过滤器文档： [ { “x.a”: { $gt: 85 } }, { “x.b”: { $gt: 80 } } ] 但是，您可以在同一标识符上指定复合条件单个过滤器文档，例如以下示例： // Example 1 [ { or: [{\"x.a\": {gt: 85}}, {“x.b”: {$gt: 80}}] } ] // Example 2 [ { and: [{\"x.a\": {gt: 85}}, {“x.b”: {$gt: 80}}] } ] // Example 3 [ { “x.a”: { $gt: 85 }, “x.b”: { $gt: 80 } } ] 例如，请参阅为 Array Update Operations 指定 arrayFilters。 version 3.6 中的新内容。 更新并返回新的文档 123456789101112131415161718192021db.user.findAndModify(&#123; query: &#123; _id: ObjectId(&quot;5e7835466b6f69d428000001&quot;) &#125;, update: &#123; $inc: &#123;age: 1&#125; &#125;, new: true &#125;) &#123; &quot;_id&quot; : ObjectId(&quot;5e7835466b6f69d428000001&quot;), &quot;name&quot; : &quot;Alice&quot;, &quot;age&quot; : 31.0, &quot;address&quot; : &#123; &quot;city&quot; : &quot;New York&quot;, &quot;state&quot; : &quot;NY&quot;, &quot;zip&quot; : &quot;10001&quot; &#125;&#125; findOneAndDelete db.collection.findOneAndDelete(filter, options) 根据 filter 和 sort 条件删除单个文档，返回已删除的文档 12345678910db.collection.findOneAndDelete( &lt;filter&gt;, &#123; writeConcern: &lt;document&gt;, projection: &lt;document&gt;, sort: &lt;document&gt;, maxTimeMS: &lt;number&gt;, collation: &lt;document&gt; &#125;) 参数 类型 描述 filter document 更新的选择标准。可以使用与find()方法相同的query selectors。 指定空文档&#123; &#125;以删除集合中返回的第一个文档。 如果未指定，则默认为空文档。 从 MongoDB 3.6.14(和 3.4.23)开始，如果查询参数不是文档，则操作错误。 projection document 可选的。 return 的字段子集。 要_返回返回文档中的所有字段，请省略此参数。 从 MongoDB 3.6.14(和 3.4.23)开始，如果投影参数不是文档，则操作错误。 sort document 可选的。为filter匹配的文档指定排序 order。 从 MongoDB 3.6.14(和 3.4.23)开始，如果 sort 参数不是文档，则操作错误。 见cursor.sort()。 maxTimeMS number 可选的。指定操作必须在其中完成的 time 限制(以毫秒为单位)。如果超出限制则引发错误。 collation document 可选的。 指定要用于操作的整理。 整理允许用户为 string 比较指定 language-specific 规则，例如字母和重音标记的规则。 排序规则选项具有以下语法： 排序规则：{ locale：， caseLevel：， caseFirst：， strength：， numericOrdering：， alternate：， maxVariable：， backwards ： } 指定排序规则时，locale字段是必填字段;所有其他校对字段都是可选的。有关字段的说明，请参阅整理文件。 如果未指定排序规则但集合具有默认排序规则(请参阅db.createCollection())，则操作将使用为集合指定的排序规则。 如果没有为集合或操作指定排序规则，MongoDB 使用先前版本中用于 string 比较的简单二进制比较。 您无法为操作指定多个排序规则。对于 example，您不能为每个字段指定不同的排序规则，或者如果使用排序执行查找，则不能对查找使用一个排序规则，而对排序使用另一个排序规则。 version 3.4 中的新内容。 按照名称排序，删除第一个文档并返回删除的文档 1234567891011121314151617181920212223db.user.findOneAndDelete( &#123; age: &#123;$gte: 30&#125; &#125;, &#123; sort: &#123; name: 1 &#125; &#125;)&#123; &quot;_id&quot; : ObjectId(&quot;5e7835466b6f69d428000001&quot;), &quot;name&quot; : &quot;Alice&quot;, &quot;age&quot; : 31.0, &quot;address&quot; : &#123; &quot;city&quot; : &quot;New York&quot;, &quot;state&quot; : &quot;NY&quot;, &quot;zip&quot; : &quot;10001&quot; &#125;&#125; findOneAndReplace db.collection.findOneAndReplace(filter, replacement, options) 根据filter和sort条件修改和替换单个文档(全量替换，除了 _id字段) 123456789101112db.collection.findOneAndReplace( &lt;filter&gt;, &lt;replacement&gt;, &#123; projection: &lt;document&gt;, sort: &lt;document&gt;, maxTimeMS: &lt;number&gt;, upsert: &lt;boolean&gt;, returnNewDocument: &lt;boolean&gt;, collation: &lt;document&gt; &#125;) 参数 类型 描述 filter document 更新的选择标准。可以使用与find()方法相同的query selectors。 指定一个空文档&#123; &#125;以替换集合中返回的第一个文档。 如果未指定，则默认为空文档。 从 MongoDB 3.6.14(和 3.4.23)开始，如果查询参数不是文档，则操作错误。 replacement document 替换文件。 不能包含更新 operators。 &lt;replacement&gt;文档无法指定与替换文档不同的_id value。 projection document 可选的。 return 的字段子集。 要_return 匹配文档中的所有字段，请省略此参数。 从 MongoDB 3.6.14(和 3.4.23)开始，如果投影参数不是文档，则操作错误。 sort document 可选的。为filter匹配的文档指定排序 order。 从 MongoDB 3.6.14(和 3.4.23)开始，如果 sort 参数不是文档，则操作错误。 见cursor.sort()。 maxTimeMS number 可选的。指定操作必须在其中完成的 time 限制(以毫秒为单位)。如果超出限制则引发错误。 upsert boolean 可选的。当true，findOneAndReplace()时： 如果没有文档与filter匹配，则从replacement参数插入文档。插入新文档后返回null，除非returnNewDocument是true。 用replacement文档替换与filter匹配的文档。 MongoDB 将_id字段添加到替换文档中，如果未在filter或replacement文档中指定。如果两者都存在_id，则值必须相等。 要避免多次 upsert，请确保query字段为唯一索引。 默认为false。 returnNewDocument boolean 可选的。当true时，返回替换文档而不是原始文档。 默认为false。 collation document 可选的。 指定要用于操作的整理。 整理允许用户为 string 比较指定 language-specific 规则，例如字母和重音标记的规则。 排序规则选项具有以下语法： 排序规则：{ locale：， caseLevel：， caseFirst：， strength：， numericOrdering：， alternate：， maxVariable：， backwards ： } 指定排序规则时，locale字段是必填字段;所有其他校对字段都是可选的。有关字段的说明，请参阅整理文件。 如果未指定排序规则但集合具有默认排序规则(请参阅db.createCollection())，则操作将使用为集合指定的排序规则。 如果没有为集合或操作指定排序规则，MongoDB 使用先前版本中用于 string 比较的简单二进制比较。 您无法为操作指定多个排序规则。对于 example，您不能为每个字段指定不同的排序规则，或者如果使用排序执行查找，则不能对查找使用一个排序规则，而对排序使用另一个排序规则。 version 3.4 中的新内容。 替换文档 1234567891011121314151617181920212223242526db.user.findOneAndReplace( &#123; _id: ObjectId(&quot;5e7835476b6f69d428000002&quot;) &#125;, &#123; &quot;name&quot; : &quot;wgf&quot;, &quot;age&quot; : 25, &quot;address&quot; : &#123; &quot;city&quot; : &quot;shenzhen&quot;, &quot;state&quot; : &quot;guangdong&quot;, &quot;zip&quot; : &quot;518111&quot; &#125; &#125;)&#123; &quot;_id&quot; : ObjectId(&quot;5e7835476b6f69d428000002&quot;), &quot;name&quot; : &quot;Bob&quot;, &quot;age&quot; : 25.0, &quot;address&quot; : &#123; &quot;city&quot; : &quot;Chicago&quot;, &quot;state&quot; : &quot;IL&quot;, &quot;zip&quot; : &quot;60606&quot; &#125;&#125; findOneAndUpdate db.collection.findOneAndUpdate(filter, update, options) 根据 filter 和 sort 条件更新单个文档 1234567891011121314db.collection.findOneAndUpdate( &lt;filter&gt;, &lt;update document or aggregation pipeline&gt;, // Changed in MongoDB 4.2 &#123; projection: &lt;document&gt;, sort: &lt;document&gt;, maxTimeMS: &lt;number&gt;, upsert: &lt;boolean&gt;, returnDocument: &lt;string&gt;, returnNewDocument: &lt;boolean&gt;, collation: &lt;document&gt;, arrayFilters: [ &lt;filterdocument1&gt;, ... ] &#125;) 参数 类型 描述 filter document 更新的选择标准。可以使用与find()方法相同的query selectors。 指定一个空文档&#123; &#125;以更新集合中返回的第一个文档。 如果未指定，则默认为空文档。 从 MongoDB 3.6.14(和 3.4.23)开始，如果查询参数不是文档，则操作错误。 update document 更新文件。 必须仅包含更新 operators。 projection document 可选的。 return 的字段子集。 要_返回返回文档中的所有字段，请省略此参数。 从 MongoDB 3.6.14(和 3.4.23)开始，如果投影参数不是文档，则操作错误。 sort document 可选的。为filter匹配的文档指定排序 order。 从 MongoDB 3.6.14(和 3.4.23)开始，如果 sort 参数不是文档，则操作错误。 见cursor.sort()。 maxTimeMS number 可选的。指定操作必须在其中完成的 time 限制(以毫秒为单位)。如果超出限制则引发错误。 upsert boolean 可选的。当true，findOneAndUpdate()时： 如果没有文件匹配filter，则创建一个新文档。有关详细信息，请参阅upsert 行为。插入新文档后返回null，除非returnNewDocument是true。 更新与filter匹配的单个文档。 要避免多次 upsert，请确保filter字段为唯一索引。 默认为false。 returnNewDocument boolean 可选的。当true时，返回更新的文档而不是原始文档。 默认为false。 collation document 可选的。 指定要用于操作的整理。 整理允许用户为 string 比较指定 language-specific 规则，例如字母和重音标记的规则。 排序规则选项具有以下语法： 排序规则：{ locale：， caseLevel：， caseFirst：， strength：， numericOrdering：， alternate：， maxVariable：， backwards ： } 指定排序规则时，locale字段是必填字段;所有其他校对字段都是可选的。有关字段的说明，请参阅整理文件。 如果未指定排序规则但集合具有默认排序规则(请参阅db.createCollection())，则操作将使用为集合指定的排序规则。 如果没有为集合或操作指定排序规则，MongoDB 使用先前版本中用于 string 比较的简单二进制比较。 您无法为操作指定多个排序规则。对于 example，您不能为每个字段指定不同的排序规则，或者如果使用排序执行查找，则不能对查找使用一个排序规则，而对排序使用另一个排序规则。 version 3.4 中的新内容。 arrayFilters array 可选的。过滤器文档的 array，用于确定要在 array 字段上为更新操作修改哪些 array 元素。 在更新文档中，使用$ []过滤后的位置 operator 来定义标识符，然后在 array 过滤器文档中进行 reference。如果标识符未包含在更新文档中，则不能为标识符提供 array 过滤器文档。 注意 &lt;identifier&gt;必须以小写字母开头，并且只包含字母数字字符。 您可以在更新文档中多次包含相同的标识符;但是，对于更新文档中的每个不同标识符($[identifier])，您必须指定恰好一个对应的 array 过滤器文档。也就是说，您不能为同一标识符指定多个 array 过滤器文档。对于 example，如果 update 语句包含标识符x(可能多次)，则不能为arrayFilters指定以下内容，其中包含 2 个单独的x过滤器文档： // INVALID [ { “x.a”: { $gt: 85 } }, { “x.b”: { $gt: 80 } } ] 但是，您可以在同一标识符上指定复合条件单个过滤器文档，例如以下示例： // Example 1 [ { or: [{\"x.a\": {gt: 85}}, {“x.b”: {$gt: 80}}] } ] // Example 2 [ { and: [{\"x.a\": {gt: 85}}, {“x.b”: {$gt: 80}}] } ] // Example 3 [ { “x.a”: { $gt: 85 }, “x.b”: { $gt: 80 } } ] 例如，请参阅为 Array Update Operations 指定 arrayFilters。 version 3.6 中的新内容。 更新当个文档，并返回旧文档 12345678910111213141516171819202122db.user.findOneAndUpdate( &#123; name: &quot;wgf&quot; &#125;, &#123; $inc: &#123; age: 1 &#125; &#125;)&#123; &quot;_id&quot; : ObjectId(&quot;5e7835476b6f69d428000002&quot;), &quot;name&quot; : &quot;wgf&quot;, &quot;age&quot; : 25.0, &quot;address&quot; : &#123; &quot;city&quot; : &quot;shenzhen&quot;, &quot;state&quot; : &quot;guangdong&quot;, &quot;zip&quot; : &quot;518111&quot; &#125;&#125; 嵌套文档查询 测试数据 1234567db.inventory.insertMany( [ &#123; item: &quot;journal&quot;, qty: 25, size: &#123; h: 14, w: 21, uom: &quot;cm&quot; &#125;, status: &quot;A&quot; &#125;, &#123; item: &quot;notebook&quot;, qty: 50, size: &#123; h: 8.5, w: 11, uom: &quot;in&quot; &#125;, status: &quot;A&quot; &#125;, &#123; item: &quot;paper&quot;, qty: 100, size: &#123; h: 8.5, w: 11, uom: &quot;in&quot; &#125;, status: &quot;D&quot; &#125;, &#123; item: &quot;planner&quot;, qty: 75, size: &#123; h: 22.85, w: 30, uom: &quot;cm&quot; &#125;, status: &quot;D&quot; &#125;, &#123; item: &quot;postcard&quot;, qty: 45, size: &#123; h: 10, w: 15.25, uom: &quot;cm&quot; &#125;, status: &quot;A&quot; &#125;]); 等值查询 12345678910111213141516171819202122db.inventory.find( &#123; size: &#123; h: 14, w: 21, uom: &quot;cm&quot; &#125; &#125;)&#123; &quot;_id&quot; : ObjectId(&quot;646c8f4baf188546f987c4f9&quot;), &quot;item&quot; : &quot;journal&quot;, &quot;qty&quot; : 25.0, &quot;size&quot; : &#123; &quot;h&quot; : 14.0, &quot;w&quot; : 21.0, &quot;uom&quot; : &quot;cm&quot; &#125;, &quot;status&quot; : &quot;A&quot;&#125; 对嵌套文档做 等值 查询时，使用查询筛选器文档 &#123; &lt;field&gt;: &lt;value&gt; &#125; ，其中 &lt;value&gt; 是要匹配的文档。 对嵌套文档做等值查询时，要求指定的 &lt;value&gt; 文档（包括字段和顺序）完全匹配，例如一下文档字段调换位置就不满足等值查询，查询不出结果 123456789db.inventory.find( &#123; size: &#123; w: 21, h: 14, uom: &quot;cm&quot; &#125; &#125;) 嵌套文档字段查询 要在嵌套文档中的字段上指定查询条件，请使用点符号 ( &quot;field.nestedField&quot; ) 注意当在查询语句中使用 &quot;.&quot;，字段和嵌套文档字段必须在引号内 下面例子选择嵌套在 size 字段中的字段 uom 等于 &quot;in&quot; 的所有文档： 1234567891011121314151617181920212223242526272829db.inventory.find( &#123; &quot;size.uom&quot;: &quot;in&quot; &#125; )/* 1 */&#123; &quot;_id&quot; : ObjectId(&quot;646c8f4baf188546f987c4fa&quot;), &quot;item&quot; : &quot;notebook&quot;, &quot;qty&quot; : 50.0, &quot;size&quot; : &#123; &quot;h&quot; : 8.5, &quot;w&quot; : 11.0, &quot;uom&quot; : &quot;in&quot; &#125;, &quot;status&quot; : &quot;A&quot;&#125;/* 2 */&#123; &quot;_id&quot; : ObjectId(&quot;646c8f4baf188546f987c4fb&quot;), &quot;item&quot; : &quot;paper&quot;, &quot;qty&quot; : 100.0, &quot;size&quot; : &#123; &quot;h&quot; : 8.5, &quot;w&quot; : 11.0, &quot;uom&quot; : &quot;in&quot; &#125;, &quot;status&quot; : &quot;D&quot;&#125; 嵌套文档组数查询 测试数据 1234567db.inventory.insertMany( [ &#123; item: &quot;journal&quot;, instock: [ &#123; warehouse: &quot;A&quot;, qty: 5 &#125;, &#123; warehouse: &quot;C&quot;, qty: 15 &#125; ] &#125;, &#123; item: &quot;notebook&quot;, instock: [ &#123; warehouse: &quot;C&quot;, qty: 5 &#125; ] &#125;, &#123; item: &quot;paper&quot;, instock: [ &#123; warehouse: &quot;A&quot;, qty: 60 &#125;, &#123; warehouse: &quot;B&quot;, qty: 15 &#125; ] &#125;, &#123; item: &quot;planner&quot;, instock: [ &#123; warehouse: &quot;A&quot;, qty: 40 &#125;, &#123; warehouse: &quot;B&quot;, qty: 5 &#125; ] &#125;, &#123; item: &quot;postcard&quot;, instock: [ &#123; warehouse: &quot;B&quot;, qty: 15 &#125;, &#123; warehouse: &quot;C&quot;, qty: 35 &#125; ] &#125;]); 等值查询 查询库存对象数组中，仓库为 “A”，库存为 5 的库存数据 123456789101112131415161718192021222324db.inventory.find( &#123; instock: &#123; warehouse: &quot;A&quot;, qty: 5 &#125; &#125;)&#123; &quot;_id&quot; : ObjectId(&quot;644628c7f6e1b1a011924011&quot;), &quot;item&quot; : &quot;journal&quot;, &quot;instock&quot; : [ &#123; &quot;warehouse&quot; : &quot;A&quot;, &quot;qty&quot; : 5.0 &#125;, &#123; &quot;warehouse&quot; : &quot;C&quot;, &quot;qty&quot; : 15.0 &#125; ]&#125; 当对整个嵌套文档使用等值匹配的时候是要求精确匹配指定文档，包括字段顺序。比如，下面的语句并没有查询到 inventory 集合中的任何文档： 1db.inventory.find( &#123; &quot;instock&quot;: &#123; qty: 5, warehouse: &quot;A&quot; &#125; &#125; ) 数组嵌套文档指定查询条件 如果你不知道嵌套在数组中的文档的索引位置，请将数组字段的名称与一个点 ( . ) 和嵌套文档中的字段名称连接起来，这样就会返回符合文档数组条件的整个文档 注意嵌套文档数组中，只要有一个嵌套文档满足条件，就会返回整个文档当在查询语句中使用 &quot;.&quot;，字段和嵌套文档字段必须在引号内 查询嵌套文档数组 instock 库存大于等于40的文档 12345678910111213141516171819202122232425262728293031323334353637383940db.inventory.find( &#123; &quot;instock.qty&quot;: &#123; $gte: 40 &#125; &#125;)/* 1 */&#123; &quot;_id&quot; : ObjectId(&quot;644628c7f6e1b1a011924013&quot;), &quot;item&quot; : &quot;paper&quot;, &quot;instock&quot; : [ &#123; &quot;warehouse&quot; : &quot;A&quot;, &quot;qty&quot; : 60.0 &#125;, &#123; &quot;warehouse&quot; : &quot;B&quot;, &quot;qty&quot; : 15.0 &#125; ]&#125;/* 2 */&#123; &quot;_id&quot; : ObjectId(&quot;644628c7f6e1b1a011924014&quot;), &quot;item&quot; : &quot;planner&quot;, &quot;instock&quot; : [ &#123; &quot;warehouse&quot; : &quot;A&quot;, &quot;qty&quot; : 40.0 &#125;, &#123; &quot;warehouse&quot; : &quot;B&quot;, &quot;qty&quot; : 5.0 &#125; ]&#125; 使用数组下标查询嵌套文档字段 查询所有文档中，instock 数组的第一个元素的 qty 小于等于5的文档 123456789101112131415161718192021222324252627282930313233343536db.inventory.find( &#123; &quot;instock.0.qty&quot;: &#123; $lte: 5 &#125; &#125;)/* 1 */&#123; &quot;_id&quot; : ObjectId(&quot;644628c7f6e1b1a011924011&quot;), &quot;item&quot; : &quot;journal&quot;, &quot;instock&quot; : [ &#123; &quot;warehouse&quot; : &quot;A&quot;, &quot;qty&quot; : 5.0 &#125;, &#123; &quot;warehouse&quot; : &quot;C&quot;, &quot;qty&quot; : 15.0 &#125; ]&#125;/* 2 */&#123; &quot;_id&quot; : ObjectId(&quot;644628c7f6e1b1a011924012&quot;), &quot;item&quot; : &quot;notebook&quot;, &quot;instock&quot; : [ &#123; &quot;warehouse&quot; : &quot;C&quot;, &quot;qty&quot; : 5.0 &#125; ]&#125; 数组中的嵌套文档多条件匹配 单个嵌套文档中的字段满足多个查询条件 $elemMatch 操作符为数组中的嵌套文档指定多个查询条件，最少一个嵌套文档同时满足所有的查询条件 查询数组中的嵌套文档，至少有一个嵌套文档满足 qty 小于等于5，并且 warehouse 为A的文档数据 12345678910111213141516171819202122232425262728db.inventory.find( &#123; instock: &#123; $elemMatch: &#123; qty: &#123; $lte: 5 &#125;, warehouse: &quot;A&quot; &#125; &#125; &#125;)&#123; &quot;_id&quot; : ObjectId(&quot;644628c7f6e1b1a011924011&quot;), &quot;item&quot; : &quot;journal&quot;, &quot;instock&quot; : [ &#123; &quot;warehouse&quot; : &quot;A&quot;, &quot;qty&quot; : 5.0 &#125;, &#123; &quot;warehouse&quot; : &quot;C&quot;, &quot;qty&quot; : 15.0 &#125; ]&#125; 多个元素联合满足查询条件 如果数组字段上的联合查询条件没有使用 $elemMatch 运算符，查询返回数组字段中多个元素联合满足所有的查询条件的所有文档 换言之，如果不使用 $elemMatch 运算符，那么对数组文档的查询的多个条件是 or 关系 1234567891011121314151617181920212223242526272829303132333435363738db.inventory.find( &#123; &quot;instock.qty&quot;: 5, &quot;instock.warehouse&quot;: &quot;A&quot; &#125;)&#123; &quot;_id&quot; : ObjectId(&quot;644628c7f6e1b1a011924011&quot;), &quot;item&quot; : &quot;journal&quot;, &quot;instock&quot; : [ &#123; &quot;warehouse&quot; : &quot;A&quot;, &quot;qty&quot; : 5.0 &#125;, &#123; &quot;warehouse&quot; : &quot;C&quot;, &quot;qty&quot; : 15.0 &#125; ]&#125;/* 2 */&#123; &quot;_id&quot; : ObjectId(&quot;644628c7f6e1b1a011924014&quot;), &quot;item&quot; : &quot;planner&quot;, &quot;instock&quot; : [ &#123; &quot;warehouse&quot; : &quot;A&quot;, &quot;qty&quot; : 40.0 &#125;, &#123; &quot;warehouse&quot; : &quot;B&quot;, &quot;qty&quot; : 5.0 &#125; ]&#125; 在这个查询中，只要数组中的多个文档组合起来，满足所有查询条件即可 数组查询 测试数据 1234567db.inventory.insertMany([ &#123; item: &quot;journal&quot;, qty: 25, tags: [&quot;blank&quot;, &quot;red&quot;], dim_cm: [ 14, 21 ] &#125;, &#123; item: &quot;notebook&quot;, qty: 50, tags: [&quot;red&quot;, &quot;blank&quot;], dim_cm: [ 14, 21 ] &#125;, &#123; item: &quot;paper&quot;, qty: 100, tags: [&quot;red&quot;, &quot;blank&quot;, &quot;plain&quot;], dim_cm: [ 14, 21 ] &#125;, &#123; item: &quot;planner&quot;, qty: 75, tags: [&quot;blank&quot;, &quot;red&quot;], dim_cm: [ 22.85, 30 ] &#125;, &#123; item: &quot;postcard&quot;, qty: 45, tags: [&quot;blue&quot;], dim_cm: [ 10, 15.25 ] &#125;]); 数组字段等值查询 数组字段做等值查询的时候，使用查询文档 &#123; &lt;field&gt;: &lt;value&gt; &#125; ，其中 &lt;value&gt; 是要精确匹配的数组，包括元素的顺序 下面的查询返回 inventory 集合中数组字段 tags 值是只包含两个元素 &quot;red&quot; 和 &quot;blank&quot; 并且按照指定顺序的数组的所有文档 1234567891011121314151617181920db.inventory.find( &#123; &quot;tags&quot;: [&quot;red&quot;, &quot;blank&quot;] &#125;)&#123; &quot;_id&quot; : ObjectId(&quot;644646dcf6e1b1a011924017&quot;), &quot;item&quot; : &quot;notebook&quot;, &quot;qty&quot; : 50.0, &quot;tags&quot; : [ &quot;red&quot;, &quot;blank&quot; ], &quot;dim_cm&quot; : [ 14.0, 21.0 ]&#125; 数组字段包含查询 如果想检索数组中包含 red和 blank两个元素并且不在乎元素顺序或者数组中是否有其它元素。可以使用 $all 操作符 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869db.inventory.find( &#123; tags: &#123; $all: [&quot;red&quot;, &quot;blank&quot;] &#125; &#125;)/* 1 */&#123; &quot;_id&quot; : ObjectId(&quot;644646dcf6e1b1a011924016&quot;), &quot;item&quot; : &quot;journal&quot;, &quot;qty&quot; : 25.0, &quot;tags&quot; : [ &quot;blank&quot;, &quot;red&quot; ], &quot;dim_cm&quot; : [ 14.0, 21.0 ]&#125;/* 2 */&#123; &quot;_id&quot; : ObjectId(&quot;644646dcf6e1b1a011924017&quot;), &quot;item&quot; : &quot;notebook&quot;, &quot;qty&quot; : 50.0, &quot;tags&quot; : [ &quot;red&quot;, &quot;blank&quot; ], &quot;dim_cm&quot; : [ 14.0, 21.0 ]&#125;/* 3 */&#123; &quot;_id&quot; : ObjectId(&quot;644646dcf6e1b1a011924018&quot;), &quot;item&quot; : &quot;paper&quot;, &quot;qty&quot; : 100.0, &quot;tags&quot; : [ &quot;red&quot;, &quot;blank&quot;, &quot;plain&quot; ], &quot;dim_cm&quot; : [ 14.0, 21.0 ]&#125;/* 4 */&#123; &quot;_id&quot; : ObjectId(&quot;644646dcf6e1b1a011924019&quot;), &quot;item&quot; : &quot;planner&quot;, &quot;qty&quot; : 75.0, &quot;tags&quot; : [ &quot;blank&quot;, &quot;red&quot; ], &quot;dim_cm&quot; : [ 22.85, 30.0 ]&#125; 查询数组字段中的元素 要查询数组字段是否包含至少一个具有指定值的元素，请使用过滤器 &#123; &lt;field&gt;: &lt;value&gt; &#125; ，其中 &lt;value&gt; 是元素值 下面的查询是对 tags 数组元素进行查询，要求数组至少包含一个 red 元素 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667db.inventory.find( &#123; tags: &quot;red&quot; &#125;)/* 1 */&#123; &quot;_id&quot; : ObjectId(&quot;644646dcf6e1b1a011924016&quot;), &quot;item&quot; : &quot;journal&quot;, &quot;qty&quot; : 25.0, &quot;tags&quot; : [ &quot;blank&quot;, &quot;red&quot; ], &quot;dim_cm&quot; : [ 14.0, 21.0 ]&#125;/* 2 */&#123; &quot;_id&quot; : ObjectId(&quot;644646dcf6e1b1a011924017&quot;), &quot;item&quot; : &quot;notebook&quot;, &quot;qty&quot; : 50.0, &quot;tags&quot; : [ &quot;red&quot;, &quot;blank&quot; ], &quot;dim_cm&quot; : [ 14.0, 21.0 ]&#125;/* 3 */&#123; &quot;_id&quot; : ObjectId(&quot;644646dcf6e1b1a011924018&quot;), &quot;item&quot; : &quot;paper&quot;, &quot;qty&quot; : 100.0, &quot;tags&quot; : [ &quot;red&quot;, &quot;blank&quot;, &quot;plain&quot; ], &quot;dim_cm&quot; : [ 14.0, 21.0 ]&#125;/* 4 */&#123; &quot;_id&quot; : ObjectId(&quot;644646dcf6e1b1a011924019&quot;), &quot;item&quot; : &quot;planner&quot;, &quot;qty&quot; : 75.0, &quot;tags&quot; : [ &quot;blank&quot;, &quot;red&quot; ], &quot;dim_cm&quot; : [ 22.85, 30.0 ]&#125; 在对数组元素进行查询时，可以使用 查询操作符 下面的查询是查询数组 dim_cm 包含至少一个值大于 25 的元素的所有文档 12345678910111213141516171819202122db.inventory.find( &#123; dim_cm: &#123; $gt: 25 &#125; &#125;)&#123; &quot;_id&quot; : ObjectId(&quot;644646dcf6e1b1a011924019&quot;), &quot;item&quot; : &quot;planner&quot;, &quot;qty&quot; : 75.0, &quot;tags&quot; : [ &quot;blank&quot;, &quot;red&quot; ], &quot;dim_cm&quot; : [ 22.85, 30.0 ]&#125; 多条件查询数组中的元素 使用多条件查询数组中的元素时，可以在查询语句中指定单个数组元素满足所有查询条件还是多个数组中的元素联合满足所有条件 使用多条件查询数组中的元素 下面对 dim_cm 数组的查询，一个元素可以满足大于 15 的条件，而另一个元素可以满足小于 20 的条件，或者一个元素可以 同时满足 这两个条件，条件之间是 OR 关系 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869db.inventory.find( &#123; dim_cm: &#123; $gt: 15, $lt: 20 &#125; &#125;)/* 1 */&#123; &quot;_id&quot; : ObjectId(&quot;644646dcf6e1b1a011924016&quot;), &quot;item&quot; : &quot;journal&quot;, &quot;qty&quot; : 25.0, &quot;tags&quot; : [ &quot;blank&quot;, &quot;red&quot; ], &quot;dim_cm&quot; : [ 14.0, 21.0 ]&#125;/* 2 */&#123; &quot;_id&quot; : ObjectId(&quot;644646dcf6e1b1a011924017&quot;), &quot;item&quot; : &quot;notebook&quot;, &quot;qty&quot; : 50.0, &quot;tags&quot; : [ &quot;red&quot;, &quot;blank&quot; ], &quot;dim_cm&quot; : [ 14.0, 21.0 ]&#125;/* 3 */&#123; &quot;_id&quot; : ObjectId(&quot;644646dcf6e1b1a011924018&quot;), &quot;item&quot; : &quot;paper&quot;, &quot;qty&quot; : 100.0, &quot;tags&quot; : [ &quot;red&quot;, &quot;blank&quot;, &quot;plain&quot; ], &quot;dim_cm&quot; : [ 14.0, 21.0 ]&#125;/* 4 */&#123; &quot;_id&quot; : ObjectId(&quot;644646dcf6e1b1a01192401a&quot;), &quot;item&quot; : &quot;postcard&quot;, &quot;qty&quot; : 45.0, &quot;tags&quot; : [ &quot;blue&quot; ], &quot;dim_cm&quot; : [ 10.0, 15.25 ]&#125; 数组中的一个元素同时满足多个查询条件 使用 $elemMatch 运算符来指定多个查询条件在数组中的元素上，数组中最少一个元素同时满足所有的查询条件 下面的查询返回数组字段 dim_cm 中最少一个元素同时满足大于 22 和 小于 30，条件之间是 AND 关系 1234567891011121314151617181920212223242526db.inventory.find( &#123; dim_cm: &#123; $elemMatch: &#123; $gt: 22, $lt: 30 &#125; &#125; &#125;)/* 1 */&#123; &quot;_id&quot; : ObjectId(&quot;644646dcf6e1b1a011924019&quot;), &quot;item&quot; : &quot;planner&quot;, &quot;qty&quot; : 75.0, &quot;tags&quot; : [ &quot;blank&quot;, &quot;red&quot; ], &quot;dim_cm&quot; : [ 22.85, 30.0 ]&#125; 数组下标查询 使用 点号，可以为数组中指定下标的元素指定查询条件，数组下标从0开始 下面的查询返回数组字段 dim_cm 中第二个元素大于 25 的所有文档 1234567891011121314151617181920212223db.inventory.find( &#123; &quot;dim_cm.1&quot;: &#123; $gt: 25 &#125; &#125;)/* 1 */&#123; &quot;_id&quot; : ObjectId(&quot;644646dcf6e1b1a011924019&quot;), &quot;item&quot; : &quot;planner&quot;, &quot;qty&quot; : 75.0, &quot;tags&quot; : [ &quot;blank&quot;, &quot;red&quot; ], &quot;dim_cm&quot; : [ 22.85, 30.0 ]&#125; 数组长度查询 使用 $size 操作符通过数组中的元素个数来进行检索 下面的查询返回数组字段 tags 中有三个元素的所有文档 123456789101112131415161718192021222324db.inventory.find( &#123; &quot;tags&quot;: &#123; $size: 3 &#125; &#125;)/* 1 */&#123; &quot;_id&quot; : ObjectId(&quot;644646dcf6e1b1a011924018&quot;), &quot;item&quot; : &quot;paper&quot;, &quot;qty&quot; : 100.0, &quot;tags&quot; : [ &quot;red&quot;, &quot;blank&quot;, &quot;plain&quot; ], &quot;dim_cm&quot; : [ 14.0, 21.0 ]&#125; 从查询中返回文档字段 默认情况下，MongoDB的查询语句返回匹配到文档的所有字段，为了限制MongoDB返回给应用的数据，可以通过 projection (投影文档)来指定或限制返回的字段 测试数据 1234567db.inventory.insertMany( [ &#123; item: &quot;journal&quot;, status: &quot;A&quot;, size: &#123; h: 14, w: 21, uom: &quot;cm&quot; &#125;, instock: [ &#123; warehouse: &quot;A&quot;, qty: 5 &#125; ] &#125;, &#123; item: &quot;notebook&quot;, status: &quot;A&quot;, size: &#123; h: 8.5, w: 11, uom: &quot;in&quot; &#125;, instock: [ &#123; warehouse: &quot;C&quot;, qty: 5 &#125; ] &#125;, &#123; item: &quot;paper&quot;, status: &quot;D&quot;, size: &#123; h: 8.5, w: 11, uom: &quot;in&quot; &#125;, instock: [ &#123; warehouse: &quot;A&quot;, qty: 60 &#125; ] &#125;, &#123; item: &quot;planner&quot;, status: &quot;D&quot;, size: &#123; h: 22.85, w: 30, uom: &quot;cm&quot; &#125;, instock: [ &#123; warehouse: &quot;A&quot;, qty: 40 &#125; ] &#125;, &#123; item: &quot;postcard&quot;, status: &quot;A&quot;, size: &#123; h: 10, w: 15.25, uom: &quot;cm&quot; &#125;, instock: [ &#123; warehouse: &quot;B&quot;, qty: 15 &#125;, &#123; warehouse: &quot;C&quot;, qty: 35 &#125; ] &#125;]); 返回指定的字段 通过 projection 指定或排除返回字段后，_id 默认是返回的。如果不需要 _id 字段，需要显式指定 1或true：表示要返回的字段 0或false：表示不返回的字段 12345678910111213141516171819202122232425262728293031db.inventory.find( &#123; status: &quot;A&quot; &#125;, &#123; item: 1, status: 1 &#125;)/* 1 */&#123; &quot;_id&quot; : ObjectId(&quot;64466a70f6e1b1a01192401b&quot;), &quot;item&quot; : &quot;journal&quot;, &quot;status&quot; : &quot;A&quot;&#125;/* 2 */&#123; &quot;_id&quot; : ObjectId(&quot;64466a70f6e1b1a01192401c&quot;), &quot;item&quot; : &quot;notebook&quot;, &quot;status&quot; : &quot;A&quot;&#125;/* 3 */&#123; &quot;_id&quot; : ObjectId(&quot;64466a70f6e1b1a01192401f&quot;), &quot;item&quot; : &quot;postcard&quot;, &quot;status&quot; : &quot;A&quot;&#125; 上面的操作等价于SQL的 1SELECT _id, item, status from inventory WHERE status = &quot;A&quot; 去除_id字段 1234567891011121314151617181920212223242526272829db.getCollection(&#x27;inventory&#x27;).find( &#123; status: &quot;A&quot; &#125;, &#123; item: 1, status: 1, _id: 0 &#125;)/* 1 */&#123; &quot;item&quot; : &quot;journal&quot;, &quot;status&quot; : &quot;A&quot;&#125;/* 2 */&#123; &quot;item&quot; : &quot;notebook&quot;, &quot;status&quot; : &quot;A&quot;&#125;/* 3 */&#123; &quot;item&quot; : &quot;postcard&quot;, &quot;status&quot; : &quot;A&quot;&#125; 注意 除 _id 字段外，不能在映射文档中同时使用 包含 和 去除 语句 去除指定字段 123456789101112131415161718192021db.getCollection(&#x27;inventory&#x27;).find( &#123; &quot;size.h&quot;: 10 &#125;, &#123; item: 0, instock: 0 &#125;)/* 1 */&#123; &quot;_id&quot; : ObjectId(&quot;64466a70f6e1b1a01192401f&quot;), &quot;status&quot; : &quot;A&quot;, &quot;size&quot; : &#123; &quot;h&quot; : 10.0, &quot;w&quot; : 15.25, &quot;uom&quot; : &quot;cm&quot; &#125;&#125; 返回嵌套文档中的指定字段 通过 点号 引用嵌套文档字段并且在映射文档中将该字段设置为1来实现返回嵌套文档中的指定字段 12345678910db.inventory.find( &#123; status: &quot;A&quot; &#125;, &#123; item: 1, status: 1, &quot;size.uom&quot;: 1 &#125;) 去除嵌套文档中的指定字段 通过 点号 引用嵌套文档字段并且在映射文档中将该字段设置为0来实现去除嵌套文档中的指定字段 12345678db.inventory.find( &#123; status: &quot;A&quot; &#125;, &#123; &quot;size.uom&quot;: 0 &#125;) 映射数组中的嵌套文档的指定字段 通过使用 点号 来映射数组中嵌套文档的指定字段 12345678910db.inventory.find( &#123; status: &quot;A&quot; &#125;, &#123; item: 1, status: 1, &quot;instock.qty&quot;: 1 &#125;) 映射返回数组中指定的数组元素 对于数组字段，MongoDB 提供了以下用于操作数组的映射运算符: $elemMatch、 $slice、 $. $elemMatch 在嵌套文档数组中，这个对象数组仅返回与 $elemMatch 匹配的 第一个元素 123456789101112131415161718192021222324db.inventory.find( &#123; item: &quot;postcard&quot; &#125;, &#123; instock: &#123; $elemMatch: &#123; qty: 35 &#125; &#125; &#125; ) /* 1 */&#123; &quot;_id&quot; : ObjectId(&quot;64466a70f6e1b1a01192401f&quot;), &quot;instock&quot; : [ &#123; &quot;warehouse&quot; : &quot;C&quot;, &quot;qty&quot; : 35.0 &#125; ]&#125; $slice 1234db.collection.find( &lt;query&gt;, &#123; &lt;arrayField&gt;: &#123; $slice: &lt;number&gt; &#125; &#125;); number &gt;= 0：返回这个数组的 n 个元素 number &lt; 0：返回这个数组的最后一个元素 1234db.collection.find( &lt;query&gt;, &#123; &lt;arrayField&gt;: &#123; $slice: [ &lt;number&gt;, &lt;number&gt; ] &#125; &#125;); 第一个参数类似 offset 第二个参数类似 limit 123456789101112131415161718192021222324252627db.inventory.find( &#123; status: &quot;A&quot;, &quot;instock.qty&quot;: 35 &#125;, &#123; item: 1, status: 1, instock: &#123; $slice: -1 &#125; &#125;)/* 1 */&#123; &quot;_id&quot; : ObjectId(&quot;64466a70f6e1b1a01192401f&quot;), &quot;item&quot; : &quot;postcard&quot;, &quot;status&quot; : &quot;A&quot;, &quot;instock&quot; : [ &#123; &quot;warehouse&quot; : &quot;C&quot;, &quot;qty&quot; : 35.0 &#125; ]&#125; $ $ 运算符限制数组返回的内容以返回与数组中的查询条件匹配的 第一个元素 12345678910111213141516171819202122232425db.inventory.find( &#123; instock: &#123; $elemMatch: &#123; warehouse : &quot;C&quot;, qty: 35.0 &#125; &#125; &#125;, &#123; &quot;instock.$&quot;: 1 &#125; )/* 1 */&#123; &quot;_id&quot; : ObjectId(&quot;64466a70f6e1b1a01192401f&quot;), &quot;instock&quot; : [ &#123; &quot;warehouse&quot; : &quot;C&quot;, &quot;qty&quot; : 35.0 &#125; ]&#125; 查询空字段或缺失字段 在MongoDB中不同的查询操作符对于 null 值处理方式不同 测试数据 1234db.inventory.insertMany([ &#123; _id: 1, item: null &#125;, &#123; _id: 2 &#125;]) 等值匹配 当使用 &#123; item : null &#125; 作为查询条件的时候，返回的是 item 字段值为 null 的文档或者 不包含item 字段的文档 1234567891011121314151617db.getCollection(&#x27;inventory&#x27;).find( &#123; item: null &#125;)/* 1 */&#123; &quot;_id&quot; : 1.0, &quot;item&quot; : null&#125;/* 2 */&#123; &quot;_id&quot; : 2.0&#125; 类型检查 当使用 &#123;item:&#123;$type:10&#125;&#125; 作为查询条件的时候，仅返回 item 字段值为 null 的文档。item 字段的值是 BSON Type Null(type number 10) 1234567891011121314db.getCollection(&#x27;inventory&#x27;).find( &#123; item: &#123; $type: 10 &#125; &#125;)/* 1 */&#123; &quot;_id&quot; : 1.0, &quot;item&quot; : null&#125; 存在检查 当使用 &#123;item:&#123;$exists:false&#125;&#125; 作为查询条件的时候，返回不包含 item 字段的文档 123456789101112db.getCollection(&#x27;inventory&#x27;).find( &#123; item: &#123; $exists: false &#125; &#125;)&#123; &quot;_id&quot; : 2.0&#125; 在mongo Shell中迭代游标 db.collection.find() 方法返回一个游标。要访问文档，您需要迭代游标。 但是，在mongo shell中，如果未使用 var 关键字将返回的游标分配给变量，则该游标将自动迭代多达20次，以打印结果中的前20个文档 （查询默认返回前20个文档） 手动迭代游标 在 mongo Shell中，当使用 var 关键字将find() 方法返回的游标分配给变量时，游标不会自动进行迭代 您可以在shell程序中调用cursor变量以进行多达20次迭代并打印匹配的文档 12345678910111213141516 var myCursor = db.user.find(&#123; age: &#123;$gt: 30&#125; &#125;); // 迭代游标 myCursor &#123; &quot;_id&quot; : ObjectId(&quot;5e7835496b6f69d428000004&quot;), &quot;name&quot; : &quot;Dave&quot;, &quot;age&quot; : 35, &quot;address&quot; : &#123; &quot;city&quot; : &quot;San Francisco&quot;, &quot;state&quot; : &quot;CA&quot;, &quot;zip&quot; : &quot;94107&quot; &#125;&#125; 您还可以使用游标方法 next() 来访问文档，作为一种替代的打印操作，请考虑使用printjson()辅助方法替换print(tojson()) 123456789101112131415161718var myCursor = db.user.find(&#123; age: &#123;$gt: 30&#125; &#125;); // 迭代游标while(myCursor.hasNext()) &#123; printjson(myCursor.next())&#125;&#123; &quot;_id&quot; : ObjectId(&quot;5e7835496b6f69d428000004&quot;), &quot;name&quot; : &quot;Dave&quot;, &quot;age&quot; : 35, &quot;address&quot; : &#123; &quot;city&quot; : &quot;San Francisco&quot;, &quot;state&quot; : &quot;CA&quot;, &quot;zip&quot; : &quot;94107&quot; &#125;&#125; 您可以使用游标方法 forEach() 来迭代游标并访问文档 12345678910111213141516var myCursor = db.user.find(&#123; age: &#123;$gt: 30&#125; &#125;); // 迭代游标myCursor.forEach(printjson);&#123; &quot;_id&quot; : ObjectId(&quot;5e7835496b6f69d428000004&quot;), &quot;name&quot; : &quot;Dave&quot;, &quot;age&quot; : 35, &quot;address&quot; : &#123; &quot;city&quot; : &quot;San Francisco&quot;, &quot;state&quot; : &quot;CA&quot;, &quot;zip&quot; : &quot;94107&quot; &#125;&#125; 迭代器索引 toArray() 方法将游标返回的所有文档加载到 RAM 中； toArray() 方法会遍历游标 123456789101112131415161718var myCursor = db.user.find(&#123; age: &#123;$gt: 30&#125; &#125;); // 游标转为数组，数据放在内存中var documentArray = myCursor.toArray();var document = documentArray[0];printjson(document);&#123; &quot;_id&quot; : ObjectId(&quot;5e7835496b6f69d428000004&quot;), &quot;name&quot; : &quot;Dave&quot;, &quot;age&quot; : 35, &quot;address&quot; : &#123; &quot;city&quot; : &quot;San Francisco&quot;, &quot;state&quot; : &quot;CA&quot;, &quot;zip&quot; : &quot;94107&quot; &#125;&#125; 此外，一些驱动程序通过使用光标上的索引（即 cursor[index] ）提供对文档的访问。这是先调用 toArray() 方法然后在结果数组上使用索引的快捷方式 1234567891011121314151617var myCursor = db.user.find(&#123; age: &#123;$gt: 30&#125; &#125;); // 快捷方式var document = myCursor[0];printjson(document);&#123; &quot;_id&quot; : ObjectId(&quot;5e7835496b6f69d428000004&quot;), &quot;name&quot; : &quot;Dave&quot;, &quot;age&quot; : 35, &quot;address&quot; : &#123; &quot;city&quot; : &quot;San Francisco&quot;, &quot;state&quot; : &quot;CA&quot;, &quot;zip&quot; : &quot;94107&quot; &#125;&#125; 更新文档 更新方法 方法 说明 db.collection.updateOne(&lt;filter&gt;, &lt;update&gt;, &lt;options&gt;) 多个文档匹配，最多更新一个文档 db.collection.updateMany((&lt;filter&gt;, &lt;update&gt;, &lt;options&gt;) 更新匹配的所有文档 db.collection.replaceOne((&lt;filter&gt;, &lt;update&gt;, &lt;options&gt;) 多个文档匹配，最多替换一个文档 db.collection.update(query, update, options)(已弃用) 更新一个或多个文档，具体操作取决于更新参数 更新参数 附加方法 方法 说明 db.collection.findOneAndReplace( filter, replacement, options ) 根据 filter 和 sort 条件修改和替换单个文档，然后返回一个文档（新或旧） db.collection.findOneAndUpdate( filter, update, options ) 根据 filter 和 sort 条件更新单个文档，然后返回一个文档（新或旧） db.collection.findAndModify(document) 修改 或 删除 并返回单个文档 db.collection.bulkWrite() 通过控制执行顺序执行多个写入操作 db.collection.save() 根据 document 参数更新现有文档或插入新文档已过时，推荐 replaceOne 方法 测试数据 123456789101112db.inventory.insertMany( [ &#123; item: &quot;canvas&quot;, qty: 100, size: &#123; h: 28, w: 35.5, uom: &quot;cm&quot; &#125;, status: &quot;A&quot; &#125;, &#123; item: &quot;journal&quot;, qty: 25, size: &#123; h: 14, w: 21, uom: &quot;cm&quot; &#125;, status: &quot;A&quot; &#125;, &#123; item: &quot;mat&quot;, qty: 85, size: &#123; h: 27.9, w: 35.5, uom: &quot;cm&quot; &#125;, status: &quot;A&quot; &#125;, &#123; item: &quot;mousepad&quot;, qty: 25, size: &#123; h: 19, w: 22.85, uom: &quot;cm&quot; &#125;, status: &quot;P&quot; &#125;, &#123; item: &quot;notebook&quot;, qty: 50, size: &#123; h: 8.5, w: 11, uom: &quot;in&quot; &#125;, status: &quot;P&quot; &#125;, &#123; item: &quot;paper&quot;, qty: 100, size: &#123; h: 8.5, w: 11, uom: &quot;in&quot; &#125;, status: &quot;D&quot; &#125;, &#123; item: &quot;planner&quot;, qty: 75, size: &#123; h: 22.85, w: 30, uom: &quot;cm&quot; &#125;, status: &quot;D&quot; &#125;, &#123; item: &quot;postcard&quot;, qty: 45, size: &#123; h: 10, w: 15.25, uom: &quot;cm&quot; &#125;, status: &quot;A&quot; &#125;, &#123; item: &quot;sketchbook&quot;, qty: 80, size: &#123; h: 14, w: 21, uom: &quot;cm&quot; &#125;, status: &quot;A&quot; &#125;, &#123; item: &quot;sketch pad&quot;, qty: 95, size: &#123; h: 22.85, w: 30.5, uom: &quot;cm&quot; &#125;, status: &quot;A&quot; &#125;] ); 更新集合中的文档 为了更新文档，MongoDB 提供了 更新运算符，例如 $set，来修改字段值 要使用更新运算符，请将以下形式的更新文档传递给更新方法： 12345&#123; &lt;update operator&gt;: &#123; &lt;field1&gt;: &lt;value1&gt;, ... &#125;, &lt;update operator&gt;: &#123; &lt;field2&gt;: &lt;value2&gt;, ... &#125;, ...&#125; 如果字段不存在，则某些更新操作符（例如 $set）将创建该字段 更新运算符 字段 名称 描述 $currentDate 将字段的值设置为当前日期，即日期或时间戳。 $inc 将字段的值增加指定的数量。 $min 仅当指定值小于现有字段值时才更新该字段。 $max 仅当指定值大于现有字段值时才更新该字段。 $mul 将字段的值乘以指定的数量。 $rename 重命名字段。 $set 设置文档中字段的值。 $setOnInsert 如果更新导致插入文档，则设置字段的值。对修改现有文档的更新操作没有影响。 $unset 从文档中删除指定的字段。 数组 名称 描述 $ 充当更新查询文档的第一个匹配项（数组元素）的占位符。 $[] 充当占位符，以更新匹配查询条件的文档的数组中的所有元素。 $[&lt;identifier&gt;] 充当占位符，以更新arrayFilters与查询条件匹配的文档中所有与条件匹配的元素。 $addToSet 仅当元素不存在于集合中时才将它们添加到数组中。 $pop 删除数组的第一项或最后一项。 $pull 删除与指定查询匹配的所有数组元素。 $push 将项目添加到数组。 $pullAll 从数组中删除所有匹配的值。 修饰符 名称 描述 $each 修改$push和$addToSet运算符以附加多个项以进行数组更新。 $position 修改$push运算符以指定要添加元素的数组中的位置。 $slice 修改$push运算符以限制更新数组的大小。 $sort 修改$push运算符以对存储在数组中的文档重新排序。 按位运算 名称 描述 $bit 执行按位AND，OR和XOR整数值的更新。 updateOne 查找与过滤器匹配的第一个文档，并应用指定的更新修改 语法 12345678910db.collection.updateOne( &lt;filter&gt;, &lt;update&gt;, &#123; upsert: &lt;boolean&gt;, writeConcern: &lt;document&gt;, collation: &lt;document&gt;, arrayFilters: [ &lt;filterdocument1&gt;, ... ] &#125;) 使用例子 12345678910111213141516171819202122232425262728293031323334353637db.inventory.updateOne( &#123; item: &quot;paper&quot; &#125;, &#123; $set: &#123; &quot;size.uom&quot;: &quot;cm&quot;, status: &quot;P&quot; &#125;, $currentDate: &#123; lastModified: true &#125; &#125;)/* 1 */&#123; &quot;acknowledged&quot; : true, &quot;matchedCount&quot; : 1.0, &quot;modifiedCount&quot; : 1.0&#125;// 修改后的数据/* 1 */&#123; &quot;_id&quot; : ObjectId(&quot;64493abb802c33de71430e81&quot;), &quot;item&quot; : &quot;paper&quot;, &quot;qty&quot; : 100.0, &quot;size&quot; : &#123; &quot;h&quot; : 8.5, &quot;w&quot; : 11.0, &quot;uom&quot; : &quot;cm&quot; &#125;, &quot;status&quot; : &quot;P&quot;, &quot;lastModified&quot; : ISODate(&quot;2023-04-26T14:58:14.173Z&quot;)&#125; 使用 $set 运算符将 size.uom 字段的值更新为 &quot;cm&quot; 并将 status 字段的值更新为 &quot;P&quot; 使用 $currentDate 运算符将 lastModified 字段的值更新为当前日期。如果 lastModified 字段不存在， $currentDate 将创建该字段 updateMany 更新与集合的指定过滤器匹配的所有文档 语法 1234567891011db.collection.updateMany( &lt;filter&gt;, &lt;update&gt;, &#123; upsert: &lt;boolean&gt;, writeConcern: &lt;document&gt;, collation: &lt;document&gt;, arrayFilters: [ &lt;filterdocument1&gt;, ... ], hint: &lt;document|string&gt; // Available starting in MongoDB 4.2.1 &#125;) 使用例子 12345678910111213141516db.inventory.updateMany( &#123; qty: &#123; $lt: 50 &#125; &#125;, &#123; $set: &#123; &quot;size.uom&quot;: &quot;in&quot;, status: &quot;P&quot; &#125;, $currentDate: &#123; lastModified: true &#125; &#125;) 更新所有 qty 小于5的文档 使用 $set 运算符将 size.uom 字段的值更新为 &quot;in&quot;，将状态字段的值更新为 &quot;P&quot; 使用 $currentDate 运算符将 lastModified 字段的值更新为当前日期。如果 lastModified 字段不存在， $currentDate 将创建该字段 replaceOne db.collection.replaceOne(filter, replacement, options) 根据过滤器替换集合中的单个文档，替换 _id 字段以外的文档的全部内容 语法 12345678910db.collection.replaceOne( &lt;filter&gt;, &lt;replacement&gt;, &#123; upsert: &lt;boolean&gt;, writeConcern: &lt;document&gt;, collation: &lt;document&gt;, hint: &lt;document|string&gt; // Available starting in 4.2.1 &#125;) 使用例子 12345678910111213141516171819202122232425262728293031323334353637383940414243db.inventory.replaceOne( &#123; item: &quot;paper&quot; &#125;, &#123; item: &quot;paper&quot;, instock: [ &#123; warehouse: &quot;A&quot;, qty: 60 &#125;, &#123; warehouse: &quot;B&quot;, qty: 40 &#125; ] &#125;)/* 1 */&#123; &quot;acknowledged&quot; : true, &quot;matchedCount&quot; : 1.0, &quot;modifiedCount&quot; : 1.0&#125;/* 1 */&#123; &quot;_id&quot; : ObjectId(&quot;64493abb802c33de71430e81&quot;), &quot;item&quot; : &quot;paper&quot;, &quot;instock&quot; : [ &#123; &quot;warehouse&quot; : &quot;A&quot;, &quot;qty&quot; : 60.0 &#125;, &#123; &quot;warehouse&quot; : &quot;B&quot;, &quot;qty&quot; : 40.0 &#125; ]&#125; 行为 原子性 MongoDB中的所有写操作都是 单个文档级别上的原子操作。有关MongoDB和原子性的更多信息，请参见原子性和事务 _id 字段 设置后，无法更新 _id 字段的值，也无法将现有文档替换为具有不同 _id 字段值的替换文档 字段顺序 对于写操作，MongoDB 保留文档字段的顺序，但以下情况除外： _id 字段始终是文档中的第一个字段 包含 rename 字段名称的更新可能会导致文档中的字段重新排序 修改或新增 如果updateOne(), updateMany(), or replaceOne() 包含 upsert：true，并且没有文档与指定的过滤器匹配，则该操作将创建一个新文档并将其插入。 如果存在匹配的文档，则该操作将修改或替换一个或多个匹配的文档 聚合管道更新 从 MongoDB 4.2 开始，可以使用聚合管道进行更新操作，通过更新操作，聚合管道可以包括以下阶段 $addFields 用于向文档中添加新的字段 $set 用于更新现有字段的值或者添加新的字段 $project 用于选择要返回的字段，并可以对这些字段进行重命名、计算表达式等操作 $unset 用于从文档中删除指定的字段 $replaceRoot 用于将文档的结构更改为指定字段的内容 $replaceWith 用于替换当前文档为指定的值 使用聚合管道允许使用表达性更强的update语句，比如根据当前字段值表示条件更新，或者使用另一个字段的值更新一个字段 使用聚合表达式变量 测试数据 12345db.students.insertMany( [ &#123; _id: 1, test1: 95, test2: 92, test3: 90, modified: new Date(&quot;01/05/2020&quot;) &#125;, &#123; _id: 2, test1: 98, test2: 100, test3: 102, modified: new Date(&quot;01/05/2020&quot;) &#125;, &#123; _id: 3, test1: 95, test2: 110, modified: new Date(&quot;01/04/2020&quot;) &#125;] ) 聚合表达式中的变量 以下 db.collection.updateOne() 操作使用聚合管道通过 _id: 3 更新文档 12345678910111213db.students.updateOne( &#123; _id: 3 &#125;, [ &#123; $set: &#123; test3: 98, modified: &quot;$$NOW&quot; &#125; &#125; ]) 具体来说，管道由一个 $set 阶段组成，该阶段将 test3 字段（并将其值设置为 98 ）添加到文档并将 modified 字段设置为当前日期时间。该操作将聚合变量 NOW 用于当前日期时间。要访问变量，请以 $$ 为前缀并用引号引起来 规范文档字段 测试数据 1234db.students2.insertMany( [ &#123; &quot;_id&quot; : 1, quiz1: 8, test2: 100, quiz2: 9, modified: new Date(&quot;01/05/2020&quot;) &#125;, &#123; &quot;_id&quot; : 2, quiz2: 5, test1: 80, test2: 89, modified: new Date(&quot;01/05/2020&quot;) &#125;,] ) 以下 db.collection.updateMany() 操作使用聚合管道来标准化文档的字段（即集合中的文档应具有相同的字段）并更新 modified 字段 12345678910111213141516171819202122232425262728293031323334353637383940414243444546db.students2.updateMany( &#123;&#125;, [ &#123; $replaceRoot: &#123; newRoot: &#123; $mergeObjects: [ &#123; quiz1: 0, quiz2: 0, test1: 0, test2: 0 &#125;, &quot;$$ROOT&quot; ] &#125; &#125; &#125;, &#123; $set: &#123; modified: &quot;$$NOW&quot; &#125; &#125; ])/* 1 */&#123; &quot;_id&quot; : 1.0, &quot;quiz1&quot; : 8.0, &quot;quiz2&quot; : 9.0, &quot;test1&quot; : 0.0, &quot;test2&quot; : 100.0, &quot;modified&quot; : ISODate(&quot;2023-04-27T07:46:36.526Z&quot;)&#125;/* 2 */&#123; &quot;_id&quot; : 2.0, &quot;quiz1&quot; : 0.0, &quot;quiz2&quot; : 5.0, &quot;test1&quot; : 80.0, &quot;test2&quot; : 89.0, &quot;modified&quot; : ISODate(&quot;2023-04-27T07:46:36.526Z&quot;)&#125; $replaceRoot 阶段，带有 $mergeObjects 表达式，可为quiz1、quiz2、test1、test2 字段设置默认值。 聚合变量 ROOT 指的是正在修改的当前文档，要访问变量，请以 $$ 为前缀并用引号引起来。当前文档字段将覆盖默认值 $set 阶段用于将修改的字段更新到当前日期时间。 对于当前日期时间，该操作将聚合变量 NOW 用于（以访问变量，以 $$ 为前缀并用引号引起来） 最终实现的效果是 $mergeObjects 将符合条件的多个文档重新进行字段规划，给出字段和默认值。然后 $replaceRoot 对当前文档进行替换，如果当前文档已存在字段则跳过，负责插入新的字段和默认值 聚合管道字段引用 测试数据 12345db.students3.insertMany( [ &#123; &quot;_id&quot; : 1, &quot;tests&quot; : [ 95, 92, 90 ], &quot;modified&quot; : ISODate(&quot;2019-01-01T00:00:00Z&quot;) &#125;, &#123; &quot;_id&quot; : 2, &quot;tests&quot; : [ 94, 88, 90 ], &quot;modified&quot; : ISODate(&quot;2019-01-01T00:00:00Z&quot;) &#125;, &#123; &quot;_id&quot; : 3, &quot;tests&quot; : [ 70, 75, 82 ], &quot;modified&quot; : ISODate(&quot;2019-01-01T00:00:00Z&quot;) &#125;] ); 以下操作使用聚合管道计算平均值，然后用平均值进行评级 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061db.students3.updateMany( &#123;&#125;, [ &#123; $set: &#123; average: &#123; $trunc: [&#123; $avg: &quot;$tests&quot; &#125;, 0] &#125;, modified: &quot;$$NOW&quot; &#125; &#125;, &#123; $set: &#123; grade: &#123; $switch: &#123; branches: [ &#123; case: &#123; $gte: [&quot;$average&quot;, 90] &#125;, then: &quot;A&quot; &#125;, &#123; case: &#123; $gte: [&quot;$average&quot;, 80] &#125;, then: &quot;B&quot; &#125;, &#123; case: &#123; $gte: [&quot;$average&quot;, 70] &#125;, then: &quot;C&quot; &#125;, &#123; case: &#123; $gte: [&quot;$average&quot;, 60] &#125;, then: &quot;D&quot; &#125; ], default: &quot;F&quot; &#125; &#125; &#125; &#125; ])/* 1 */&#123; &quot;_id&quot; : 1.0, &quot;tests&quot; : [ 95.0, 92.0, 90.0 ], &quot;modified&quot; : ISODate(&quot;2023-04-27T08:44:41.551Z&quot;), &quot;average&quot; : 92.0, &quot;grade&quot; : &quot;A&quot;&#125;/* 2 */&#123; &quot;_id&quot; : 2.0, &quot;tests&quot; : [ 94.0, 88.0, 90.0 ], &quot;modified&quot; : ISODate(&quot;2023-04-27T08:44:41.551Z&quot;), &quot;average&quot; : 90.0, &quot;grade&quot; : &quot;A&quot;&#125;/* 3 */&#123; &quot;_id&quot; : 3.0, &quot;tests&quot; : [ 70.0, 75.0, 82.0 ], &quot;modified&quot; : ISODate(&quot;2023-04-27T08:44:41.551Z&quot;), &quot;average&quot; : 75.0, &quot;grade&quot; : &quot;C&quot;&#125; 先使用管道计算出平均值， $trunc 用于截断平均数，让平均分数为整数，然后再使用 NOW 聚合变量修改时间字段 $set 阶段使用 $switch 表达式在 average 的基础上添加 grade 字段 聚合管道数组拼接 测试数据 12345db.students4.insertMany([ &#123; &quot;_id&quot; : 1, &quot;quizzes&quot; : [ 4, 6, 7 ] &#125;, &#123; &quot;_id&quot; : 2, &quot;quizzes&quot; : [ 5 ] &#125;, &#123; &quot;_id&quot; : 3, &quot;quizzes&quot; : [ 10, 10, 10 ] &#125;]) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546db.students4.updateOne( &#123; _id: 2 &#125;, [ &#123; $set: &#123; quizzes: &#123; $concatArrays: [ &quot;$quizzes&quot;, // 数组字段引用 [ 8, 6 ] // 多个数组拼接 ] &#125; &#125; &#125; ])/* 1 */&#123; &quot;_id&quot; : 1.0, &quot;quizzes&quot; : [ 4.0, 6.0, 7.0 ]&#125;/* 2 */&#123; &quot;_id&quot; : 2.0, &quot;quizzes&quot; : [ 5.0, 8.0, 6.0 ]&#125;/* 3 */&#123; &quot;_id&quot; : 3.0, &quot;quizzes&quot; : [ 10.0, 10.0, 10.0 ]&#125; 聚合管道新字段计算 测试数据 12345db.temperatures.insertMany([ &#123; &quot;_id&quot; : 1, &quot;date&quot; : ISODate(&quot;2019-06-23&quot;), &quot;tempsC&quot; : [ 4, 12, 17 ] &#125;, &#123; &quot;_id&quot; : 2, &quot;date&quot; : ISODate(&quot;2019-07-07&quot;), &quot;tempsC&quot; : [ 14, 24, 11 ] &#125;, &#123; &quot;_id&quot; : 3, &quot;date&quot; : ISODate(&quot;2019-10-30&quot;), &quot;tempsC&quot; : [ 18, 6, 8 ] &#125;]) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960db.temperatures.updateMany( &#123; &#125;, [ &#123; $addFields: &#123; &quot;tempsF&quot;: &#123; // 添加一个字段 $map: &#123; // 映射转换 input: &quot;$tempsC&quot;, as: &quot;celsius&quot;, // 别名 in: &#123; $add: [ &#123; $multiply: [&quot;$$celsius&quot;, 9/5 ] &#125;, 32 ] &#125; // 字段计算 &#125; &#125; &#125; &#125; ])/* 1 */&#123; &quot;_id&quot; : 1.0, &quot;date&quot; : ISODate(&quot;2019-06-23T00:00:00.000Z&quot;), &quot;tempsC&quot; : [ 4.0, 12.0, 17.0 ], &quot;tempsF&quot; : [ 39.2, 53.6, 62.6 ]&#125;/* 2 */&#123; &quot;_id&quot; : 2.0, &quot;date&quot; : ISODate(&quot;2019-07-07T00:00:00.000Z&quot;), &quot;tempsC&quot; : [ 14.0, 24.0, 11.0 ], &quot;tempsF&quot; : [ 57.2, 75.2, 51.8 ]&#125;/* 3 */&#123; &quot;_id&quot; : 3.0, &quot;date&quot; : ISODate(&quot;2019-10-30T00:00:00.000Z&quot;), &quot;tempsC&quot; : [ 18.0, 6.0, 8.0 ], &quot;tempsF&quot; : [ 64.4, 42.8, 46.4 ]&#125; 具体来说，该管道由一个 $addFields 阶段组成，用于添加一个包含华氏温度的新数组字段 tempsF 。为了将 tempsC 数组中的每个摄氏温度转换为华氏温度，该阶段使用 $map 表达式以及 $add 和 $multiply 表达式 删除文档 删除方法 方法 描述 db.collection.deleteOne() 最多删除一个与指定过滤器匹配的文档，即使过滤器匹配到多个文档 db.collection.deleteMany() 删除与指定过滤器匹配的所有文档 db.collection.remove() 删除与指定过滤器匹配的单个文档或所有文档 其他方法 以下方法也可以从集合中删除文档: db.collection.findOneAndDelete(). findOneAndDelete() 提供了一个排序选项。该选项允许删除按指定顺序排序的第一个文档 db.collection.findAndModify(). 提供了一个排序选项。该选项允许删除按指定顺序排序的第一个文档 db.collection.bulkWrite(). 批量写操作 删除所有文档 要从集合中删除所有文档，请将空 过滤器 文档 &#123;&#125; 传递给 db.collection.deleteMany() 方法 1db.inventory.deleteMany(&#123;&#125;) 删除符合条件的所有文档 12345db.inventory.deleteMany( &#123; status: &quot;A&quot; &#125;) 删除 status 为 A 的所有文档 仅删除一个符合条件的文档 要删除最多一个与指定过滤器匹配的文档（即使多个文档可以与指定过滤器匹配），请使用 db.collection.deleteOne() 方法 12345db.inventory.deleteOne( &#123; status: &quot;D&quot; &#125;) 删除行为 索引 即使从集合中删除所有文档，删除操作也不会删除索引 原子性 MongoDB 中的所有写操作在单个文档级别上都是原子的。有关 MongoDB 和原子性的更多信息，请参阅 原子性和事务 写确认 对于写入问题，您可以指定从MongoDB请求的写入操作的确认级别。 有关详细信息，请参见 写确认 批量写入操作 MongoDB为客户端提供了批量写操作的能力。 批量写入操作会影响 单个集合。 MongoDB允许应用程序确定批量写入操作所需的可接受的确认级别 db.collection.bulkWrite() 方法提供了执行批量 插入，更新和 删除 操作的能力。对于批量插入而言，MongoDB也支持 db.collection.insertMany() 方法批量插入 有序 VS 无序操作 批量写操作可以是有序的，也可以无序的 使用操作的有序列表，MongoDB 串行 地执行操作。 如果在某个单独的写操作的处理过程中发生错误，MongoDB将直接返回而不再继续处理列表中任何剩余的写操作。参考 有序的批量写入 使用无序的操作列表，MongoDB可以 并行 地执行操作，但是不能保证此行为。 如果某个单独的写操作的处理过程中发生错误，MongoDB将继续处理列表中剩余的写操作。参考 无序的批量写入 在分片集合上执行有序的批量写操作通常比执行无序批量写操作要慢。这是因为对于有序列表而言，每个操作都必须等待上一个操作完成后才能执行 默认情况下，bulkWrite() 执行 有序 的写入。 要指定 无序 的写入，请在选项文档中设置 ordered：false 支持的操作 insertOne updateOne updateMany replaceOne deleteOne deleteMany 用法 测试数据 12345db.pizzas.insertMany( [ &#123; _id: 0, type: &quot;pepperoni&quot;, size: &quot;small&quot;, price: 4 &#125;, &#123; _id: 1, type: &quot;cheese&quot;, size: &quot;medium&quot;, price: 7 &#125;, &#123; _id: 2, type: &quot;vegan&quot;, size: &quot;large&quot;, price: 8 &#125;] ) 使用 insertOne 添加两个文档 使用 updateOne 更新文档 使用 deleteOne 删除文档 使用 replaceOne 替换文档 1234567891011121314151617181920212223242526272829303132try &#123; db.pizzas.bulkWrite( [ &#123; insertOne: &#123; document: &#123; _id: 3, type: &quot;beef&quot;, size: &quot;medium&quot;, price: 6 &#125; &#125; &#125;, &#123; insertOne: &#123; document: &#123; _id: 4, type: &quot;sausage&quot;, size: &quot;large&quot;, price: 10 &#125; &#125; &#125;, &#123; updateOne: &#123; filter: &#123; type: &quot;cheese&quot; &#125;, update: &#123; $set: &#123; price: 8 &#125; &#125; &#125; &#125;, &#123; deleteOne: &#123; filter: &#123; type: &quot;pepperoni&quot;&#125; &#125; &#125;, &#123; replaceOne: &#123; filter: &#123; type: &quot;vegan&quot; &#125;, replacement: &#123; type: &quot;tofu&quot;, size: &quot;small&quot;, price: 4 &#125; &#125; &#125; ] )&#125; catch( error ) &#123; print( error )&#125;/* 1 */&#123; &quot;acknowledged&quot; : true, &quot;deletedCount&quot; : 1.0, &quot;insertedCount&quot; : 2.0, &quot;matchedCount&quot; : 2.0, &quot;upsertedCount&quot; : 0.0, &quot;insertedIds&quot; : &#123; &quot;0&quot; : 3.0, &quot;1&quot; : 4.0 &#125;, &quot;upsertedIds&quot; : &#123;&#125;&#125; 批量插入分片集合的策略 大批量插入操作，包括初始数据插入或例行数据导入，会影响分片集群的性能。对于批量插入，请考虑以下策略： 对分片集合进行预拆分 如果分片集合为空，则该集合只有一个存储在单个分片上的初始数据块，MongoDB必须花一些时间来接收数据，创建拆分并将拆分的块分发到其他分片上。为了避免这种性能开销，您可以对分片集合进行预拆分，请参考 分片集群中的数据块拆分 中的描述 对mongos的无序写入 要提高对分片集群的写入性能，请使用 bulkWrite() 并将可选参数 ordered 设置为 false 。 mongos 可以尝试同时将写入发送到多个分片。对于空集合，首先按照 分片集群中的数据块拆分 中的描述预先拆分集合 避免单调插入带来的瓶颈 如果您的分片键在插入过程中是单调递增的，那么所有插入的数据都会插入到该分片集合的最后一个数据块中，也就是说会落到某单个分片上。因此，集群的插入能力将永远不会超过该单个分片的插入性能（木桶的短板原理） 如果插入量大于单个分片可以处理的数据量，并且无法避免单调递增的分片键，那么可以考虑对应用程序进行如下修改： 反转分片键的二进制位。这样可以保留信息并避免将插入顺序与增加的值序列相关联 交换第一个和最后16比特来实现“随机”插入 mongo 分片默认选择范围分片，单调递增的分片键会让数据集中插入到某个分片中 文本搜索 对于自我管理（非 Atlas）部署，MongoDB 的文本搜索功能支持执行字符串内容文本搜索的查询操作。为了执行文本搜索，MongoDB 使用 文本索引 和 $text 运算符 执行文本搜索 此示例演示如何构建文本索引并在指定文本段的情况下使用它来查找咖啡店 测试数据 123456789db.stores.insertMany( [ &#123; _id: 1, name: &quot;Java Hut&quot;, description: &quot;Coffee and cakes&quot; &#125;, &#123; _id: 2, name: &quot;Burger Buns&quot;, description: &quot;Gourmet hamburgers&quot; &#125;, &#123; _id: 3, name: &quot;Coffee Shop&quot;, description: &quot;Just coffee&quot; &#125;, &#123; _id: 4, name: &quot;Clothes Clothes Clothes&quot;, description: &quot;Discount clothing&quot; &#125;, &#123; _id: 5, name: &quot;Java Shopping&quot;, description: &quot;Indonesian goods&quot; &#125; ]) 创建文本索引 123456db.stores.createIndex( &#123; name: &quot;text&quot;, description: &quot;text&quot; &#125;) 准确的短语搜索 可以通过用双引号将它们括起来来搜索确切的短语。如果 $search 字符串包含短语和单个术语，则文本搜索将仅匹配包含该短语的文档 例如，以下将查找包含 coffee shop 的所有文档： 123456789101112131415db.stores.find( &#123; $text: &#123; $search: &quot;\\&quot;coffee shop\\&quot;&quot; &#125; &#125;)/* 1 */&#123; &quot;_id&quot; : 3.0, &quot;name&quot; : &quot;Coffee Shop&quot;, &quot;description&quot; : &quot;Just coffee&quot;&#125; 全文搜索例子 搜索单个单词 匹配任何搜索词 搜索短语 排除包含的术语 搜索不同的语言 地理空间查询 GeoJSON 地理位置数据类型 TODO 可重试写入 可重试写入允许MongoDB驱动程序在遇到网络错误或在复制集或分片群集中找不到正常的主节点时自动重试特定的写操作一次 前提条件 需要集群部署支持 可重试写入需要副本集或分片集群，并且不支持独立实例 支持的存储引擎 可重试写入需要支持文档级锁定的存储引擎，例如 WiredTiger 或 内存 存储引擎 3.6+版本程序驱动 客户端需要为 MongoDB 3.6 或更高版本更新 MongoDB 驱动程序 可重试写入和多文档事务 事务 提交和 中止 操作是可重试的写操作。如果提交操作或中止操作遇到错误，MongoDB 驱动程序将重试一次操作，而不管 retryWrites 是否设置为 false 无论 retryWrites 的值如何，事务内的写操作都不可单独重试 启用可重试写入 官方的MongoDB 3.6和4.0兼容驱动程序需要在连接字符串中包含 retryWrites=true 选项，以启用该连接的可重试写操作 MongoDB 4.2 及更高版本兼容的驱动程序 默认启用 可重试写入。较早的驱动程序需要 retryWrites=true 选项。在使用与 MongoDB 4.2 及更高版本兼容的驱动程序的应用程序中，可以省略 retryWrites=true 选项 要禁用可重试写入，使用与 MongoDB 4.2 及更高版本兼容的驱动程序的应用程序必须在连接字符串中包含 retryWrites=false Mongo shell mongosh 中默认启用可重试写入。要禁用可重试写入，请使用 --retryWrites=false 命令行选项： 1mongosh --retryWrites=false 可重试的写操作 当发出已确认的写关注时，可以重试以下写操作; 例如 Write Concern 不能为 &#123;w：0&#125; 事务内的写操作不可单独重试 方法 说明 db.collection.insertOne() db.collection.insert() db.collection.insertMany() 插入操作 db.collection.updateOne() db.collection.replaceOne() db.collection.save() db.collection.update() where multi is false 单文档更新操作 db.collection.deleteOne() db.collection.remove() where justOne is true 单个文档删除操作 db.collection.findAndModify() db.collection.findOneAndDelete() db.collection.findOneAndReplace() db.collection.findOneAndUpdate() findAndModify 操作。所有 findAndModify 操作都是单文档操作 db.collection.bulkWrite() 具有以下写操作： insertOne . updateOne . replaceOne . deleteOne 仅包含单文档写入操作的批量写入操作。可重试的批量操作可以包括指定写操作的任意组合，但不能包括任何多文档写操作，例如 updateMany Bulk operations for: Bulk.find.removeOne() . Bulk.find.replaceOne() . Bulk.find.replaceOne() 仅包含单文档写入操作的批量写入操作。可重试的批量操作可以包括指定写操作的任意组合，但不能包括任何多文档写操作，例如 update 为 multi 选项指定 true 可重试写入行为 持久性网络错误 MongoDB 可重试写入仅进行一次重试。这有助于解决暂时性网络错误和副本集选举，但不会解决持久性网络错误 故障转移期 如果驱动程序在目标副本集或分片集群分片中找不到健康的主节点，则驱动程序会等待 serverSelectionTimeoutMS 毫秒以确定新的主节点，然后再重试。可重试写入不会解决故障转移时间超过 serverSelectionTimeoutMS 的实例 可重试读取 可重试读取允许MongoDB驱动程序在遇到某些 网络 或 服务器错误 时，可以一次自动重试某些读取操作 前提条件 最低的驱动程序版本 官方MongoDB驱动兼容MongoDB服务器 4.2 和以后支持重试读取 最低服务器版本 如果连接到MongoDB Server 3.6 或更高版本，驱动程序只能重试读取操作 启用可重试读取 MongoDB Server 4.2 及更高版本兼容的官方 MongoDB 驱动程序 默认启用 可重试读取。要显式禁用可重试读取，请在部署的连接字符串中指定 retryReads=false 可重试读取操作 方法 内容描述 Collection.aggregate Collection.count Collection.countDocuments Collection.distinct Collection.estimatedDocumentCount Collection.find Database.aggregate CRUD API 读取操作 Collection.watch Database.watch MongoClient.watch 更改流操作 MongoClient.listDatabasesDatabase.listCollections Collection.listIndexes 枚举操作 由 Collection.find 支持的 GridFS 操作（例如 GridFSBucket.openDownloadStream ） GridFS 文件下载操作 行为 持久性网络错误 MongoDB 可重试写入仅进行一次重试。这有助于解决暂时性网络错误和副本集选举，但不会解决持久性网络错误 故障转移期 如果驱动程序在目标副本集或分片集群分片中找不到健康的主节点，则驱动程序会等待 serverSelectionTimeoutMS 毫秒以确定新的主节点，然后再重试。可重试写入不会解决故障转移时间超过 serverSelectionTimeoutMS 的实例 读取关注 readConcern 选项允许你控制从 复制集 和 分片集群 读取数据的一致性和隔离性 通过有效地使用 写关注 和 读关注，你可以适当地调整 一致性 和 可用性 的保证级别，例如等待以保证更强的一致性，或放松一致性要求以提供更高的可用性 将MongoDB驱动程序更新到MongoDB 3.2或更高版本以支持读关注 从 MongoDB 4.4 开始，副本集和分片集群支持设置全局默认读关注。未指定显式读取关注的操作会继承全局默认读取关注设置。有关详细信息，请参阅 setDefaultRWConcern 读关注级别 读关注详解 类似读隔离级别 local 返回单个节点最新数据，主节点同时进行写操作 可能会读取到不一致的数据 available 牺牲一致性，提供一种最低延迟的读取。可能会返回孤儿文档 majority 保证读取的数据已被大多数副本集成员写入确认 linearizable 保证线性一致性读取，意味着读取操作会等待前面的写操作完成 snapshot 快照读取，确保读取的是已提交的数据，只能在副本集和分片集群下使用，需要维护和创建快照 使用 对于不在多文档事务中的操作，可以指定 readConcern 级别作为支持读关注的命令和方法的选项 1readConcern: &#123; level: &lt;level&gt; &#125; db.collection.find() 指定读取关注级别： 1db.collection.find().readConcern(&lt;level&gt;) 写入关注 写关注描述了从 MongoDB 请求对独立 mongod 或副本集或分片集群的写入操作的确认级别。在分片集群中， mongos 实例会将写关注传递给分片 对于多文档事务，您在事务级别设置写关注，而不是在单个操作级别。不要为事务中的单个写操作显式设置写关注如果您为多文档事务指定了一个 &quot;majority&quot; 写关注点，并且该事务未能复制到计算出的大多数副本集成员，那么该事务可能不会立即在副本集成员上回滚。副本集最终是一致的。始终在所有副本集成员上应用或回滚事务 写关注规范 1&#123; w: &lt;value&gt;, j: &lt;boolean&gt;, wtimeout: &lt;number&gt; &#125; w 选项请求确认写操作已传播到指定数量的mongod实例或具有指定标记的mongod实例 0：表示不进行写入确认 1：主节点的写入确认 j 选项请求 MongoDB 确认写入操作已写入磁盘日志，以便在系统出现故障时进行恢复 wtimeout 此选项指定写入关注的时间限制（以毫秒为单位）. wtimeout 仅适用于大于 1 的 w 值 具体用法 CURD概念 查询计划、性能和分析 查询计划 查询优化 分析查询性能 写操作性能 原子性、一致性和分布式操作 原子性和事务 读隔离性，一致性和近因性 分布式查询 查询计划 对于查询，MongoDB查询优化器在给定可用索引的情况下选择并缓存效率最高的查询计划。最有效的查询计划的评估是基于查询执行计划在查询计划评估候选计划时执行的“工作单元”(works)的数量 Work units 是一种计算模型，用于确定 MongoDB 数据库中查询和更新操作的执行时间Work units 的计算基于 MongoDB 的查询和更新操作，包括 find、insert、update 和 delete 等操作。每次查询或更新操作都会消耗一个或多个 work units 关联的计划缓存条目用于具有相同查询形状的后续查询 计划缓存条目状态 查询计划是指MongoDB查询优化器生成的用于执行查询的计划查询形状（Query Shape）指的是查询的结构或模式，包括查询的字段条件和操作符等。它描述了查询逻辑结构，例如使用哪些字段进行过、排序或聚合等操作。查询形状可以通过查询语句来定义，如使用find()方法或聚合管道操作符查询条目（Query Stage）查询计划中的每个步骤或阶段，用于处理查询的不同操作。每个查询阶段都会接收输入数据，并根据查询形状的定义对数据进行处理然后将结果传递下一个阶段。常见的查询段包括索引描、过滤、投影、排序和聚合等查询形状和查询条目之间存在联系。查询形状定义了查询的逻辑结构，而查询条目则是实际执行查询的步骤MongoDB 通过查询形状来定义查询操作，并指导查询引擎如何检索文档。查询形状中的每个部分都可以用来表示查询的不同方面，例如查询条件、查询范围和查询模式等。开发人员可以根据查询形状来设计和优化查询语句，以获得最佳的查询性能 从 MongoDB 4.2 开始，缓存条目与状态相关联： State Description Missing （缺失） 缓存中不存在此形状的条目。 对于查询，如果形状的缓存条目状态为 Missing： ⁣⁣⁣⁣ ⁣⁣⁣⁣1.对候选计划进行评估并选出一个获胜的计划。 ⁣⁣⁣⁣ ⁣⁣⁣⁣2.所选计划以其 works 值添加到处于 Inactive 状态的缓存中。 Inactive （不活跃） 缓存中的条目是此形状的占位符条目。也就是说，计划者已经看到了形状并计算了其成本（works值）并存储为占位符条目，但查询形状不用于生成查询计划。 对于查询，如果形状的缓存条目状态 Inactive: ⁣⁣⁣⁣ 1.对候选计划进行评估并选出一个获胜的计划。⁣⁣⁣⁣ 2. 将所选计划的 works 值与非活动条目的值进行比较。如果所选计划的 works 值为： 小于或等于 Inactive 条目的 所选计划将替换占位符 Inactive 条目，并具有 Active 状态 如果在替换发生之前， Inactive 条目变为 Active（例如，由于另一个查询操作），则仅当新活动条目的works值大于所选计划时，才会替换该新活动条目 大于 Inactive 条目的 Inactive 条目保留，但其 works 值增加 Active （活跃） 缓存中的条目用于获胜计划。规划器可以使用该条目来生成查询计划。对于查询，如果形状的缓存条目状态为 Active: 活动条目用于生成查询计划。 规划器还会评估条目的性能，如果条目的 works值不再符合选择标准，它将转换为非活动状态。 有关触发计划缓存更改的其他场景，请参阅 计划缓存刷新 查询计划和高速缓存 要查看给定查询的查询计划信息，可以使用 db.collection.explain() 或 cursor.explain() 从 MongoDB 4.2 开始，您可以使用 $planCacheStats 聚合阶段来查看集合的计划缓存信息 计划缓存刷新 如果 mongod 重新启动或关闭，查询计划缓存不会保留。此外： 索引或集合删除等目录操作会清除计划缓存 最近最少使用（LRU）高速缓存替换机制将清除最近最少访问的高速缓存条目 用户还可以： 使用 PlanCache.clear() 方法手动清除整个计划缓存 使用 PlanCache.clearPlansByQuery()方法手动清除特定计划缓存条目 查询优化 索引通过减少查询操作需要处理的数据量来提高读操作的效率。这简化了与在MongoDB中完成查询相关的工作 创建索引以支持读操作 如果应用程序查询特定字段或字段集上的集合，那么查询字段上的 索引 或字段集上的 复合索引 可以防止查询扫描整个集合来查找和返回查询结果。有关索引的更多信息，请参阅 MongoDB中索引中完整文档 例如 应用程序在 type 字段上查询 inventory 集合。 type 字段的值是用户驱动的 12var typeValue = &lt;someUserInput&gt;;db.inventory.find( &#123; type: typeValue &#125; ); 要提高此查询的性能，请向 type 字段上的 inventory 集合添加升序或降序索引。在 mongosh 中，可以使用 db.collection.createIndex() 方法创建索引： 1db.inventory.createIndex( &#123; type: 1 &#125; ) 该索引可以防止上述对 type 的查询扫描整个集合以返回结果 其他优化 查询选择性 指查询条件排除或过滤掉集合中文档的程度。查询选择性可以决定查询是否可以有效地使用索引，甚至根本不使用索引 更具选择性的查询匹配较小比例的文档。例如，对唯一 _id 字段的等式匹配具有高度选择性，因为它最多可以匹配一个文档 例如，不等运算符 $nin 和 $ne 的选择性不是很强，因为它们通常匹配索引的很大一部分。因此，在许多情况下，带有索引的 $nin 或 $ne 查询的性能可能并不比必须扫描集合中所有文档的 $nin 或 $ne 查询好 索引覆盖查询 查询中的所有字段都是索引的一部分 结果中返回的所有字段都在同一个索引中 嵌套文档 索引覆盖同样适用在嵌套文档中 多键覆盖 如果索引跟踪哪个或哪些字段导致索引成为多键，则多键索引可以覆盖对非数组字段的查询 多键索引 不能覆盖对 数组字段 的查询 性能 为索引包含查询所需的所有字段，所以MongoDB既可以匹配 查询条件 ，又可以仅使用索引返回结果 仅查询索引要比查询索引之外的文档快得多。索引键通常比它们编目的文档小，索引通常在RAM中可用，或按顺序位于磁盘上 局限性 地理空间索引无法覆盖查询 多键索引 不能覆盖对数组字段的查询 在 mongos 上运行时，如果索引包含分片键，则索引只能覆盖分片集合上的查询 分析查询性能 查询计划返回结果 可能会因 MongoDB 版本而异 cursor.explain(&quot;executionStats&quot;) 和 db.collection.explain(&quot;executionStats&quot;) 方法提供有关查询性能的统计信息。这些统计信息可用于衡量查询是否以及如何使用索引。有关详细信息，请参阅 db.collection.explain() 测试数据 1234567891011121314db.inventory.insertMany( [ &#123; &quot;_id&quot; : 1, &quot;item&quot; : &quot;f1&quot;, type: &quot;food&quot;, quantity: 500 &#125;, &#123; &quot;_id&quot; : 2, &quot;item&quot; : &quot;f2&quot;, type: &quot;food&quot;, quantity: 100 &#125;, &#123; &quot;_id&quot; : 3, &quot;item&quot; : &quot;p1&quot;, type: &quot;paper&quot;, quantity: 200 &#125;, &#123; &quot;_id&quot; : 4, &quot;item&quot; : &quot;p2&quot;, type: &quot;paper&quot;, quantity: 150 &#125;, &#123; &quot;_id&quot; : 5, &quot;item&quot; : &quot;f3&quot;, type: &quot;food&quot;, quantity: 300 &#125;, &#123; &quot;_id&quot; : 6, &quot;item&quot; : &quot;t1&quot;, type: &quot;toys&quot;, quantity: 500 &#125;, &#123; &quot;_id&quot; : 7, &quot;item&quot; : &quot;a1&quot;, type: &quot;apparel&quot;, quantity: 250 &#125;, &#123; &quot;_id&quot; : 8, &quot;item&quot; : &quot;a2&quot;, type: &quot;apparel&quot;, quantity: 400 &#125;, &#123; &quot;_id&quot; : 9, &quot;item&quot; : &quot;t2&quot;, type: &quot;toys&quot;, quantity: 50 &#125;, &#123; &quot;_id&quot; : 10, &quot;item&quot; : &quot;f4&quot;, type: &quot;food&quot;, quantity: 75 &#125; ]) 没有索引的查询 12345678910111213db.inventory.find( &#123; quantity: &#123; $gte: 100, $lte: 200 &#125; &#125;)&#123; &quot;_id&quot; : 2, &quot;item&quot; : &quot;f2&quot;, &quot;type&quot; : &quot;food&quot;, &quot;quantity&quot; : 100 &#125;&#123; &quot;_id&quot; : 3, &quot;item&quot; : &quot;p1&quot;, &quot;type&quot; : &quot;paper&quot;, &quot;quantity&quot; : 200 &#125;&#123; &quot;_id&quot; : 4, &quot;item&quot; : &quot;p2&quot;, &quot;type&quot; : &quot;paper&quot;, &quot;quantity&quot; : 150 &#125; 要查看所选的查询计划，请将 cursor.explain(&quot;executionStats&quot;) 游标方法链接到查找命令的末尾： 12345678db.inventory.find( &#123; quantity: &#123; $gte: 100, $lte: 200 &#125; &#125;).explain(&quot;executionStats&quot;) 返回以下执行计划： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105&#123; &quot;explainVersion&quot; : &quot;1&quot;, &quot;queryPlanner&quot; : &#123; &quot;namespace&quot; : &quot;test.inventory&quot;, &quot;indexFilterSet&quot; : false, &quot;parsedQuery&quot; : &#123; &quot;$and&quot; : [ &#123; &quot;quantity&quot; : &#123; &quot;$lte&quot; : 200.0 &#125; &#125;, &#123; &quot;quantity&quot; : &#123; &quot;$gte&quot; : 100.0 &#125; &#125; ] &#125;, &quot;maxIndexedOrSolutionsReached&quot; : false, &quot;maxIndexedAndSolutionsReached&quot; : false, &quot;maxScansToExplodeReached&quot; : false, &quot;winningPlan&quot; : &#123; &quot;stage&quot; : &quot;COLLSCAN&quot;, &quot;filter&quot; : &#123; &quot;$and&quot; : [ &#123; &quot;quantity&quot; : &#123; &quot;$lte&quot; : 200.0 &#125; &#125;, &#123; &quot;quantity&quot; : &#123; &quot;$gte&quot; : 100.0 &#125; &#125; ] &#125;, &quot;direction&quot; : &quot;forward&quot; &#125;, &quot;rejectedPlans&quot; : [] &#125;, &quot;executionStats&quot; : &#123; &quot;executionSuccess&quot; : true, &quot;nReturned&quot; : 3, &quot;executionTimeMillis&quot; : 24, &quot;totalKeysExamined&quot; : 0, &quot;totalDocsExamined&quot; : 10, &quot;executionStages&quot; : &#123; &quot;stage&quot; : &quot;COLLSCAN&quot;, &quot;filter&quot; : &#123; &quot;$and&quot; : [ &#123; &quot;quantity&quot; : &#123; &quot;$lte&quot; : 200.0 &#125; &#125;, &#123; &quot;quantity&quot; : &#123; &quot;$gte&quot; : 100.0 &#125; &#125; ] &#125;, &quot;nReturned&quot; : 3, &quot;executionTimeMillisEstimate&quot; : 0, &quot;works&quot; : 12, &quot;advanced&quot; : 3, &quot;needTime&quot; : 8, &quot;needYield&quot; : 0, &quot;saveState&quot; : 1, &quot;restoreState&quot; : 1, &quot;isEOF&quot; : 1, &quot;direction&quot; : &quot;forward&quot;, &quot;docsExamined&quot; : 10 &#125; &#125;, &quot;command&quot; : &#123; &quot;find&quot; : &quot;inventory&quot;, &quot;filter&quot; : &#123; &quot;quantity&quot; : &#123; &quot;$gte&quot; : 100.0, &quot;$lte&quot; : 200.0 &#125; &#125;, &quot;$db&quot; : &quot;test&quot; &#125;, &quot;serverInfo&quot; : &#123; &quot;host&quot; : &quot;4ffcfe6b7fdf&quot;, &quot;port&quot; : 27017, &quot;version&quot; : &quot;5.0.5&quot;, &quot;gitVersion&quot; : &quot;d65fd89df3fc039b5c55933c0f71d647a54510ae&quot; &#125;, &quot;serverParameters&quot; : &#123; &quot;internalQueryFacetBufferSizeBytes&quot; : 104857600, &quot;internalQueryFacetMaxOutputDocSizeBytes&quot; : 104857600, &quot;internalLookupStageIntermediateDocumentMaxSizeBytes&quot; : 104857600, &quot;internalDocumentSourceGroupMaxMemoryBytes&quot; : 104857600, &quot;internalQueryMaxBlockingSortMemoryUsageBytes&quot; : 104857600, &quot;internalQueryProhibitBlockingMergeOnMongoS&quot; : 0, &quot;internalQueryMaxAddToSetBytes&quot; : 104857600, &quot;internalDocumentSourceSetWindowFieldsMaxMemoryBytes&quot; : 104857600 &#125;, &quot;ok&quot; : 1.0&#125; 执行计划返回结果详解 queryPlanner.winningPlan.queryPlan.stage 显示 COLLSCAN 以指示集合扫描 COLLSCAN 用于集合扫描 IXSCAN 用于扫描索引键 FETCH 用于检索文档 GROUP 用于对文档进行分组 SHARD_MERGE 用于合并分片的结果 SHARDING_FILTER 用于从分片中过滤掉孤立文档 集合扫描表明 mongod 必须逐个文档扫描整个集合文档才能识别结果。这通常是一项昂贵的操作，可能会导致查询速度变慢 executionStats.nReturned 显示 3 表示查询匹配并返回三个文档 executionStats.totalKeysExamined 显示 0 以指示此查询未使用索引 executionStats.totalDocsExamined 显示 10 表示 MongoDB 必须扫描十个文档（即集合中的所有文档）才能找到三个匹配的文档 匹配 文档的数量和 检查 文档的数量之间的差异可能表明，为了提高效率，查询可能会受益于索引的使用 使用索引 12345db.inventory.createIndex( &#123; quantity: 1 &#125;) 查看执行计划： 12345678db.inventory.find( &#123; quantity: &#123; $gte: 100, $lte: 200 &#125; &#125;).explain(&quot;executionStats&quot;) 返回的查询计划： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131&#123; &quot;explainVersion&quot; : &quot;1&quot;, &quot;queryPlanner&quot; : &#123; &quot;namespace&quot; : &quot;test.inventory&quot;, &quot;indexFilterSet&quot; : false, &quot;parsedQuery&quot; : &#123; &quot;$and&quot; : [ &#123; &quot;quantity&quot; : &#123; &quot;$lte&quot; : 200.0 &#125; &#125;, &#123; &quot;quantity&quot; : &#123; &quot;$gte&quot; : 100.0 &#125; &#125; ] &#125;, &quot;maxIndexedOrSolutionsReached&quot; : false, &quot;maxIndexedAndSolutionsReached&quot; : false, &quot;maxScansToExplodeReached&quot; : false, &quot;winningPlan&quot; : &#123; &quot;stage&quot; : &quot;FETCH&quot;, &quot;inputStage&quot; : &#123; &quot;stage&quot; : &quot;IXSCAN&quot;, &quot;keyPattern&quot; : &#123; &quot;quantity&quot; : 1.0 &#125;, &quot;indexName&quot; : &quot;quantity_1&quot;, &quot;isMultiKey&quot; : false, &quot;multiKeyPaths&quot; : &#123; &quot;quantity&quot; : [] &#125;, &quot;isUnique&quot; : false, &quot;isSparse&quot; : false, &quot;isPartial&quot; : false, &quot;indexVersion&quot; : 2, &quot;direction&quot; : &quot;forward&quot;, &quot;indexBounds&quot; : &#123; &quot;quantity&quot; : [ &quot;[100.0, 200.0]&quot; ] &#125; &#125; &#125;, &quot;rejectedPlans&quot; : [] &#125;, &quot;executionStats&quot; : &#123; &quot;executionSuccess&quot; : true, &quot;nReturned&quot; : 3, &quot;executionTimeMillis&quot; : 1, &quot;totalKeysExamined&quot; : 3, &quot;totalDocsExamined&quot; : 3, &quot;executionStages&quot; : &#123; &quot;stage&quot; : &quot;FETCH&quot;, &quot;nReturned&quot; : 3, &quot;executionTimeMillisEstimate&quot; : 0, &quot;works&quot; : 4, &quot;advanced&quot; : 3, &quot;needTime&quot; : 0, &quot;needYield&quot; : 0, &quot;saveState&quot; : 0, &quot;restoreState&quot; : 0, &quot;isEOF&quot; : 1, &quot;docsExamined&quot; : 3, &quot;alreadyHasObj&quot; : 0, &quot;inputStage&quot; : &#123; &quot;stage&quot; : &quot;IXSCAN&quot;, &quot;nReturned&quot; : 3, &quot;executionTimeMillisEstimate&quot; : 0, &quot;works&quot; : 4, &quot;advanced&quot; : 3, &quot;needTime&quot; : 0, &quot;needYield&quot; : 0, &quot;saveState&quot; : 0, &quot;restoreState&quot; : 0, &quot;isEOF&quot; : 1, &quot;keyPattern&quot; : &#123; &quot;quantity&quot; : 1.0 &#125;, &quot;indexName&quot; : &quot;quantity_1&quot;, &quot;isMultiKey&quot; : false, &quot;multiKeyPaths&quot; : &#123; &quot;quantity&quot; : [] &#125;, &quot;isUnique&quot; : false, &quot;isSparse&quot; : false, &quot;isPartial&quot; : false, &quot;indexVersion&quot; : 2, &quot;direction&quot; : &quot;forward&quot;, &quot;indexBounds&quot; : &#123; &quot;quantity&quot; : [ &quot;[100.0, 200.0]&quot; ] &#125;, &quot;keysExamined&quot; : 3, &quot;seeks&quot; : 1, &quot;dupsTested&quot; : 0, &quot;dupsDropped&quot; : 0 &#125; &#125; &#125;, &quot;command&quot; : &#123; &quot;find&quot; : &quot;inventory&quot;, &quot;filter&quot; : &#123; &quot;quantity&quot; : &#123; &quot;$gte&quot; : 100.0, &quot;$lte&quot; : 200.0 &#125; &#125;, &quot;$db&quot; : &quot;test&quot; &#125;, &quot;serverInfo&quot; : &#123; &quot;host&quot; : &quot;4ffcfe6b7fdf&quot;, &quot;port&quot; : 27017, &quot;version&quot; : &quot;5.0.5&quot;, &quot;gitVersion&quot; : &quot;d65fd89df3fc039b5c55933c0f71d647a54510ae&quot; &#125;, &quot;serverParameters&quot; : &#123; &quot;internalQueryFacetBufferSizeBytes&quot; : 104857600, &quot;internalQueryFacetMaxOutputDocSizeBytes&quot; : 104857600, &quot;internalLookupStageIntermediateDocumentMaxSizeBytes&quot; : 104857600, &quot;internalDocumentSourceGroupMaxMemoryBytes&quot; : 104857600, &quot;internalQueryMaxBlockingSortMemoryUsageBytes&quot; : 104857600, &quot;internalQueryProhibitBlockingMergeOnMongoS&quot; : 0, &quot;internalQueryMaxAddToSetBytes&quot; : 104857600, &quot;internalDocumentSourceSetWindowFieldsMaxMemoryBytes&quot; : 104857600 &#125;, &quot;ok&quot; : 1.0&#125; queryPlanner.winningPlan.queryPlan.inputStage.stage 显示 IXSCAN 说明使用索引 executionStats.nReturned 显示 3 表示查询匹配并返回三个文档 executionStats.totalKeysExamined 显示 3 表示 MongoDB 扫描了三个索引条目。检查的键数与返回的文档数相匹配，这意味着 mongod 只需检查索引键即可返回结果。 mongod 不必扫描所有文档，只需将三个匹配的文档拉入内存即可。这导致非常有效的查询 executionStats.totalDocsExamined 显示 3 表示MongoDB扫描了三个文档 如果没有索引，查询将扫描整个 10 文档集合以返回 3 匹配文档。查询还必须扫描每个文档的全部内容，可能会将它们拉入内存。这会导致昂贵且可能很慢的查询操作 使用索引运行时，查询会扫描 3 索引条目和 3 文档以返回 3 匹配文档，从而实现非常高效的查询 原子性和事务 原子性 在MongoDB中，写操作在单个文档级别上是原子的，即使该操作修改了单个文档中嵌入的多个文档 多文档事务 当单个写操作（例如 db.collection.updateMany() ）修改多个文档时，每个文档的修改是原子的，但整个操作不是原子的 当执行多文档写操作时，无论是通过单个写操作还是通过多个写操作，其他操作都可能会交错 对于需要对多个文档（在单个或多个集合中）进行读写原子性的情况，MongoDB 支持多文档事务： 在 4.0 版本中，MongoDB 支持副本集上的多文档事务 在 4.2 版本中，MongoDB 引入了分布式事务，增加了对分片集群上的多文档事务的支持，并合并了现有的对副本集上的多文档事务的支持 有关 MongoDB 中事务的详细信息，请参阅 事务 页面 在大多数情况下，多文档事务会比单文档写入产生更大的性能成本，并且多文档事务的可用性不应取代有效的模式设计。对于许多场景，非规范化数据模型（嵌入式文档和数组）将继续成为您的数据和用例的最佳选择。也就是说，对于许多场景，适当地建模数据将最大限度地减少对多文档事务的需求。 并发控制 并发控制允许多个应用程序并发运行，而不会导致数据不一致或冲突 对文档的 findAndModify 操作是原子的：如果查找条件与文档匹配，则对该文档执行更新。在当前更新完成之前，对该文档的并发查询和其他更新不会受到影响 例如 包含两个文档的集合： 1234db.myCollection.insertMany( [ &#123; _id: 0, a: 1, b: 1 &#125;, &#123; _id: 1, a: 1, b: 1 &#125;] ) 以下两个 findAndModify 操作同时运行： 1234db.myCollection.findAndModify( &#123; query: &#123; a: 1 &#125;, update: &#123; $inc: &#123; b: 1 &#125;, $set: &#123; a: 2 &#125; &#125;&#125; ) findAndModify 操作完成后，保证两个文档中的 a 和 b 都设置为 2 说明并发是互斥的 还可以在字段上创 唯一索引 ，以便它只能具有唯一值。这可以防止插入和更新创建重复数据。您可以在多个字段上创建唯一索引，以确保字段值的组合是唯一的。有关示例，请参阅 findAndModify() 使用唯一索引更新或插入 读隔离性，一致性和因果一致 隔离保证 读未提交 根据读取的关注点，客户端可以在 持久化 写入之前看到写入的结果： 无论写操作的写关注点如何，使用 “local” 或 “available” 读取关注点的其他客户端可以在写操作被确认给发出该写操作的客户端之前看到写操作的结果 使用 “local” 或 “available” 读取关注点的客户端可以读取可能在副本集故障切换期间被回滚的数据 在多文档事务中，当事务提交时，事务中所做的所有数据更改都会保存并在事务外部可见。也就是说，事务不会只提交其中的一些更改同时回滚其他更改 在事务提交之前，事务中所做的数据更改对事务外部是不可见的 然而，当事务写入多个分片时，并不是所有外部读取操作都需要等待已提交事务的结果在所有分片间可见。例如，如果事务已提交，写入 1 在分片 A 上可见，但写入 2 在分片 B 上尚不可见，在使用“local”读取关注点时，可以读取写入 1 的结果而无需看到写入 2 读未提交是 默认的隔离级别，适用于mongod独立实例以及复制集和分片群集 一致性保证 读未提交和单文档原子性 写操作对于单个文档是原子的；也就是说，如果写操作正在更新文档中的多个字段，那么读操作永远不会看到只更新了一部分字段的文档。然而，即使客户端可能没有看到部分更新的文档，读取未提交意味着并发读操作仍然可以在更改变得持久之前看到更新的文档 在独立的 mongod 实例中，对于单个文档的一组读取和写入操作是可串行化的。在副本集中，只有在没有回滚的情况下，对于单个文档的一系列读取和写入操作才是可串行化的 读未提交和多文档写入 当单个写操作（例如 db.collection.updateMany()）修改多个文档时，每个文档的修改是原子的，但整个操作不是原子的 执行多文档写入操作时，无论是通过单个写操作还是多个写操作，其他操作都可能交错执行 对于需要对多个文档进行读取和写入操作（在单个或多个集合中）的情况，MongoDB 支持多文档事务 在 4.0 版本中，MongoDB 在副本集上支持多文档事务 在 4.2 版本中，MongoDB 引入分布式事务，为分片集群添加了对多文档事务的支持，并整合了对副本集的多文档事务支持 在大多数情况下，相对于单文档写入，多文档事务会带来更大的性能开销，并且多文档事务的可用性不应该取代有效的架构设计。对于许多场景，规范化的数据模型（嵌入式文档和数组）仍然是数据和使用案例的最佳选择。也就是说，对于许多场景，适当地对数据进行建模将最小化对多文档事务的需求 游标快照 在某些情况下，MongoDB 游标可以返回同一文档多次。当游标返回文档时，其他操作可以与查询交错执行。如果这些操作之一更改了查询所使用的索引上的索引字段，那么游标可能会多次返回相同的文档 使用唯一索引的查询在某些情况下可能会返回重复的值。如果一个使用唯一索引的游标与具有相同唯一值的文档的删除和插入交错执行，那么游标可能会从不同的文档中两次返回相同的唯一值 可以使用读关注点的 snapshot 来解决这个问题 实时顺序 将读关注设置为 linearizable，将写关注设置为 majority，那么这种读写模型组合可以使多个线程可以在单个文档上执行读写操作，就好像单个线程实时执行了这些操作一样 ; 也就是说，这些读写的相应计划被认为是线性的 因果一致性 如果一个操作在逻辑上依赖于前一个操作，那么这两个操作之间存在因果关系。例如，一个删除基于特定条件的所有文档的写操作和一个后续的读操作来验证删除操作之间存在因果关系。 使用因果一致性会话，MongoDB 按照它们的因果关系对操作进行排序，并保证客户端所观察到的结果与这些因果关系一致 客户端会话和因果一致性保证 为了提供因果一致性，MongoDB 3.6 在客户端会话中启用了因果一致性。因果一致性会话表示具有 majority 读关注点的读操作序列和具有 majority 写关注点的写操作序列之间具有因果关系，并通过它们的顺序来体现。应用程序必须确保一次只能有一个线程在客户端会话中执行这些操作 重要提示：客户端会话仅对以下情况保证因果一致性： 具有 majority 读关注点的读操作，即返回的数据已被副本集中的大多数成员确认并持久化 具有 majority 写关注点的写操作，即要求写操作在副本集的多数投票成员上应用之后才返回确认信息 当客户端发出带有 majority 读关注点的读操作和带有 majority 写关注点的写操作序列时，客户端会在每个操作中包含会话信息 对于与会话关联的每个带有 majority 读关注点的读操作和带有 majority 写关注点的写操作，MongoDB 返回操作时间和集群时间，即使操作出现错误。客户端会话会跟踪操作时间和集群时间（线性一致） 因果一致性会话中的操作与会话外的操作不隔离。如果并发写操作在会话的写操作和读操作之间交错执行，会话的读操作可能会返回反映在会话的写操作之后发生的写操作的结果 因果一致性使用 考虑一个名为 items 的集合，该集合维护各种物品的当前和历史数据。只有历史数据具有非空的结束日期。如果某个物品的 SKU 值发生更改，需要将具有旧 SKU 值的文档进行更新，并在之后插入一个新文档以包含当前的 SKU 值。客户端可以使用因果一致性会话确保更新发生在插入之前 12345678910111213ClientSession session1 = client.startSession(ClientSessionOptions.builder().causallyConsistent(true).build());Date currentDate = new Date();MongoCollection&lt;Document&gt; items = client.getDatabase(&quot;test&quot;) .withReadConcern(ReadConcern.MAJORITY) .withWriteConcern(WriteConcern.MAJORITY.withWTimeout(1000, TimeUnit.MILLISECONDS)) .getCollection(&quot;test&quot;);items.updateOne(session1, eq(&quot;sku&quot;, &quot;111&quot;), set(&quot;end&quot;, currentDate));Document document = new Document(&quot;sku&quot;, &quot;nuts-111&quot;) .append(&quot;name&quot;, &quot;Pecans&quot;) .append(&quot;start&quot;, currentDate);items.insertOne(session1, document); 聚合 聚合操作处理数据记录并返回计算结果(诸如统计平均值，求和等)。聚合操作组值来自多个文档，可以对分组数据执行各种操作以返回单个结果。聚合操作包含三类： 单一作用聚合：提供了对常见聚合过程的简单访问，操作都从单个集合聚合文档 聚合管道是一个数据聚合的框架，模型基于数据处理流水线的概念。文档进入多级管道，将文档转换为聚合结果 从 MongoDB 5.0 开始，不推荐使用 map-reduce 聚合管道操作符 单一作用聚合 这类单一作用的聚合函数。 所有这些操作都聚合来自单个集合的文档。虽然这些操作提供了对公共聚合过程的简单访问，但它们缺乏聚合管道和map-Reduce的灵活性和功能 方法 说明 db.collection.estimatedDocumentCount() 忽略查询条件，返回集合或视图中所有文档的计数 db.collection.count() 返回与find()集合或视图的查询匹配的文档计数 。等同于 db.collection.find(query).count()构造 db.collection.distinct() 在单个集合或视图中查找指定字段的不同值，并在数组中返回结果 测试数据 1234567891011121314db.inventory.insertMany( [ &#123; &quot;_id&quot; : 1, &quot;item&quot; : &quot;f1&quot;, type: &quot;food&quot;, quantity: 500 &#125;, &#123; &quot;_id&quot; : 2, &quot;item&quot; : &quot;f2&quot;, type: &quot;food&quot;, quantity: 100 &#125;, &#123; &quot;_id&quot; : 3, &quot;item&quot; : &quot;p1&quot;, type: &quot;paper&quot;, quantity: 200 &#125;, &#123; &quot;_id&quot; : 4, &quot;item&quot; : &quot;p2&quot;, type: &quot;paper&quot;, quantity: 150 &#125;, &#123; &quot;_id&quot; : 5, &quot;item&quot; : &quot;f3&quot;, type: &quot;food&quot;, quantity: 300 &#125;, &#123; &quot;_id&quot; : 6, &quot;item&quot; : &quot;t1&quot;, type: &quot;toys&quot;, quantity: 500 &#125;, &#123; &quot;_id&quot; : 7, &quot;item&quot; : &quot;a1&quot;, type: &quot;apparel&quot;, quantity: 250 &#125;, &#123; &quot;_id&quot; : 8, &quot;item&quot; : &quot;a2&quot;, type: &quot;apparel&quot;, quantity: 400 &#125;, &#123; &quot;_id&quot; : 9, &quot;item&quot; : &quot;t2&quot;, type: &quot;toys&quot;, quantity: 50 &#125;, &#123; &quot;_id&quot; : 10, &quot;item&quot; : &quot;f4&quot;, type: &quot;food&quot;, quantity: 75 &#125; ]) estimatedDocumentCount() 忽略查询条件，返回集合或视图中所有文档的计数 1234db.inventory.estimatedDocumentCount()10 count() 12345678910db.inventory.count( &#123; quantity: &#123; $gte: 300 &#125; &#125;)4 distinct() type 字段去重 查询条件 quantity &gt;= 100 12345678910111213141516db.inventory.distinct( &quot;type&quot;, &#123; quantity: &#123; $gte: 100 &#125; &#125;)[ &quot;apparel&quot;, &quot;food&quot;, &quot;paper&quot;, &quot;toys&quot;] 聚合管道 SQL和Mongo聚合对比 聚合框架 MongoDB 聚合框架（Aggregation Framework）是一个计算框架，它可以： 作用在一个或几个集合上 对集合中的数据进行的一系列运算 将这些数据转化为期望的形式 从效果而言，聚合框架相当于 SQL 查询中的GROUP BY、 LEFT OUTER JOIN 、 AS等 聚合与管道 聚合管道阶段文档 整个聚合运算过程称为管道（Pipeline），它是由多个阶段（Stage）组成的， 每个管道： 接受一系列文档（原始数据） 每个阶段对这些文档进行一系列运算 结果文档输出给下一个阶段 完整的聚合管道示例 123456789101112131415161718db.orders.insertMany( [ &#123; _id: 0, name: &quot;Pepperoni&quot;, size: &quot;small&quot;, price: 19, quantity: 10, date: ISODate( &quot;2021-03-13T08:14:30Z&quot; ) &#125;, &#123; _id: 1, name: &quot;Pepperoni&quot;, size: &quot;medium&quot;, price: 20, quantity: 20, date : ISODate( &quot;2021-03-13T09:13:24Z&quot; ) &#125;, &#123; _id: 2, name: &quot;Pepperoni&quot;, size: &quot;large&quot;, price: 21, quantity: 30, date : ISODate( &quot;2021-03-17T09:22:12Z&quot; ) &#125;, &#123; _id: 3, name: &quot;Cheese&quot;, size: &quot;small&quot;, price: 12, quantity: 15, date : ISODate( &quot;2021-03-13T11:21:39.736Z&quot; ) &#125;, &#123; _id: 4, name: &quot;Cheese&quot;, size: &quot;medium&quot;, price: 13, quantity:50, date : ISODate( &quot;2022-01-12T21:23:13.331Z&quot; ) &#125;, &#123; _id: 5, name: &quot;Cheese&quot;, size: &quot;large&quot;, price: 14, quantity: 10, date : ISODate( &quot;2022-01-12T05:08:13Z&quot; ) &#125;, &#123; _id: 6, name: &quot;Vegan&quot;, size: &quot;small&quot;, price: 17, quantity: 10, date : ISODate( &quot;2021-01-13T05:08:13Z&quot; ) &#125;, &#123; _id: 7, name: &quot;Vegan&quot;, size: &quot;medium&quot;, price: 18, quantity: 10, date : ISODate( &quot;2021-01-13T05:10:13Z&quot; ) &#125;] ) 12345678910111213141516171819202122232425262728293031323334353637383940db.orders.aggregate( [ // 第 1 阶段：按披萨大小过滤披萨订单文档 &#123; $match: &#123;size: &quot;medium&quot;&#125; &#125;, // 第 2 阶段：按披萨名称分组进行聚合运算 &#123; $group: &#123; _id: &quot;$name&quot;, totalQuantity: &#123; $sum: &quot;$quantity&quot; &#125;, totalPrice: &#123;$sum:&#123; $multiply: [&quot;$price&quot;, &quot;$quantity&quot;]&#125;&#125; &#125; &#125; ])/* 1 */&#123; &quot;_id&quot; : &quot;Vegan&quot;, &quot;totalQuantity&quot; : 10.0, &quot;totalPrice&quot; : 180.0&#125;/* 2 */&#123; &quot;_id&quot; : &quot;Pepperoni&quot;, &quot;totalQuantity&quot; : 20.0, &quot;totalPrice&quot; : 400.0&#125;/* 3 */&#123; &quot;_id&quot; : &quot;Cheese&quot;, &quot;totalQuantity&quot; : 50.0, &quot;totalPrice&quot; : 650.0&#125; $match 阶段： 过滤 size 为 medium 的比萨订单 将剩余的文档传递到 $group 阶段 $group 阶段： 按 pizza name 对剩余文档进行分组 使用 $sum 计算每个比萨 name 的总订单 quantity 。总数存储在聚合管道返回的 totalQuantity 字段中 聚合阶段 阶段 描述 $addFields 向文档添加新字段。与project类似，addFields重塑了流中的每个文档;具体而言，通过向输出文档添加新字段，该文档包含输入文档和新添加字段中的现有字段。 $set是的别名$addFields。 $bucket 根据指定的表达式和存储段边界将传入文档分类为称为存储段的组。 $bucketAuto 根据指定的表达式将传入的文档分类为特定数量的组(称为存储桶)。自动确定存储桶边界，以尝试将文档均匀地分配到指定数量的存储桶中。 $collStats 返回有关集合或视图的统计信息。 $count 返回聚合管道此阶段的文档数量计数。 $facet 在同一阶段的同一组输入文档上处理多个聚合管道。支持在一个阶段中创建能够表征多维或多面数据的多面聚合。 $geoNear 基于与地理空间点的接近度返回有序的文档流。将$match，$sort和$limit的功能合并到地理空间数据中。输出文档包括附加距离字段，并且可以包括位置标识符字段。 $graphLookup 对集合执行递归搜索。对于每个输出文档，添加一个新的 array 字段，该字段包含该文档的递归搜索的遍历结果。 $group 按指定的标识符表达式对文档进行分组，并将累加器表达式(如果指定)应用于每个 group。消耗所有输入文档，并为每个不同的 group 输出一个文档。输出文档仅包含标识符字段，如果指定，则包含累积字段。 $indexStats 返回有关集合的每个索引的使用的统计信息。 $limit 将未修改的前 n 个文档传递给管道，其中 n 是指定的限制。对于每个输入文档，输出一个文档(对于前 n 个文档)或零文档(在前 n 个文档之后)。 $listSessions 列出所有活动时间已足够长以传播到system.sessions集合的会话。 $lookup 对同一数据库中的另一个集合执行左外连接，以从“已连接”集合中过滤文档以进行处理。 $match 过滤文档流以仅允许匹配的文档未经修改地传递到下一个管道阶段。 $match使用标准的 MongoDB 查询。对于每个输入文档，输出一个文档(匹配)或零文档(不匹配)。 $merge 将聚合管道的结果文档写入集合。该阶段可以将结果合并（插入新文档，合并文档，替换文档，保留现有文档，使操作失败，使用自定义更新管道处理文档）将结果合并到输出集合中。要使用该$merge阶段，它必须是管道中的最后一个阶段。 4.2版中的新功能。 $out 将聚合管道的结果文档写入集合。要使用$out阶段，它必须是管道中的最后一个阶段。 $planCacheStats 返回集合的计划缓存信息。 $project 重塑流中的每个文档，例如通过添加新字段或删除现有字段。对于每个输入文档，输出一个文档。 另请参阅$unset删除现有字段。 $redact 通过基于文档本身中存储的信息限制每个文档的内容来重塑流中的每个文档。包含$project和$match的功能。可用于实现字段级编辑。对于每个输入文档，输出一个或零个文档。 $replaceRoot 用指定的嵌入文档替换文档。该操作将替换输入文档中的所有现有字段，包括_id字段。指定嵌入在输入文档中的文档，以将嵌入的文档提升到顶部级别。 $replaceWith是$replaceRoot阶段的别名 。 $replaceWith 用指定的嵌入文档替换文档。该操作将替换输入文档中的所有现有字段，包括_id字段。指定嵌入在输入文档中的文档，以将嵌入的文档提升到顶部级别。 $replaceWith是$replaceRoot阶段的别名 。 $sample 从输入中随机选择指定数量的文档。 $set 将新字段添加到文档。与$project相似，$set重塑流中的每个文档；具体而言，通过向输出文档添加新字段，该输出文档既包含输入文档中的现有字段，又包含新添加的字段。 $set是$addFields阶段的别名。 $skip 跳过前 n 个文档，其中 n 是指定的跳过编号，并将未修改的其余文档传递给管道。对于每个输入文档，输出零文档(对于前 n 个文档)或一个文档(如果在前 n 个文档之后)。 $sort 按指定的排序 key 重新排序文档流。只有顺序改变;文档保持不变。对于每个输入文档，输出一个文档。 sortByCount 根据指定表达式的 value 对传入文档进行分组，然后计算每个不同 group 中的文档计数。 $unset 从文档中删除/排除字段。 $unset是$project删除字段的阶段的别名。 $unwind 将数组展开后形成一个独立的文档。每个输出文档都使用元素 value 替换 array。对于每个输入文档，输出 n 个文档，其中 n 是 array 元素的数量，对于空 array 可以为零。 $match 过滤文档以仅允许匹配的文档未经修改地传递到下一个管道阶段 1&#123; $match: &#123; &lt;query&gt; &#125; &#125; 测试数据 1234567891011db.articles.insertMany( [ &#123; &quot;_id&quot; : ObjectId(&quot;512bc95fe835e68f199c8686&quot;), &quot;author&quot; : &quot;dave&quot;, &quot;score&quot; : 80, &quot;views&quot; : 100 &#125;, &#123; &quot;_id&quot; : ObjectId(&quot;512bc962e835e68f199c8687&quot;), &quot;author&quot; : &quot;dave&quot;, &quot;score&quot; : 85, &quot;views&quot; : 521 &#125;, &#123; &quot;_id&quot; : ObjectId(&quot;55f5a192d4bede9ac365b257&quot;), &quot;author&quot; : &quot;ahn&quot;, &quot;score&quot; : 60, &quot;views&quot; : 1000 &#125;, &#123; &quot;_id&quot; : ObjectId(&quot;55f5a192d4bede9ac365b258&quot;), &quot;author&quot; : &quot;li&quot;, &quot;score&quot; : 55, &quot;views&quot; : 5000 &#125;, &#123; &quot;_id&quot; : ObjectId(&quot;55f5a1d3d4bede9ac365b259&quot;), &quot;author&quot; : &quot;annT&quot;, &quot;score&quot; : 60, &quot;views&quot; : 50 &#125;, &#123; &quot;_id&quot; : ObjectId(&quot;55f5a1d3d4bede9ac365b25a&quot;), &quot;author&quot; : &quot;li&quot;, &quot;score&quot; : 94, &quot;views&quot; : 999 &#125;, &#123; &quot;_id&quot; : ObjectId(&quot;55f5a1d3d4bede9ac365b25b&quot;), &quot;author&quot; : &quot;ty&quot;, &quot;score&quot; : 95, &quot;views&quot; : 1000 &#125; ]) 简单数据匹配 12345678910111213141516171819202122db.articles.aggregate( [ &#123;$match: &#123;author: &quot;dave&quot;&#125;&#125; ])/* 1 */&#123; &quot;_id&quot; : ObjectId(&quot;512bc95fe835e68f199c8686&quot;), &quot;author&quot; : &quot;dave&quot;, &quot;score&quot; : 80.0, &quot;views&quot; : 100.0&#125;/* 2 */&#123; &quot;_id&quot; : ObjectId(&quot;512bc962e835e68f199c8687&quot;), &quot;author&quot; : &quot;dave&quot;, &quot;score&quot; : 85.0, &quot;views&quot; : 521.0&#125; $project 将具有请求字段的文档传递到管道中的下一阶段。指定的字段可以是输入文档中的现有字段或新计算的字段 1&#123; $project: &#123; &lt;specification(s)&gt; &#125; &#125; 测试数据 1234567891011db.books.insertMany( [ &#123; &quot;_id&quot; : 1, title: &quot;abc123&quot;, isbn: &quot;0001122223334&quot;, author: &#123; last: &quot;zzz&quot;, first: &quot;aaa&quot; &#125;, copies: 5 &#125; ]) 下面的聚合仅展示 title 和 author 字段（_id 是默认展示的） 123456789101112131415db.books.aggregate( [ &#123;$project: &#123;title:1, author:1&#125;&#125; ])&#123; &quot;_id&quot; : 1.0, &quot;title&quot; : &quot;abc123&quot;, &quot;author&quot; : &#123; &quot;last&quot; : &quot;zzz&quot;, &quot;first&quot; : &quot;aaa&quot; &#125;&#125; $count 返回聚合管道此阶段的文档数量计数 1&#123; $count: &lt;string&gt; &#125; 测试数据 12345678910db.scores.insertMany( [ &#123; &quot;_id&quot; : 1, &quot;subject&quot; : &quot;History&quot;, &quot;score&quot; : 88 &#125;, &#123; &quot;_id&quot; : 2, &quot;subject&quot; : &quot;History&quot;, &quot;score&quot; : 92 &#125;, &#123; &quot;_id&quot; : 3, &quot;subject&quot; : &quot;History&quot;, &quot;score&quot; : 97 &#125;, &#123; &quot;_id&quot; : 4, &quot;subject&quot; : &quot;History&quot;, &quot;score&quot; : 71 &#125;, &#123; &quot;_id&quot; : 5, &quot;subject&quot; : &quot;History&quot;, &quot;score&quot; : 79 &#125;, &#123; &quot;_id&quot; : 6, &quot;subject&quot; : &quot;History&quot;, &quot;score&quot; : 83 &#125; ]) 统计成绩高于80分的人数 1234567891011db.scores.aggregate( [ &#123;$match: &#123;score: &#123;$gt: 80&#125;&#125;&#125;, &#123;$count: &quot;countNum&quot;&#125; ])&#123; &quot;countNum&quot; : 4&#125; $group 按指定的标识符表达式对文档进行分组，并将累加器表达式(如果指定)应用于每个 group 支持的累加运算符 12345678&#123; $group: &#123; _id: &lt;expression&gt;, // 分组字段 &lt;field1&gt;: &#123; &lt;accumulator1&gt; : &lt;expression1&gt; &#125;, // 定义累加字段名，累加运算符，累加表达式 ... &#125; &#125; 测试数据 12345678910db.sales.insertMany([ &#123; &quot;_id&quot; : 1, &quot;item&quot; : &quot;abc&quot;, &quot;price&quot; : NumberDecimal(&quot;10&quot;), &quot;quantity&quot; : NumberInt(&quot;2&quot;), &quot;date&quot; : ISODate(&quot;2014-03-01T08:00:00Z&quot;) &#125;, &#123; &quot;_id&quot; : 2, &quot;item&quot; : &quot;jkl&quot;, &quot;price&quot; : NumberDecimal(&quot;20&quot;), &quot;quantity&quot; : NumberInt(&quot;1&quot;), &quot;date&quot; : ISODate(&quot;2014-03-01T09:00:00Z&quot;) &#125;, &#123; &quot;_id&quot; : 3, &quot;item&quot; : &quot;xyz&quot;, &quot;price&quot; : NumberDecimal(&quot;5&quot;), &quot;quantity&quot; : NumberInt( &quot;10&quot;), &quot;date&quot; : ISODate(&quot;2014-03-15T09:00:00Z&quot;) &#125;, &#123; &quot;_id&quot; : 4, &quot;item&quot; : &quot;xyz&quot;, &quot;price&quot; : NumberDecimal(&quot;5&quot;), &quot;quantity&quot; : NumberInt(&quot;20&quot;) , &quot;date&quot; : ISODate(&quot;2014-04-04T11:21:39.736Z&quot;) &#125;, &#123; &quot;_id&quot; : 5, &quot;item&quot; : &quot;abc&quot;, &quot;price&quot; : NumberDecimal(&quot;10&quot;), &quot;quantity&quot; : NumberInt(&quot;10&quot;) , &quot;date&quot; : ISODate(&quot;2014-04-04T21:23:13.331Z&quot;) &#125;, &#123; &quot;_id&quot; : 6, &quot;item&quot; : &quot;def&quot;, &quot;price&quot; : NumberDecimal(&quot;7.5&quot;), &quot;quantity&quot;: NumberInt(&quot;5&quot; ) , &quot;date&quot; : ISODate(&quot;2015-06-04T05:08:13Z&quot;) &#125;, &#123; &quot;_id&quot; : 7, &quot;item&quot; : &quot;def&quot;, &quot;price&quot; : NumberDecimal(&quot;7.5&quot;), &quot;quantity&quot;: NumberInt(&quot;10&quot;) , &quot;date&quot; : ISODate(&quot;2015-09-10T08:43:00Z&quot;) &#125;, &#123; &quot;_id&quot; : 8, &quot;item&quot; : &quot;abc&quot;, &quot;price&quot; : NumberDecimal(&quot;10&quot;), &quot;quantity&quot; : NumberInt(&quot;5&quot; ) , &quot;date&quot; : ISODate(&quot;2016-02-06T20:20:13Z&quot;) &#125;,]) 根据 item 分组，统计每组 item 的文档数量 12345678910111213141516171819202122232425262728293031323334353637db.sales.aggregate( [ &#123; $group: &#123; _id: &quot;$item&quot;, count: &#123; $count: &#123;&#125; &#125; &#125; &#125; ])/* 1 */&#123; &quot;_id&quot; : &quot;jkl&quot;, &quot;count&quot; : 1&#125;/* 2 */&#123; &quot;_id&quot; : &quot;def&quot;, &quot;count&quot; : 2&#125;/* 3 */&#123; &quot;_id&quot; : &quot;abc&quot;, &quot;count&quot; : 3&#125;/* 4 */&#123; &quot;_id&quot; : &quot;xyz&quot;, &quot;count&quot; : 2&#125; 按照 item 分组，求每组最大的 quantity 12345678910111213141516171819202122232425262728293031323334353637db.sales.aggregate( [ &#123; $group: &#123; _id: &quot;$item&quot;, maxQuantity: &#123; $max: &quot;$quantity&quot; &#125; &#125; &#125; ])/* 1 */&#123; &quot;_id&quot; : &quot;xyz&quot;, &quot;maxQuantity&quot; : 20&#125;/* 2 */&#123; &quot;_id&quot; : &quot;jkl&quot;, &quot;maxQuantity&quot; : 1&#125;/* 3 */&#123; &quot;_id&quot; : &quot;abc&quot;, &quot;maxQuantity&quot; : 10&#125;/* 4 */&#123; &quot;_id&quot; : &quot;def&quot;, &quot;maxQuantity&quot; : 10&#125; 实现having 先按照 item 分组计算销售总价，然后再过滤销售总价少于100的数据 12345678910111213141516171819202122232425262728293031323334353637383940db.sales.aggregate( [ &#123; $group: &#123; _id: &quot;$item&quot;, total: &#123; $sum: &#123; $multiply: [&quot;$price&quot;, &quot;$quantity&quot;] &#125; &#125; &#125; &#125;, &#123; $match: &#123; total: &#123; $gt: 100 &#125; &#125; &#125; ])/* 1 */&#123; &quot;_id&quot; : &quot;xyz&quot;, &quot;total&quot; : NumberDecimal(&quot;150&quot;)&#125;/* 2 */&#123; &quot;_id&quot; : &quot;abc&quot;, &quot;total&quot; : NumberDecimal(&quot;170&quot;)&#125;/* 3 */&#123; &quot;_id&quot; : &quot;def&quot;, &quot;total&quot; : NumberDecimal(&quot;112.5&quot;)&#125; $unwind 将数组字段进行展开，然后每个数组元素重新生成一份新的文档 1&#123; $unwind: &lt;field path&gt; &#125; 测试数据 1db.inventory.insertOne(&#123; &quot;_id&quot; : 1, &quot;item&quot; : &quot;ABC1&quot;, sizes: [ &quot;S&quot;, &quot;M&quot;, &quot;L&quot;] &#125;) 将数组 $sizes 进行展开 123456789101112131415161718192021222324252627db.inventory.aggregate( [ &#123;$unwind: &quot;$sizes&quot;&#125; ])/* 1 */&#123; &quot;_id&quot; : 1.0, &quot;item&quot; : &quot;ABC1&quot;, &quot;sizes&quot; : &quot;S&quot;&#125;/* 2 */&#123; &quot;_id&quot; : 1.0, &quot;item&quot; : &quot;ABC1&quot;, &quot;sizes&quot; : &quot;M&quot;&#125;/* 3 */&#123; &quot;_id&quot; : 1.0, &quot;item&quot; : &quot;ABC1&quot;, &quot;sizes&quot; : &quot;L&quot;&#125; $limit 限制传递到管道中下一阶段的文档数量 1&#123; $limit: &lt;positive 64-bit integer&gt; &#125; 测试数据 12345678910db.sales.insertMany([ &#123; &quot;_id&quot; : 1, &quot;item&quot; : &quot;abc&quot;, &quot;price&quot; : NumberDecimal(&quot;10&quot;), &quot;quantity&quot; : NumberInt(&quot;2&quot;), &quot;date&quot; : ISODate(&quot;2014-03-01T08:00:00Z&quot;) &#125;, &#123; &quot;_id&quot; : 2, &quot;item&quot; : &quot;jkl&quot;, &quot;price&quot; : NumberDecimal(&quot;20&quot;), &quot;quantity&quot; : NumberInt(&quot;1&quot;), &quot;date&quot; : ISODate(&quot;2014-03-01T09:00:00Z&quot;) &#125;, &#123; &quot;_id&quot; : 3, &quot;item&quot; : &quot;xyz&quot;, &quot;price&quot; : NumberDecimal(&quot;5&quot;), &quot;quantity&quot; : NumberInt( &quot;10&quot;), &quot;date&quot; : ISODate(&quot;2014-03-15T09:00:00Z&quot;) &#125;, &#123; &quot;_id&quot; : 4, &quot;item&quot; : &quot;xyz&quot;, &quot;price&quot; : NumberDecimal(&quot;5&quot;), &quot;quantity&quot; : NumberInt(&quot;20&quot;) , &quot;date&quot; : ISODate(&quot;2014-04-04T11:21:39.736Z&quot;) &#125;, &#123; &quot;_id&quot; : 5, &quot;item&quot; : &quot;abc&quot;, &quot;price&quot; : NumberDecimal(&quot;10&quot;), &quot;quantity&quot; : NumberInt(&quot;10&quot;) , &quot;date&quot; : ISODate(&quot;2014-04-04T21:23:13.331Z&quot;) &#125;, &#123; &quot;_id&quot; : 6, &quot;item&quot; : &quot;def&quot;, &quot;price&quot; : NumberDecimal(&quot;7.5&quot;), &quot;quantity&quot;: NumberInt(&quot;5&quot; ) , &quot;date&quot; : ISODate(&quot;2015-06-04T05:08:13Z&quot;) &#125;, &#123; &quot;_id&quot; : 7, &quot;item&quot; : &quot;def&quot;, &quot;price&quot; : NumberDecimal(&quot;7.5&quot;), &quot;quantity&quot;: NumberInt(&quot;10&quot;) , &quot;date&quot; : ISODate(&quot;2015-09-10T08:43:00Z&quot;) &#125;, &#123; &quot;_id&quot; : 8, &quot;item&quot; : &quot;abc&quot;, &quot;price&quot; : NumberDecimal(&quot;10&quot;), &quot;quantity&quot; : NumberInt(&quot;5&quot; ) , &quot;date&quot; : ISODate(&quot;2016-02-06T20:20:13Z&quot;) &#125;,]) 传递2条数据到聚合下个阶段 123456789101112131415161718192021222324db.sales.aggregate( [ &#123;$limit: 2&#125; ])/* 1 */&#123; &quot;_id&quot; : 1.0, &quot;item&quot; : &quot;abc&quot;, &quot;price&quot; : NumberDecimal(&quot;10&quot;), &quot;quantity&quot; : 2, &quot;date&quot; : ISODate(&quot;2014-03-01T08:00:00.000Z&quot;)&#125;/* 2 */&#123; &quot;_id&quot; : 2.0, &quot;item&quot; : &quot;jkl&quot;, &quot;price&quot; : NumberDecimal(&quot;20&quot;), &quot;quantity&quot; : 1, &quot;date&quot; : ISODate(&quot;2014-03-01T09:00:00.000Z&quot;)&#125; $skip 取一个正整数，指定要跳过的最大文档数，并将剩余的文档传递到管道中的下一阶段 1&#123; $skip: &lt;positive 64-bit integer&gt; &#125; 测试数据 12345678910db.sales.insertMany([ &#123; &quot;_id&quot; : 1, &quot;item&quot; : &quot;abc&quot;, &quot;price&quot; : NumberDecimal(&quot;10&quot;), &quot;quantity&quot; : NumberInt(&quot;2&quot;), &quot;date&quot; : ISODate(&quot;2014-03-01T08:00:00Z&quot;) &#125;, &#123; &quot;_id&quot; : 2, &quot;item&quot; : &quot;jkl&quot;, &quot;price&quot; : NumberDecimal(&quot;20&quot;), &quot;quantity&quot; : NumberInt(&quot;1&quot;), &quot;date&quot; : ISODate(&quot;2014-03-01T09:00:00Z&quot;) &#125;, &#123; &quot;_id&quot; : 3, &quot;item&quot; : &quot;xyz&quot;, &quot;price&quot; : NumberDecimal(&quot;5&quot;), &quot;quantity&quot; : NumberInt( &quot;10&quot;), &quot;date&quot; : ISODate(&quot;2014-03-15T09:00:00Z&quot;) &#125;, &#123; &quot;_id&quot; : 4, &quot;item&quot; : &quot;xyz&quot;, &quot;price&quot; : NumberDecimal(&quot;5&quot;), &quot;quantity&quot; : NumberInt(&quot;20&quot;) , &quot;date&quot; : ISODate(&quot;2014-04-04T11:21:39.736Z&quot;) &#125;, &#123; &quot;_id&quot; : 5, &quot;item&quot; : &quot;abc&quot;, &quot;price&quot; : NumberDecimal(&quot;10&quot;), &quot;quantity&quot; : NumberInt(&quot;10&quot;) , &quot;date&quot; : ISODate(&quot;2014-04-04T21:23:13.331Z&quot;) &#125;, &#123; &quot;_id&quot; : 6, &quot;item&quot; : &quot;def&quot;, &quot;price&quot; : NumberDecimal(&quot;7.5&quot;), &quot;quantity&quot;: NumberInt(&quot;5&quot; ) , &quot;date&quot; : ISODate(&quot;2015-06-04T05:08:13Z&quot;) &#125;, &#123; &quot;_id&quot; : 7, &quot;item&quot; : &quot;def&quot;, &quot;price&quot; : NumberDecimal(&quot;7.5&quot;), &quot;quantity&quot;: NumberInt(&quot;10&quot;) , &quot;date&quot; : ISODate(&quot;2015-09-10T08:43:00Z&quot;) &#125;, &#123; &quot;_id&quot; : 8, &quot;item&quot; : &quot;abc&quot;, &quot;price&quot; : NumberDecimal(&quot;10&quot;), &quot;quantity&quot; : NumberInt(&quot;5&quot; ) , &quot;date&quot; : ISODate(&quot;2016-02-06T20:20:13Z&quot;) &#125;,]) 跳过前两条数据 12345678910111213141516171819202122232425262728293031323334353637db.sales.aggregate( [ &#123;$skip: 2&#125;, &#123;$project: &#123;_id: 1&#125;&#125; ])/* 1 */&#123; &quot;_id&quot; : 3.0&#125;/* 2 */&#123; &quot;_id&quot; : 4.0&#125;/* 3 */&#123; &quot;_id&quot; : 5.0&#125;/* 4 */&#123; &quot;_id&quot; : 6.0&#125;/* 5 */&#123; &quot;_id&quot; : 7.0&#125;/* 6 */&#123; &quot;_id&quot; : 8.0&#125; $sort 对所有输入文档进行排序并按排序顺序将它们返回到管道 1&#123; $sort: &#123; &lt;field1&gt;: &lt;sort order&gt;, &lt;field2&gt;: &lt;sort order&gt; ... &#125; &#125; 1: ASC -1: DESC 测试数据 12345678910db.sales.insertMany([ &#123; &quot;_id&quot; : 1, &quot;item&quot; : &quot;abc&quot;, &quot;price&quot; : NumberDecimal(&quot;10&quot;), &quot;quantity&quot; : NumberInt(&quot;2&quot;), &quot;date&quot; : ISODate(&quot;2014-03-01T08:00:00Z&quot;) &#125;, &#123; &quot;_id&quot; : 2, &quot;item&quot; : &quot;jkl&quot;, &quot;price&quot; : NumberDecimal(&quot;20&quot;), &quot;quantity&quot; : NumberInt(&quot;1&quot;), &quot;date&quot; : ISODate(&quot;2014-03-01T09:00:00Z&quot;) &#125;, &#123; &quot;_id&quot; : 3, &quot;item&quot; : &quot;xyz&quot;, &quot;price&quot; : NumberDecimal(&quot;5&quot;), &quot;quantity&quot; : NumberInt( &quot;10&quot;), &quot;date&quot; : ISODate(&quot;2014-03-15T09:00:00Z&quot;) &#125;, &#123; &quot;_id&quot; : 4, &quot;item&quot; : &quot;xyz&quot;, &quot;price&quot; : NumberDecimal(&quot;5&quot;), &quot;quantity&quot; : NumberInt(&quot;20&quot;) , &quot;date&quot; : ISODate(&quot;2014-04-04T11:21:39.736Z&quot;) &#125;, &#123; &quot;_id&quot; : 5, &quot;item&quot; : &quot;abc&quot;, &quot;price&quot; : NumberDecimal(&quot;10&quot;), &quot;quantity&quot; : NumberInt(&quot;10&quot;) , &quot;date&quot; : ISODate(&quot;2014-04-04T21:23:13.331Z&quot;) &#125;, &#123; &quot;_id&quot; : 6, &quot;item&quot; : &quot;def&quot;, &quot;price&quot; : NumberDecimal(&quot;7.5&quot;), &quot;quantity&quot;: NumberInt(&quot;5&quot; ) , &quot;date&quot; : ISODate(&quot;2015-06-04T05:08:13Z&quot;) &#125;, &#123; &quot;_id&quot; : 7, &quot;item&quot; : &quot;def&quot;, &quot;price&quot; : NumberDecimal(&quot;7.5&quot;), &quot;quantity&quot;: NumberInt(&quot;10&quot;) , &quot;date&quot; : ISODate(&quot;2015-09-10T08:43:00Z&quot;) &#125;, &#123; &quot;_id&quot; : 8, &quot;item&quot; : &quot;abc&quot;, &quot;price&quot; : NumberDecimal(&quot;10&quot;), &quot;quantity&quot; : NumberInt(&quot;5&quot; ) , &quot;date&quot; : ISODate(&quot;2016-02-06T20:20:13Z&quot;) &#125;,]) 按照 price 升序 quantity 降序 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273db.sales.aggregate( [ &#123; $sort: &#123; price: 1, quantity: -1 &#125; &#125;, &#123; $project: &#123; price: 1, quantity: 1 &#125; &#125; ])/* 1 */&#123; &quot;_id&quot; : 4.0, &quot;price&quot; : NumberDecimal(&quot;5&quot;), &quot;quantity&quot; : 20&#125;/* 2 */&#123; &quot;_id&quot; : 3.0, &quot;price&quot; : NumberDecimal(&quot;5&quot;), &quot;quantity&quot; : 10&#125;/* 3 */&#123; &quot;_id&quot; : 7.0, &quot;price&quot; : NumberDecimal(&quot;7.5&quot;), &quot;quantity&quot; : 10&#125;/* 4 */&#123; &quot;_id&quot; : 6.0, &quot;price&quot; : NumberDecimal(&quot;7.5&quot;), &quot;quantity&quot; : 5&#125;/* 5 */&#123; &quot;_id&quot; : 5.0, &quot;price&quot; : NumberDecimal(&quot;10&quot;), &quot;quantity&quot; : 10&#125;/* 6 */&#123; &quot;_id&quot; : 8.0, &quot;price&quot; : NumberDecimal(&quot;10&quot;), &quot;quantity&quot; : 5&#125;/* 7 */&#123; &quot;_id&quot; : 1.0, &quot;price&quot; : NumberDecimal(&quot;10&quot;), &quot;quantity&quot; : 2&#125;/* 8 */&#123; &quot;_id&quot; : 2.0, &quot;price&quot; : NumberDecimal(&quot;20&quot;), &quot;quantity&quot; : 1&#125; $lookup 主要用来实现多表关联查询， 相当关系型数据库中多表左外连接。每个输入待处理的文档，经过$lookup 阶段的处理，输出的新文档中会包含一个新生成的数（可根据需要命名新key ）。数组列存放的数据是来自被Join集合的适配文档，如果没有，集合为空 123456789&#123; $lookup: &#123; from: &lt;collection to join&gt;, localField: &lt;field from the input documents&gt;, foreignField: &lt;field from the documents of the &quot;from&quot; collection&gt;, as: &lt;output array field&gt; &#125;&#125; 测试数据 12345db.orders.insertMany( [ &#123; &quot;_id&quot; : 1, &quot;item&quot; : &quot;almonds&quot;, &quot;price&quot; : 12, &quot;quantity&quot; : 2 &#125;, &#123; &quot;_id&quot; : 2, &quot;item&quot; : &quot;pecans&quot;, &quot;price&quot; : 20, &quot;quantity&quot; : 1 &#125;, &#123; &quot;_id&quot; : 3 &#125;] ) 12345678db.inventory.insertMany( [ &#123; &quot;_id&quot; : 1, &quot;sku&quot; : &quot;almonds&quot;, &quot;description&quot;: &quot;product 1&quot;, &quot;instock&quot; : 120 &#125;, &#123; &quot;_id&quot; : 2, &quot;sku&quot; : &quot;bread&quot;, &quot;description&quot;: &quot;product 2&quot;, &quot;instock&quot; : 80 &#125;, &#123; &quot;_id&quot; : 3, &quot;sku&quot; : &quot;cashews&quot;, &quot;description&quot;: &quot;product 3&quot;, &quot;instock&quot; : 60 &#125;, &#123; &quot;_id&quot; : 4, &quot;sku&quot; : &quot;pecans&quot;, &quot;description&quot;: &quot;product 4&quot;, &quot;instock&quot; : 70 &#125;, &#123; &quot;_id&quot; : 5, &quot;sku&quot;: null, &quot;description&quot;: &quot;Incomplete&quot; &#125;, &#123; &quot;_id&quot; : 6 &#125;] ) 以下对 orders 集合的聚合操作使用来自 orders 集合的字段 item 和来自 inventory 集合的 sku 字段将来自 orders 的文档与来自 inventory 集合的文档连接起来 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960db.orders.aggregate( [ &#123; $lookup: &#123; from: &quot;inventory&quot;, localField: &quot;item&quot;, foreignField: &quot;sku&quot;, as: &quot;inventory_docs&quot; &#125; &#125; ])/* 1 */&#123; &quot;_id&quot; : 1.0, &quot;item&quot; : &quot;almonds&quot;, &quot;price&quot; : 12.0, &quot;quantity&quot; : 2.0, &quot;inventory_docs&quot; : [ &#123; &quot;_id&quot; : 1.0, &quot;sku&quot; : &quot;almonds&quot;, &quot;description&quot; : &quot;product 1&quot;, &quot;instock&quot; : 120.0 &#125; ]&#125;/* 2 */&#123; &quot;_id&quot; : 2.0, &quot;item&quot; : &quot;pecans&quot;, &quot;price&quot; : 20.0, &quot;quantity&quot; : 1.0, &quot;inventory_docs&quot; : [ &#123; &quot;_id&quot; : 4.0, &quot;sku&quot; : &quot;pecans&quot;, &quot;description&quot; : &quot;product 4&quot;, &quot;instock&quot; : 70.0 &#125; ]&#125;/* 3 */&#123; &quot;_id&quot; : 3.0, &quot;inventory_docs&quot; : [ &#123; &quot;_id&quot; : 5.0, &quot;sku&quot; : null, &quot;description&quot; : &quot;Incomplete&quot; &#125;, &#123; &quot;_id&quot; : 6.0 &#125; ]&#125; Map-Reduce 从MongoDB 5.0开始，map-reduce被弃用，推荐使用管道聚合 Map-reduce是一种数据处理范例，用于将大量数据压缩为有用的聚合结果。MapReduce操作将大量的数据处理工作拆分成多个线程并行处理，然后将结果合并在一起。MongoDB提供的Map-Reduce非常灵活，对于大规模数据分析也相当实用 MapReduce具有两个阶段： 将具有相同Key的文档数据整合在一起的map阶段 组合map操作的结果进行统计输出的reduce阶段 123456789101112131415db.collection.mapReduce( &lt;map&gt;, &lt;reduce&gt;, &#123; out: &lt;collection&gt;, query: &lt;document&gt;, sort: &lt;document&gt;, limit: &lt;number&gt;, finalize: &lt;function&gt;, scope: &lt;document&gt;, jsMode: &lt;boolean&gt;, verbose: &lt;boolean&gt;, bypassDocumentValidation: &lt;boolean&gt; &#125; ) map，将数据拆分成键值对，交给reduce函数 reduce，根据键将值做统计运算 out，可选，将结果汇入指定表 quey，可选筛选数据的条件，筛选的数据送入map sort，排序完后，送入map视图 limit，限制送入map的文档数 finalize，可选，修改reduce的结果后进行输出 scope，可选，指定map、reduce、finalize的全局变量 jsMode，可选，默认false。在mapreduce过程中是否将数据转换成bson格式。 verbose，可选，是否在结果中显示时间，默认false bypassDocmentValidation，可选，是否略过数据校验 用法 测试数据 123456789101112db.orders.insertMany([ &#123; _id: 1, cust_id: &quot;Ant O. Knee&quot;, ord_date: new Date(&quot;2020-03-01&quot;), price: 25, items: [ &#123; sku: &quot;oranges&quot;, qty: 5, price: 2.5 &#125;, &#123; sku: &quot;apples&quot;, qty: 5, price: 2.5 &#125; ], status: &quot;A&quot; &#125;, &#123; _id: 2, cust_id: &quot;Ant O. Knee&quot;, ord_date: new Date(&quot;2020-03-08&quot;), price: 70, items: [ &#123; sku: &quot;oranges&quot;, qty: 8, price: 2.5 &#125;, &#123; sku: &quot;chocolates&quot;, qty: 5, price: 10 &#125; ], status: &quot;A&quot; &#125;, &#123; _id: 3, cust_id: &quot;Busby Bee&quot;, ord_date: new Date(&quot;2020-03-08&quot;), price: 50, items: [ &#123; sku: &quot;oranges&quot;, qty: 10, price: 2.5 &#125;, &#123; sku: &quot;pears&quot;, qty: 10, price: 2.5 &#125; ], status: &quot;A&quot; &#125;, &#123; _id: 4, cust_id: &quot;Busby Bee&quot;, ord_date: new Date(&quot;2020-03-18&quot;), price: 25, items: [ &#123; sku: &quot;oranges&quot;, qty: 10, price: 2.5 &#125; ], status: &quot;A&quot; &#125;, &#123; _id: 5, cust_id: &quot;Busby Bee&quot;, ord_date: new Date(&quot;2020-03-19&quot;), price: 50, items: [ &#123; sku: &quot;chocolates&quot;, qty: 5, price: 10 &#125; ], status: &quot;A&quot;&#125;, &#123; _id: 6, cust_id: &quot;Cam Elot&quot;, ord_date: new Date(&quot;2020-03-19&quot;), price: 35, items: [ &#123; sku: &quot;carrots&quot;, qty: 10, price: 1.0 &#125;, &#123; sku: &quot;apples&quot;, qty: 10, price: 2.5 &#125; ], status: &quot;A&quot; &#125;, &#123; _id: 7, cust_id: &quot;Cam Elot&quot;, ord_date: new Date(&quot;2020-03-20&quot;), price: 25, items: [ &#123; sku: &quot;oranges&quot;, qty: 10, price: 2.5 &#125; ], status: &quot;A&quot; &#125;, &#123; _id: 8, cust_id: &quot;Don Quis&quot;, ord_date: new Date(&quot;2020-03-20&quot;), price: 75, items: [ &#123; sku: &quot;chocolates&quot;, qty: 5, price: 10 &#125;, &#123; sku: &quot;apples&quot;, qty: 10, price: 2.5 &#125; ], status: &quot;A&quot; &#125;, &#123; _id: 9, cust_id: &quot;Don Quis&quot;, ord_date: new Date(&quot;2020-03-20&quot;), price: 55, items: [ &#123; sku: &quot;carrots&quot;, qty: 5, price: 1.0 &#125;, &#123; sku: &quot;apples&quot;, qty: 10, price: 2.5 &#125;, &#123; sku: &quot;oranges&quot;, qty: 10, price: 2.5 &#125; ], status: &quot;A&quot; &#125;, &#123; _id: 10, cust_id: &quot;Don Quis&quot;, ord_date: new Date(&quot;2020-03-23&quot;), price: 25, items: [ &#123; sku: &quot;oranges&quot;, qty: 10, price: 2.5 &#125; ], status: &quot;A&quot; &#125;]) 求每位客户订单的总额 123456789101112131415// 定义map函数，需要返回一个键值对var mapFun = function() &#123; emit(this.cust_id, this.price);&#125;// 定义reduce函数，实现价格统计var reduceFun = function(cust_id, price) &#123; return Array.sum(price);&#125;db.orders.mapReduce( mapFun, reduceFun, &#123; out: &quot;map_reduce_example&quot; &#125; // 将结果输出到一个集合) 1234567891011121314151617181920212223242526db.getCollection(&#x27;map_reduce_example&#x27;).find(&#123;&#125;)/* 1 */&#123; &quot;_id&quot; : &quot;Don Quis&quot;, &quot;value&quot; : 155.0&#125;/* 2 */&#123; &quot;_id&quot; : &quot;Cam Elot&quot;, &quot;value&quot; : 60.0&#125;/* 3 */&#123; &quot;_id&quot; : &quot;Busby Bee&quot;, &quot;value&quot; : 125.0&#125;/* 4 */&#123; &quot;_id&quot; : &quot;Ant O. Knee&quot;, &quot;value&quot; : 95.0&#125; 视图 只读 MongoDB 视图是一个只读的可查询对象，其内容由其他集合或视图上的聚合管道定义，通过视图进行写操作会报错 MongoDB不会将视图内容保存到磁盘。当客户端查询视图时，将按需计算视图的内容 视图种类 MongoDB提供两种不同的视图类型：标准视图和物化视图。这两种视图类型都从聚合管道返回结果 标准视图是在读取视图时计算的，不会存储到磁盘(和mysql一样，对查询进行封装) 视图存储在磁盘上并从磁盘读取。他们使用 $merge 或 $out 阶段来更新保存的数据 物化视图 物化视图（Materialized Views）是一种预计算和存储查询结果的数据结构。它们类似于传统数据库中的视图，但与视图不同，物化视图会将查询结果实际存储在磁盘上，而不是每次查询时动态计算。 MongoDB 4.2 版本引入了物化视图的概念。使用物化视图，您可以定义一个查询，并将其结果存储在集合中。这样，当数据发生变化时，您可以通过刷新物化视图来更新存储的查询结果，而不需要重新执行整个查询。 123456789101112131415161718// 创建源集合db.sourceCollection.insertMany([ &#123; _id: 1,: &quot;John&quot;, age: 25 &#125;, &#123; _id: 2,: &quot;Jane&quot;, age: 30 &#125;, &#123; _id: 3, name: &quot;Bob&quot;, age: 35 &#125;]);// 定义聚合管道var pipeline = [ &#123; $match: &#123; age: &#123; $gte: 30 &#125; &#125; &#125;, &#123; $project: &#123; _id: 0,: 1 &#125; &#125;];// 创建物化视图db.createView(&quot;myMaterializedView&quot;, &quot;sourceCollection&quot;, pipeline);// 刷新物化视图db.myMaterializedView.refresh(); 索引 标准视图使用基础集合的索引。因此，您无法直接在标准视图上创建、删除或重新生成索引，也无法在视图上获取索引列表 可以直接在物化视图上创建索引，因为它们存储在磁盘上 创建视图 语法 12345678db.createView( &quot;&lt;viewName&gt;&quot;, // 视图名称 &quot;&lt;source&gt;&quot;, // 数据源 [&lt;pipeline&gt;], // 管道 &#123; &quot;collation&quot; : &#123; &lt;collation&gt; &#125; &#125;) 限制 不支持 db.collection.mapReduce() $text 运算符，因为聚合中的 $text 操作仅对第一阶段有效 $geoNear 重命名视图 创建单集合视图 测试数据 12345678910db.students.insertMany( [ &#123; sID: 22001, name: &quot;Alex&quot;, year: 1, score: 4.0 &#125;, &#123; sID: 21001, name: &quot;bernie&quot;, year: 2, score: 3.7 &#125;, &#123; sID: 20010, name: &quot;Chris&quot;, year: 3, score: 2.5 &#125;, &#123; sID: 22021, name: &quot;Drew&quot;, year: 1, score: 3.2 &#125;, &#123; sID: 17301, name: &quot;harley&quot;, year: 6, score: 3.1 &#125;, &#123; sID: 21022, name: &quot;Farmer&quot;, year: 1, score: 2.2 &#125;, &#123; sID: 20020, name: &quot;george&quot;, year: 3, score: 2.8 &#125;, &#123; sID: 18020, name: &quot;Harley&quot;, year: 5, score: 2.8 &#125;,] ) 创建一个只有一年级学生的视图 12345678db.createView( &quot;firstYear&quot;, // 视图名称 &quot;students&quot;, // 集合名称 [ &#123; $match: &#123; year: 1 &#125; &#125;, // 数据过滤 &#123; $sort: &#123; score: 1 &#125; &#125; // 分数排序 ]) 1234567891011121314151617181920212223242526272829db.firstYear.find(&#123;&#125;)/* 1 */&#123; &quot;_id&quot; : ObjectId(&quot;64552b676378f2a57f74e957&quot;), &quot;sID&quot; : 21022.0, &quot;name&quot; : &quot;Farmer&quot;, &quot;year&quot; : 1.0, &quot;score&quot; : 2.2&#125;/* 2 */&#123; &quot;_id&quot; : ObjectId(&quot;64552b676378f2a57f74e955&quot;), &quot;sID&quot; : 22021.0, &quot;name&quot; : &quot;Drew&quot;, &quot;year&quot; : 1.0, &quot;score&quot; : 3.2&#125;/* 3 */&#123; &quot;_id&quot; : ObjectId(&quot;64552b676378f2a57f74e952&quot;), &quot;sID&quot; : 22001.0, &quot;name&quot; : &quot;Alex&quot;, &quot;year&quot; : 1.0, &quot;score&quot; : 4.0&#125; 创建多集合的视图 测试数据 12345db.orders.insertMany( [ &#123; &quot;_id&quot; : 1, &quot;item&quot; : &quot;almonds&quot;, &quot;price&quot; : 12, &quot;quantity&quot; : 2 &#125;, &#123; &quot;_id&quot; : 2, &quot;item&quot; : &quot;pecans&quot;, &quot;price&quot; : 20, &quot;quantity&quot; : 1 &#125;, &#123; &quot;_id&quot; : 3 &#125;] ) 12345678db.inventory.insertMany( [ &#123; &quot;_id&quot; : 1, &quot;sku&quot; : &quot;almonds&quot;, &quot;description&quot;: &quot;product 1&quot;, &quot;instock&quot; : 120 &#125;, &#123; &quot;_id&quot; : 2, &quot;sku&quot; : &quot;bread&quot;, &quot;description&quot;: &quot;product 2&quot;, &quot;instock&quot; : 80 &#125;, &#123; &quot;_id&quot; : 3, &quot;sku&quot; : &quot;cashews&quot;, &quot;description&quot;: &quot;product 3&quot;, &quot;instock&quot; : 60 &#125;, &#123; &quot;_id&quot; : 4, &quot;sku&quot; : &quot;pecans&quot;, &quot;description&quot;: &quot;product 4&quot;, &quot;instock&quot; : 70 &#125;, &#123; &quot;_id&quot; : 5, &quot;sku&quot;: null, &quot;description&quot;: &quot;Incomplete&quot; &#125;, &#123; &quot;_id&quot; : 6 &#125;] ) 创建一个视图，使用 orders 的 item 字段关联 inventory 集合的 sku ,并输出 item 和 sku 字段 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748db.createView( &quot;orderDetail&quot;, &quot;orders&quot;, [ &#123; $lookup: &#123; from: &quot;inventory&quot;, localField: &quot;item&quot;, foreignField:&quot;sku&quot;, as: &quot;product&quot;&#125; &#125;, &#123; $project: &#123; item:1, &quot;product.sku&quot;:1 &#125; &#125; ])db.getCollection(&#x27;orderDetail&#x27;).find(&#123;&#125;)/* 1 */&#123; &quot;_id&quot; : 1.0, &quot;item&quot; : &quot;almonds&quot;, &quot;product&quot; : [ &#123; &quot;sku&quot; : &quot;almonds&quot; &#125; ]&#125;/* 2 */&#123; &quot;_id&quot; : 2.0, &quot;item&quot; : &quot;pecans&quot;, &quot;product&quot; : [ &#123; &quot;sku&quot; : &quot;pecans&quot; &#125; ]&#125;/* 3 */&#123; &quot;_id&quot; : 3.0, &quot;product&quot; : [ &#123; &quot;sku&quot; : null &#125;, &#123;&#125; ]&#125; 修改视图 12345db.runCommand( &#123; collMod: &quot;lowStock&quot;, // 视图名 viewOn: &quot;products&quot;, // 集合 &quot;pipeline&quot;: [] // 要求修改的管道&#125; ) 测试数据 12345678910db.students.insertMany( [ &#123; sID: 22001, name: &quot;Alex&quot;, year: 1, score: 4.0 &#125;, &#123; sID: 21001, name: &quot;bernie&quot;, year: 2, score: 3.7 &#125;, &#123; sID: 20010, name: &quot;Chris&quot;, year: 3, score: 2.5 &#125;, &#123; sID: 22021, name: &quot;Drew&quot;, year: 1, score: 3.2 &#125;, &#123; sID: 17301, name: &quot;harley&quot;, year: 6, score: 3.1 &#125;, &#123; sID: 21022, name: &quot;Farmer&quot;, year: 1, score: 2.2 &#125;, &#123; sID: 20020, name: &quot;george&quot;, year: 3, score: 2.8 &#125;, &#123; sID: 18020, name: &quot;Harley&quot;, year: 5, score: 2.8 &#125;,] ) 创建一张只包含1年级学生的视图 12345db.createView( &quot;studentsView&quot;, &quot;students&quot;, [ &#123; $match: &#123; year: 1 &#125; &#125; ]) 视图修改，改为只包含2年级学生的视图 1234567db.runCommand( &#123; collMod: &quot;studentsView&quot;, viewOn: &quot;students&quot;, pipeline: [ &#123; $match: &#123; year: 2 &#125; &#125; ] &#125;) 删除视图 1db.view.drop() 1db.studentsView.drop() 索引 索引支持在MongoDB中高效执行查询。如果没有索引，MongoDB必须执行集合扫描，即扫描集合中的每个文档，以选择与查询语句匹配的文档。如果查询存在适当的索引，MongoDB可以使用该索引来限制它必须检查的文档数量 索引是特殊的数据结构（B+树，官网说的B树实际上是文化差异，老外认为B+树是B树的变种） ，它以易于遍历的形式存储集合数据集的一小部分。索引存储特定字段或一组字段的值，按字段值排序。索引条目的排序支持高效的相等匹配和基于范围的查询操作。此外，MongoDB可以使用索引中的排序返回排序结果 从根本上说，MongoDB中的索引类似于其他数据库系统中的索引。MongoDB在集合级别定义索引，并支持MongoDB集合中文档的任何字段或子字段上的索引 默认 _id 索引 MongoDB在创建集合期间在 _id 字段上创建一个 唯一索引。 _id 索引可防止客户端为 _id 字段插入两个具有相同值的文档。不能将 _id 字段的索引删除 在分片集群中，如果你不使用 _id 字段作为分片键，那么你的应用程序必须确保 _id 字段中值的唯一性以防止错误。这通常是通过使用标准的自动生成的 ObjectId 来完成的 创建索引 要在 Mongo Shell 中创建索引，请使用 db.collection.createIndex() 1db.collection.createIndex( &lt;key and index type specification&gt;, &lt;options&gt; ) 参考索引属性 1db.collection.createIndex( &#123; name: -1 &#125; ) 索引名称 索引的默认名称是索引键和索引中每个键排序（即 1 或 -1）的串联，使用下划线作为分隔符。例如，在 &#123; item : 1, quantity: -1 &#125; 上创建的索引的名称为 item_1_quantity_-1 可以自定义索引名称 1234db.products.createIndex( &#123; item: 1, quantity: -1 &#125; , &#123; name: &quot;query for inventory&quot; &#125;) 查看索引 12查看索引信息 db.books.getIndexes() 查看索引键 db.books.getIndexKeys() 查看索引占用空间 1db.collection.totalIndexSize([is_detail]) is_detail：可选参数，传入除0或false外的任意数据，都会显示该集合中每个索引的大小及总大小。如果传入0或false则只显示该集合中所有索引的总大小。默认值为false 删除索引 12删除集合指定索引 db.col.dropIndex(&quot;索引名称&quot;)删除集合所有索引 db.col.dropIndexes() 索引类型 MongoDB提供了许多不同的索引类型来支持特定类型的数据和查询 单字段索引 MongoDB为文档集合中任何字段的索引提供了完整的支持。默认情况下，所有集合在 _id 字段上都有一个索引，应用程序和用户可以添加其他索引来支持重要的查询和操作 创建一个单字段索引 1db.records.createIndex( &#123; score: 1 &#125; ) 1: 索引升序排序 -1: 索引降序排序 在嵌套字段建立索引 测试数据 1234567db.records.insertOne( &#123; &quot;_id&quot;: ObjectId(&quot;570c04a4ad233577f97dc459&quot;), &quot;score&quot;: 1034, &quot;location&quot;: &#123; state: &quot;NY&quot;, city: &quot;New York&quot; &#125; &#125;) 以下操作在 location.state 字段上创建索引： 1234567891011121314db.records.createIndex(&#123;&quot;location.state&quot;: 1&#125;)db.records.getIndexKeys()[ &#123; &quot;_id&quot; : 1 &#125;, &#123; &quot;location.state&quot; : 1.0 &#125;] 在嵌套文档上创建索引 支持对整个嵌套文档创建索引 location 字段是一个嵌套文档，包含嵌入字段 city 和 state 。以下命令在整个 location 字段上创建索引： 1db.records.createIndex( &#123; location: 1 &#125; ) 复合索引 复合索引是多个字段组合而成的索引，其性质和单字段索引类似。但不同的是，复合索引中字段的顺序、字段的升降序对查询性能有直接的影响，因此在设计复合索引时则需要考虑不同的查询场景 MongoDB对任何复合索引施加了32个字段的限制 创建索引 测试数据 12345678910db.products.insertOne( &#123; &quot;_id&quot;: 1, &quot;item&quot;: &quot;Banana&quot;, &quot;category&quot;: [&quot;food&quot;, &quot;produce&quot;, &quot;grocery&quot;], &quot;location&quot;: &quot;4th Street Store&quot;, &quot;stock&quot;: 4, &quot;type&quot;: &quot;cases&quot; &#125;) 以下操作在 item 和 stock 字段上分别创建升序和降序索引： 123456db.products.createIndex( &#123; item: 1, stock: -1 &#125;) 复合索引和SQL一样，在使用时需要遵循 最左匹配原则，并且复合索引的排序要根据业务查询的实际排序为准 多键索引 多键索引范围 在 数组 的属性上建立索引。MongoDB为数组中的每个元素创建一个索引键。这些多键索引支持对数组字段进行高效查询 索引基本数组 测试数据 123db.survey.insertOne( &#123; _id: 1, item: &quot;ABC&quot;, ratings: [ 2, 5, 9 ], range: [8, 10, 15] &#125;) 为数组字段 ratings 创建多键索引 12345db.survey.createIndex( &#123; ratings: 1 &#125;) 由于 ratings 字段包含一个数组，因此 ratings 上的索引是多键的。多键索引包含以下三个索引键，每个索引键指向同一文档： 2 5 9 注意对于一个复合索引而言，一个索引文档只能包含一个多键索引（一个基本数组类型的字段索引），否则创建索引会报错12345678910111213db.survey.createIndex( &#123; ratings: 1, range: 1 &#125;)&#123; &quot;ok&quot; : 0.0, &quot;errmsg&quot; : &quot;Index build failed: 9a1ff53c-5550-4315-a94f-74998bd81ab4: Collection test.survey ( 6d437930-01ee-4911-82e5-a3653088c692 ) :: caused by :: cannot index parallel arrays [range] [ratings]&quot;, &quot;code&quot; : 171, &quot;codeName&quot; : &quot;CannotIndexParallelArrays&quot;&#125; 索引嵌套文档数组 测试数据 1234567891011db.inventory.insertOne( &#123; _id: 1, item: &quot;abc&quot;, stock: [ &#123; size: &quot;S&quot;, color: &quot;red&quot;, quantity: 25 &#125;, &#123; size: &quot;S&quot;, color: &quot;blue&quot;, quantity: 10 &#125;, &#123; size: &quot;M&quot;, color: &quot;blue&quot;, quantity: 50 &#125; ] &#125;) 以下操作在 stock.size 和 stock.quantity 字段上创建多键索引： 123456db.inventory.createIndex( &#123; &quot;stock.size&quot;: 1, &quot;stock.quantity&quot;: 1 &#125;) 复合多键索引可以支持具有谓词的查询，这些谓词既包括索引字段，也包括仅包括索引前缀 stock.size 的谓词。如以下例子所示: 12db.inventory.find( &#123; &quot;stock.size&quot;: &quot;M&quot; &#125; )db.inventory.find( &#123; &quot;stock.size&quot;: &quot;S&quot;, &quot;stock.quantity&quot;: &#123; $gt: 20 &#125; &#125; ) 文本索引 一个集合只能有一个文本搜索索引，但该索引可以涵盖多个字段 MongoDB的文本索引功能存在诸多限制，而官方并未提供中文分词的功能，这使得该功能的应用场景十分受限 用法 1db.reviews.createIndex( &#123; comments: &quot;text&quot;, description: &quot;text&quot; &#125; ) 测试数据 12345678910db.inventory.insertMany( [ &#123; _id: 1, dept: &quot;tech&quot;, description: &quot;lime green computer&quot; &#125;, &#123; _id: 2, dept: &quot;tech&quot;, description: &quot;wireless red mouse&quot; &#125;, &#123; _id: 3, dept: &quot;kitchen&quot;, description: &quot;green placemat&quot; &#125;, &#123; _id: 4, dept: &quot;kitchen&quot;, description: &quot;red peeler&quot; &#125;, &#123; _id: 5, dept: &quot;food&quot;, description: &quot;green apple&quot; &#125;, &#123; _id: 6, dept: &quot;food&quot;, description: &quot;red potato&quot; &#125; ]) 为 description 字段创建文本索引 123db.inventory.createIndex( &#123; description: &quot;text&quot; &#125;) 使用文本索引搜索 123456789101112131415161718192021222324252627db.inventory.find( &#123; $text: &#123; $search: &quot;green&quot; &#125; &#125;)/* 1 */&#123; &quot;_id&quot; : 5.0, &quot;dept&quot; : &quot;food&quot;, &quot;description&quot; : &quot;green apple&quot;&#125;/* 2 */&#123; &quot;_id&quot; : 3.0, &quot;dept&quot; : &quot;kitchen&quot;, &quot;description&quot; : &quot;green placemat&quot;&#125;/* 3 */&#123; &quot;_id&quot; : 1.0, &quot;dept&quot; : &quot;tech&quot;, &quot;description&quot; : &quot;lime green computer&quot;&#125; $text 操作符可以在有text index的集合上执行文本检索。$text 将会使用 空格 和 标点符号 作为分隔符对检索字符串进行分词（查询条件和索引数据都会分词）， 并且对检索字符串中所有的分词结果进行一个逻辑上的 OR 操作 通配符索引 MongoDB支持在一个或一组字段上创建索引，以支持查询。由于MongoDB支持动态模式（动态添加字段），应用程序可以查询不能提前知道名称或任意名称的字段 例如， product_catalog 集合中的文档可能包含 product_attributes 字段。 product_attributes 字段可以包含任意嵌套字段，包括嵌入的文档和数组： 12345678910111213141516171819202122232425db.product_catalog.insertMany( [ &#123; &quot;product_name&quot; : &quot;Spy Coat&quot;, &quot;product_attributes&quot; : &#123; &quot;material&quot; : [ &quot;Tweed&quot;, &quot;Wool&quot;, &quot;Leather&quot; ], &quot;size&quot; : &#123; &quot;length&quot; : 72, &quot;units&quot; : &quot;inches&quot; &#125; &#125; &#125;, &#123; &quot;product_name&quot; : &quot;Spy Pen&quot;, &quot;product_attributes&quot; : &#123; &quot;colors&quot; : [ &quot;Blue&quot;, &quot;Black&quot; ], &quot;secret_feature&quot; : &#123; &quot;name&quot; : &quot;laser&quot;, &quot;power&quot; : &quot;1000&quot;, &quot;units&quot; : &quot;watts&quot;, &#125; &#125; &#125; ]) 以下操作在 product_attributes 字段上创建通配符索引： 123db.product_catalog.createIndex( &#123; &quot;product_attributes.$**&quot;: 1 &#125;) 通配符索引可以支持对 product_attributes 或其嵌入字段进行 任意单字段 查询： 123db.products_catalog.find( &#123; &quot;product_attributes.size.length&quot; : &#123; $gt : 60 &#125; &#125; )db.products_catalog.find( &#123; &quot;product_attributes.material&quot; : &quot;Leather&quot; &#125; )db.products_catalog.find( &#123; &quot;product_attributes.secret_feature.name&quot; : &quot;laser&quot; &#125; ) 使用限制 不能使用通配符索引来分片集合。在要分片的一个或多个字段上创建一个非通配符索引 不能创建复合索引 不能为通配符索引指定以下属性： TTL Unique 您不能使用通配符语法创建以下索引类型： 2d (Geospatial) 2dsphere (Geospatial) Hashed 哈希索引 不同于传统的B+Tree索引,哈希索引使用hash函数来创建索引。在索引字段上进行 精确匹配,但不支持 范围查询 ,不支持 多键hash 使用散列分片键对集合进行分片会导致数据分布更均匀。有关更多详细信息，请参阅 哈希分片 Hashed索引使用hashing函数来计算索引字段值的哈希。hashing函数折叠嵌入式文档并计算整个值的hash，但不支持多键（即数组）索引 创建哈希索引 若要创建哈希索引，请指定 hashed 作为索引键的值，如以下示例所示： 1db.collection.createIndex( &#123; _id: &quot;hashed&quot; &#125; ) 创建复合哈希索引 从MongoDB 4.4开始，MongoDB支持创建包含 单个哈希字段 的复合索引。若要创建复合哈希索引，请在创建索引时指定 hashed 作为任何单个索引键的值： 1db.collection.createIndex( &#123; &quot;fieldA&quot; : 1, &quot;fieldB&quot; : &quot;hashed&quot;, &quot;fieldC&quot; : -1 &#125; ) 地理位置索引 TODO 索引属性 Option Type 描述 background Boolean 建索引过程会阻塞其它数据库操作，background可指定以后台方式创建索引，即增加 “background” 可选参数。“background” 默认值为false unique Boolean 建立的索引是否唯一。指定为true创建唯一索引。默认值为false name string 索引的名称。如果未指定，MongoDB的通过连接索引的字段名和排序顺序生成一个索引名称 dropDups Boolean 3.0+版本已废弃。在建立唯一索引时是否删除重复记 录,指定 true 创建唯一索引。默认值为 false sparse Boolean 对文档中不存在的字段数据不启用索引；这个参数需 要特别注意，如果设置为true的话，在索引字段中不 会查询出不包含对应字段的文档.。默认值为 false expireAfterSeconds integer 指定一个以秒为单位的数值，完成 TTL设定，设定集合的生存时间 v index version 索引的版本号。默认的索引版本取决于mongod创建索引时运行的版本 weights document 索引权重值，数值在 1 到 99,999 之间，表示该索引 相对于其他索引字段的得分权重 default_language string 对于文本索引，该参数决定了停用词及词干和词器的 规则的列表。 默认为英语 language_override string 对于文本索引，该参数指定了包含在文档中的字段 名，语言覆盖默认的language，默认值为 language 唯一索引 唯一索引可确保索引字段不存储重复值;即强制索引字段的唯一性。默认情况下，MongoDB 在创建集合期间在 _id 字段上创建一个唯一索引 创建唯一索引 1db.members.createIndex( &#123; &quot;user_id&quot;: 1 &#125;, &#123; unique: true &#125; ) 限制 如果集合字段已经有重复值，则MongoDB无法在指定的索引字段上创建唯一索引（同一字段有重复值） 不能对 哈希索引 指定唯一约束 缺失字段 唯一性索引对于文档中缺失的字段，会使用null值代替，因此不允许存在多个文档缺失索引字段的情况 例如，集合在 x 上具有唯一索引： 1db.collection.createIndex( &#123; &quot;x&quot;: 1 &#125;, &#123; unique: true &#125; ) 如果集合尚未包含缺少字段 x 的文档，则唯一索引允许插入不带字段 x 的文档： 1db.collection.insertOne( &#123; y: 1 &#125; ) 但是，如果集合已包含缺少字段 x 的文档，则插入没有字段 x 的文档时会出现唯一索引错误： 12db.collection.insertOne( &#123; z: 1 &#125; )// duplicate key error 创建复合唯一索引 1db.collection.createIndex( &#123; &quot;a.loc&quot;: 1, &quot;a.qty&quot;: 1 &#125;, &#123; unique: true &#125; ) 在复制集和分片上建立唯一索引 对于副本集和分片集群，使用 rolling procedure (滚动过程) 创建唯一索引需要在过程中停止对集合的所有写操作。如果在此过程中无法停止对集合的所有写入，请不要使用滚动过程。相反，请通过以下方式在集合上构建唯一索引： 在副本集的主数据库上调用 db.collection.createIndex() 或者，在分片集群的 mongos 上调用 db.collection.createIndex() 分片集群和唯一索引 对于范围分片集合，只有以下索引是唯一的： 分片键上的索引 一个复合索引，其中分片键是一个 前缀（分片键必须作为唯一索引的前缀） 默认 _id 索引;但是如果 _id 字段不是分片键或分片键的前缀，则 _id 索引仅强制实施每个分片的唯一性约束 部分索引 部分索引只索引集合中满足指定筛选器表达式的文档。通过索引集合中文档的子集，部分索引可以降低存储需求，并降低创建和维护索引的性能成本 使用 db.collection.createIndex() 方法与 partialFilterExpression 选项一起使用创建部分索引，partialFilterExpression 选项接受使用以下方法指定筛选条件的文档： 相等表达式（即 field: value 或使用 $eq 运算符） $exists: true 表达式 $gt, $gte, $lt, $lte 表达式 $type 表达式 $and 运算符 $or 运算符 $in 运算符 部分索引生效必须满足 partialFilterExpression 1234567db.users.insertMany( [ &#123; &quot;_id&quot; : ObjectId(&quot;56424f1efa0358a27fa1f99a&quot;), &quot;username&quot; : &quot;david&quot;, &quot;age&quot; : 29 &#125;, &#123; &quot;_id&quot; : ObjectId(&quot;56424f37fa0358a27fa1f99b&quot;), &quot;username&quot; : &quot;amanda&quot;, &quot;age&quot; : 35 &#125;, &#123; &quot;_id&quot; : ObjectId(&quot;56424fe2fa0358a27fa1f99c&quot;), &quot;username&quot; : &quot;rajiv&quot;, &quot;age&quot; : 57 &#125; ]) 1234db.users.createIndex( &#123; username: 1 &#125;, // 对username字段创建所索引 &#123; partialFilterExpression: &#123; age: &#123; $gt: 30 &#125; &#125; &#125; // 部分索引，只有年龄大于30的数据才创建和使用索引) 以下查询将使用部分索引 123db.users.find( &#123; username: &quot;david&quot; , age: 35&#125;) 以下查询不会使用部分索引 123db.users.find( &#123; username: &quot;rajiv&quot;&#125;) 因为查询条件没有满足或缺失满足部分索引的 partialFilterExpression 条件，则不使用部分索引 部分索引导致唯一索引失效 分部索引仅索引集合中满足指定筛选表达式的文档。如果同时指定 partialFilterExpression 和唯一约束，则唯一约束仅适用于满足筛选表达式的文档。如果文档不符合筛选条件，则具有唯一约束的部分索引不会阻止插入不符合唯一约束的文档 引用上面的测试数据，假设 users 创建一个索引 1234db.users.createIndex( &#123; username: 1 &#125;, &#123; unique: true, partialFilterExpression: &#123; age: &#123; $gte: 30 &#125; &#125; &#125;) 那么这个部分唯一索引对于 age 大于等于30的数据可以保证 username 唯一，但是 age 小于30则不保证 username 唯一 稀疏索引 索引的稀疏属性确保索引只包含具有索引字段的文档的条目，索引将跳过没有索引字段的文档 特性： 只对存在字段的文档进行索引（包括字段值为null的文档） 集合上的稀疏索引无法返回完整结果 1234567db.score.insertMany( [ &#123; &quot;_id&quot; : ObjectId(&quot;523b6e32fb408eea0eec2647&quot;), &quot;userid&quot; : &quot;newbie&quot; &#125;, &#123; &quot;_id&quot; : ObjectId(&quot;523b6e61fb408eea0eec2648&quot;), &quot;userid&quot; : &quot;abby&quot;, &quot;score&quot; : 82 &#125;, &#123; &quot;_id&quot; : ObjectId(&quot;523b6e6ffb408eea0eec2649&quot;), &quot;userid&quot; : &quot;nina&quot;, &quot;score&quot; : 90 &#125; ]) 创建一个稀疏索引 1db.score.createIndex( &#123; score: 1 &#125; , &#123; sparse: true &#125; ) 下面查询不会返回 userid 为 newbie 的文档，因为强制使用了稀疏索引，该文档没有 score 字段，不会被加入索引 123456789101112131415db.score.find().sort( &#123; score: -1 &#125; ).hint( &#123;score: 1&#125; )/* 1 */&#123; &quot;_id&quot; : ObjectId(&quot;523b6e6ffb408eea0eec2649&quot;), &quot;userid&quot; : &quot;nina&quot;, &quot;score&quot; : 90.0&#125;/* 2 */&#123; &quot;_id&quot; : ObjectId(&quot;523b6e61fb408eea0eec2648&quot;), &quot;userid&quot; : &quot;abby&quot;, &quot;score&quot; : 82.0&#125; 需要返回完整的查询结果就不能暗示使用稀疏索引 1db.score.find().sort( &#123; score: -1 &#125; ) 具有唯一约束的稀疏索引 可以使用以下命令创建具有唯一约束的稀疏索引 1db.score.createIndex( &#123; score: 1 &#125; , &#123; sparse: true, unique: true &#125; ) 以下数据可以插入到集合中 12345db.score.insertMany( [ &#123; &quot;userid&quot;: &quot;newbie&quot;, &quot;score&quot;: 43 &#125;, &#123; &quot;userid&quot;: &quot;abby&quot;, &quot;score&quot;: 34 &#125;, &#123; &quot;userid&quot;: &quot;nina&quot; &#125; // 原本唯一索引只允许集合中字段有一个为null值，但是使用稀疏索引后，缺失字段的文档可以插入多个，但是不会被索引] ) 以下数据不能插入到集合中，因为 score 值为 82 和 90 的文档已经存在： 1234db.score.insertMany( [ &#123; &quot;userid&quot;: &quot;newbie&quot;, &quot;score&quot;: 82 &#125;, &#123; &quot;userid&quot;: &quot;abby&quot;, &quot;score&quot;: 90 &#125;] ) TTL索引 通过设置TTL使集合中的数据过期 TTL索引是一种特殊的 单字段索引，MongoDB可以使用它在一定的时间或特定的时钟时间后自动从集合中删除文档。数据过期对于某些类型的信息很有用，比如机器生成的事件数据、日志和会话信息，这些信息只需要在数据库中保存有限的时间 测试数据 创建一个 user 集合，根据 createTime 创建一个TTL索引，数据在10秒后过期 1db.user.createIndex( &#123; &quot;createTime&quot;: 1 &#125;, &#123; expireAfterSeconds: 10 &#125; ) 插入一条用户数据，数据将在10秒后自动删除 12345678db.user.insertOne( &#123; &quot;_id&quot; : ObjectId(&quot;56424f1efa0358a27fa1f99a&quot;), &quot;username&quot; : &quot;david&quot;, &quot;createTime&quot; : new Date() &#125;)// 数据将在10秒后自动删除 隐藏索引 通过对规划器隐藏索引，用户可以评估删除索引而不实际删除索引的潜在影响。如果影响是负面的，用户可以取消隐藏索引，而不必重新创建已删除的索引 123456789101112创建隐藏索引 db.restaurants.createIndex(&#123; fileName: 1 &#125;,&#123; hidden: true &#125;);隐藏现有索引 db.restaurants.hideIndex( &#123; fileName: 1&#125; );或者db.restaurants.hideIndex( &quot;索引名称&quot; ) 取消隐藏索引 db.restaurants.unhideIndex( &#123; fileName: 1&#125; ); 或者db.restaurants.unhideIndex( &quot;索引名称&quot; ); 索引使用建议 为每一个查询建立合适的索引 这个是针对于数据量较大比如说超过几十上百万（文档数目）数量级的集合。如果没有索引MongoDB需要把所有的Document从盘上读到内存，这会对MongoDB服务器造成较大的压力并影响到其他请求的执行。 创建合适的复合索引，不要依赖于交叉索引 如果你的查询会使用到多个字段，MongoDB有两个索引技术可以使用：交叉索引和复合索引。交叉索引就是针对每个字段单独建立一个单字段索引，然后在查询执行时候使用相应的单字段索引进行索引交叉而得到查询结果。交叉索引目前触发率较低，所以如果你有一个多字段查询的时候，建议使用复合索引能够保证索引正常的使用。 复合索引字段顺序：匹配条件在前，范围条件在后 Equality First, Range After 前面的例子，在创建复合索引时如果条件有匹配和范围之分，那么匹配条件（sport: “marathon”) 应该在复合索引的前面。范围条件(age: &lt;30)字段应该放在复合索引的后面，使用复合索引也要遵循最左匹配原则。 尽可能使用覆盖索引 使用索引覆盖的好处是查询的数据在索引上都能获取到，直接查询索引后返回数据，提高查询效率。 建索引要在后台运行 在对一个集合创建索引时，该集合所在的数据库将不接受其他读写操作。对大数据量的集合建索引，建议使用后台运行选项 {background: true} SpringBoot整合 Pom配置 123456789101112131415161718192021222324252627&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-mongodb&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; yml配置 1234567891011server: port: 9090spring: data: mongodb: host: 127.0.0.1 port: 27017 username: test password: 123456 database: test Mongo配置 12345678910111213141516171819202122232425262728293031323334@Configuration@RequiredArgsConstructorpublic class MongoConfig &#123; private final MongoDatabaseFactory mongoDbFactory; private final MongoMappingContext mongoMappingContext; /** * 转换类配置 * * @return 转换类 */ @Bean public MappingMongoConverter mappingMongoConverter() &#123; DbRefResolver dbRefResolver = new DefaultDbRefResolver(mongoDbFactory); MappingMongoConverter converter = new MappingMongoConverter(dbRefResolver, mongoMappingContext); //不保存 _class 属性到mongo converter.setTypeMapper(new DefaultMongoTypeMapper(null)); return converter; &#125; /** * 定义事务管理器 * * @param mongoDbFactory * @return */ @Bean public MongoTransactionManager mongoTransactionManager(MongoDatabaseFactory mongoDatabaseFactory) &#123; // 设置事务级别的读/写关注级别 TransactionOptions transactionOptions = TransactionOptions.builder().readConcern(ReadConcern.SNAPSHOT).writeConcern(WriteConcern.MAJORITY).build(); return new MongoTransactionManager(mongoDatabaseFactory, transactionOptions); &#125;&#125; 实体 1234567891011121314@Data@ToString@Accessors(chain = true)@Document(collection = &quot;product&quot;)public class Product &#123; @MongoId private ObjectId id; private String productName; private BigDecimal price; private Integer num;&#125; VO 1234567@Datapublic class ProductAggVo &#123; @MongoId private ObjectId id; private Integer total;&#125; 单元测试 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071@Slf4j@SpringBootTestpublic class CURDTest &#123; @Autowired private MongoTemplate mongoTemplate; @Test public void insertTest() &#123; List&lt;Product&gt; productList = new ArrayList&lt;&gt;(); productList.add( new Product() .setProductName(&quot;iphone 14&quot;) .setPrice(new BigDecimal(&quot;1234&quot;)) .setNum(5)); productList.add( new Product() .setProductName(&quot;iphone 15&quot;) .setPrice(new BigDecimal(&quot;2134&quot;)) .setNum(6)); productList.add( new Product() .setProductName(&quot;iphone xiaomi&quot;) .setPrice(new BigDecimal(&quot;3333&quot;)) .setNum(7)); this.mongoTemplate.insert(productList, Product.class); &#125; @Test public void selectTest() &#123; Query query = new Query(Criteria.where(&quot;productName&quot;).is(&quot;iphone 14&quot;)); Product product = this.mongoTemplate.findOne(query, Product.class); log.info(product.toString()); &#125; @Test public void updateTest() &#123; Query query = new Query(Criteria.where(&quot;productName&quot;).is(&quot;iphone 14&quot;)); Update update = new Update().inc(&quot;num&quot;, 1); UpdateResult updateResult = this.mongoTemplate.updateFirst(query, update, Product.class); log.info(updateResult.toString()); &#125; @Test public void removeTest() &#123; Query query = new Query(Criteria.where(&quot;productName&quot;).is(&quot;iphone 14&quot;)); DeleteResult deleteResult = this.mongoTemplate.remove(query, Product.class); log.info(deleteResult.toString()); &#125; @Test public void aggTest() &#123; MatchOperation match = Aggregation.match(Criteria.where(&quot;num&quot;).gte(6)); GroupOperation group = Aggregation.group(&quot;_id&quot;).sum(&quot;$num&quot;).as(&quot;total&quot;); Aggregation aggregation = Aggregation.newAggregation( match, group ); AggregationResults&lt;ProductAggVo&gt; aggregate = this.mongoTemplate.aggregate(aggregation, Product.class, ProductAggVo.class); log.info(aggregate.toString()); &#125;&#125; 存储引擎 存储引擎是数据库的组成部分，负责管理数据在内存和磁盘上的存储方式。MongoDB支持多个存储引擎，因为不同的引擎对特定的工作负载性能更好。 WiredTiger 是MongoDB 3.2开始的默认存储引擎。它非常适用于大多数工作负载，并且推荐在新部署中使用。WiredTiger提供了文档级的并发模型、检查点和压缩等功能。 内存存储引擎在MongoDB 商业版中可用。它不是将文档存储在磁盘上，而是将它们保留在内存中，以实现更可预测的数据延迟。 WiredTiger 从MongoDB 3.2开始，WiredTiger存储引擎是默认的存储引擎。 文档级并发 WiredTiger 使用文档级并发控制进行写入操作。因此，多个客户端可以同时修改集合的不同文档。 对于大多数读写操作，WiredTiger使用 乐观并发 控制模式。 WiredTiger仅在全局、数据库和集合级别使用意向锁。 当存储引擎检测到两个操作之间存在冲突时，将引发写冲突，从而导致MongoDB自动重试该操作。 一些全局操作（通常是涉及多个数据库的短暂操作）仍然需要全局“实例范围级别的”锁。 其他一些操作（例如删除集合）仍然需要独占数据库锁。 内存快照和检查点 WiredTiger 存储引擎使用 MVCC（多版本并发控制）技术。在进行操作时，WiredTiger 为操作提供数据的一个时间点快照。快照提供了内存中数据的一致视图。 写入磁盘时，WiredTiger将所有数据文件中的快照中的所有数据以一致的方式写入磁盘。 现在持久的数据充当数据文件中的 检查点。 该检查点可确保数据文件直到最后一个检查点（包括最后一个检查点）都保持一致； 即检查点可以充当恢复点。 从3.6版本开始，MongoDB将WiredTiger配置为以 60秒 的间隔创建 检查点（即将内存快照数据写入磁盘）。 在早期版本中，MongoDB将检查点设置为在WiredTiger中以60秒的间隔或在写入 2GB 日志数据时对用户数据进行检查，以先到者为准。 在写入新检查点期间，以前的检查点仍然有效。因此，即使MongoDB在写入新检查点时终止或遇到错误，在重新启动时，MongoDB也可以从上一个有效的检查点恢复。 当 WiredTiger 的元数据表以原子方式更新以引用新检查点时，当新的检查点可用户时。一旦可以访问新的检查点，WiredTiger 就会从旧检查点中释放内存page。 使用WiredTiger，即使没有日志，MongoDB也可以从最后一个检查点恢复;但是，若要恢复在最后一个检查点之后所做的更改，请使用日记功能运行。 检查点检查点是指定时将内存中的数据写入到磁盘中并创建一个检查点文件用于数据恢复的操作 Journal(日志) 写入关注的 j 选项 可以控制 mongodb 必须要成功写入 Journal 才返回 WiredTiger 将预写日志（即 journal）与检查点结合使用，以确保数据的持久性。 WiredTiger 日志（journal）保留检查点之间的所有数据修改。如果MongoDB在检查点之间宕机，它将使用日志重放自上次检查点以来修改的所有数据。 WiredTiger 日志使用 snappy 压缩库进行压缩。若要指定不同的压缩算法或不压缩，请使用storage.wiredTiger.engineConfig.journalCompressor 设置（超过128M才会开启压缩）。 可以通过将 storage.journal.enabled 设置为 false 来禁用独立实例的日记功能，这可以减少维护日志的开销。对于独立实例，不使用日志意味着当MongoDB意外宕机时，最后一个检查点后面的数据会丢失。 压缩 使用WiredTiger，MongoDB支持对所有集合和索引进行压缩。 压缩可最大程度地减少存储空间的使用量，但会增加CPU的开销。 默认情况下，WiredTiger 对所有集合使用块压缩和 snappy 压缩库，对所有索引使用前缀压缩。 对于集合，可选的压缩库有 zlib zstd (MongoDB 4.2) 对于索引，若要禁用前缀压缩，请使用 storage.wiredTiger.indexConfig.prefixCompression 设置。 压缩设置还可以在集合和索引创建期间基于每个集合和每个索引进行配置。 内存使用 使用WiredTiger，MongoDB同时利用WiredTiger内部缓存和文件系统缓存。 从MongoDB 3.4开始，默认的WiredTiger内部缓存大小是以下两者中的较大者： （内存大小 -1GB） * 50% 256MB 例如，在总共具有 4GB RAM 的系统上，WiredTiger 缓存将使用 1.5GB 的 RAM （ 0.5 * (4 GB - 1 GB) = 1.5 GB ） 相反，总内存为1.25 GB的系统将为WiredTiger缓存分配256 MB，因为这是总RAM的一半以上减去一GB（0.5* （1.25 GB-1 GB）= 128 MB 内存存储引擎 商业版本提供 GridFS GridFS是MongoDB提供的一种存储和检索大文件（如图片、视频等）的机制。它允许将一个大文件分割成多个小文件进行存储，并且可以方便地进行文件的读写操作。GridFS会自动处理文件的切分和重组（每个chunk 255KB），同时还可以提供基于文件名、类型、大小等条件进行搜索的功能。 GridFS 不仅可用于存储超过 16 MB 的文件，还可用于存储要访问的任何文件，而无需将整个文件加载到内存中。 使用场景 在MongoDB中，使用GridFS存储大于16 MB的文件。 在某些情况下，在MongoDB数据库中存储大文件可能比在系统级文件系统中更有效: 如果文件系统限制目录中的文件数，则可以使用 GridFS 根据需要存储任意数量的文件。 如果要访问大文件部分的信息，而不必将整个文件加载到内存中，则可以使用 GridFS 调用文件的各个部分，而无需将整个文件读入内存（比如视频）。 当你希望保持文件和元数据在多个系统和设施之间自动同步和部署时，可以使用GridFS。使用地理分布的复制集时，MongoDB可以自动将文件及其元数据分发到多个mongod实例和设施 如何使用 GridFS 将文件存储在两个集合中： chunks 存储二进制块 files 存储文件的元数据 GridFS通过使用存储桶名称为每个集合添加前缀，将集合放置在一个公共存储桶中。默认情况下，GridFS使用两个集合以及一个名为fs的存储桶： fs.files fs.chunks chunks 集合 chunks 集合中的每个文档都表示 GridFS 中表示的文件的不同块。 123456&#123; &quot;_id&quot; : &lt;ObjectId&gt;, &quot;files_id&quot; : &lt;ObjectId&gt;, &quot;n&quot; : &lt;num&gt;, &quot;data&quot; : &lt;binary&gt;&#125; 字段 说明 chunks._id 块的唯一ObjectId chunks.files_id “父”文档的 _id ，在 files 集合中指定 chunks.n 区块的序列号。GridFS 对所有块进行编号，从 0 开始 chunks.data BSON 二进制类型 files 集合 files 集合中的每个文档都表示 GridFS 中的一个文件 1234567891011&#123; &quot;_id&quot; : &lt;ObjectId&gt;, &quot;length&quot; : &lt;num&gt;, &quot;chunkSize&quot; : &lt;num&gt;, &quot;uploadDate&quot; : &lt;timestamp&gt;, &quot;md5&quot; : &lt;hash&gt;, &quot;filename&quot; : &lt;string&gt;, &quot;contentType&quot; : &lt;string&gt;, &quot;aliases&quot; : &lt;string array&gt;, &quot;metadata&quot; : &lt;any&gt;,&#125; 字段 描述 files._id 该文档的唯一标识符。 _id 是您为原始文档选择的数据类型。 MongoDB 文档的默认类型是 BSON ObjectId。 files.length 文件大小，以字节为单位。 files.chunkSize 每个块的大小，以字节为单位。 GridFS 将文档分成大小为 chunkSize 的块，除了最后一个块外，它只有所需的大小。 默认大小为 255kB。 files.uploadDate 文件首次被 GridFS 存储的日期。此值的类型为 Date。 files.md5 已弃用。 FIPS 140-2 禁止使用 MD5 算法。 MongoDB 驱动程序已弃用 MD5 支持，并将在未来版本中删除 MD5 生成。需要文件摘要的应用程序应在 GridFS 外部实现它并存储在 files.metadata 中。返回完整文件的 MD5 哈希由 filemd5 命令返回。此值的类型为 String。 files.filename 可选。适用于 GridFS 文件的人类可读名称。 files.contentType 已弃用。可选。适用于 GridFS 文件的有效 MIME 类型。仅供应用程序使用。请使用 files.metadata 存储与 GridFS 文件的 MIME 类型相关的信息。 files.aliases 已弃用。可选。别名字符串数组。仅供应用程序使用。请使用 files.metadata 存储与 GridFS 文件的 MIME 类型相关的信息。 files.metadata 可选。元数据字段可以是任何数据类型，并且可以保存您希望存储的任何其他信息。如果您希望向 files 集合中的文档添加其他任意字段，请将它们添加到 metadata 字段中的对象中。 添加文件 我们使用 GridFS 的 put 命令来存储 mp3 文件。 调用 MongoDB 安装目录下bin的 mongofiles 工具 1mongofiles -d gridfs put song.mp3 -d gridfs 指定存储文件的数据库名称，如果不存在该数据库，MongoDB会自动创建。如果不存在该数据库，MongoDB会自动创建。Song.mp3 是音频文件名 切换数据库后查询文件 1db.fs.files.find() 查询数据块 1db.fs.chunks.find() 副本集 Replica Sets MongoDB中的副本集（Replica Set）是一组维护相同数据集的mongod服务。 副本集可提供 数据冗余 和 高可用性，是所有生产部署的基础。 也可以说，副本集类似于有自动故障恢复功能的主从集群。通俗的讲就是用多台机器进行同一数据的异步复制，从而使多台机器拥有同一数据的多个副本，并且当主库宕机时在不需要用户干预的情况下自动切换其他备份服务器做主库。而且还可以利用副本服务器做只读服务器，实现读写分离，提高负载。 冗余和数据可用性 复制提供冗余并提高数据可用性。 通过在不同数据库服务器上提供多个数据副本，复制提供了针对单个数据库服务器宕机的容错级别 在某些情况下，复制可以提高读取性能，因为客户端可以将读取操作发送到不同的服务上， 在不同数据中心维护数据副本可以增加分布式应用程序的数据位置和可用性。 您还可以为专用目的维护其他副本，例如灾难恢复，报告或备份。 MongoDB中的复制 副本集是一组维护相同数据集的 mongod 实例。副本集包含多个数据承载节点和一个可选的仲裁节点。在数据承载节点中，一个且只有一个成员被视为主节点，而其他节点被视为辅助节点。 主节点接收所有写入操作。一个副本集只能有一个能够确认具有 &#123; w: &quot;majority&quot; &#125; 写入关注的写入的主节点，尽管在某些情况下，另一个Mongod实例可能会暂时认为自己也是主节点（网络分区），主节点在其操作日志中记录其数据集的所有更改，记录 oplog. 辅助节点复制主节点的 oplog 并将操作应用于其数据集，完成辅助节点和主节点的数据同步。 如果主节点宕机，则在符合条件的副本节点中重新选举主节点。 主从复制和副本集区别 主从集群和副本集最大的区别就是副本集没有固定的 主节点。整个集群会选出一个“主节点”，当其挂掉后，又在剩下的从节点中选中其他节点为“主节点”，副本集总有一个活跃点(主、primary)和一个或多个备份节点(从、secondary)。 副本集角色 副本集有两种类型三种角色 两种类型： 主节点：数据操作的主要连接点，可读写 辅助节点：数据冗余备份，可读或选举 三种角色 主节点 主节点负责所有写请求操作，然后主节点使用 oplog 记录写操作。 辅助节点复制 oplog 完成数据同步 主节点宕机。会触发新一轮主节点选举，会从辅助节点中重新选举出一个新的主节点 辅助节点 辅助节点维护主节点的数据副本。辅助节点使用主节点的 oplog 将数据从主节点复制本辅助节点，当主节点宕机辅助节点有机会晋升为主节点。 辅助节点不能进行写操作，通过额外配置后辅助节点可以进行读操作。 仲裁节点 不同步任何数据的副本，只具有投票选举作用。当然也可以将仲裁节点维护为副本集的一部分，即副本节点同时也可以是仲裁节点。也是一种从节点类型 可以将额外的mongod实例添加到副本集作为仲裁节点。 仲裁节点不维护数据集。 仲裁节点的目的是通过响应其他副本集成员的心跳和选举请求来维护副本集中的仲裁。 因为它们不存储数据集，所以仲裁器可以是提供副本集仲裁功能的好方法，其占用资源更低。 如果你的副本+主节点的个数是偶数，建议加一个仲裁节点，形成奇数，容易满足大多数的投票。 如果你的副本+主节点的个数是奇数，可以不加仲裁节点。 Oplog Oplog同步 oplog（操作日志）是一个特殊的 有限集合 ，它保留修改数据库中存储的数据的所有操作的滚动记录。 与其他有上限的集合不同，oplog 可以超过其配置的大小限制，避免误删提交点。 MongoDB的所有写操作请求都是在 主节点 上进行的，然后将这些操作记录到主节点的 oplog 上，最后 辅助节点 会以异步复制的方式同步这些日志（4.4后是推送模式）。所有副本集成员都包含一个 oplog 的副本，其位于local.oplog.rs 集合中，该集合可以让副本集成员维护数据库的当前状态。 local.oplog.rs 是 MongoDB 中的一个特殊集合，用于支持复制和sharding。它存储了所有对MongoDB数据库进行更改的操作，包括插入、更新和删除等操作，这些操作都是通过primary节点执行的。每个secondary节点都会从primary节点获取并应用这些操作（回放），从而保证数据在各个节点上的一致性。 为了便于复制，所有副本集成员将心跳(ping)发送给所有其他成员。任何从节点都可以从任何其他节点导入oplog条目。 oplog中的每个操作都是 幂等 的。也就是说，oplog 操作无论应用于目标数据集一次还是多次，都会产生相同的结果。 为了确保不会重复复制操作，每个辅助节点在复制过程中会追踪自己已经复制和执行过的 oplog 条目的状态。每个副本集成员都有一个叫做 “复制进程 ID（replication processId）” 的标识符，用来标记已经复制过的 oplog 条目。当一个副本集成员读取主节点的 oplog 时，它会记住自己在最近的复制过程中读取的最后一个条目的复制进程 ID。下一次复制过程开始时，它会使用该复制进程 ID 作为起点，只复制新的操作 操作日志大小 默认操作日志大小 Unix和Windows系统 存储引擎 默认Oplog大小 下限 上限 In-Memory 物理内存的5% 50MB 50GB WiredTiger 可用磁盘空间的5% 990MB 50GB macOS系统 存储引擎 默认Oplog大小 In-Memory 192MB物理内存 WiredTiger 192MB磁盘空间 在大多数情况下，默认的oplog大小就足够了。例如，如果一个oplog是空闲磁盘空间的5%，并且可容纳24小时的操作记录，那么从节点从oplog停止复制条目的时间可以长达24小时，并且不会因oplog条目变得太陈旧而无法继续复制 在 mongod 创建操作日志之前，可以使用 oplogSizeMB 选项指定其大小。首次启动副本集成员后，请使用 replSetResizeOplog 管理命令更改 oplog 大小，不用重启进程。 要查看操作日志状态（包括操作的大小和时间范围），请使用rs.printReplicationInfo() ,有关操作日志状态的详细信息，请参考 Check the Size of the Oplog. 主节点选举原则 MongoDB在副本集中，会自动进行主节点的选举，主节点选举的触发条件： 主节点宕机 主节点网络抖动（默认心跳超时时间为10秒） 人工干预（rs.stepDown(600)） 一旦触发选举，就要根据一定规则来选主节点。 选举规则是根据票数来决定谁当选： 票数最高，且获得了半数以上票数的从节点当选主节点（N/2 + 1）;当复制集内存活的成员数量不满足总节点半数以上的数量时，无法进行主节点选举，此时服务处于只读状态 若票数相同，且票数都满足总节点半数以上，数据新的节点当选；数据的新旧是通过操作日志 oplog 来对比的 优先级 在获得票数的时候，优先级（priority）参数影响重大，优先级默认为 1 可以通过设置优先级（priority）来设置额外票数。优先级即权重，取值为0-1000，相当于可额外增加0-1000的票数，优先级的值越大，就越可能获得多数成员的投票（votes）数。指定较高的值可使成员更有资格成为主节点 调整副本集成员的优先级 故障转移期间的数据回滚 只有在主节点宕机之前进行了写操作并且写操作没有同步到副本节点时才需要回滚。当宕机的主节点以副本节点重新加入副本集时，它将回滚未完成同步的写操作，以确保和其他节点的数据保持一致. 读偏好 MongoDB的读偏好选项是指在进行读取操作时，优先选择从哪个节点或副本集成员读取数据 读偏好选项 描述 primary 将读操作发送到主节点（Primary）。这是默认的读偏模式，适用于大多数情况。 PrimaryPreferred 首选将读操作发送到主节点（Primary），但如果主节点不可用，则可以从副本集中的任何成员读取数据。 Secondary 将读操作发送到副本集中的次要节点（Secondary）。这可以用分担主节点的读负载或现读扩展。 secondaryPreferred 首选将操作发送到副本中的次要节点（Secondary），但如果没有次要节点可用，则可以从主节点读取数据。 nearest 将读操作发送到距离客端最近的节点，无论其角色是主节点还次要节点。这个模式适用于需要最小读延迟的场景。 副本集架构搭建 目标：一主一副本一仲裁 主节点 1234日志和数据目录mkdir -p /data/mongodb/rs/27020/logmkdir -p /data/mongodb/rs/27020/datamkdir -p /data/mongodb/rs/27020/config 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748新建配置vi /data/mongodb/rs/27020/config/mongod.conf# mongod.conf# for documentation of all options, see:# http://docs.mongodb.org/manual/reference/configuration-options/# Where and how to store data.#storage:# dbPath: /data/db# engine:# wiredTiger:# where to write logging data.systemLog: destination: file logAppend: true path: /data/log/mongo.log# network interfaces#net:# port: 27017# bindIp: 127.0.0.1# how the process runs#processManagement:# timeZoneInfo: /usr/share/zoneinfo# 安全项先保持注释状态，配好集群后放开。mongo.key后面步骤里会有生成。# security:# keyFile: /etc/mongo/mongo.key# authorization: enabled#operationProfiling:# 配副本集名replication: replSetName: &quot;rs&quot; #sharding:## Enterprise-Only Options:#auditLog:#snmp: 123456789启动服务docker run -d \\--name mongod_27020 \\-p 0.0.0.0:27020:27017 \\-v /data/mongodb/rs/27020/data:/data/db \\-v /data/mongodb/rs/27020/log:/data/log \\-v /data/mongodb/rs/27020/config:/data/conf \\--privileged=true mongo \\--replSet &quot;rs&quot; 副本节点 1234日志和数据目录mkdir -p /data/mongodb/rs/27021/logmkdir -p /data/mongodb/rs/27021/datamkdir -p /data/mongodb/rs/27021/config 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748新建配置vi /data/mongodb/rs/27021/config/mongod.conf# mongod.conf# for documentation of all options, see:# http://docs.mongodb.org/manual/reference/configuration-options/# Where and how to store data.#storage:# dbPath: /data/db# engine:# wiredTiger:# where to write logging data.systemLog: destination: file logAppend: true path: /data/log/mongo.log# network interfaces#net:# port: 27017# bindIp: 127.0.0.1# how the process runs#processManagement:# timeZoneInfo: /usr/share/zoneinfo# 安全项先保持注释状态，配好集群后放开。mongo.key后面步骤里会有生成。# security:# keyFile: /etc/mongo/mongo.key# authorization: enabled#operationProfiling:# 配副本集名replication: replSetName: &quot;rs&quot; #sharding:## Enterprise-Only Options:#auditLog:#snmp: 123456789启动服务docker run -d \\--name mongod_27021 \\-p 0.0.0.0:27021:27017 \\-v /data/mongodb/rs/27021/data:/data/db \\-v /data/mongodb/rs/27021/log:/data/log \\-v /data/mongodb/rs/27021/config:/data/conf \\--privileged=true mongo \\--replSet &quot;rs&quot; 仲裁节点 1234日志和数据目录mkdir -p /data/mongodb/rs/27022/logmkdir -p /data/mongodb/rs/27022/datamkdir -p /data/mongodb/rs/27022/config 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950新建配置vi /data/mongodb/rs/27022/config/mongod.confvi /data/mongodb/rs/27021/config/mongod.conf# mongod.conf# for documentation of all options, see:# http://docs.mongodb.org/manual/reference/configuration-options/# Where and how to store data.#storage:# dbPath: /data/db# engine:# wiredTiger:# where to write logging data.systemLog: destination: file logAppend: true path: /data/log/mongo.log# network interfaces#net:# port: 27017# bindIp: 127.0.0.1# how the process runs#processManagement:# timeZoneInfo: /usr/share/zoneinfo# 安全项先保持注释状态，配好集群后放开。mongo.key后面步骤里会有生成。# security:# keyFile: /etc/mongo/mongo.key# authorization: enabled#operationProfiling:# 配副本集名replication: replSetName: &quot;rs&quot; #sharding:## Enterprise-Only Options:#auditLog:#snmp: 123456789启动服务docker run -d \\--name mongod_27022 \\-p 0.0.0.0:27022:27017 \\-v /data/mongodb/rs/27022/data:/data/db \\-v /data/mongodb/rs/27022/log:/data/log \\-v /data/mongodb/rs/27022/config:/data/conf \\--privileged=true mongo \\--replSet &quot;rs&quot; 添加节点和仲裁节点 添加，删除副本集节点参考 rs 命令 1234567rs.initiate(&#123;_id: &quot;rs&quot;,members: [&#123; _id: 0, host: &quot;192.168.0.181:27020&quot; &#125;,&#123; _id: 1, host: &quot;192.168.0.181:27021&quot; &#125;,&#123; _id: 2, host: &quot;192.168.0.181:27022&quot;, arbiterOnly: true &#125;] &#125;) 查看集群状态 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136rs:PRIMARY&gt; rs.status()&#123; &quot;set&quot; : &quot;rs&quot;, &quot;date&quot; : ISODate(&quot;2023-05-10T16:00:35.077Z&quot;), &quot;myState&quot; : 1, &quot;term&quot; : NumberLong(1), &quot;syncSourceHost&quot; : &quot;&quot;, &quot;syncSourceId&quot; : -1, &quot;heartbeatIntervalMillis&quot; : NumberLong(2000), &quot;majorityVoteCount&quot; : 2, &quot;writeMajorityCount&quot; : 2, &quot;votingMembersCount&quot; : 3, &quot;writableVotingMembersCount&quot; : 2, &quot;optimes&quot; : &#123; &quot;lastCommittedOpTime&quot; : &#123; &quot;ts&quot; : Timestamp(1683734432, 3), &quot;t&quot; : NumberLong(1) &#125;, &quot;lastCommittedWallTime&quot; : ISODate(&quot;2023-05-10T16:00:32.456Z&quot;), &quot;readConcernMajorityOpTime&quot; : &#123; &quot;ts&quot; : Timestamp(1683734432, 3), &quot;t&quot; : NumberLong(1) &#125;, &quot;appliedOpTime&quot; : &#123; &quot;ts&quot; : Timestamp(1683734432, 3), &quot;t&quot; : NumberLong(1) &#125;, &quot;durableOpTime&quot; : &#123; &quot;ts&quot; : Timestamp(1683734432, 3), &quot;t&quot; : NumberLong(1) &#125;, &quot;lastAppliedWallTime&quot; : ISODate(&quot;2023-05-10T16:00:32.456Z&quot;), &quot;lastDurableWallTime&quot; : ISODate(&quot;2023-05-10T16:00:32.456Z&quot;) &#125;, &quot;lastStableRecoveryTimestamp&quot; : Timestamp(1683734409, 1), &quot;electionCandidateMetrics&quot; : &#123; &quot;lastElectionReason&quot; : &quot;electionTimeout&quot;, &quot;lastElectionDate&quot; : ISODate(&quot;2023-05-10T15:58:29.320Z&quot;), &quot;electionTerm&quot; : NumberLong(1), &quot;lastCommittedOpTimeAtElection&quot; : &#123; &quot;ts&quot; : Timestamp(1683734298, 1), &quot;t&quot; : NumberLong(-1) &#125;, &quot;lastSeenOpTimeAtElection&quot; : &#123; &quot;ts&quot; : Timestamp(1683734298, 1), &quot;t&quot; : NumberLong(-1) &#125;, &quot;numVotesNeeded&quot; : 2, &quot;priorityAtElection&quot; : 1, &quot;electionTimeoutMillis&quot; : NumberLong(10000), &quot;numCatchUpOps&quot; : NumberLong(0), &quot;newTermStartDate&quot; : ISODate(&quot;2023-05-10T15:58:29.377Z&quot;), &quot;wMajorityWriteAvailabilityDate&quot; : ISODate(&quot;2023-05-10T15:58:30.274Z&quot;) &#125;, &quot;members&quot; : [ &#123; &quot;_id&quot; : 0, &quot;name&quot; : &quot;192.168.0.181:27020&quot;, &quot;health&quot; : 1, &quot;state&quot; : 1, &quot;stateStr&quot; : &quot;PRIMARY&quot;, // 主节点 &quot;uptime&quot; : 605, &quot;optime&quot; : &#123; &quot;ts&quot; : Timestamp(1683734432, 3), &quot;t&quot; : NumberLong(1) &#125;, &quot;optimeDate&quot; : ISODate(&quot;2023-05-10T16:00:32Z&quot;), &quot;lastAppliedWallTime&quot; : ISODate(&quot;2023-05-10T16:00:32.456Z&quot;), &quot;lastDurableWallTime&quot; : ISODate(&quot;2023-05-10T16:00:32.456Z&quot;), &quot;syncSourceHost&quot; : &quot;&quot;, &quot;syncSourceId&quot; : -1, &quot;infoMessage&quot; : &quot;&quot;, &quot;electionTime&quot; : Timestamp(1683734309, 1), &quot;electionDate&quot; : ISODate(&quot;2023-05-10T15:58:29Z&quot;), &quot;configVersion&quot; : 1, &quot;configTerm&quot; : 1, &quot;self&quot; : true, &quot;lastHeartbeatMessage&quot; : &quot;&quot; &#125;, &#123; &quot;_id&quot; : 1, &quot;name&quot; : &quot;192.168.0.181:27021&quot;, &quot;health&quot; : 1, &quot;state&quot; : 2, &quot;stateStr&quot; : &quot;SECONDARY&quot;, // 副本节点 &quot;uptime&quot; : 136, &quot;optime&quot; : &#123; &quot;ts&quot; : Timestamp(1683734432, 3), &quot;t&quot; : NumberLong(1) &#125;, &quot;optimeDurable&quot; : &#123; &quot;ts&quot; : Timestamp(1683734432, 3), &quot;t&quot; : NumberLong(1) &#125;, &quot;optimeDate&quot; : ISODate(&quot;2023-05-10T16:00:32Z&quot;), &quot;optimeDurableDate&quot; : ISODate(&quot;2023-05-10T16:00:32Z&quot;), &quot;lastAppliedWallTime&quot; : ISODate(&quot;2023-05-10T16:00:32.456Z&quot;), &quot;lastDurableWallTime&quot; : ISODate(&quot;2023-05-10T16:00:32.456Z&quot;), &quot;lastHeartbeat&quot; : ISODate(&quot;2023-05-10T16:00:33.337Z&quot;), &quot;lastHeartbeatRecv&quot; : ISODate(&quot;2023-05-10T16:00:34.358Z&quot;), &quot;pingMs&quot; : NumberLong(0), &quot;lastHeartbeatMessage&quot; : &quot;&quot;, &quot;syncSourceHost&quot; : &quot;192.168.0.181:27020&quot;, &quot;syncSourceId&quot; : 0, &quot;infoMessage&quot; : &quot;&quot;, &quot;configVersion&quot; : 1, &quot;configTerm&quot; : 1 &#125;, &#123; &quot;_id&quot; : 2, &quot;name&quot; : &quot;192.168.0.181:27022&quot;, &quot;health&quot; : 1, &quot;state&quot; : 7, &quot;stateStr&quot; : &quot;ARBITER&quot;, // 仲裁节点 &quot;uptime&quot; : 136, &quot;lastHeartbeat&quot; : ISODate(&quot;2023-05-10T16:00:33.338Z&quot;), &quot;lastHeartbeatRecv&quot; : ISODate(&quot;2023-05-10T16:00:33.351Z&quot;), &quot;pingMs&quot; : NumberLong(0), &quot;lastHeartbeatMessage&quot; : &quot;&quot;, &quot;syncSourceHost&quot; : &quot;&quot;, &quot;syncSourceId&quot; : -1, &quot;infoMessage&quot; : &quot;&quot;, &quot;configVersion&quot; : 1, &quot;configTerm&quot; : 1 &#125; ], &quot;ok&quot; : 1, &quot;$clusterTime&quot; : &#123; &quot;clusterTime&quot; : Timestamp(1683734432, 3), &quot;signature&quot; : &#123; &quot;hash&quot; : BinData(0,&quot;AAAAAAAAAAAAAAAAAAAAAAAAAAA=&quot;), &quot;keyId&quot; : NumberLong(0) &#125; &#125;, &quot;operationTime&quot; : Timestamp(1683734432, 3)&#125; 集群安全生成 key文件 副本集的读写操作 在主节点上执行写操作 1234567use test;rs:PRIMARY&gt; db.user.insertOne( &#123; userName:&quot;wgf&quot;, sex:1 &#125; )&#123; &quot;acknowledged&quot; : true, &quot;insertedId&quot; : ObjectId(&quot;645bc114a1d8b7b70ea2dbd0&quot;)&#125; 在副本节点查询插入的数据 12345678910111213141516171819rs:SECONDARY&gt; db.user.find(&#123;&#125;)Error: error: &#123; &quot;topologyVersion&quot; : &#123; &quot;processId&quot; : ObjectId(&quot;645bbe1c68a22fb5732aef90&quot;), &quot;counter&quot; : NumberLong(4) &#125;, &quot;ok&quot; : 0, &quot;errmsg&quot; : &quot;not master and slaveOk=false&quot;, // 不是 slave节点，不能读取 &quot;code&quot; : 13435, &quot;codeName&quot; : &quot;NotPrimaryNoSecondaryOk&quot;, &quot;$clusterTime&quot; : &#123; &quot;clusterTime&quot; : Timestamp(1683734919, 1), &quot;signature&quot; : &#123; &quot;hash&quot; : BinData(0,&quot;AAAAAAAAAAAAAAAAAAAAAAAAAAA=&quot;), &quot;keyId&quot; : NumberLong(0) &#125; &#125;, &quot;operationTime&quot; : Timestamp(1683734919, 1)&#125; 发现，不能读取集合的数据。当前从节点只是一个备份，不是奴隶节点，无法读取数据，写当然更不行。因为默认情况下，从节点是没有读写权限的，可以增加读的权限，但需要进行设置。 设置为奴隶节点 设置为奴隶节点，允许在从成员上运行读的操作 12345在副本节点执行rs.slaveOk()或rs.slaveOk(true) 再次在 奴隶节点 执行查询请求，数据已经被同步到副本节点 12rs:SECONDARY&gt; db.user.find(&#123;&#125;)&#123; &quot;_id&quot; : ObjectId(&quot;645bc114a1d8b7b70ea2dbd0&quot;), &quot;userName&quot; : &quot;wgf&quot;, &quot;sex&quot; : 1 &#125; 副本集故障测试 副本节点宕机 关闭 mongod_27021 节点 1docker stop mongod_27021 主节点和仲裁节点对 mongod_27021 的心跳失败。因为主节点还在，因此，没有触发投票选举。 如果此时，在主节点写入数据,写入不受影响 12345db.user.insertOne( &#123; userName:&quot;test&quot;, age:30 &#125; )&#123; &quot;acknowledged&quot; : true, &quot;insertedId&quot; : ObjectId(&quot;645c5203a79b5534f6f3e61a&quot;)&#125; 重新启动 mongod_27021 节点 123rs.slaveOk(true) #设置为奴隶节点db.user.find( &#123; userName:&quot;test&quot; &#125; )&#123; &quot;_id&quot; : ObjectId(&quot;645c5203a79b5534f6f3e61a&quot;), &quot;userName&quot; : &quot;test&quot;, &quot;age&quot; : 30 &#125; 再启动从节点，会发现，主节点写入的数据，会自动同步给从节点。 主节点宕机 关闭 mongod_27020 节点 1docker stop mongod_27020 发现，从节点和仲裁节点对 mongod_27020 的心跳失败，当失败超过10秒，此时因为没有主节点了，会自动发起投票。 mongod_27022 向 mongod_27021 投了一票，mongod_27021 本身自带一票，因此共两票，满足过半原则，mongod_27020 成为新主节点 123456789# 连接新的主节点mongo 192.168.0.181:27021# 插入数据rs:PRIMARY&gt; db.user.insertOne( &#123; userName:&quot;test2&quot;, age:30 &#125; )&#123; &quot;acknowledged&quot; : true, &quot;insertedId&quot; : ObjectId(&quot;645c5bc84518ca10db10c508&quot;)&#125; 重新启动 mongod_27020 节点，发现其变成从节点，会向新的主节点重新同步数据 123456use testrs.slaveOk(true)rs:SECONDARY&gt; db.user.find()&#123; &quot;_id&quot; : ObjectId(&quot;645bc114a1d8b7b70ea2dbd0&quot;), &quot;userName&quot; : &quot;wgf&quot;, &quot;sex&quot; : 1 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;645c5203a79b5534f6f3e61a&quot;), &quot;userName&quot; : &quot;test&quot;, &quot;age&quot; : 30 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;645c5bc84518ca10db10c508&quot;), &quot;userName&quot; : &quot;test2&quot;, &quot;age&quot; : 30 &#125; 服务降级 关闭仲裁节点和从节点 12docker stop mongod_27020docker stop mongod_27022 登录 mongod_27021 节点，查看副本集状态 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115rs:SECONDARY&gt; rs.status()&#123; &quot;set&quot; : &quot;rs&quot;, &quot;date&quot; : ISODate(&quot;2023-05-11T03:13:32.135Z&quot;), &quot;myState&quot; : 2, &quot;term&quot; : NumberLong(3), &quot;syncSourceHost&quot; : &quot;&quot;, &quot;syncSourceId&quot; : -1, &quot;heartbeatIntervalMillis&quot; : NumberLong(2000), &quot;majorityVoteCount&quot; : 2, &quot;writeMajorityCount&quot; : 2, &quot;votingMembersCount&quot; : 3, &quot;writableVotingMembersCount&quot; : 2, &quot;optimes&quot; : &#123; &quot;lastCommittedOpTime&quot; : &#123; &quot;ts&quot; : Timestamp(1683774683, 1), &quot;t&quot; : NumberLong(3) &#125;, &quot;lastCommittedWallTime&quot; : ISODate(&quot;2023-05-11T03:11:23.552Z&quot;), &quot;readConcernMajorityOpTime&quot; : &#123; &quot;ts&quot; : Timestamp(1683774683, 1), &quot;t&quot; : NumberLong(3) &#125;, &quot;appliedOpTime&quot; : &#123; &quot;ts&quot; : Timestamp(1683774703, 1), &quot;t&quot; : NumberLong(3) &#125;, &quot;durableOpTime&quot; : &#123; &quot;ts&quot; : Timestamp(1683774703, 1), &quot;t&quot; : NumberLong(3) &#125;, &quot;lastAppliedWallTime&quot; : ISODate(&quot;2023-05-11T03:11:43.552Z&quot;), &quot;lastDurableWallTime&quot; : ISODate(&quot;2023-05-11T03:11:43.552Z&quot;) &#125;, &quot;lastStableRecoveryTimestamp&quot; : Timestamp(1683774683, 1), &quot;members&quot; : [ &#123; &quot;_id&quot; : 0, &quot;name&quot; : &quot;192.168.0.181:27020&quot;, &quot;health&quot; : 0, &quot;state&quot; : 8, &quot;stateStr&quot; : &quot;(not reachable/healthy)&quot;, &quot;uptime&quot; : 0, &quot;optime&quot; : &#123; &quot;ts&quot; : Timestamp(0, 0), &quot;t&quot; : NumberLong(-1) &#125;, &quot;optimeDurable&quot; : &#123; &quot;ts&quot; : Timestamp(0, 0), &quot;t&quot; : NumberLong(-1) &#125;, &quot;optimeDate&quot; : ISODate(&quot;1970-01-01T00:00:00Z&quot;), &quot;optimeDurableDate&quot; : ISODate(&quot;1970-01-01T00:00:00Z&quot;), &quot;lastAppliedWallTime&quot; : ISODate(&quot;2023-05-11T03:11:23.552Z&quot;), &quot;lastDurableWallTime&quot; : ISODate(&quot;2023-05-11T03:11:23.552Z&quot;), &quot;lastHeartbeat&quot; : ISODate(&quot;2023-05-11T03:13:32.067Z&quot;), &quot;lastHeartbeatRecv&quot; : ISODate(&quot;2023-05-11T03:11:25.994Z&quot;), &quot;pingMs&quot; : NumberLong(0), &quot;lastHeartbeatMessage&quot; : &quot;Error connecting to 192.168.0.181:27020 :: caused by :: Connection refused&quot;, &quot;syncSourceHost&quot; : &quot;&quot;, &quot;syncSourceId&quot; : -1, &quot;infoMessage&quot; : &quot;&quot;, &quot;configVersion&quot; : 1, &quot;configTerm&quot; : 3 &#125;, &#123; &quot;_id&quot; : 1, &quot;name&quot; : &quot;192.168.0.181:27021&quot;, &quot;health&quot; : 1, &quot;state&quot; : 2, &quot;stateStr&quot; : &quot;SECONDARY&quot;, &quot;uptime&quot; : 2823, &quot;optime&quot; : &#123; &quot;ts&quot; : Timestamp(1683774703, 1), &quot;t&quot; : NumberLong(3) &#125;, &quot;optimeDate&quot; : ISODate(&quot;2023-05-11T03:11:43Z&quot;), &quot;lastAppliedWallTime&quot; : ISODate(&quot;2023-05-11T03:11:43.552Z&quot;), &quot;lastDurableWallTime&quot; : ISODate(&quot;2023-05-11T03:11:43.552Z&quot;), &quot;syncSourceHost&quot; : &quot;&quot;, &quot;syncSourceId&quot; : -1, &quot;infoMessage&quot; : &quot;&quot;, &quot;configVersion&quot; : 1, &quot;configTerm&quot; : 3, &quot;self&quot; : true, &quot;lastHeartbeatMessage&quot; : &quot;&quot; &#125;, &#123; &quot;_id&quot; : 2, &quot;name&quot; : &quot;192.168.0.181:27022&quot;, &quot;health&quot; : 0, &quot;state&quot; : 8, &quot;stateStr&quot; : &quot;(not reachable/healthy)&quot;, &quot;uptime&quot; : 0, &quot;lastHeartbeat&quot; : ISODate(&quot;2023-05-11T03:13:32.048Z&quot;), &quot;lastHeartbeatRecv&quot; : ISODate(&quot;2023-05-11T03:11:37.565Z&quot;), &quot;pingMs&quot; : NumberLong(0), &quot;lastHeartbeatMessage&quot; : &quot;Error connecting to 192.168.0.181:27022 :: caused by :: Connection refused&quot;, &quot;syncSourceHost&quot; : &quot;&quot;, &quot;syncSourceId&quot; : -1, &quot;infoMessage&quot; : &quot;&quot;, &quot;configVersion&quot; : 1, &quot;configTerm&quot; : 3 &#125; ], &quot;ok&quot; : 1, &quot;$clusterTime&quot; : &#123; &quot;clusterTime&quot; : Timestamp(1683774703, 1), &quot;signature&quot; : &#123; &quot;hash&quot; : BinData(0,&quot;AAAAAAAAAAAAAAAAAAAAAAAAAAA=&quot;), &quot;keyId&quot; : NumberLong(0) &#125; &#125;, &quot;operationTime&quot; : Timestamp(1683774703, 1)&#125; 由于副本集存活的节点数少于总节点数的一半，服务进入降级模式，主节点自动降级为副本节点，此时的服务只能够读取不能够写入。当重新加入的节点数过半后，重新选举主节点，副本集写入功能恢复 连接副本集 连接副本集写法 1mongodb://&lt;replica-set-hostname&gt;/&lt;database-name&gt;?replicaSet=&lt;replica-set-name&gt; mongod 命令连接副本集 1mongo &quot;mongodb://192.168.0.181:27021,192.168.0.181:27021,192.168.0.181:27022/test?replicaSet=rs&quot; 分片集群 分片 是一种将数据分配到多个机器上的方法。MongoDB通过分片技术来支持具有海量数据集和高吞吐量操作的部署方案。 数据库系统的数据集或应用的吞吐量比较大的情况下，会给单台服务器的处理能力带来极大的挑战。例如，高查询率会耗尽服务器的CPU资源。工作的数据集大于系统的内存压力、磁盘驱动器的I/O容量。 分片是水平扩展的一种方案，通过将系统数据集划分至多台机器，并根据需要添加服务器来提升容量。虽然单个机器的总体速度或容量可能不高，但每台机器只需处理整个数据集的某个子集，所以可能会提供比单个高速大容量服务器更高的效率，而且机器的数量只需要根据数据集大小来进行扩展，与单个机器的高端硬件相比，这个方案可以降低总体成本。不过，这种方式会提高基础设施部署维护的复杂性。 分片集群组件 分片：每个分片都包含分片数据的一个子集。从MongoDB 3.6开始，分片必须部署为副本集。 路由器： mongos 充当查询路由器，在客户端应用程序和分片集群之间提供接口。从MongoDB 4.4开始， mongos 可以支持 对冲读取 以最大程度地减少延迟。 配置服务器：配置服务器存储群集的元数据和配置设置。从MongoDB 3.4开始，配置服务器必须部署为副本集（CSRS） 分片集群角色 角色 描述 configsvr 该角色表示当前节点是分片集群的配置服务器。配置服务器负责管理集群的元数据信息，如分片键、分片状态等等。 shardsvr 该角色表示当前节点是一个数据分片节点。数据分片节点存储了实际的数据，并负责对数据进行读写操作。 mongos 该角色表示当前节点是 mongos 路由器节点。mongos 负责将客户端请求路由到正确的分片节点上，并合并结果返回给客户端。 集群搭建 参考搭建 创建第一个分片副本集 1234567891011121314151617181920212223242526docker run --name mongo_rs1_1 --network bridge -d mongo --replSet rs1 --shardsvr --port 27017docker run --name mongo_rs1_2 --network bridge -d mongo --replSet rs1 --shardsvr --port 27017docker run --name mongo_rs1_3 --network bridge -d mongo --replSet rs1 --shardsvr --port 27017# 查看3个容器的ip地址docker inspect 容器ID | grep IPAddress# ip分别为172.17.0.2172.17.0.3172.17.0.4docker exec -it mongo_rs1_1 bashmongo# 初始化副本集rs.initiate( &#123; _id:&quot;rs1&quot;, members:[ &#123;_id:0,host:&quot; 172.17.0.2:27017&quot;&#125;, &#123;_id:1,host:&quot; 172.17.0.3:27017&quot;&#125;, &#123;_id:2,host:&quot; 172.17.0.4:27017&quot;&#125; ] &#125;) 创建第二个分片副本集 1234567891011121314151617181920212223242526docker run --name mongo_rs2_1 --network bridge -d mongo --replSet rs2 --shardsvr --port 27017docker run --name mongo_rs2_2 --network bridge -d mongo --replSet rs2 --shardsvr --port 27017docker run --name mongo_rs2_3 --network bridge -d mongo --replSet rs2 --shardsvr --port 27017# 查看3个容器的ip地址docker inspect 容器ID | grep IPAddress# ip分别为172.17.0.5172.17.0.6172.17.0.7docker exec -it mongo_rs2_1 bashmongo# 初始化副本集rs.initiate( &#123; _id:&quot;rs2&quot;, members:[ &#123;_id:0,host:&quot; 172.17.0.5:27017&quot;&#125;, &#123;_id:1,host:&quot; 172.17.0.6:27017&quot;&#125;, &#123;_id:2,host:&quot; 172.17.0.7:27017&quot;&#125; ] &#125;) 创建配置服务副本集 1234567891011121314151617181920212223242526docker run --name mongo_config_1 --network bridge -d mongo --replSet rsconfig --configsvr --port 27017docker run --name mongo_config_2 --network bridge -d mongo --replSet rsconfig --configsvr --port 27017docker run --name mongo_config_3 --network bridge -d mongo --replSet rsconfig --configsvr --port 27017# 查看3个容器的ip地址docker inspect 容器ID | grep IPAddress# ip分别为172.17.0.8172.17.0.9172.17.0.10docker exec -it mongo_config_1 bashmongo# 初始化副本集rs.initiate( &#123; _id:&quot;rsconfig&quot;, members:[ &#123;_id:0,host:&quot; 172.17.0.8:27017&quot;&#125;, &#123;_id:1,host:&quot; 172.17.0.9:27017&quot;&#125;, &#123;_id:2,host:&quot; 172.17.0.10:27017&quot;&#125; ] &#125;) 创建路由服务 12345docker run -p 28000:27018 --name mongo_router_1 --network bridge -d mongo --bind_ip 0.0.0.0docker exec -it mongo_router_1 bash# 启动mongo路由服务，指向配置服务器地址mongos --configdb rsconfig/172.17.0.8:27017,172.17.0.9:27017,172.17.0.10:27017 --port 27018 --bind_ip 0.0.0.0 打开新的shell 1234docker exec -it mongo_router_1 bash# 连接mongo路由mongo --port 27018 1234567891011121314151617use testdb.user.insert( &#123;userName:1&#125; )WriteCommandError(&#123; &quot;ok&quot; : 0, &quot;errmsg&quot; : &quot;Database test could not be created :: caused by :: No shards found&quot;, &quot;code&quot; : 70, &quot;codeName&quot; : &quot;ShardNotFound&quot;, &quot;$clusterTime&quot; : &#123; &quot;clusterTime&quot; : Timestamp(1683909745, 1), &quot;signature&quot; : &#123; &quot;hash&quot; : BinData(0,&quot;AAAAAAAAAAAAAAAAAAAAAAAAAAA=&quot;), &quot;keyId&quot; : NumberLong(0) &#125; &#125;, &quot;operationTime&quot; : Timestamp(1683909745, 1)&#125;) 插入失败的原因：通过路由节点操作，现在只是连接了配置节点，还没有连接分片数据节点，因此无法写入业务数据。 在路由节点上进行分片操作 将第一套分片副本集添加进来 使用语法 1sh.addShard(&quot;副本集名称/IP:Port&quot;) 1234567891011121314151617mongo --port 27018sh.addShard(&quot;rs1/172.17.0.2:27017,172.17.0.3:27017,172.17.0.4:27017&quot;)&#123; &quot;shardAdded&quot; : &quot;rs1&quot;, &quot;ok&quot; : 1, &quot;$clusterTime&quot; : &#123; &quot;clusterTime&quot; : Timestamp(1684141648, 4), &quot;signature&quot; : &#123; &quot;hash&quot; : BinData(0,&quot;AAAAAAAAAAAAAAAAAAAAAAAAAAA=&quot;), &quot;keyId&quot; : NumberLong(0) &#125; &#125;, &quot;operationTime&quot; : Timestamp(1684141648, 4)&#125; 查看分片状态 1234567891011121314151617181920212223242526272829mongos&gt; sh.status()--- Sharding Status --- sharding version: &#123; &quot;_id&quot; : 1, &quot;minCompatibleVersion&quot; : 5, &quot;currentVersion&quot; : 6, &quot;clusterId&quot; : ObjectId(&quot;645e6ad0efcc6b2b53143174&quot;) &#125; shards: &#123; &quot;_id&quot; : &quot;rs1&quot;, &quot;host&quot; : &quot;rs1/172.17.0.2:27017,172.17.0.3:27017,172.17.0.4:27017&quot;, &quot;state&quot; : 1, &quot;topologyTime&quot; : Timestamp(1684141648, 1) &#125; active mongoses: &quot;5.0.5&quot; : 1 autosplit: Currently enabled: yes balancer: Currently enabled: yes Currently running: no Failed balancer rounds in last 5 attempts: 0 Migration results for the last 24 hours: No recent migrations databases: &#123; &quot;_id&quot; : &quot;config&quot;, &quot;primary&quot; : &quot;config&quot;, &quot;partitioned&quot; : true &#125; config.system.sessions shard key: &#123; &quot;_id&quot; : 1 &#125; unique: false balancing: true chunks: rs1 1024 too many chunks to print, use verbose if you want to force print 添加第二套分片副本集 123456789101112131415sh.addShard(&quot;rs2/172.17.0.5:27017,172.17.0.6:27017,172.17.0.7:27017&quot;)&#123; &quot;shardAdded&quot; : &quot;rs2&quot;, &quot;ok&quot; : 1, &quot;$clusterTime&quot; : &#123; &quot;clusterTime&quot; : Timestamp(1684142090, 4), &quot;signature&quot; : &#123; &quot;hash&quot; : BinData(0,&quot;AAAAAAAAAAAAAAAAAAAAAAAAAAA=&quot;), &quot;keyId&quot; : NumberLong(0) &#125; &#125;, &quot;operationTime&quot; : Timestamp(1684142090, 4)&#125; 查看分片状态 12345678910111213141516171819202122232425262728293031mongos&gt; sh.status()--- Sharding Status --- sharding version: &#123; &quot;_id&quot; : 1, &quot;minCompatibleVersion&quot; : 5, &quot;currentVersion&quot; : 6, &quot;clusterId&quot; : ObjectId(&quot;645e6ad0efcc6b2b53143174&quot;) &#125; shards: &#123; &quot;_id&quot; : &quot;rs1&quot;, &quot;host&quot; : &quot;rs1/172.17.0.2:27017,172.17.0.3:27017,172.17.0.4:27017&quot;, &quot;state&quot; : 1, &quot;topologyTime&quot; : Timestamp(1684141648, 1) &#125; &#123; &quot;_id&quot; : &quot;rs2&quot;, &quot;host&quot; : &quot;rs2/172.17.0.5:27017,172.17.0.6:27017,172.17.0.7:27017&quot;, &quot;state&quot; : 1, &quot;topologyTime&quot; : Timestamp(1684142090, 2) &#125; active mongoses: &quot;5.0.5&quot; : 1 autosplit: Currently enabled: yes balancer: Currently enabled: yes Currently running: no Failed balancer rounds in last 5 attempts: 0 Migration results for the last 24 hours: 30 : Success databases: &#123; &quot;_id&quot; : &quot;config&quot;, &quot;primary&quot; : &quot;config&quot;, &quot;partitioned&quot; : true &#125; config.system.sessions shard key: &#123; &quot;_id&quot; : 1 &#125; unique: false balancing: true chunks: rs1 994 rs2 30 too many chunks to print, use verbose if you want to force print 如果添加分片失败，需要先手动移除分片，检查添加分片的信息的正确性后，再次添加分片。移除分片参考：1db.runCommand( &#123; removeShard: &quot;rs1&quot; &#125; )如果只剩下最后一个shard，是无法删除的，移除时会自动转移分片数据，需要一个时间过程。完成后，再次执行删除分片命令才能真正删除。 数据库开启分片功能 sh.enableSharding(database, primaryShard) 显式创建数据库。使用 mongosh 方法 sh.shardCollection() 对数据库上的集合进行分片（从 MongoDB 6.0 开始，对集合进行分片不需要此方法） sh.shardCollection(namespace, key, unique, options) 使用 key 作为分片键对集合进行分片。分片键确定MongoDB如何在分片之间分发集合的文档 1234567891011121314mongos&gt; sh.enableSharding(&quot;test&quot;)&#123; &quot;ok&quot; : 1, &quot;$clusterTime&quot; : &#123; &quot;clusterTime&quot; : Timestamp(1684143030, 5), &quot;signature&quot; : &#123; &quot;hash&quot; : BinData(0,&quot;AAAAAAAAAAAAAAAAAAAAAAAAAAA=&quot;), &quot;keyId&quot; : NumberLong(0) &#125; &#125;, &quot;operationTime&quot; : Timestamp(1684143030, 4)&#125; 将名为 “mydb” 的数据库中的 “mycollection” 集合按照 “_id” 字段进行分片 1sh.shardCollection(&quot;mydb.mycollection&quot;, &#123;_id: 1&#125;) 分片键和分片策略 分片键可以是单个索引字段，也可以是复合索引涵盖的多个字段，复合索引确定集合文档在集群分片中的分布。 MongoDB将分片键值（或使用 Hash散列分片键值）的范围划分为分片键值的非重叠范围。每个范围都与一个块（ chunk）相关联。 chunk特定分片内连续的分片键值范围。区块范围包括下边界，不包括上限。MongoDB在块增长超过配置的块大小（默认情况下为128MB）时会拆分块当一个分片包含相对于其他分片的集合的区块过多时，MongoDB会迁移块。参考 分片集群负载均衡 分片键的选择： 分片键的基数 分片键的基数代表分片键最大能有多少个块（chunk） 分片键频率 考虑分片键值在文档出现的频率，尽量选择重复出现频率不会差异很大的字段 非单调变更 在范围分片时，单调递增的主键要注意minKey和maxKey的分片，如果单调递增，大部分数据可能会在maxKey的数据块上 Hash分片策略 哈希分片使用单字段哈希索引或复合哈希索引（4.4 版新功能）作为分片键，用于跨分片集群对数据进行分区。如无范围查询需求，使用 _id 进行Hash分片是一个不错的选择。 下面的例子是使用nickname作为片键，根据其值的哈希值进行数据分片 1234567891011121314mongos&gt; sh.shardCollection(&quot;articledb.comment&quot;,&#123;&quot;nickname&quot;:&quot;hashed&quot;&#125;)&#123; &quot;collectionsharded&quot; : &quot;articledb.comment&quot;, &quot;collectionUUID&quot; : UUID(&quot;ddea6ed8-ee61-4693-bd16-196acc3a45e8&quot;), &quot;ok&quot; : 1, &quot;operationTime&quot; : Timestamp(1564612840, 28), &quot;$clusterTime&quot; : &#123; &quot;clusterTime&quot; : Timestamp(1564612840, 28), &quot;signature&quot; : &#123; &quot;hash&quot; : BinData(0,&quot;AAAAAAAAAAAAAAAAAAAAAAAAAAA=&quot;), &quot;keyId&quot; : NumberLong(0) &#125; &#125;&#125; 范围分片策略（默认） 基于范围的分片会将数据划分为由片键值确定的连续范围。 在此模型中，具有 相邻 分片键值的文档可能位于相同的块或分片中。 这有 效提高范围查询效率，但是如果分片键没选择好，则容易导致数据倾斜。 每个chunk的数据都存储在同一个Shard上，每个Shard可以存储很多个chunk，chunk存储在哪个shard的信息会存储在Config server中，mongos也会根据各个shard上的chunk的数量来自动做负载均衡。 下例是使用年龄进行范围分片 1234567891011121314mongos&gt; sh.shardCollection(&quot;articledb.author&quot;,&#123;&quot;age&quot;:1&#125;)&#123; &quot;collectionsharded&quot; : &quot;articledb.author&quot;, &quot;collectionUUID&quot; : UUID(&quot;9a47bdaa-213a-4039-9c18-e70bfc369df7&quot;), &quot;ok&quot; : 1, &quot;operationTime&quot; : Timestamp(1567512803, 13), &quot;$clusterTime&quot; : &#123; &quot;clusterTime&quot; : Timestamp(1567512803, 13), &quot;signature&quot; : &#123; &quot;hash&quot; : BinData(0,&quot;eE9QT5yE5sL1Tyr7+3U8GRy5+5Q=&quot;), &quot;keyId&quot; : NumberLong(&quot;6732061237309341726&quot;) &#125; &#125;&#125; 一旦对一个集合分片，分片键和分片值就不可改变。 集合进行分片 语法： 1sh.shardCollection(namespace, key, unique, options) Parameter Type Description namespace string 要分片的集合的命名空间，格式为 &quot;&lt;database&gt;.&lt;collection&gt;&quot; key document 指定要用作分片键的一个或多个字段的文档；字段值为1，用于范围分片；字段值为 “hash”，用于hash分片；分片键必须由索引支持。除非集合为空，否则索引必须在shardCollection 命令之前存在。如果集合为空，MongoDB会在分片集合之前创建索引，如果可以支持分片键的索引尚不存在。 unique boolean 可选，指定 true 以确保基础索引强制实施唯一约束。默认为 false options 可选，包含可选字段的文档，包括 numInitialChunks 和 collation 对 test 数据库的 user 表进行hash分片 1234567891011121314mongos&gt; sh.shardCollection(&quot;test.user&quot;, &#123;&quot;_id&quot;: &quot;hashed&quot;&#125;)&#123; &quot;collectionsharded&quot; : &quot;test.user&quot;, &quot;ok&quot; : 1, &quot;$clusterTime&quot; : &#123; &quot;clusterTime&quot; : Timestamp(1684150040, 29), &quot;signature&quot; : &#123; &quot;hash&quot; : BinData(0,&quot;AAAAAAAAAAAAAAAAAAAAAAAAAAA=&quot;), &quot;keyId&quot; : NumberLong(0) &#125; &#125;, &quot;operationTime&quot; : Timestamp(1684150040, 28)&#125; 插入数据测试 123456789101112use testmongos&gt; for (var i=1; i&lt;=100; i++) &#123; db.user.insertOne( &#123; _id: i+&quot;&quot;, userName: &quot;wgf_&quot;+i &#125; ) &#125; &#123; &quot;acknowledged&quot; : true, &quot;insertedId&quot; : &quot;100&quot; &#125; 登陆第一个副本集 mongo_rs1_1 123456docker exec -it mongo_rs1_1 bashmongouse testrs1:PRIMARY&gt; db.user.find().count()58 登陆第二个副本集 mongo_rs2_1 123456docker exec -it mongo_rs2_1 bashmongouse tests2:PRIMARY&gt; db.user.find().count()42 可以看到，100条数据近似均匀的分布到了2个shard上。是根据片键的哈希值分配的。 分片负载均衡 MongoDB 平衡器是一个后台进程，用于监控每个分片集合的每个分片上的数据量。当给定分片上分片集合的数据量达到特定的 迁移阈值 时，平衡器会尝试在分片之间自动迁移数据，并在遵守区域的情况下达到每个分片的均匀数据量。默认情况下，始终启用平衡器进程。 分片集群的平衡过程对用户和应用程序层是无感知的，但是在执行该过程时可能会对性能产生一些影响。 MongoDB可以执行并行数据迁移，但一个分片一次最多可以参与一个迁移。对于具有 n 个分片的分片集群，MongoDB 最多可以同时执行 n/2（向下舍入）迁移 迁移阈值 为了最大程度地减少均衡对集群的影响，均衡器仅在分片集合的数据分布达到特定阈值后才开始自我平衡。 如果分片之间的数据差异（对于该集合）小于集合配置的范围大小的 三倍，则认为集合是平衡的。对于默认范围大小 128MB ，对于给定集合，两个分片的数据大小差异必须至少为 384MB 才能进行迁移。 分片集群与SpringBoot时，连接的是Router节点1234spring: data: mongodb: uri: mongodb://172.17.0.11:27018/test # 多个逗号分隔 事务 事务支持和被限制的操作 在MongoDB中，对单个文档的操作是原子的。由于可以使用嵌入式文档和数组来组织单个文档结构中数据之间的关系，而不是跨多个文档和集合进行规范化（范式），因此这种单文档原子性消除了许多实际用例对多文档事务的需求。 对于需要对多个文档（在单个或多个集合中）进行读取和写入的原子性的情况，MongoDB支持多文档事务。使用分布式事务可以跨多个操作、集合、数据库、文档和分片使用。 分布式事务和多文档事务从MongoDB 4.2开始，这两个术语是同义词。分布式事务是指分片集群和副本集上的多文档事务。多文档事务（无论是在分片集群还是副本集上）也称为从MongoDB 4.2开始的分布式事务。分布式事务是非常影响数据库性能的，官方推荐尽量使用反范式的设计规避分布式事务问题，不建议在Mongo大量使用多文档事务。 事务和原子性 对于需要读取和写入多个文档（在单个或多个集合中）的原子性的情况，MongoDB支持多文档事务： 在4.0版本中，MongoDB支持副本集上的多文档事务。 在 4.2 版本中，MongoDB 引入了分布式事务，它增加了对分片集群上的多文档事务的支持，并结合了对副本集上多文档事务的现有支持。 多文档事务是原子的（即提供 全有或全无 的语义）： 当一个事务提交后，所有数据的修改都会被保存并且变得可见。也就是说，在一个事务中，它所做的所有更改要么全部提交，要么全部回滚，保证了数据的一致性。 在一个事务提交之前，该事务所做的数据更改在事务外部是不可见的。 然而，当一个事务向多个Shard写入时，并不是所有的外部读操作都需要等待事务提交结果后在所有分片上都可见才能进行。 例如，如果一个事务已经提交，并且在Shard A上的写入1可见，但在Shard B上的写入2还不能被看到，对于使用 &quot;local&quot; 读关注点的外部读取操作可以读取到写入1的结果，而不会看到写入2（Seata也是如此，为了提高读取性能，全局事务未提交下，允许外部读取提交的分支事务）。 当一个事务中止时，所有在该事务中作出的数据更改都将被回滚。 事务和操作 分布式事务可以跨多个操作、集合、数据库、文档以及从 MongoDB 4.2 开始的 分片 中使用。 关于事务： 可以对现有集合指定读/写 （CRUD） 操作。 MongoDB 4.4开始，可以在 事务中创建集合和索引；但是，不能在跨分片的写事务中创建新集合。例如，如果你想对一个分片中已存在的集合进行写入且在另外一个不同的分片中隐式地创建集合，那么MongoDB无法在同一事务中执行这两种操作。 事务中使用的集合可以位于不同的数据库中 capped collections 不能使用事务 MongoDB 5.0开始，capped collections 不能使用取关注点 &quot;snapshot&quot; 无法返回支持的操作的查询计划（即 explain ) 对于在事务外部创建的游标，不能在事务内部调用 getMore 对于在事务中创建的游标，不能在事务外部调用 getMore 事务与会话 事务是与某个会话相关联的；即你为一个会话启动一个事务。 在任何给定时间，一个会话最多可以有一个打开的事务。 使用驱动程序时，事务中的每个操作都必须与会话相关联。 如果一个会话结束了并且它有一个打开的事务，则事务会中止。 事务和读取偏好 在使用驱动时，可以在事务开始时设置事务级别的 read preference ： 如果事务级别的读偏好没有设置，事务级的读偏好默认为会话级的读偏好 如果未设置事务级别和会话级别读取偏好，则事务将使用客户端级别的读取偏好。默认情况下，客户端级别的读取偏好项为 primary 包含读取操作的多文档事务必须使用 primary 读取偏好。给定事务中的所有操作都必须路由到同一成员。 事务和读取关注（事务读隔离） 事务中的操作使用事务级读取关注点。也就是说，在 集合 和 数据库 级别设置的任何读取关注点都将在事务中被忽略。 可以在事务启动时设置事务级读取关注点 如果未设置事务级读取关注点，则事务级读取关注点默认为会话级读取关注点。 如果未设置事务级和会话级读取关注点，则事务级读取关注点默认为客户端级读取关注点。默认情况下，对于针对主数据库的读取，客户端级读取关注点为 &quot;local&quot; 在分布式事务中，local 意味着 读未提交，因为它返回节点最新的可用数据，在分片集群事务中，它可以读取到分支事务（分片事务）提交而全局事务未提交的更改数据 读取关注 说明 &quot;local（默认）&quot; 读取关注点 &quot;local&quot; 返回节点中可用的最新数据，但可以回滚（读未提交）。对于分片集群上的事务， &quot;local&quot; 读取关注点无法保证数据来自同一个跨分片的快照视图，如果需要快照隔离，请使用 &quot;snapshot&quot; 读取关注点MongoDB 4.4开始，可以在事务中创建集合和索引。如果显式创建集合或索引，则事务必须使用读取关注点 &quot;local&quot; 。 集合的隐式创建可以使用可用于事务的任何读取关注点。 &quot;majority&quot; 读取关注点 &quot;majority&quot; 返回已由大多数副本集成员确认的数据（即数据无法回滚，读已提交），如果事务以写关注点&quot;majority&quot;提交对于分片集群上的事务， &quot;local&quot; 读取关注点无法保证数据来自分片上的同一快照视图（视图未完成同步），如果需要快照隔离，请使用 &quot;snapshot&quot; 读取关注点 &quot;snapshot&quot; 如果事务以写入关注点&quot;majority&quot;提交，则读取关注点 &quot;snapshot&quot; 会从一个大多数已提交数据的快照中返回数据（可重复读）。对于分片集群上的事务，数据的 &quot;snapshot&quot; 视图在分片之间同步。 事务和写入关注（写一致性） 隐式默认写关注 MongoDB事务中的写入操作必须使用默认的写入关注，并在提交时使用事务级别的写入关注来提交写入操作。 注意不要为事务中的各个写入操作显式设置写入关注点。为事务中的各个写入操作设置写入关注点会导致异常。 可以在事务启动时设置事务级写入关注点： 写关注点 说明 w: 1 使用 w: 1 写入关注点提交时，事务级 &quot;majority&quot; 读取关注点无法保证事务中的读取操作能读取大多数已提交的数据。使用 w: 1 写入关注点提交时，事务级 &quot;snapshot&quot; 读取关注不保证事务中的读取操作使用多数提的交快照数据。 w: &quot;majority(默认)&quot; 在提交已应用于多数（ M）有投票权的成员后，写关注 w: &quot;majority&quot; ;即提交到主节点和（M-1）个辅助节点。使用 w: &quot;majority&quot; 写入关注点提交时，事务级 &quot;majority&quot; 读关注会保证操作已经读取了大多数提交的数据。 对于分片群集上的事务，大多数提交的数据的视图不会在分片之间同步。使用 w: &quot;majority&quot; 写入关注点提交时，事务级 &quot;snapshot&quot; 读取关注点可确保操作来自大多数提交数据的同步快照。 执行时长限制 默认情况下，事务的运行时间必须小于 一分钟。您可以使用 transactionLifetimeLimitSeconds 对 mongod 实例修改此限制。对于分片集群，必须修改所有分片副本集成员的参数。超过此限制的事务被视为已过期，并将通过定期清理过程中止。 Oplog大小限制 MongoDB根据需要创建尽可能多的oplog条目来封装事务中的所有写入操作，而不是为事务中的所有写入操作创建一个条目。这将删除单个 oplog 条目对其所有写入操作施加的事务的 16MB 总大小限制。尽管删除了总大小限制，但每个oplog条目仍必须在16MB的BSON文档大小限制范围内。 获取锁 默认情况下，事务最多等待 5 毫秒来获取事务中操作所需的锁。如果事务无法在 5 毫秒内获取其所需的锁，则事务将中止。 事务在中止或提交时释放所有锁。 在开始事务之前立即创建或删除集合时，如果需要在事务内访问该集合，则在进行创建或删除操作时使用写关注 &quot;majority&quot; 可以保证事务能获取到请求的锁。 可以使用 maxTransactionLockRequestTimeoutMillis 参数来调整事务等待获取锁的时间。增加 maxTransactionLockRequestTimeoutMillis 允许事务中的操作等待指定的时间来获取所需的锁。这有助于避免在瞬时并发锁获取（如快速运行的元数据操作）上中止事务。但是，这可能会延迟死锁事务操作的中止。 正在进行的事务和写入冲突 如果事务正在进行中，但事务外部的写入修改了该事务之后尝试修改的文档（写冲突），则事务会因写入冲突而中止。 如果一个事务正在进行并且已经锁定修改文档，那么当事务外部的写操作试图修改同一个文档时，写操作会一直等到事务结束。 正在进行的事务和过时的读取 事务内的读取操作可能会返回历史版本数据。也就是说，事务内的读操作不能保证看到其他已提交的事务或非事务性写入的内容。例如，假设有以下操作顺序： 事务正在进行中 事务外部的写入删除文档 事务内部的读取操作能够读取现在删除的文档，因为该操作使用的是写入之前的快照。 为避免事务内部单个文档的读取过时，可以使用 db.collection.findOneAndUpdate() 方法。例如： 123456789session.startTransaction( &#123; readConcern: &#123; level: &quot;snapshot&quot; &#125;, writeConcern: &#123; w: &quot;majority&quot; &#125; &#125; );employeesCollection = session.getDatabase(&quot;hr&quot;).employees;employeeDoc = employeesCollection.findOneAndUpdate( &#123; _id: 1, employee: 1, status: &quot;Active&quot; &#125;, &#123; $set: &#123; employee: 1 &#125; &#125;, &#123; returnNewDocument: true &#125;); 如果员工文档在事务之外发生更改，则事务将中止。 如果员工文档未更改，则事务将返回该文档并锁定该文档。 正在进行的事务和块迁移 区块迁移 在某些阶段获取独占集合锁。 如果正在进行的事务持有集合上的锁，并且涉及该集合的块迁移刚开始，则这些迁移阶段必须等待事务释放集合上的锁，从而会影响块迁移的性能。 如果块迁移与事务交错进行（例如，如果事务在块迁移正在进行时开始，并且迁移在事务锁定集合之前完成），则事务在提交期间出错并中止。 提交期间的外部读取 在事务提交期间，外部的读操作可能会尝试读取将被事务修改的相同文档。如果事务写入多个分片，则在跨分片提交尝试期间。 使用读取关注点 &quot;snapshot&quot; 或 &quot;linearizable&quot; 的外部读取，或者是因果一致性会话的一部分（即包括 afterClusterTime）等待事务的所有写入都可见。 使用其他读取关注点的外部读取不会等待事务的所有写入都可见，而是读取可用文档的事务的历史版本。 事务操作 Spring Data MongoDB 事务整合 SpringBoot整合 123456789101112131415161718192021222324252627&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-mongodb&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; Mongo配置类 12345678910111213141516171819202122232425262728293031323334@Configuration@RequiredArgsConstructorpublic class MongoConfig &#123; private final MongoDatabaseFactory mongoDbFactory; private final MongoMappingContext mongoMappingContext; /** * 转换类配置 * * @return 转换类 */ @Bean public MappingMongoConverter mappingMongoConverter() &#123; DbRefResolver dbRefResolver = new DefaultDbRefResolver(mongoDbFactory); MappingMongoConverter converter = new MappingMongoConverter(dbRefResolver, mongoMappingContext); //不保存 _class 属性到mongo converter.setTypeMapper(new DefaultMongoTypeMapper(null)); return converter; &#125; /** * 定义事务管理器 * * @param mongoDbFactory * @return */ @Bean public MongoTransactionManager mongoTransactionManager(MongoDatabaseFactory mongoDatabaseFactory) &#123; // 设置事务级别的读/写关注级别，目前副本集就三个节点，写关注故意配置为4 TransactionOptions transactionOptions = TransactionOptions.builder().readConcern(ReadConcern.LOCAL).writeConcern(new WriteConcern(4)).build(); return new MongoTransactionManager(mongoDatabaseFactory, transactionOptions); &#125;&#125; 单元测试 使用上一章节做好分片的 user 集合 12345678910111213141516171819202122@SpringBootTestpublic class TransactionTest &#123; @Autowired private UserService userService; @Test public void insertTest() &#123; User user1 = new User(); user1.setId(UUID.randomUUID().toString()) .setUserName(UUID.randomUUID().toString()); User user2 = new User(); user2.setId(UUID.randomUUID().toString()) .setUserName(UUID.randomUUID().toString()); User user3 = new User(); user3.setId(UUID.randomUUID().toString()) .setUserName(UUID.randomUUID().toString()); this.userService.insert(user1, user2, user3); &#125;&#125; 123456789101112131415161718@Service@RequiredArgsConstructorpublic class UserService &#123; private final MongoTemplate mongoTemplate; /** * 分片集群事务 * @param user1 * @param user2 * @param user3 */ @Transactional(value = &quot;mongoTransactionManager&quot;) public void insert(User user1, User user2, User user3) &#123; mongoTemplate.insert(user1); mongoTemplate.insert(user2); mongoTemplate.insert(user3); &#125;&#125; 报错是因为写关注没有足够的数据节点承载，原因是副本集只有三个数据节点，事务配置正常 分布式事务回滚测试 123456@Beanpublic MongoTransactionManager mongoTransactionManager(MongoDatabaseFactory mongoDatabaseFactory) &#123; // 设置事务级别的读/写关注级别 TransactionOptions transactionOptions = TransactionOptions.builder().readConcern(ReadConcern.LOCAL).writeConcern(WriteConcern.MAJORITY).build(); return new MongoTransactionManager(mongoDatabaseFactory, transactionOptions);&#125; 修改事务级别的写关注为 MAJORITY 单元测试 12345678910111213141516@Testpublic void insertErrorTest() &#123; User user1 = new User(); user1.setId(UUID.randomUUID().toString()) .setUserName(UUID.randomUUID().toString()); User user2 = new User(); user2.setId(UUID.randomUUID().toString()) .setUserName(UUID.randomUUID().toString()); User user3 = new User(); user3.setId(UUID.randomUUID().toString()) .setUserName(UUID.randomUUID().toString()); this.userService.insertError(user1, user2, user3);&#125; UserService 添加事务回滚方法 12345678@Transactional(value = &quot;mongoTransactionManager&quot;, rollbackFor = Exception.class)public void insertError(User user1, User user2, User user3) &#123; mongoTemplate.insert(user1); mongoTemplate.insert(user2); mongoTemplate.insert(user3); // 抛出异常，让分片事务回滚 throw new RuntimeException();&#125; 异常，事务终止提交 数据没有新增到Mongodb 生产可用配置 123456@Beanpublic MongoTransactionManager mongoTransactionManager(MongoDatabaseFactory mongoDatabaseFactory) &#123; // 设置事务级别的读/写关注级别 TransactionOptions transactionOptions = TransactionOptions.builder().readConcern(ReadConcern.SNAPSHOT).writeConcern(WriteConcern.MAJORITY).build(); return new MongoTransactionManager(mongoDatabaseFactory, transactionOptions);&#125; 123456@Transactional(value = &quot;mongoTransactionManager&quot;)public void insert(User user1, User user2, User user3) &#123; mongoTemplate.insert(user1); mongoTemplate.insert(user2); mongoTemplate.insert(user3);&#125; 单元测试 12345678910111213141516@Testpublic void insertErrorTest() &#123; User user1 = new User(); user1.setId(UUID.randomUUID().toString()) .setUserName(UUID.randomUUID().toString()); User user2 = new User(); user2.setId(UUID.randomUUID().toString()) .setUserName(UUID.randomUUID().toString()); User user3 = new User(); user3.setId(UUID.randomUUID().toString()) .setUserName(UUID.randomUUID().toString()); this.userService.insertError(user1, user2, user3);&#125;","categories":[],"tags":[]},{"title":"DDD领域驱动设计","slug":"DDD领域驱动设计","date":"2022-12-21T06:22:37.000Z","updated":"2023-07-14T06:49:14.410Z","comments":true,"path":"2022/12/21/DDD领域驱动设计/","link":"","permalink":"https://wugengfeng.cn/2022/12/21/DDD%E9%A2%86%E5%9F%9F%E9%A9%B1%E5%8A%A8%E8%AE%BE%E8%AE%A1/","excerpt":"","text":"背景和来源 导读 一文看懂DDD 本文章侧重于DDD的实践和落地，对于理论概念涉及交浅，建议配合《领域驱动设计 软件核心复杂性应对之道》这本书加深理解 领域：从事一种专门活动或事业的范围、部类或部门 背景 实体经济和ToB业务会成为未来的主战场 深化数字化转型 简单的系统已经被开发得差不多了（开箱即用的脚手架） 随着技术的发展，以后软件系统的开发方向有可能是挑战历史做不到的事情，这类系统往往庞大而复杂。而面对复杂的系统我们传统的设计模式便会显得捉襟见肘，亟需要新的理论来帮助我们设计复杂系统模型，而领域驱动设计便是一套指导复杂系统分析和设计的方法论 DDD的起源 DDD其实早在2003年便诞生了，刚诞生时并不火。直到微服务架构的流行，发现DDD能够解决微服务架构那些亟需解决的问题，包括项目复杂度的控制和服务的划分 核心思想和解决的痛点 模型和建模 模型是对现实的抽象和模拟 建模是针对特定问题建立领域的合理模型 总结：同样的事物在面对不同领域（问题）的时候，它的模型是不一样的。建模一定是针对领域问题来建模。上图中的两个系统中，进销存中的商品条码对超市来说比较重要。而在电商系统中的商品属性，标签，图片对电商系统特别重要 软件系统复杂性来源 建模阶段：把领域知识和业务需求转换成领域模型 设计阶段：把模型转化成代码，需要考虑的是技术实现 业务复杂导致模型复杂 由上图可知，同样的事物在不同的领域（财务、印钞、防伪）的复杂度是不一样的 假设我们现在要为铸币厂设计一个内部系统，我们把上图的三个领域都放在一个模型中进行系统设计，那么我们将得到一个复杂的系统模型。这其实就是传统的面向对象思想存在的一个问题，在DDD出现之前，这一直是个痛点问题。当业务变得越来越复杂后我们依然采用面向对象思想在一个单一的模型里描述一个系统，这个系统的复杂度将不可控制！ 解决方案：模型分解 技术实现引入额外复杂性 假设开发一个电商平台系统，系统中有两个团队进行开发，第一个团队专门负责数据接入开发，第二个团队负责商品服务开发 第一个团队：业务逻辑简单，当商家建立商品数据、门店时，只需要针对商品、门店建立实体写入表中 第二个团队：假设在商品搜索服务时，需要加入门店信息的搜索时，为了搜索的实时性，可能要求第一团队生成一张门店商品表（引入复杂性） 门店商品：商品服务需要实现根据门店距离实现进行商品搜索，那这个时候商品表和门店表的数据同步如何保持一致，无论哪张表延迟同步都有可能导致商品搜索不出来，因此又引入门店商品表，搜索时使用门店商品表进行搜索（门店商品表数据是在商家系统数据接入时生成的，同步给商品服务） 解决痛点 领域驱动设计（英语：domain driven design，缩写 DDD）是一种用于指导软件设计的方法论，也是一种设计思维方式，用于解决软件复杂性问题，旨在加速那些必须处理复杂领域的软件项目的开发 解决痛点：解决了面向对象思想无法解决的软件设计的复杂性问题（复杂领域） 核心思想 模型分解 对于一个复杂的系统，可以对模型进行分解，划分出不同的子领域，然后再针对不同的子领域进行建模，面向不同的子领域解决不同的问题，分而治之思想 DDD提供两个工具对模型进行分解 领域划分：面向问题空间 限界上下文：面向解决方案空间 模型驱动设计 Model Driven Design，通过分层架构隔离领域层、仔细选择模型和设计方案等措施保持实现与模型的一致 OOAD敏捷和DDD的区别与联系 在DDD诞生之前，在软件开发领域已经诞生一些思想和理论，其中的佼佼者就是面向对象程序设计和敏捷开发 DDD与OOAD OOAD：面向对象程序设计其实和DDD属于同一层面，因为它们都属于设计层级，目的都是将问题(领域)和需求转换成模型，再将模型转换成方案 OOP：DDD的落地需要依赖于面向对象编程语言的实现 OOAD的不足：OOAD在面向大型系统设计时其实是有局限性的。原因是OOAD体系其实没有一个对系统有效划分的方法论，面向大规模复杂的系统，它会将多个子领域的问题放到一个模型中去考虑，得出的模型不管是对象本身的复杂性或对象和对象之间关系的复杂性都会变得异常复杂 DDD的优势：有战略设计的部分，它会面对系统遇到的领域进行拆分，最后得到一个拆分后的模型，是一种分而治之的方案，最后系统复杂性也得到控制 DDD与敏捷开发 在上个时代，每个软件工程的主体都是放在一个版本里开发的，软件工程会把这个版本划分为几个阶段，有细化阶段，需求分析阶段，实现阶段，测试阶段，部署阶段等。每个阶段的输出是下一个阶段的输入数据，顺序不会逆转回来，等项目上线后，一切尘埃落定。这种模式有一个明显的弊端，如果是前期的一些错误可能到项目的后期才会被发现，比如需求分析阶段，假设到了测试阶段才发现需求分析阶段的问题，那么纠正分析阶段的问题成本将异常的高，软件失败的风险也非常高。在这种缺陷下，敏捷开发应运而生。 敏捷开发主要包含两个方面的内容 开发流程：把一个大的目标分成多个迭代去完成，每个迭代为一个冲刺（传统开发好比马拉松，敏捷是把这个马拉松划分为若干个百米冲刺，敏捷开发不提倡开发前期的整体设计） 开发文化：敏捷宣言 战略设计 智慧零售项目介绍 背景：线下零售依然占据GDP一定的比重，线上销售面临流量枯竭 智慧零售，自动售货机线下新零售。智慧零售的主要解决人、货、场的问题 团队背景 A公司：SaaS公司，主要面向零售企业客户 SmartRM产品团队：一位资深产品经理，几位产品策划 SmartRM研发团队：一位架构师，10+开发，资深开发3至4位 客户背景 B集团: 零售行业某头部商家 全国各地有大量商超和便利店，有成熟的内部系统和供应链 基于自动售卖机的零售业务是新业务 案例优势 复杂度可覆盖DDD大部分知识点并体现其价值 智慧零售市场庞大且场景接近日常生活 有数据分析需求，可结合大数据分析场景 建模和设计的流程 目的：对DDD和在项目中的应用有一个全局的认识 重点 软件系统从需求到最终技术方案包含的环节 各环节的目标和概要 常用建模方法 所谓的建模和设计就是把领域知识和需求转换成代码的过程，它可以分为四个阶段 挖掘用户故事 建立通用语言 战略设计 战术设计 这四个阶段的开始时间有先后，但并不是上一个阶段结束下一个阶段才开始，而是重叠的。对用户故事的挖掘和建立通用语言是最先开始的，可能会贯穿整个建模设计的过程。同样，战术设计的开始也不意味着战略设计的结束，在战术设计的过程中我们有可能发现战略设计的不合理从而回过头对战略设计进行优化。同时在战略设计中，我们也不断会对通用语言进行丰富，甚至发现业务流程中的优化点，从而回过头重新研究用户故事(建模涡流思想) 建模参与人员 领域专家：非常重要的角色，指的是在某个领域有深度经验的专业人士（比如医疗系统，医生就是领域专家） 产品团队：现实中产品团队经常扮演领域专家角色，在有必要的情况下还是需要邀请领域专家进行讨论 研发团队：负责将建立起来的模型以编码形式落地 用户故事 什么角色 希望做什么 最终达到什么目的 用户故事更多的是对问题的描述，而不是解决方案，通过从问题出发，通过整个团队的讨论认证（领域专家，产品经理，研发人员，讨论过程中固化其图文）最终达到一个接近的解决方案 售卖机用户故事：作为一个消费者，我希望通过手机支付从售卖机中购买商品，从而能够更方便购物 给出用户故事后团队就可以开始对故事进行讨论，一边讨论一边画图，通过图文可快速把讨论结果固化 通过这一讨论过程，会发现慢慢从问题空间进入方法空间（消费者怎么通过售卖机购买商品） 通用语言 目的是拉齐各方人员的领域认知 在讨论模型和定义模型时，团队使用的同一种语言 领域知识需要在团队内部高效流转，模型需要描述 通用语言要体现在代码里（类名 属性名等） 在对用户故事进行挖掘的时候，就已经实在建立通用语言了，在讨论的时候，大部分时间是领域专家进行讲解。其他人要做的是理解领域专家提到的各种概念，及时提问同时进行知识固化，对讨论中存在歧义的关键词汇建立统一认知，甚至固化下来形成文档 战略设计 DDD中对问题空间和解决方案空间进行分解的过程 目的是分解模型以控制复杂性 是DDD与传统建模和设计方法的核心区别之一 包括内容 领域划分 寻找限界上下文(BC) 确定上下文映射：上下文之间的关系，确定上下文映射的过程还会加入防腐层（领域自我保护） 战术设计 对各个限界上下文的细节设计过程 限界上下文内部的模型结构与完整技术方案 内容包含 实体 值对象 服务 模块 聚合 工厂 资料库 这些元素确定下来后代码基本也就确定下来了，战术设计包括编码环节（Eric Evans认为设计人员或者架构师是需要深度参与编码的） 建模方法 领域叙事 Domain Storytelling 事件风暴 Event Storming 四色建模法 4C 从用户故事到通用语言 用户故事和建立通用语言和战略设计的关系 因为建模一定要从问题空间出发，而且通用语言也要尽早开始建立，因为无论是描述模型还是团队沟通都需要通用语言。所以在正式开始战略设计之前，从用户故事开始对领域进行探索，可以使得开发人员迅速迅速学习领域知识并且在团队内部初步建立通用语言，为后面的领域划分寻找限界上下文等工作做好铺垫 什么是用户故事 在软件开发中，用户故事是一种对软件系统特性的非正式的自然语言描述，是敏捷软件开发中从终端用户的角度对软件系统特性进行捕捉的一种方式。用户故事描述了不同类型的用户需要什么以及为什么需要，它可以帮助我们创建需求的简单描述。 总结：用户故事就是对问题的描述 在软件开发的需求阶段，就让产品经理对软件的功能进行详细的设计（可能设计并不能落地），不仅是时间上的浪费，也会让团队过早陷入细节。用户故事就提供了一种恰到好处的粒度，直接对需求（问题）进行描述 用户故事的构建 简单描述用户需求； 围绕简单描述进行讨论; 明确如何验证； 分别对应用户故事的三个元素，也就是3C： Card(卡片)、Conversation(谈话)、Confirmation(验证) Card (卡片) 卡片就是指对用户故事的简述(传统上人们通过便利贴在白板上构建用户故事)，一个好的用户故事卡片包括三 个要素: 谁：谁需要这个功能; 需要什么：想通过系统完成什么事情; 为什么：为什么需要这个功能，这个功能带来什么样的价值; Conversation (谈话) 谈话是指用户、领域专家、产品经理、研发之间围绕用户故事进行的讨论，谈话是明确需求细节的必要环节。可以用文字对谈话进行简要记录，此外，也可以基于图形或其他工具进行讨论。(比如使用 domain storytelling 挖掘用户故事) Confirmation (验证) 验证代表了验收测试，描述了客户或者产品owner怎样确定用户故事已经被实现，且能够满足需求。一般可以用如 下模板写Confirmation: 123假设我是&lt;角色&gt;，在xxx情况下，当我&lt;操作&gt;，那么&lt;结果&gt;。 用户故事必须是可以验证的 SmartRM通用语言文档 场景模拟 售卖机扫码支付购物 卡片 作为用户， 我希望在售卖机上通过手机扫码支付购买商品， 以便快速便捷地购买商品。 谈话 P：用户在设备屏幕上选择商品后，设备展示支付二维码，用户使用微信扫描二维码，完成支付后，设备完成出货，交易结束，设备屏幕上回到商品列表界面 D：这里的设备是指自动售卖机吗? P：是的。 D：那么我们以后统一用”售卖机“这个词吧? 英文用Vending Machine。 P：没问题。 D： 如果用户支付失败，会怎么样? P：售卖机会等待一段时间，然后取消交易，回到商品列表界面 D：售卖机出货会失败吗? P：有可能，不过我们还是要找懂这套售卖机的人了解下 D： 是的，我把运营人员O拉进来聊下。 P：0，我们在对SmartRM系统进行建模，想咨询一些售卖机相关的问题。售卖机出货会失败吗? 0：我们应用的售卖机主要包括自动称重式柜门机、弹货道售卖机、蛇形货道售卖机。其中，称重式柜门机特点是售卖的商品类型多，不会卡货，主要用于办公楼、商超、小区等室内外公共场所的饮料、零食、生鲜等商品的售卖，弹簧货道售卖机的特点是成本低，售卖商品类型多，卡货概率高，主要用于室内外等公共场所的饮料、零食的售卖，蛇形货道售卖机的特点是体积一般较大、库存容量大、卡货概率低、省电，但是只能用于饮料的售卖，主要放在体育场、工厂、学校、公园等室内或室外公共场所用于饮料售卖。目前我们有一部分存量弹簧货道售卖机，后面大部分室内的会替换成自动称重柜门机，但是仍然会保留一部分。我们的售卖机可靠性很优秀，但是还是会有几种出货失败的情况。比较常见的出货失败的情况有以下几种： 1) 卡货，2)售卖机网络问题，3) 库存错误 意义：用户故事的讨论能够让我们快速的明确流程，熟悉领域知识同时建立通用语言，同时让团队之间的理解达成一致 验收标准 假设我是一名用户，货道售卖机屏幕的商品列表上有商品A,B,C当我在售卖机屏幕上选择了商品A，并扫描展示的二维码完成支付后那么商品A就会从售卖机中弹出，我可以拿到商品A 研发人员：单元测试的依据 测试人员：测试计划的依据 产品人员：验收的依据 Domain Storytelling 通过上面的谈话我们能够发现其实是比较松散的，可以借助 领域故事陈述 Domain Storytelling 将关键细节记录起来，建立通用语言，形成简要文档 domain story modeler 领域故事陈述工具 通过领域故事陈述建立起的模型可以将不同角色的认知进行对齐，并且图例的关键词的统一其实就是建立通用语言的过程 通用语言 一种描述模型且基于模型的语言 团队在进行所有交流时都使用它 代码中也要体现 包含的内容 类和操作的名称 施加于模型之上的规则和约束（如商品补充的库存不能超过容量） 应用于领域模型的模式（工厂，资料库…） 我们对用户故事进行storytelling，其实就是建立领域通用语言的过程，storytelling的输出结果 (上述的storytelling图)也就包含了领域通用语言的完整语句，对象、角色、活动、以及体现它们相互作用的完整语句在图中都可以一览无余。这里我们可以更进一步，将通用语言中的词汇提炼出来，将其中英文都列在通用语言词汇表中，这些词汇将会贯穿整个建模和设计过程，最终也会体现在代码中，因此团队中所有成员，都需要明确理解其含义，并且在相关讨论、模型、以及代码中使用它们 领域划分和子域 内容 什么是领域划分和子域 为什么要进行领域划分 基于用户故事分解的领域划分方法 什么是领域划分 领域：DDD会按规则细分业务领域，细分到一定程度，会将问题范围限定在特定边界，在该边界内建立领域模型，进而用代码实现该领域模型，解决相应业务问题，领域就是该边界内要解决的业务问题域（对业务进行边界划分） 子域：领域可进步划分为子领域。划分出来的多个子领域称为子域，每个子域对应一个更小的问题域或业务范围 领域划分是以分离关注点为原则对问题空间的划分 子域是领域中某个方面的问题和解决它所涉及的一切 为什么进行领域划分 领域划分是一个分而治之的思想，核心是将一个大而复杂的问题拆解成一个个较小的问题 问题点和领域知识重叠：图中的两个研发都需要关注三个问题点，存在重复劳动，并且工作上可能还出现重叠 模型重叠：抽象出的商品，订单，售卖机的模型设计工作由谁展开？ 不同子域聚焦不同问题：职责明确 工作效率低 职责冲突 smartRM项目的六大顶级用户故事 售卖机扫码支付购物 柜门机免密购物 售卖机投放 补货 售卖机撤销 经营分析 初步领域划分 按照面向的用户不同，可以把六大用户故事初步划分为交易域和运营域 怎样进行领域划分 基于故事分解的领域划分 模拟交易域的用户故事分解 然后再根据拆分的用户故事进行归类，最终得到下面的图 最终根据团队的不断优化，一个系统被拆分为以下的领域 核心域和精炼 子域的类型 核心域：决定产品和公司核心竞争力的子域是核心域，它是业务成功的主要因素和公司的核心竞争力。 通用域：没有太多个性化的诉求，同时被多个子域使用的通用功能子域是通用域。 支撑域：还有一种功能子域是必需的，但既不包含决定产品和公司核心竞争力的功能，也不包含通用功能的子域，它就是支撑域。 核心域和研发力分配 DDD认为，对于项目和产品来说，最核心人员应该把精力投入到核心域的设计和开发里来，尽量减少团队在其他部分的投入，这样才能提升产品和业务的核心竞争力 精炼 精炼就是不断提炼和压缩的过程 通过精炼，分理出领域普通的部分，最终得到一个核心域。精炼可以让团队用尽可能小的代价换取最大的成功概率 案例中的精炼 战略设计要明确核心域，团队尽量减少非核心域投入 核心域的建立总是伴随着精炼，精炼有两种方法 限界上下文 限界上下文是一种语义上的上下文边界。意思是在这个边界里的软件模型组件都有它特定的含义并且做特定的事。一个限界上下文内的组件都是上下文特定的并且语义明确的 前文已经提到，同一个事物在面向不同的场景、问题或领域(或限界上下文)时，表现的复杂度和语义是不一样的 同一台售卖机在运营眼里，用户眼里是不一样的。在运营眼里，它是一台库存不断消耗的又需要及时补充库存的商品容器，在用户眼里它就是一个购物平台。所以在运营和用户（或者叫上下文）眼里提到的售卖机其实是有差别的。我们要让这个词具备单一的含义，单一的作用那就需要明确指出来，它在哪个上下文。或者换句话说它的限界上下文是什么，这就是限界上下文本质的含义 为什么需要限界上下文 自然语言具有模糊性（同一个词在不场合在不同人眼里通常具有不同的意义） 同一个事物面向不同场景有不同模型（上文的售卖机） 软件系统需要分解模型以控制复杂性（需要通过限界上下文来分解复杂的模型） 限界上下文是分工的单位（高内聚低耦合特性，一个限界上下文可以交给一个研发或者团队负责） 如何划分限界上下文 Domain Storytelling (领域故事陈述法) 这里识别边界主要利用了概念之间的语义差别 Domain storytelling中边界特征 单向联系（库存计划到配送订单的箭头） 语义区别 活动的触发方式不一样（比如图中售卖机的投放和库存计划就是不同的触发方式，没有划分边界主要是考虑到售卖机的投放和库存计划的制订是紧密相关的，本质都是解决库存问题，设计前期的原则是宁缺毋滥） Event Storming (事件风暴法 推荐) 目前最热门的方法，DDD中最重要的建模方法，后续会讲 基于子域概念提取 通过分别从各子领域的用户故事中提取关键概念，审视它们之间的关系，以及它们与外部系统之间的关系，我们可以梳理出系统中的限界上下文。如下图所示 把子域和限界上下文的划分结果整合 限界上下文和微服务 DDD和微服务 微服务是限界上下文的实现方式，一般一个限界上下文对应一个服务 上下文映射模式 限界上下文并不是孤立的，它们之间需要协作才能完成系统的功能。划分之后的限界上下文以及团队之间如何进行协作，怎样去理解限界上下文之间的关系，这就引出了上下文映射的概念 上下文映射(Context Mapping) 上下文映射是指限界上下文之间的模型映射关系 描述团队之间的协作关系以及上下文之间的集成关系 决定上下文之间如何集成以及如何设置防腐层（代理层，将其他模型转换本上下文模型） 上图的上下文中存在概念重叠的地方，比如支付二维码和商品，这种现象是正常的，这就是限界上下文的意义所在。但是在系统实现的过程中需要处理一个问题，当我们集成两个概念重叠的上下文需要怎么处理？是将其中一个转换成另一个或多个？还是其中一个向另外一个看齐？其实这就是模型和模型之间的映射关系。当需要集成另一个上下文，就需要看到这个上下文暴露出来的概念 上下文映射模式 模式名称 备注 Partenership 合伙人 Shared Kernel 共享内核 Customer/Supplier 客户/供应商 Conformist 顺从者 Anticorruption Layer 防腐层 Separate Ways 分道扬镰（独立方法） Open Host Service 开放主机服务 Published Language 公开语言 Big Ball Of Mud 大泥球 开放主机服务 服务提供方为所有消费方提供一套公共的API 针对通用的功能和模型 微信支付是支付上下文的上游，微信支付提供通用的支付API。通用API服务因为难以定制化，其隐含了另一种模式顺从者 顺从者 没有模型到模型的转换 一个上下文沿用另一个上下文的部分模型 顺从者模式隐藏一个风险，当你顺从一个上下文的时候，其实也就表明了你允许其对你的侵入。如果有多个支付渠道，那么也就意味着需要引入多个上下文模型，如果不采用特殊方法处理，当前上下文可能会变得混乱可能会变成大泥球 大泥球 由混杂的模型构成的糟糕系统，模型不稳定且难于维护 与大泥球合作的上下文要确保自身不被污染，设置防腐层 防腐层 把上游上下文的模型转换成自己上下文的模型 是下游上下文中访问外部模型的一个代理层 共享内核 两个上下文共享部分模型 比如把核心源码封装为一个jar或者starter甚至是数据表等 慎用，仅当团队紧密合作且共享部分稳定，合作紧密的团队又隐含另一模式 合伙人 技术无关，是一种团队协作关系 两个团队之间可以随时互通有无，协同变更 客户/供应商 下游上下文可以向上游上下文提需求 一般用于核心域与非核心域之间的协作 核心域（下游）会向非核心域提需求（现实中支撑子域可能由外包团队负责） 分道扬镳（独立方法） 两个上下文无协作，各自独立 当两个上下文之间的集成成本过高 比较常见的是新旧系统的集成，原本旧系统如果就是一个大泥球，旧系统的集成就会相对困难。新系统可能会放弃集成，自己实现 公开语言 标准化与协议化的模型 所有上下文都可以与公开语言中的模型进行转换 对接了公开语言的上下文之间可以实现组件化对接 例子 蓝牙协议、tcp/ip Java生态的jdbc、jvm标准等 SQL 分层架构 严格按照领域模型来编写代码(通用语言命名) 建模和实现中都有破坏该原则的因素 架构分层能够避免模型在实现过程中被省略或者污染 建立分层架构的目标是实现领域驱动设计中的一个重要原则，模型驱动设计。模型驱动设计简而言之就是我们需要遵循模型来编写代码，使得代码的设计不受到其他因素的干扰，代码和模型实现一致。但现实中这种原则很容易受到干扰，比如出于数据库性能考虑，我们代码会创造出领域不存在的实体，因此分层架构用于避免模型在实现过程中被省略或者污染 传统分层 传统分层围绕数据结构编码 传统架构分层的入口是controller,处理业务会调用service，如果处理的业务逻辑有远程调用则会调用client,最后会查询或保存数据调用dao层，返回数据用到entity和vo层。这样的分层架构看上去中规中矩，其实就是我们要尽量避免的反模式。因为在entity包中可能存在大量非领域实体的实体，在这种分层架构下，其实我们一直是围绕数据在编写代码，领域模型其实已经被我们忽略掉了，这个模型其实属于贫血模型。在这种分层架构下我们的设计其实都是在围绕这数据在设计（或者说围绕数据表结构设计，先设计表结构然后开始编码），这种模式最后会演变为业务代码被存储层绑架，业务逻辑和技术实现混杂在一起，领域模型最终也被技术方案绑架，传统的架构模型存在以下问题 领域模型易被省略，变成贫血模型 容易演变成基于数据的设计，一切从表结构开始 领域模型与技术实现混杂，易被技术实现绑架 四层架构 目的：让各层之间形成一个良性的单向依赖 四层架构是DDD中经典的架构，把架构分为 分层 英文 描述 表现层 User Interface 用户界面层，或者表现层，负责向用户显示解释用户命令 应用层 Application Layer 定义软件要完成的任务，并且指挥协调领域对象进行不同的操作还包含事务的控制。该层不包含业务领域知识。 领域层 Domain Layer 或称为模型层，系统的核心，负责表达业务概念，业务状态信息以及业务规则。即包含了该领域（问题域）所有复杂的业务知识抽象和规则定义。该层主要精力要放在领域对象分析上，可以从实体，值对象，聚合（聚合根），领域服务，领域事件，仓储，工厂等方面入手 基础设施层 Infrastructure Layer 主要有2方面内容，一是为领域模型提供持久化机制，当软件需要持久化能力时候才需要进行规划；一是对其他层提供通用的技术支持能力，如消息通信，通用工具，配置等的实现； 四层架构落地 接口层（表现层） 领域层 应用层 应用服务不在访问数据层（直接操作dao类），应用服务会去协调领域层的元素，实体，资源库，领域服务等让它们配合完成请求。具体的业务逻辑被放到了领域层。通过四层架构，这里的设计和编码已经不围绕表结构去进行了（资料库是实体和表结构的解耦），领域模型就可以变得相对独立（模型不受表结构影响），不再被技术方案所绑架，这样设计人员才能把精力聚焦到领域模型的建立和实现上 基础设施层 四层架构解决的问题 分离关注点 让领域模型层更独立 单向依赖 缺陷 领域层对基础设施层仍然有感知，领域模型和技术实现耦合（因为资料库的实现依赖于dao类） 六边形架构 洋葱架构，六边形架构 保持领域层的纯粹性，不受其他因素干扰 便于践行模型驱动设计，代码跟随模型 便于把团队精力集中到领域模型 四层架构演化而来的，核心思想是把领域模型放到了核心层，领域层变得存粹和独立，本文参考了六边形架构和洋葱架构落地，将洋葱架构的User Interface层改为Adapter层 应用层 和上文没有太大区别 领域层 资源库的实现类被移到了适配器层，领域层只保留了资料库的接口。因为资源库本质完成的是领域模型和存储层之间的数据交换 基础设施层 把原先基础设施层远程服务调用拆分成两部分，接口放到了领域层，实现放到了适配器层。因为调用其他上下文的服务其实就是对上下文映射的一种实现，是领域模型之间的数据交换。需要在adapter层中完成其他模型到本模型数据的转换，防腐层可以在适配器层中实现 适配器层 实现消息的收发（controller） 六边形架构的特点是让领域模型变得非常干净，领域模型非常独立，不再受到业务无关因素的影响 初涉战术设计 环境准备，约定 项目 smartrm-monolith 语言 Java IDE IntelliJ IDEA Jdk Java SE Development Kit 8 依赖管理和构建应 Maven 3 应用开发框架 Spring boot 2.5.4 关系型数据库访问ORM Mybatis 3.5.6 通用工具库 Guava 30.1.1-jre 定时任务调度 Quratz 2.3.2 命名约定 对象含义 所处层 业内常用命 阿里规范 采用命名 视图层对象 接口 (适配) 层 VO VO VO (能省则省) 数据传输对象 应用层 DTO DTO DTO 数据存储对象 基础设施层 PO(Persistent Object) DO(Data Object) DO 领域对象 领域层 DO(Domain Object) BO(business object) 根据领域通用语言命名 前期先使用单体架构实现DDD，后续重构为微服务架构 项目结构 包结构 交易域准备工作 交易域业务流程熟悉 针对交易域进行战术设计分析 核心域上下文的依赖准备工作 货道售卖机操作流程 柜门售卖机操作流程 在战略设计时（场景模拟），我们通过用户故事使用 Domain Storytelling(领域故事陈述法)建立起一个模型。在这个图中我们能够看到重要的角色和活动(标记序号的是活动)，角色有可能是领域中的某种用户或者某一种系统，活动就是从角色触发的领域中的某一种行为。在途中我们能看到角色和活动以及活动发生的顺序 ，但是我们看不到系统中的重要对象他们之间的关系。因为在战略设计的用户故事建模中我们围绕的就是对角色和活动进行分析，基本上没有机会去探讨对象之间的关系。所以通过这个图我们还是不知道系统中关键对象的特性是什么，对象之间又是怎样进行协作的，以至于我们不知道如何编写代码 因此我们必须针对战术设计进行分析，需要分析出对象之间的关联 对象间关系 一个对象为另一个对象的状态变更提供数据 比如售卖机商品列表，就需要商品库存提供信息过滤商品，也需要商品信息提供商品明细进行展示 一个对象的状态变更导致另一个对象的状态变更 订单状态的变化影响支付，支付状态的变化影响订单状态 战术设计阶段就是要尽量挖掘对象之间的关系，和战略设计分析的思路不同，重点不再围绕用户和其他领域中的角色以及从这些角色出发的活动进行分析。而转变到挖掘领域中的对象和对象之间的关联 domain-story-modeler 是以用户，角色，活动为核心，只有从角色出发的箭头才有标号（设备上下文到交易上下文无法放置标号），因此需要借用UML建模里的时序图 整合 将战术故事的领域故事陈述结合时序图后，就得到上下文交互图。箭头的标注我们能够看到上下文提供的接口 实体和值对象 参考 实体和值对象 实体：是指描述了领域中唯一的且可持续变化的抽象模型，有ID标识，有生命周期，有状态（用值对象来描述状态），实体通过ID进行区分；其二是要跟踪状态的变化； 值对象：值对象的核心本质是值，与是否有复杂类型无关，值对象没有生命周期，通过两个值对象的值是否相同区分是否是同一个值对象 特征 实体 ID相等性 要跟踪状态变化 比如身份证上的身份证号，头像 值对象 属性相等性 可互换 不变性 比如身份证上的出生地址 区分的原因 值对象往往更轻量级 尽可能用值对象而不是实体 值对象不用跟踪变化 实体和值对象在领域中扮演的角色不一样 实体可以作为一个聚合根，而值对象不可以 区分 通过特征区分 是否只读 生命周期是否跨越活动 以身份证举例 12345678910111213141516171819202122232425262728293031323334353637383940414243444546// 实体public class IdCard &#123; // id private Long id; // 身份证号 private String IdNumber; // 头像 private String avatar; // 地址 private Address address; public Long getId() &#123; return id; &#125; public void setId(Long id) &#123; this.id = id; &#125; public String getIdNumber() &#123; return IdNumber; &#125; public void setIdNumber(String idNumber) &#123; IdNumber = idNumber; &#125; public String getAvatar() &#123; return avatar; &#125; public void setAvatar(String avatar) &#123; this.avatar = avatar; &#125; public Address getAddress() &#123; return address; &#125; public void setAddress(Address address) &#123; this.address = address; &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859// 值对象public class Address &#123; public Address(String province, String city, String address, String detail) &#123; this.province = province; this.city = city; this.address = address; this.detail = detail; &#125; // 省份 private String province; // 城市 private String city; // 区 private String address; // 详细地址 private String detail; public String getProvince() &#123; return province; &#125; public String getCity() &#123; return city; &#125; public String getAddress() &#123; return address; &#125; public String getDetail() &#123; return detail; &#125; @Override public boolean equals(Object o) &#123; if (this == o) return true; if (o == null || getClass() != o.getClass()) return false; Address address1 = (Address) o; if (province != null ? !province.equals(address1.province) : address1.province != null) return false; if (city != null ? !city.equals(address1.city) : address1.city != null) return false; if (address != null ? !address.equals(address1.address) : address1.address != null) return false; return detail != null ? detail.equals(address1.detail) : address1.detail == null; &#125; @Override public int hashCode() &#123; int result = province != null ? province.hashCode() : 0; result = 31 * result + (city != null ? city.hashCode() : 0); result = 31 * result + (address != null ? address.hashCode() : 0); result = 31 * result + (detail != null ? detail.hashCode() : 0); return result; &#125;&#125; 领域对象的构造 Order 对象构造是谁的职责，如何确保相关对象的一致性 使用工厂模式解决领域对象的构造。一个聚合的领域对象一般是由聚合根提供构造方法的，而聚合根的构造一般在领域服务或者应用层被构造 工厂方法模式 抽象工厂模式 如何兼顾对象构造的简便性和对象的封装性 建造者模式 实体ID应该如何生成 基于已有信息的拼接 基于数据库表自增ID 基于独立的ID生成器 资源库与持久化 OrderRepository OrderRepositoryImpl 什么是资源库 为每种需要全局访问的对象类型创建一个对象，这个对象相当于该类型的所有对象在内存中的一个集合的“替身”。通过一个众所周知的全局接口来提供访问 带必要管理功能的领域对象容器，与技术实现无关 资源库的意义 提供一个管理领域对象的简单模型 使领域模型和持久化技术解耦，它可以屏蔽存储层的技术细节 在领域层提供一个资源库接口暴露给上层使用，大多数情况下，资源库底层也是使用Dao类实现，主要完成表对象和领域对象的转换和解耦 资源库的实现 领域实体通常也会使用充血模式，实体除包含get set方法，还包含对象的持久化操作 1234567891011121314public interface OrderRepository &#123; // 根据订单id获取订单 Order getOrderById(long orderId); // 新增订单 void addOrder(Order order); // 更新订单信息 void updateOrder(Order order); // 新增或修改订单信息 void addOrUpdate(Order order);&#125; 12345678910111213141516171819202122232425262728293031323334353637@Repositorypublic class OrderRepositoryImpl implements OrderRepository &#123; @Autowired OrderMapper orderMapper; @Autowired DomainEventBus eventBus; @Override public Order getOrderById(long orderId) &#123; // 表实体 OrderDo orderDo = orderMapper.selectOne(orderId); try &#123; // 字符串转对象 StockedCommodityDo[] commodityDos = new ObjectMapper() .readValue(orderDo.getCommodities(), StockedCommodityDo[].class); // 表实体转领域实体，实现领域模型和底层存储的解耦 return Order.Builder() .orderId(orderId) .paymentId(orderDo.getPaymentId()) .type(OrderType.of(orderDo.getType())) .machineId(orderDo.getMachineId()) .state(OrderState.of(orderDo.getState())) .commodities(Arrays.stream(commodityDos).map( d -&gt; new StockedCommodity(d.getCommodityId(), d.getName(), d.getImageUrl(), d.getPrice(), d.getCount())).collect( Collectors.toList())) .eventBus(eventBus).build(); &#125; catch (JsonProcessingException e) &#123; throw new DomainException(CommonError.PersistentDataError); &#125; &#125; ... 聚合 聚合根提供外部访问聚合内部对象接口 限界上下文和聚合：一个限界上下文可能包含多个聚合 聚合就是一组相关对象的集合，我们把它作为数据修改的单元。每个聚合都有一个根和一个边界。聚合根是聚合所包含的一个特定实体。对聚合而言，外部对象只可以引用聚合根，而边界内部的对象之间则可以互相引用 聚合是拥有事务一致性 (强一致性)的领域对象组合 聚合内的实体适用事务一致性 聚合之间适用最终一致性 不脱离聚合根修改聚合内部对象 聚合根有全局唯一标识，聚合内部实体只有局部标识 聚合根可以从资源库获取，聚合内部实体不能 比如一辆汽车，汽车就是聚合根，而引擎就是一个和汽车相关的对象。比如汽车引擎的维修必须通过汽车对象，首先必须把汽车拖进维修厂 聚合解决什么问题 优雅地实现一致性 聚合是限界上下文粒度的下限 聚合的识别 实体是否在所有活动中都协同变更 在本项目中，货道售卖机，订单，支付三个实体，是否属于同一个聚合？货道售卖机和订单不管是用户在选择商品之后或者是用户超时取消订单之后或扫码支付之后它们的状态是协同变更的，要么货道售卖机售卖机处于交易状态，订单处于开始状态；要么货道售卖机处于就绪状态，订单处于成功或取消状态。我们可以认为货道售卖机和订单属于同一个聚合，判断聚合根需要两个因素 直接面向用户请求 谁的生命周期更长 项目中货道售卖机是直接面向用户请求的，而且货道售卖机在处理一个订单后可以处理下一个新的订单，货道售卖机的生命周期会比订单长。所以货道售卖机是聚合根。支付这个实体和货道售卖机不属于同一个聚合，因为支付属于外部系统，无法保证一致性，因此支付和货道售卖机不属于同一个聚合 实现聚合 应用层服务：AppTradeService 12345678910111213141516171819202122232425262728293031323334353637383940/** * 用户选择商品 */@Transactionalpublic PaymentQrCode selectCommodity(SelectCommodityCmdDto cmd) &#123; // 获取聚合根 SlotVendingMachine machine = machineRepository.getSlotVendingMachineById(cmd.getMachineId()); if (machine == null) &#123; LOGGER.warn(&quot;vending machine not found:&#123;&#125;&quot;, cmd.getMachineId()); throw new DomainException(TradeError.VendingMachineNotFound); &#125; // 校验商品 CommodityInfo commodityInfo = commodityService.getCommodityDetail(cmd.getCommodityId()); if (commodityInfo == null) &#123; LOGGER.warn(&quot;commodity not exist:&#123;&#125;&quot;, cmd.getCommodityId()); throw new DomainException(TradeError.CommodityNotExist); &#125; // 构建库存商品 StockedCommodity commodity = new StockedCommodity( commodityInfo.getCommodityId(), commodityInfo.getCommodityName(), commodityInfo.getImageUrl(), commodityInfo.getPrice(), 1 ); // 远程调用，开始支付，获取二维码 // 支付和货到售卖机不属于同一个聚合 PaymentQrCode code = machine .selectCommodity(Lists.newArrayList(commodity), deviceService, payService, cmd.getPlatformType()); Map&lt;String, Object&gt; params = Maps.newHashMap(); params.put(&quot;orderId&quot;, machine.getCurOrder().getOrderId()); params.put(&quot;machineId&quot;, machine.getMachineId()); // 超时支付设置 scheduler.scheduleRetry(TradeExpireExecutor.class, params, 30 * 1000, 1000); return code;&#125; 聚合根：SlotVendingMachine 聚合根的聚合对象的持久化交给聚合根的资料库操作，聚合根负责和外部对象交互，被依赖的对象只能通过聚合根和外部对象交互 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071// 选择商品public PaymentQrCode selectCommodity(Collection&lt;StockedCommodity&gt; commodities, TradeDeviceService deviceService, TradePayService payService, PlatformType platformType) &#123; //校验设备状态 if (state != SlotVendingMachineState.Ready) &#123; throw new DomainException(TradeError.VendingMachineStateNotRight); &#125; //校验库存 if (!checkInventory(commodities, deviceService)) &#123; throw new DomainException(TradeError.InventoryCheckFail); &#125; // 生成订单 curOrder = this.generateOrder(commodities); emitEvent(new OrderCreatedEvent(this.machineId, curOrder)); state = SlotVendingMachineState.Trading; // 远程调用，开始支付，获取支付二维码 PaymentQrCode ret = payService.startQrCodePayForOrder(platformType, curOrder); // AOP切面，更新订单 curOrder.setPaymentId(ret.getPaymentId()); // 乐观锁机制 incVersion(); return ret;&#125;// 订单和货道售卖机是聚合关系，售卖机可以看作是订单的聚合根private Order generateOrder(Collection&lt;StockedCommodity&gt; commodities) &#123; return Order.Builder().commodities(commodities) .orderId(UniqueIdGeneratorUtil.instance().nextId()) .state(OrderState.Start) .type(OrderType.SlotQrScanePaid) .machineId(this.machineId) .eventBus(eventBus) .build();&#125;// 完成订单public void finishOrder(long orderId, TradeDeviceService deviceService) throws Exception &#123; if (this.curOrder == null || this.curOrder.getOrderId() != orderId) &#123; LOGGER.warn(&quot;order finished when slot vending machine has release it:&#123;&#125;,&#123;&#125;&quot;, machineId, orderId); return; &#125; //弹出商品 if (curOrder.getCommodities().size() &gt; 1) &#123; throw new DomainException(CommonError.UnExpected) .withMsg(&quot;slot vending machine only support one commodity order&quot;); &#125; for (StockedCommodity commodity : curOrder.getCommodities()) &#123; deviceService .popCommodity(curOrder.getMachineId(), commodity.getCommodityId(), curOrder.getOrderId()); &#125; this.curOrder.succeed(); this.state = SlotVendingMachineState.Popping; incVersion();&#125;// 取消订单public void cancelOrder() &#123; if (curOrder == null || curOrder.getState() == OrderState.Canceled) &#123; LOGGER.warn(&quot;cancel order state not right.&quot;); return; &#125; curOrder.cancel(); state = SlotVendingMachineState.Ready; //curOrder = null; // 乐观锁机制 incVersion();&#125; 聚合根资料库：VendingMachineRepository 12345678910111213141516171819202122public void updateSlotVendingMachine(SlotVendingMachine machine) &#123; if (!machine.isVersionInc()) &#123; //版本号未改变时直接跳过 return; &#125; TradeSlotVendingMachineDo machineDo = new TradeSlotVendingMachineDo(); machineDo.setMachineId(machine.getMachineId()); machineDo.setState(machine.getState().code()); machineDo.setCurOrderId(machine.getCurOrder() != null ? machine.getCurOrder().getOrderId() : 0); machineDo.setVersion(machine.getVersion()); // 聚合根资料库持久化聚合对象 if (machine.getCurOrder() != null) &#123; orderRepository.addOrUpdate(machine.getCurOrder()); &#125; int updated = slotVendingMachineMapper.update(machineDo); if (updated == 0) &#123; LOGGER.error(&quot;fail to update slot machine, version:&quot; + machineDo.getVersion()); throw new DomainException(CommonError.ConcurrencyConflict); &#125; LOGGER.info(&quot;update slot machine, version:&quot; + machineDo.getVersion());&#125; 这样聚合根就完成了聚合中的所有对象的操作，收敛了所有聚合对象的操作（只能通过聚合根操作） 领域服务 TradeCommodityService TradeCommodityServiceImpl，实现在适配层，因为需要进行模型转换 当领域中的某个重要的过程或转换操作不是实体或值对象的自然职责时应该在模型中添加一个作为独立接口的操作，并将其声明为领域服务定义接口时要使用模型语言，并确保操作名称是通用语言中的术语。此外应该使领域服务成为无状态的，领域服务只包含业务逻辑 场景：比如货道售卖机此时开展了运营活动，如果是新用户第一次购买商品时打八折，由于运营活动不是实体和值对象，因此可以使用领域服务声明一个ActivityService，用于计算新用户商品总额 应用层 定义软件要完成的任务，并且指挥表达领域概念的对象来解决问题。这一层所负责的工作对业务来说意义重大，也是与其他系统的应用层进行交互的必要渠道。应用层要尽量简单，不包含业务规则或者知识，而只为下一层中的领域对象协调任务，分配工作，使它们互相协作 比如在一个汽车模型中，汽车的领域层包括，开门，关门，启动，刹车，转弯，倒车。而应用层是“从家里开车到公司”，那么应用层要实现汽车从家里开到公司只能是协调汽车的开门，关门，启动等操作后，最终才能完成家到公司的需求 应用层的职责 应用层服务：AppTradeService 事务控制 身份认证和访问权限 定时任务调度 事件订阅 事件监听 (适配层，可能包含事件对象转换) 事件处理(应用层） 深入战术设计 领域事件为什么重要 在DDD中，聚合根的读写操作，出于对性能的考虑，其读模型和写模型可能是相对独立的。写模型落地到数据库后通过领域事件通知读模型进行数据同步，读模型对于性能的考量，采取性能更优秀的NoSql存储，比如Redis 领域事件能够驱动建模 领域事件和很多重要思想相关 CQRS 命令查询责任分离 CQRS 架构 Event Soucing 事件溯源 领域事件和大数据处理和分析相关 建模工具 事件风暴建模法 便利贴：线下开会 Miro：线上工具 领域事件 订单支付使用领域事件：Order DomainEvent 领域事件，单机模式使用spring自定义事件实现 什么是领域事件 领域中发生的任何领域专家感兴趣的事情 领域事件一般由聚合产生 领域事件不是技术概念 事件命名和基本属性 命名方法：动词+名词 finishOrder 事件ID：全局唯一 产生时间 发布订阅方式 外部系统 API定向通知，比如支付的回调 API定时拉取，提供公开API 消息队列 内部系统 观察者模式，单体架构 数据库流水（binlog） 消息队列 事件存储 直接使用消息中间件的存储（内存或文件） 基于数据库（mongodb就是不错的选择） 事件处理的要求 顺序性 聚合ID 存储分片 消费分组 幂等性：（用幂等性代替分布式事务，最终一致性保证） 领域事件和大数据分析 事件风暴建模法 事件风暴 事件风暴讲解 事件风暴讲解2 一种协作式的对复杂业务领域进行探索的讨论形式 一种灵活易调整的的轻量级的适用于DDD的建模方法 应用场景 评估已有业务线的健康度并发现优化点 探索一个新业务模型的可行性 设想为各个参与方能带来最大利益的新服务 设计整洁的可维护的软件以支持快速推进的业务 事件风暴核心 领域事件 聚台 决策命令 角色 读模型 策略 外部系统 问题/热点 参与角色 4 ~ 8人规模，人太多沟通效率太低 研发人员 产品经理 领域专家 列出主要领域事件 橙色便利贴 动词过去式 和领域专家相关 收集关注点和问题 紫色便利贴 问题 风险/关注点 假设 讨论点 通过命令深入领域 蓝色代表命令 黄色代表角色 找到聚合 从领域事件反向驱动出命令后就要找到聚合，聚合链接了领域事件和决策命令 处理领域逻辑 处理命令 产生领域事件 找出读模型 帮助用户做出决策 数据查询 策略 响应式逻辑 响应领域事件 触发命令 外部系统 第三方服务 对当前领域来说是外部 比如微信支付 事件风暴的几个任务 Big Picture（描绘出全景图） 业务处理流程 软件设计 高效事件风暴的注意事项 首先关注学习和倾听 谈话和例子很关键 锚定到具体的业务用例 澄清模糊概念 正向驱动 反向驱动 运营域事件风暴建模 防腐层构建 图中的运营上下文和ERP系统上下文间就存在一个防腐层，使用ACL标记。防腐层就是上下文之间的一个转换层，它的作用就是防止上游上下文的复杂和混乱扩散到下游上下文（上下文的自我保护层） 在案例中，当运营人员决定在某个地方投放安装售卖机的时候，需要在系统中下投放订单。最终提交到客户（第三方）的ERP系统内部，由客户的采购和运营人员最终执行，这里的ERP系统是一个业务复杂度非常高的系统，和我们的领域区别也比较大。我们的系统和客户ERP系统交互的过程中，如果没有任何隔离措施，那么ERP内部系统的复杂性，一些系统概念就会扩散到我们系统 上图为ERP的对接文档，这些字段在我们的上下文中不好理解，在领域上下文应该只要关注运营相关的概念就可以了，不需要关注外部系统的概念，否则外部系统概念入侵到运营域，会增加运营域的复杂度 防腐层落地 将运营上下文的投放订单和客户ERP的采购订单进行转换 投放订单 VendingMachineInstallOrder ERP采购订单对象放在适配层 ERPNumberId ERPPurchaseOrder FPOOrderEntry 投放订单服务 DevicePurchaseService 投放订单服务实现放到设配层，因为需要对投放订单和ERP的采购订单进行转换 DevicePurchaseServiceImpl：实现了领域值对象和外部系统对象的转换，将外部系统的业务逻辑隔离在适配层，方式外部系统的业务入侵 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081/** * @description: erp防腐层示例，调用采购服务申请安装售卖机 */@Servicepublic class DevicePurchaseServiceImpl implements DevicePurchaseService &#123; private static String ERP_BUSSINESS_TYPE = &quot;&quot;; //业务类型 private static String ERP_BILL_TYPE = &quot;&quot;; //单据类型 // 售卖机型号Mapper @Autowired private VendingMachineModelMapper machineModelMapper; private K3CloudApi client; @PostConstruct public void init() &#123; client = new K3CloudApi(); &#125; @Override public void placeInstallOrder(VendingMachineInstallOrder order) &#123; // 获取售卖机物料信息 VendingMachineModelDo modelData = machineModelMapper .selectByCode(order.getDeviceModel().code()); //售卖机投放订单 -&gt; ERP系统采购订单 FPOOrderEntry orderEntry = new FPOOrderEntry(); orderEntry.setFEntryID(0); //物料信息 orderEntry.setFMaterialId(new ERPNumberId(modelData.getMaterialId())); orderEntry.setFMaterialDesc(modelData.getMaterialDesc()); orderEntry.setFProductType(modelData.getProductType()); orderEntry.setFProcesser(new ERPNumberId(modelData.getProcessor())); orderEntry.setFBomId(new ERPNumberId(modelData.getBomId())); //数量 orderEntry.setFStockQty(order.getCount()); //TODO: 填充订单项目更多字段 //创建采购订单 ERPPurchaseOrder purchaseOrder = new ERPPurchaseOrder(); purchaseOrder.setFBusinessType(ERP_BUSSINESS_TYPE); purchaseOrder.setFBillTypeID(new ERPNumberId(ERP_BILL_TYPE)); purchaseOrder.setFPurchaseOrgId(new ERPNumberId(modelData.getPurchaseOrgId())); purchaseOrder.setFPurchaseDeptId(new ERPNumberId(modelData.getPurchaseDeptId())); purchaseOrder.setFPurchaserId(new ERPNumberId(modelData.getPurchaserId())); purchaseOrder.setFDate(order.getCreatedTime().format(DateTimeFormatter.BASIC_ISO_DATE)); purchaseOrder.addEntry(orderEntry); //TODO: 填充采购订单更多字段 // 提交采购订单 try &#123; ObjectMapper objectMapper = new ObjectMapper(); String json = objectMapper.writeValueAsString(purchaseOrder); String resultString = client.save(&quot;PUR_PurchaseOrder&quot;, json); JsonNode resultJson = objectMapper.readTree(resultString); if (!resultJson.get(&quot;Result&quot;).get(&quot;ResponseStatus&quot;).get(&quot;IsSuccess&quot;).asBoolean()) &#123; throw new DomainException(OperationError.ERPError); &#125; else &#123; String id = resultJson.get(&quot;Result&quot;).get(&quot;ID&quot;).asText(); String number = resultJson.get(&quot;Result&quot;).get(&quot;Number&quot;).asText(); order.setOrderId(new InstallOrderId(id, number)); //提交订单 ObjectNode submitRequest = objectMapper.createObjectNode(); submitRequest.putArray(&quot;Ids&quot;).add(id); submitRequest.putArray(&quot;Numbers&quot;).add(number); String submitResult = client .submit(&quot;PUR_PurchaseOrder&quot;, objectMapper.writeValueAsString(submitRequest)); resultJson = objectMapper.readTree(submitResult); if (!resultJson.get(&quot;Result&quot;).get(&quot;ResponseStatus&quot;).get(&quot;IsSuccess&quot;).asBoolean()) &#123; throw new DomainException(OperationError.ERPError); &#125; &#125; &#125; catch (DomainException e) &#123; throw e; &#125; catch (Exception e) &#123; throw new RuntimeException(e); &#125; &#125;&#125; 大数据服务实现经营数据分析 准备 使用JMeter 或 Locust 压测框架生成测试数据 kafka数据同步到OSS 阿里巴巴大数据治理平台 DataWorks 使用压测工具模拟请求 交易上下文发布领域事件到Kafka 使用Datax消费Kafka数据，并同步到OSS对象存储 使用阿里巴巴 DataWorks 大数据解决方案，OSS作为数据输入源，配置数据形成大数据报表 DDD和微服务 单体架构开发虽然简单，但是多个上下文都在同一个项目中集成部署，上下文中遇到性能瓶颈非常难以排查。多个上下文共享同一个DB,当业务越来越庞大后，DB面临的读写压力也会越来越大 微服务架构的好处 技术异构性：不同的服务可以使用不同的技术栈，甚至使用不同的语言开发 容错性：服务之间的性能问题和故障不会相互影响 灵活扩展：不同服务可以根据业务动态扩缩容 简化部署：大服务拆分成模块化部署 与组织结构匹配：多个开发团队可以同时开发不同的微服务模块 可组合性：多个服务可以组合成新的应用 方便替代和升级：当需要替换或重构某个服务的时候不会牵连其他服务（保证暴露接口的一致性即可） 微服务的基础 服务注册和发现 服务监控 熔断降级（高峰期服务节点分配给核心业务，周边服务节点让出资源） 流量控制 安全性 配置管理 微服务的问题和DDD的答案 服务划分 如何划分限界上下文 服务划分一直是微服务落地一个比较具有争议性的问题，服务划分的边界和粒度按照不同的理解有不同的划分手段，DDD给出的答案是根据限界上下文划分微服务，每个限界上下文都是一个独立的服务，上下文之间互不影响 微服务框架基础设施 微服务架构的落地需要解决服务治理问题，而服务治理依赖良好的底层方案。当前，微服务的底层方案总的来说可以分为两种：微服务SDK务框架）和服务网格 微服务SDK 应用程序通过接入SDK来实现服务治理，SDK运行在应用程序的上下文（相同进程），构建后成为应用程序的一部分，常见有实现方式有 Spring Cloud OpenFeign 打包 jar包暴露服务API Dubbo 将服务Interface打包成 jar包暴露服务 SDK的方式通常伴随代码入侵，当SDK升级不向下兼容时，下游的服务不管是否有业务变更也得被迫升级 服务网格 通过Sidecar模式，用单独的代理进程接管应用程序的网络流量，从而实现服务治理，借助代理进程，可以实现服务的流量控制（访问权限控制流、熔断等等）、服务发现、负载均衡等等服务治理相关功能 Istio 服务网格从逻辑上分为数据平面和控制平面。 数据平面 由一组智能代理（Envoy）组成，被部署为 Sidecar。这些代理负责协调和控制微服务之间的所有网络通信。它们还收集和报告所有网格流量的遥测数据。 控制平面 管理并配置代理来进行流量路由 Istio Istio组件 网格可视化 微服务框架的选择 服务发现对比 SpringCloud Istio Istio 服务发现与负载 基于DDD思想进行服务拆分 smartrm-micro-services 使用限界上下文拆分服务 领域事件改造 DomainEvent：领域事件接口，微服务不再使用Spring自定义事件 DomainEventBus：时间总线接口，用于发布事件， 队列名称就是事件类名 SimpleEventBusImpl：消息总线实现，底层使用Kafka发布事件 DomainEventHandler：事件处理器接口 DomainEventListener：领域事件监听器，其底层主要是kafkaConsumer，利用构造函数传入的事件类型和Handler监听和处理事件 DomainEventListenerAppRunner：Kafka配置 交易上下文微服务改造 因为交易上下文和其他上下文交互最多，因此选择交易上下文作为例子 涉及到跨领域调用的需要在交易服务添加一个共享层。共享模型可以考虑抽取公共jar包或者放在共享内核（弊端是可能会导致共享内核频繁升级） AppTradeService 123456789101112131415161718192021222324252627282930@Transactionalpublic PaymentQrCode selectCommodity(SelectCommodityCmdDto cmd) &#123; SlotVendingMachine machine = machineRepository.getSlotVendingMachineById(cmd.getMachineId()); if (machine == null) &#123; LOGGER.warn(&quot;vending machine not found:&#123;&#125;&quot;, cmd.getMachineId()); throw new DomainException(TradeError.VendingMachineNotFound); &#125; // 远程服务调用，接口订单在领域层，实现在适配层，因为需要进行Dto到本领域实体的转换 CommodityInfo commodityInfo = commodityService.getCommodityDetail(cmd.getCommodityId()); if (commodityInfo == null) &#123; LOGGER.warn(&quot;commodity not exist:&#123;&#125;&quot;, cmd.getCommodityId()); throw new DomainException(TradeError.CommodityNotExist); &#125; StockedCommodity commodity = new StockedCommodity( commodityInfo.getCommodityId(), commodityInfo.getCommodityName(), commodityInfo.getImageUrl(), commodityInfo.getPrice(), 1 ); PaymentQrCode code = machine .selectCommodity(Lists.newArrayList(commodity), deviceService, payService, cmd.getPlatformType()); Map&lt;String, Object&gt; params = Maps.newHashMap(); params.put(&quot;orderId&quot;, machine.getCurOrder().getOrderId()); params.put(&quot;machineId&quot;, machine.getMachineId()); scheduler.scheduleRetry(TradeExpireExecutor.class, params, 30 * 1000, 1000); return code;&#125; 应用层使用远程服务调用商品信息，远程服务调用底层使用的是RestTemplate RestTemplateConfig：RestTemplate配置类 TradeCommodityServiceImpl：商品远程服务调用实现 123456789101112131415161718192021@Autowired private RestTemplate commodityRestTemplate; @Override public CommodityInfo getCommodityDetail(String commodityId) &#123; String path = &quot;/detail/&quot; + commodityId; ParameterizedTypeReference&lt;CommonResponse&lt;CommodityInfoDto&gt;&gt; reference = new ParameterizedTypeReference&lt;CommonResponse&lt;CommodityInfoDto&gt;&gt;() &#123; &#125;; ResponseEntity&lt;CommonResponse&lt;CommodityInfoDto&gt;&gt; response = commodityRestTemplate .exchange(path, HttpMethod.GET, null, reference); if (!response.getStatusCode().is2xxSuccessful()) &#123; throw new DomainException(CommonError.UnExpected) .withMsg(response.getStatusCode().toString()); &#125; else if (response.getBody().getCode() != CommonError.NoError.getCode()) &#123; throw new DomainException(CommonError.UnExpected).withMsg(response.getBody().getMsg()); &#125; // Dto和领域实体转换 CommodityInfoDto dto = response.getBody().getData(); return new CommodityInfo(dto.getCommodityId(), dto.getCommodityName(), dto.getImageUrl(), dto.getPrice()); &#125; 改造好后的微服务架构图 k8s容器编排 Kubernetes是容器集群管理系统，是一个开源的平台 硬件资源管理调度、应用部署的事实标准 业务架构师需要懂运维架构 K8S架构原理 K8S 服务类型 Kubernetes集群由一个Master节点（为了高可用也可以使用多Master互为备份）和多个Worker节点构成 ​ Master节点中主要包含三个组件：API Server、Scheduler、Controller。其中API Server负责对集群内外提供集群信息的restful接口，从etcd读据；Controller是集群的管理控制组件，负责感知和调整集群状态，根据用户的请求控制集群，Controller包含多种不同的Controller，如ReplicationController，Node Controller等等，分别执行集群中不同方面的管理工作；Scheduler是调度器，负责为应用的Pod分配部署的Worker节点 ​ Worker节点主要由两部分构成：kubelet 和 kube-proxy。kubelet主要负责Worker节点上的容器管理，它会向Master汇报当前节点的状态信息，从Master节点获取和执行指令，对节点上Pod的生命周期进行管理，使节点的状态向目标状态靠拢；kube-proxy则负责节点上的网络管理，负责消息的路由转发（确切说是负责相应路由规则的配置） 服务网格 Istio核心组件 Trace:整个调用链 Span:某个服务调用 链路调用跟踪的核心是在每个请求中生成一个TraceID,当A服务调用B服务调用C服务将TraceID向下传递(其实就是埋点)，最终达到全链路跟踪目的 常用开源的组件有 zipkin和SkyWalking 实践中的问题和关键 持续集成 什么是CI/CD CI(持续集成) 通过自动化流程持续把各个开发者的工作集成到一起避免过大的集成成本 CD(持续交付) 通过自动化测试和部署流程使软件系统随时处于可发布状态 CI/CD也是微服务的重要基础 持续集成工具对比：Flow vs Jenkins CI/CD核心 单元测试：代码覆盖率（尽量代码覆盖） 集成测试：上下文边界 功能测试 回归测试 所有的测试工作尽可能通过代码自动进行 领域沟通和建模避免漏掉重要细节 深层模型 若开发人员识别出设计中隐含的某个概念或是在讨论中受到启发而发现一个概念时，就会对领域模型和相应的代码进行许多转换，在模型中加入一个或多个对象或关系，从而将此概念显式地表达出来。有时这种从隐式概念到显式概念的转换可能是一次突破，使我们得到一个深层模型 在之前的篇章中，我们和领域专家在谈话的过程中使用领域叙事构建了上图柜门及免密购物的模型，在整个谈话的过程中，我们都是围绕用户的行为在做一系列讨论，整个过程很顺畅，没有什么问题，但是和下图我们最终建立起的模型后发现有很大的区别 我们漏掉了一个很重要的实体账号，导致漏掉了整个用户上下文，在前期和领域专家的谈话过程中，我们根本就没有发现用户这个实体，因为在领域专家或业务专家的眼里是看不到用户这个概念的，他们只能看到用户拿着手机在购物，以及用户和微信的交互，他们可能不清楚免密支付是需要用户签署支付协议，下单支付时需要依赖用户实体的（技术层面）。所以在谈话过程中就没有出现账号这个词，但是在我们的系统实现过程中，是离不开用户这个对象的。首先遇到的问题就是判断用户是否要打开柜门这个权限，并且其他子域中可能对用户上下文也有依赖，比如运营域要对用户进行深入分析… 原因 以活动作为建模的核心，模型过于偏向业务 漏掉重要分支 复杂系统难免漏掉细节 应对方法 不要单纯以角色的行为(活动)为中心进行沟通和建模 领域沟通过程中，研发人员发挥主动性 场景走查 建模是一个不断完善自我修复的过程，建模完成后不是一成不变的 DDD的争论和局限性 资源库与领域服务的区别 资源库：负责实体发的读写逻辑 领域服务：负责处理业务逻辑 DDD是否过度设计 “按照DDD写代码的话似乎对编码的要求与系统理解更复杂了，一个业务是由多个领域对象同时分担处理，要是此时项目紧急加人进来做新业务，完全无法了解这些领域对象究竟具体提供了哪些服务，有一种系统被过度设计的感觉。” DDD落地到业务逻辑简单，性能要求高的系统 模型问题，DDD就是根据需求建立通用语言后设计的模型，领域专家和技术专家应该能够理解领域对象 DDD的出现只是辅助复杂系统的分析和设计，并不是解决所有代码结构问题 战略设计和战术设计 DDD就是一套方法论，一个作用是用于拉齐项目中各个角色对于需求的认知，高效率的让项目中涉及的”知识”在团队内流转，这个是战略设计的作用，另一个作用就是将战略设计划分的领域模型，通过使用战术设计的各种“武器”，比如实体，值对象，仓储层，防腐层等等，将领域模型落地成高度抽象且领域层稳定的代码 DDD是否被神化 《领域驱动设计 软件核心复杂性应对之道》书名已经给出了答案，觉得DDD被神化之前，首先需要有一个概念，什么事复杂系统？一个由几个微服务组成的系统算复杂系统吗？ 个人认为，觉得DDD被神化可能有一个原因是拿DDD和目前正在进行的项目进行模拟落地对比。要知道，DDD不是一套通用的方法论，他提供的是一套面向复杂系统指导和设计的方法论。我们可以向淘宝，京东看齐，他们的电商系统有多少个类？面对这样庞大的复杂系统，DDD能够做的是指导架构设计合理化，以便架构能够适应业务的落地和变化，降低架构设计失误的风险（前几年有多少借着微服务的风口重构了不适应业务变化的系统？），这才是大厂核心部门要推行DDD的原因。所以觉得DDD被神化需要看到的它对复杂系统架构的指导意义，不要盲目拿DDD和现有系统进行落地比较 DDD是一套不完善的方法论 DDD其实是一套由工程师梳理出来的方法论和模式（聚合，聚合根，值对象），它指导了复杂系统的分析和设计，贴合业务建模是它的侧重点（建立通用语言，战略设计）。但是它在对理论落地的性能方面并没有相关指导，比如聚合根的读写效率（一个庞大的聚合根的读写可能最后会是压垮DB的最后一根稻草）。因此DDD寻找的是一种贴合业务（对象使用领域实体命名），便于业务理解的建模和代码落地方法论。落地DDD性能调优也将会是一个大的挑战","categories":[],"tags":[]},{"title":"响应式编程","slug":"响应式编程","date":"2022-11-12T15:01:00.000Z","updated":"2023-10-31T08:15:58.317Z","comments":true,"path":"2022/11/12/响应式编程/","link":"","permalink":"https://wugengfeng.cn/2022/11/12/%E5%93%8D%E5%BA%94%E5%BC%8F%E7%BC%96%E7%A8%8B/","excerpt":"","text":"探讨篇 起源 技术发展趋势 背景：最近几年，随着Go、Node 等新语言、新技术的出现，Java 作为服务器端开发语言老大的地位受到了不小的挑战。虽然Java 的市场地位在短时间内并不会发生改变，但Java 社区还是将挑战视为机遇，并努力、不断地提高自身应对高并发服务器端开发场景的能力 2009 年，微软提出了一个更优雅地实现异步编程的方式—— Reactive Programming ，我们称之为响应式编程 JavaScript 语言就在ES6 中通过Promise 机制引入了类似的异步编程方式 2017 年9 月28 日，Spring 5 正式发布。Spring 5 发布最大的意义在于，它将响应式编程技术的普及向前推进了一大步。而同时，作为在背后支持Spring 5 响应式编程的框架Spring Reactor 是什么？ 面向流 变化传播 异步编程范式 响应式编程是一种面向数据流和变化传播的异步编程范式。这意味着可以在编程语言中很方便地表达静态或动态的数据流，而相关的计算模型会自动将变化的值通过数据流进行传播 – 百度百科 流：官方的解释是一组有序的数据发布者。所谓流，就是数据的流动、传输。通过各种形式将数据从一个地方带到另一个地方。一次Http请求是流，将请求参数带到服务端。一次数据库查询也是流，提交查询参数返回查询结果… 静态数据流：Java代码中的一个数组、一个List、一次数据库查询结果。它们的特点是数据是固定的，有限的 动态数据流：一个商城的页面埋点就是一个动态数据流（无限流），你永远不知道数据什么时候到来，什么时候结束 变化传播：一个值改变之后，会像多米诺骨牌一样，导致直接和间接引用它的值均发生相应变化 有什么特点？ 异步编程：提供了合适的异步编程模型，能够挖掘多核 CPU 的能力、提高效率、降低延迟和阻塞等 数据流：基于数据流模型，响应式编程提供一套统一的 Stream 风格的数据处理接口 变化传播：简单来说就是以一个数据流为输入，经过一连串操作转化为另一个数据流，然后分发给各个订阅者的过程 适用场景 以更少的服务器资源承载更高并发的场景 技术路线 ProjectReactor是 Spring 响应式编程的基石，ProjectReactor项目由vmware和Pivotal两家公司合作研发 Spring Webflux 底层的响应式流实现就依赖于ProjectReactor Spring Cloud Gateway 需要 Spring Boot 和 Spring Webflux 提供的 Netty 运行。它在传统的 Servlet 容器中或构建为 WAR 时不起作用 概念篇 ProjecReactor 官方参考文档地址 中文文档地址(3.2.0) 中文文档源码 reactive-streams 概念 响应式编程思想 响应式编程，就像装配一条流水线。Publisher 规定了数据如何生产，中间会有 Operators（操作符）对流水线的数据进行解析，校验，转换等等操作，最终处理好的数据流转到 Subscriber 发布订阅流程 Publisher 发布者 123public interface Publisher &lt; T &gt; &#123; public void subscribe（Subscriber &lt;？ super T &gt; s）;&#125; Publisher是一个或无限发布者元素的发布者，根据从其Subscriber接收到的需求发布它们 一个Publisher可以在不同的时间点动态地服务于多个Subscriber的订阅 在被订阅之前，大部分Publisher并不会主动发布元素(冷与热)，通过 订阅，可以将 Publisher 与 Subscriber 进行绑定，从而触发整个链中的数据流动。这是在内部实现的，通过单个 request 信号从 Subscriber 传播到上游，一直传回到 Publisher Mono：一个可以发出[0,1]个元素的异步发布者 Flux：一个可以发出[1,无穷]个元素的异步发布者 Subscriber 订阅者 123456public interface Subscriber &lt; T &gt; &#123; public void onSubscribe（Subscription s）; public void onNext（T t）; public void onError（Throwable t）; public void onComplete（）;&#125; 在将Subscriber的实例传递给Publisher.subscribe(Subscriber)后，将收到一次对onSubscribe(Subscription)的调用 在调用Subscription.request(long)之前不会收到进一步的通知 单次调用onError(Throwable)或onComplete()发出终止状态信号，之后将不会发送更多事件(订阅终止) 只要Subscriber实例能够处理更多元素，就可以通过Subscription.request(long)发出需求信号(long 是背压数) BaseSubscriber：如果需要自定义订阅者，推荐继承这个类 Subscription 订阅 1234public interface Subscription &#123; public void request(long n); public void cancel();&#125; 表示Subscriber消费Publisher发布的一个消息的生命周期 它只能由单个Subscriber使用一次 它用于表示对数据的需求和取消需求（并允许资源清理） Processor 处理器 12public interface Processor&lt;T, R&gt; extends Subscriber&lt;T&gt;, Publisher&lt;R&gt; &#123;&#125; Processor代表一个处理阶段，它既是Subscriber又是Publisher并且遵守两者的契约 也就是说它是一个 订阅元素-发布元素-处理的处理过程（3.5版本开始废弃，改用Sinks） Mono Mono 使用手册 单元测试 一个可以发出[0,1]个元素的发布者 简单发布订阅 1234567@Testpublic void test() &#123; Mono&lt;String&gt; source = Mono.just(&quot;test&quot;) // 定义发布数据的逻辑 .map(String::toUpperCase); // 操作符，处理发布的数据 source.subscribe(System.out::println); // 订阅数据&#125; Flux 单元测试 一个可以发出[1,无穷]个元素的发布者 简单发布订阅 1234567@Testpublic void test() &#123; Flux&lt;String&gt; source = Flux.just(&quot;test&quot;) // 定义发布数据的逻辑 .map(String::toUpperCase); // 操作符，处理发布的数据 source.subscribe(System.out::println); // 订阅数据&#125; Disposable 所有基于 lambda 的subscribe()都有一个 Disposable 返回类型。在这种情况下，Disposable 接口表示可以通过调用其 dispose() 方法取消订阅 对于 Flux 或 Mono，dispose() 是源停止生成元素的信号。但是，它不能保证马上取消，某些源可能会非常快地生成元素，因为在接收到取消指令之前源已完成 发布元素 just：热发布运算符，在定义后，数据即刻发布(冷与热) generate：同步单线程且每次回调只能发布一个元素 1234567Flux&lt;String&gt; flux = Flux.generate( () -&gt; 0, (state, sink) -&gt; &#123; // SynchronousSink sink.next(&quot;3 x &quot; + state + &quot; = &quot; + 3*state); if (state == 10) sink.complete(); return state + 1; &#125;); create：异步多线程发布元素 1234567Flux.create(fluxSink -&gt; &#123; for (int i = 0; i &lt; 5; i++) &#123; fluxSink.next(i); // 发布元素 &#125; fluxSink.complete(); // 完成发布&#125;); push：异步单线程发布元素 1234Flux.push(fluxSink -&gt; &#123; fluxSink.next(&quot;test&quot;); fluxSink.complete();&#125;); 背压 向上游传播信号也用作实现 背压，我们在组装流水线类比中将其描述为，当工作站的处理速度比上游工作站慢时，沿生产线向上发送反馈信号。背压其实就是一种下游向上游传递信号控制上游元素发布速率的一种手段，目的是均衡上下游的生产消费速率，保证稳定。(相当于MQ的拉取模式) 背压策略 IGNORE： 完全忽略下游背压请求，这可能会在下游队列积满的时候导致 IllegalStateException ERROR： 当下游跟不上节奏的时候发出一个 IllegalStateException 的错误信号 DROP： 当下游没有准备好接收新的元素的时候抛弃这个元素 LATEST： 让下游只得到上游最新的元素 BUFFER： （默认的）缓存所有下游没有来得及处理的元素（这个不限大小的缓存可能导致 OutOfMemoryError） 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849@Testpublic void test() throws InterruptedException &#123; CountDownLatch countDownLatch = new CountDownLatch(1); Flux&lt;Long&gt; flux = Flux.&lt;Long&gt;create(sink -&gt; &#123; for (long i = 0; i &lt; 100; i++) &#123; sink.next(i); try &#123; TimeUnit.MILLISECONDS.sleep(10); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; sink.complete(); &#125;, FluxSink.OverflowStrategy.LATEST) .publishOn(Schedulers.newSingle(&quot;newSingle&quot;), 1); flux.subscribe(new BaseSubscriber&lt;Long&gt;() &#123; @Override protected void hookOnSubscribe(Subscription subscription) &#123; // 订阅时设置每次请求元素个数 request(1); &#125; @Override protected void hookOnNext(Long value) &#123; log.info(&quot;消费流数据：&#123;&#125;&quot;, value); try &#123; TimeUnit.MILLISECONDS.sleep(50); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; // 每处理完1个数据，就再请求1个 request(1); &#125; @Override protected void hookOnError(Throwable throwable) &#123; log.error(throwable.getMessage(), throwable); &#125; @Override protected void hookOnComplete() &#123; countDownLatch.countDown(); &#125; &#125;); countDownLatch.await();&#125; 冷与热 冷 的发布者为每个订阅者重新生成数据。如果没有创建订阅，则永远不会生成数据。例如，如果源包装了一个 HTTP 调用，则会为每个订阅发出一个新的 HTTP 请求（在订阅前什么都不会发生） 热 的发布者不会为每个订阅者从头开始。相反，迟到的订阅者会收到他们订阅后发出的信号。但是请注意，一些 热 反应流可以缓存或重播全部或部分发布的元素。从一般的角度来看，热发布者甚至可以在没有订阅者监听时发布（打破在订阅前什么都不会发生，可能立即发布元素，即使没有订阅者） just就是为数不多的热发布者运算符，它在组装时直接捕获值，然后将其重播给任何订阅它的人。再次使用 HTTP 调用类比，如果捕获的数据是 HTTP 调用的结果，那么在实例化时只进行一次网络调用 defer可以将热发布者转变为冷发布者，它将我们示例中的 HTTP 请求推迟到订阅时触发（并且会导致对每个新订阅进行单独的网络调用） share() replay(… )可用于将冷发布者变成热发布者（至少在第一次订阅发生后） 冷发布者重播行为 123456789101112131415161718 @Test public void clodTest() &#123; Flux&lt;String&gt; source = Flux.fromIterable(Arrays.asList(&quot;blue&quot;, &quot;green&quot;, &quot;orange&quot;, &quot;purple&quot;)) .map(String::toUpperCase); source.subscribe(d -&gt; System.out.println(&quot;Subscriber 1: &quot;+d)); source.subscribe(d -&gt; System.out.println(&quot;Subscriber 2: &quot;+d)); &#125;// Subscriber 1: BLUE// Subscriber 1: GREEN// Subscriber 1: ORANGE// Subscriber 1: PURPLE// Subscriber 2: BLUE// Subscriber 2: GREEN// Subscriber 2: ORANGE// Subscriber 2: PURPLE 热发布者广播行为 12345678910111213141516171819202122 @Test public void hotTest() &#123; DirectProcessor&lt;String&gt; hotSource = DirectProcessor.create(); Flux&lt;String&gt; hotFlux = hotSource.map(String::toUpperCase); // 第一个订阅者 hotFlux.subscribe(d -&gt; System.out.println(&quot;Subscriber 1 to Hot Source: &quot;+d)); hotSource.onNext(&quot;blue&quot;); hotSource.onNext(&quot;green&quot;); // 第二个订阅者 hotFlux.subscribe(d -&gt; System.out.println(&quot;Subscriber 2 to Hot Source: &quot;+d)); hotSource.onNext(&quot;orange&quot;); hotSource.onNext(&quot;purple&quot;); hotSource.onComplete(); &#125;// Subscriber 1 to Hot Source: BLUE// Subscriber 1 to Hot Source: GREEN// Subscriber 1 to Hot Source: ORANGE// Subscriber 2 to Hot Source: ORANGE// Subscriber 1 to Hot Source: PURPLE// Subscriber 2 to Hot Source: PURPLE 调度器 要用什么线程池由用户自己决定，只提供配置线程池的方法 元素的发布和订阅可以使用publishOn和subscribeOn方法指定使用的调度线程 Reactor， 就像 RxJava，也可以被认为是 并发无关（concurrency agnostic） 的。意思就是， 它并不强制要求任何并发模型。更进一步，它将选择权交给开发者。不过，它还是提供了一些方便 进行并发执行的库 使用当前线程Schedulers.immediate() 可重用的单线程Schedulers.single()。注意，这个方法对所有调用者都提供同一个线程来使用 专用单线程Schedulers.newSingle()。如果想为每个调用都使用一个单独的线程执行则使用这个 弹性线程池Schedulers.elastic()。它根据需要创建一个线程池，重用空闲线程。线程池如果空闲时间过长 （默认为 60s）就会被废弃，没有线程数上限（新版本已废弃） 有界弹性线程池Schedulers.boundedElastic()。可以设置最大线程数和最大任务数的弹性线程池 固定大小线程池（Schedulers.parallel()）。所创建线程池的大小与 CPU 个数等同 使用JDK或第三方的线程池 Schedulers.fromExecutorService(ExecutorService) 123456789101112131415161718192021222324/** * Flux.create 异步多线程发布 */@Testpublic void createTest() throws InterruptedException &#123; CountDownLatch countDownLatch = new CountDownLatch(100); Scheduler scheduler = Schedulers.boundedElastic(); Flux.create(fluxSink -&gt; &#123; for (int i = 0; i &lt; 10; i++) &#123; ThreadUtil.sleep(100, TimeUnit.MILLISECONDS); log.info(&quot;发布元素&quot;); fluxSink.next(i); &#125; &#125;) .publishOn(scheduler) // 指定发布的线程池 .subscribeOn(scheduler) // 指定订阅的线程池 .subscribe(data -&gt; &#123; log.info(&quot;订阅元素：&#123;&#125;&quot;, data); countDownLatch.countDown(); &#125;); countDownLatch.await();&#125; 错误处理 在响应式流中，错误（error）是终止（terminal）事件。当有错误发生时，它会导致流序列停止， 并且错误信号会沿着操作链条向下传递，直至遇到你定义的 Subscriber 及其 onError 方法 Reactor 提供了其他的用于在链中处理错误的方法，即错误处理操作（error-handling operators） 静态缺省值 onErrorReturn：当Publisher出现错误时，返回一个默认值 12345678910@Testpublic void test() &#123; Mono&lt;Integer&gt; source = Mono.just(10) .map(item -&gt; item / 0) // 除零错误 .onErrorReturn(0); // 异常返回默认值，有重载方法，可根据异常类型，条件决定返回值 StepVerifier.create(source) .expectNext(0) .verifyComplete();&#125; 捕获并吃掉错误 onErrorComplete：吃掉Error信号，只需将 onError 信号替换为 onComplete 信号即可完成序列 12345678910111213@Testpublic void test() &#123; Mono source = Mono.just(1) .map(item -&gt; &#123; throw new RuntimeException(); &#125;) .onErrorComplete(); source.subscribe(AssertSubscriber.create() .assertNoError() .assertNoValues() .assertNotComplete());&#125; 异常处理方法 onErrorResume：当Publisher出现错误时，进行方法回调 12345678910111213141516171819202122232425@Test public void test() &#123; Mono&lt;String&gt; source = Mono.just(&quot;&quot;) .map(item -&gt; &#123; if (null == item || &quot;&quot;.equals(item)) &#123; throw new NullPointerException(); &#125; return item; &#125;) .onErrorResume(this::getDefault); StepVerifier.create(source) .expectNext(&quot;default&quot;) .verifyComplete(); &#125; /** * 异常回调方法 * @param throwable * @return */ private Mono&lt;String&gt; getDefault(Throwable throwable) &#123; return Mono.just(&quot;default&quot;); &#125; 捕获并重新抛出 onErrorMap：捕获异常，并重新转换一个新的异常然后继续向下传递 123456789101112@Test public void test() &#123; Mono source = Mono.just(1) .map(item -&gt; item / 0) .onErrorMap(e -&gt; &#123; return new RuntimeException(&quot;error&quot;); &#125;); StepVerifier.create(source) .expectError(RuntimeException.class) .verify(); &#125; 异常终止触发行为 doOnError：添加Mono因异常而终止时触发的行为，首先处理程序被执行，然后 onError 信号被传播到下游 123456789101112131415@Testpublic void test() &#123; Mono source = Mono.just(1) .map(item -&gt; &#123; throw new RuntimeException(); &#125;) .doOnError(throwable -&gt; &#123; // TODO 异常业务处理 throwable.printStackTrace(); &#125;); StepVerifier.create(source) .expectError() .verify();&#125; 使用Finally doFinally：Mono因任何原因终止后添加行为触发（包括取消、终止、完成），会传递一个信号量类型通知你 12345678910111213141516@Test public void test() &#123; Mono source = Mono.just(1) .map(item -&gt; &#123; throw new RuntimeException(); &#125;) .doFinally(signalType -&gt; &#123; if (SignalType.ON_ERROR.equals(signalType)) &#123; System.out.println(&quot;异常信号&quot;); &#125; &#125;); StepVerifier.create(source) .expectError() .verify(); &#125; try-with-resource using：主要将一个资源使用工厂方法方式为每个订阅者生成资源，第一个参数在订阅时创建资源，第二个参数 一个Mono工厂创建 Mono， 第三个参数 资源清理方法 1234567891011121314@Testpublic void test() &#123; Mono&lt;Object&gt; source = Mono.using( () -&gt; 1, resource -&gt; Mono.error(new RuntimeException()), item -&gt; &#123; // 类似 finally,完成或异常后会执行 System.out.println(&quot;释放资源&quot;); &#125;); StepVerifier.create(source) .expectError(RuntimeException.class) .verify();&#125; Exceptions 全局 Reactor 核心异常处理和操作工具，使用工具类传播和拆包异常 12345678910111213141516171819@Test public void test() &#123; Mono&lt;Integer&gt; source = Mono.just(1) .map(item -&gt; &#123; if (item == 1) &#123; // 包装传播一个异常 throw Exceptions.propagate(new IllegalArgumentException()); &#125; return item; &#125;); source.subscribe(System.out::print, e -&gt; &#123; // 订阅时处理传播的异常 if (Exceptions.unwrap(e) instanceof IllegalArgumentException) &#123; System.out.println(&quot;参数非法异常&quot;); &#125; &#125;); &#125; Sinks 在3.5.0版本之前使用的是Processor，Sinks在新版本之后用于淘汰Processor 在 Reactor 中，sink 是一个允许以独立方式安全手动触发信号的类，创建一个类似于 Publisher 的结构，能够处理多个 Subscriber(单播类型除外) Sinks.emitNext： 发布元素，EmitFailureHandler,发布失败处理程序 Sinks.Many 终止：（通常通过调用其 emitError(Throwable) 或 emitComplete() 方法），它会允许更多订阅者订阅，但会立即向他们重播终止信号 Sinks.UnicastSpec 单播规则 Sinks.ManySpec 多播规则 Sinks.MulticastSpec 多播规则 Sinks.MulticastReplaySpec 多播重播规则 多线程环安全类 Sinks.One Sinks.Many Sinks 建造者为主要支持的生产者类型提供了一个引导式 API。您会发现 Flux 中的一些行为，例如 onBackpressureBuffer 123456789101112131415161718@Test public void test() &#123; ExecutorService executorService = Executors.newFixedThreadPool(5); // 定义一个sinks Sinks.Many&lt;Integer&gt; sinks = Sinks.many().replay().all(); // 多线程手动调用发布元素 executorService.execute(() -&gt; sinks.emitNext(1, Sinks.EmitFailureHandler.FAIL_FAST)); executorService.execute(() -&gt; sinks.emitNext(2, Sinks.EmitFailureHandler.FAIL_FAST)); executorService.execute(() -&gt; sinks.emitNext(3, Sinks.EmitFailureHandler.FAIL_FAST)); // Sinks.Many 可以转为Flux。同理，Sinks.One可以转为Mono Flux&lt;Integer&gt; flux = sinks.asFlux().log(); // 多线程发布元素，因此输出的顺序不一定是1,2,3 flux.subscribe(System.out::println); &#125; 单播 仅允许一个订阅者订阅，多个订阅者订阅除了第一个订阅之外，其他订阅者会出现IllegalStateException异常 123456789@Testpublic void test() &#123; Sinks.Many&lt;Integer&gt; sinks = Sinks.many().unicast().onBackpressureBuffer(); sinks.emitNext(1, Sinks.EmitFailureHandler.FAIL_FAST); Flux&lt;Integer&gt; flux = sinks.asFlux(); flux.subscribe(AssertSubscriber.create().assertNoError().assertNotComplete()); flux.subscribe(AssertSubscriber.create().assertError(IllegalStateException.class).assertNotComplete());&#125; 多播 它只会将新推送的数据传输给它的多个订阅者，同时为每个订阅者提供背压 123456789@Testpublic void test() &#123; Sinks.Many&lt;Integer&gt; sinks = Sinks.many().multicast().onBackpressureBuffer(); sinks.emitNext(1, Sinks.EmitFailureHandler.FAIL_FAST); Flux&lt;Integer&gt; flux = sinks.asFlux(); flux.subscribe(AssertSubscriber.create().assertNoError().assertNotComplete()); flux.subscribe(AssertSubscriber.create().assertNoError().assertNotComplete());&#125; 重播 它将向多个Subscriber广播，并能够保留和重播所有历史的元素 123456789@Testpublic void test() &#123; Sinks.Many&lt;Integer&gt; sinks = Sinks.many().replay().all(); sinks.emitNext(1, Sinks.EmitFailureHandler.FAIL_FAST); Flux&lt;Integer&gt; flux = sinks.asFlux(); flux.subscribe(AssertSubscriber.create().assertNoError().assertNotComplete()); flux.subscribe(AssertSubscriber.create().assertNoError().assertNotComplete());&#125; Debug调试 在命令模式开发中，错误代码根据异常堆栈是相对容易定位到的。但是在响应式编程中，由于Publish封装了其操作符，并且调用链可能会很长，根据异常堆栈定位问题相对困难 123456789@Testpublic void test3() &#123; Mono&lt;String&gt; source = Flux.just(&quot;hello&quot;, &quot;world&quot;) .filter(item -&gt; null != item) .map(item -&gt; item.toLowerCase()) .single(); Disposable subscribe = source.subscribe();&#125; 1234567891011121314151617181920Caused by: java.lang.IndexOutOfBoundsException: Source emitted more than one item at reactor.core.publisher.MonoSingle$SingleSubscriber.onNext(MonoSingle.java:134) at reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onNext(FluxMapFuseable.java:129) at reactor.core.publisher.FluxFilterFuseable$FilterFuseableSubscriber.onNext(FluxFilterFuseable.java:118) at reactor.core.publisher.FluxArray$ArrayConditionalSubscription.fastPath(FluxArray.java:340) at reactor.core.publisher.FluxArray$ArrayConditionalSubscription.request(FluxArray.java:263) at reactor.core.publisher.FluxFilterFuseable$FilterFuseableSubscriber.request(FluxFilterFuseable.java:191) at reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.request(FluxMapFuseable.java:171) at reactor.core.publisher.MonoSingle$SingleSubscriber.doOnRequest(MonoSingle.java:103) at reactor.core.publisher.Operators$MonoInnerProducerBase.request(Operators.java:2841) at reactor.core.publisher.LambdaMonoSubscriber.onSubscribe(LambdaMonoSubscriber.java:121) at reactor.core.publisher.MonoSingle$SingleSubscriber.onSubscribe(MonoSingle.java:115) at reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onSubscribe(FluxMapFuseable.java:96) at reactor.core.publisher.FluxFilterFuseable$FilterFuseableSubscriber.onSubscribe(FluxFilterFuseable.java:87) at reactor.core.publisher.FluxArray.subscribe(FluxArray.java:50) at reactor.core.publisher.FluxArray.subscribe(FluxArray.java:59) at reactor.core.publisher.Mono.subscribe(Mono.java:4716) at reactor.core.publisher.Mono.subscribeWith(Mono.java:4784) at reactor.core.publisher.Mono.subscribe(Mono.java:4544) at reactor.core.publisherMonoBlockTest.test3(MonoBlockTest.java:66) 从异常堆栈中我们得到一个IndexOutOfBoundsException，它告诉我们这个源发布了太多的元素。我们只能猜测是由于Flux发布了多个元素最终调用single方法收敛为一个Mono时元素过多，但是在这个堆栈中我们提取不到任何有价值的信息，这显然对我们进行调试很不友好 激活调试模式 尽管堆栈跟踪仍然能够提取到一些有价值的信息，但我们可以看到，在更高级的情况下，它本身并不理想。幸运的是，Reactor 带有专为调试而设计的汇编时检测，这是通过在应用程序启动时通过 Hooks.onOperatorDebug() 方法激活全局调试模式来完成的（或者至少在可以实例化有异常的 Flux 或 Mono 之前） 通过包装操作符的构造并在那里捕获堆栈跟踪来检测对 Reactor 操作符方法的调用（它们被组装到链中）。由于这是在声明运算符链时完成的，因此应该在此之前激活Hook，因此最安全的方法是在应用程序开始时立即激活它 现在继续沿用上面的那段异常代码，我们加入激活调试模式再看看异常堆栈 12345678910@Testpublic void test3() &#123; Hooks.onOperatorDebug(); Mono&lt;String&gt; source = Flux.just(&quot;hello&quot;, &quot;world&quot;) .filter(item -&gt; null != item) .map(item -&gt; item.toLowerCase()) .single(); // 代码第65行 Disposable subscribe = source.subscribe();&#125; 12345678910111213reactor.core.Exceptions$ErrorCallbackNotImplemented: java.lang.IndexOutOfBoundsException: Source emitted more than one itemCaused by: java.lang.IndexOutOfBoundsException: Source emitted more than one item at reactor.core.publisher.MonoSingle$SingleSubscriber.onNext(MonoSingle.java:134)⑴ Suppressed: reactor.core.publisher.FluxOnAssembly$OnAssemblyException: ⑵Assembly trace from producer [reactor.core.publisher.MonoSingle] :⑶ reactor.core.publisher.Flux.single(Flux.java:8203) reactor.core.publisher.MonoBlockTest.test3(MonoBlockTest.java:65)Error has been observed at the following site(s):⑷ *__Flux.single ⇢ at reactor.core.publisher.MonoBlockTest.test3(MonoBlockTest.java:65)⑸Original Stack Trace:⑹ at reactor.core.publisher.MonoSingle$SingleSubscriber.onNext(MonoSingle.java:134)⑺ at reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onNext(FluxMapFuseable.java:129) at reactor.core.publisher.FluxFilterFuseable$FilterFuseableSubscriber.onNext(FluxFilterFuseable.java:118) ⑴：原始堆栈跟踪到MonoSingle,也就是说我们的元素被截断 ⑵：我们看到了捕获堆栈的包装器运算符。这是回溯开始出现的地方 ⑶：我们得到一些关于操作位置的细节MonoSingle ⑷：我们得到了错误传播的运算符链的观点，从头到尾（错误站点到订阅站点） ⑸：提到了错误的每个操作以及使用它的用户类和所在行数，这里我们有一个“根” ⑹：跟踪堆栈的其余部分 ⑺：展示了一些操作的内部结构 最终根据“根”的提示和分析，我们定位到了single()方法出现异常 checkpoint 调试模式是全局的，会影响应用程序中组装到 Flux 或 Mono 中的每个运算符 正如我们之前看到的，这种全局知识是以影响性能为代价的（由于填充的堆栈跟踪的数量）。如果我们知道可能存在问题的操作，则可以降低该成本。但是，我们通常不知道哪些操作符可能有问题，除非复现BUG调试 可以将checkpoint()运算符链接到方法链中。检查点运算符的工作方式类似于Hook，但仅适用于该特定的方法链 1234567891011@Testpublic void test3() &#123; Mono&lt;String&gt; source = Flux.just(&quot;hello&quot;, &quot;world&quot;) .filter(item -&gt; null != item) .map(item -&gt; item.toLowerCase()) .checkpoint(&quot;1&quot;) .single() .checkpoint(&quot;2&quot;); Disposable subscribe = source.subscribe();&#125; 我们可以在一个方法链中插入一些检查点，最终这些检查点会协助我们定位到出现问题的代码，上面代码插入2个检查点，最终在异常堆栈中只输出检查点2，因此我们可以定位问题介于检查点1和2之间 12345678910reactor.core.Exceptions$ErrorCallbackNotImplemented: java.lang.IndexOutOfBoundsException: Source emitted more than one itemCaused by: java.lang.IndexOutOfBoundsException: Source emitted more than one item at reactor.core.publisher.MonoSingle$SingleSubscriber.onNext(MonoSingle.java:134) Suppressed: reactor.core.publisher.FluxOnAssembly$OnAssemblyException: Error has been observed at the following site(s): *__checkpoint ⇢ 2Original Stack Trace: at reactor.core.publisher.MonoSingle$SingleSubscriber.onNext(MonoSingle.java:134) at reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onNext(FluxMapFuseable.java:129) at reactor.core.publisher.FluxFilterFuseable$FilterFuseableSubscriber.onNext(FluxFilterFuseable.java:118) 现成的全局调试方案 Project Reactor 附带一个单独的 Java 代理，它可以检测您的代码并添加调试信息，而无需在每次操作调用时捕获堆栈跟踪。该行为与激活调试模式非常相似 - 也称为回溯，但没有运行时性能开销 它需要使用Maven进行依赖集成 1234&lt;dependency&gt; &lt;groupId&gt;io.projectreactor&lt;/groupId&gt; &lt;artifactId&gt;reactor-tools&lt;/artifactId&gt;&lt;/dependency&gt; 它还需要显式初始化 1ReactorDebugAgent.init(); 由于该实现将在加载类时对其进行检测，因此最好将其放置在 main(String[]) 方法中的其他所有内容之前 1234public static void main(String[] args) &#123; ReactorDebugAgent.init(); SpringApplication.run(Application.class, args);&#125; ReactorDebugAgent 作为 Java Agent 实现，并使用 ByteBuddy 进行字节码扩展。可能无法在某些 JVM 上工作，请参阅 ByteBuddy 的文档以获取更多详细信息 如果当前开发环境不支持 ByteBuddy 字节码扩展，可以将 reactor-tools 作为 Java Agent 运行 1java -javaagent reactor-tools.jar -jar app.jar 修改代码 12345678910@Testpublic void test() &#123; ReactorDebugAgent.init(); Mono&lt;String&gt; source = Flux.just(&quot;hello&quot;, &quot;world&quot;) .filter(item -&gt; null != item) .map(item -&gt; item.toLowerCase()) .single(); Disposable subscribe = source.subscribe();&#125; 123456789101112131415Test class reactor.tools.agent.ReactorDebugJavaAgentTest[ERROR] (Test worker) Operator called default onErrorDropped - reactor.core.Exceptions$ErrorCallbackNotImplemented: java.lang.IndexOutOfBoundsException: Source emitted more than one itemreactor.core.Exceptions$ErrorCallbackNotImplemented: java.lang.IndexOutOfBoundsException: Source emitted more than one itemCaused by: java.lang.IndexOutOfBoundsException: Source emitted more than one item at reactor.core.publisher.MonoSingle$SingleSubscriber.onNext(MonoSingle.java:134) Suppressed: The stacktrace has been enhanced by Reactor, refer to additional information below: Assembly trace from producer [reactor.core.publisher.MonoSingle] : reactor.core.publisher.Flux.single reactor.tools.agent.ReactorDebugJavaAgentTest.test(ReactorDebugJavaAgentTest.java:62)Error has been observed at the following site(s): *__Flux.single ⇢ at reactor.tools.agent.ReactorDebugJavaAgentTest.test(ReactorDebugJavaAgentTest.java:62) // 根Original Stack Trace: at reactor.core.publisher.MonoSingle$SingleSubscriber.onNext(MonoSingle.java:134) at reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onNext(FluxMapFuseable.java:129) at reactor.core.publisher.FluxFilterFuseable$FilterFuseableSubscriber.onNext(FluxFilterFuseable.java:118) 日志记录 日志操作使用 Loggers 类，它通过 SLF4J 选择常见的日志记录框架，例如 Log4J 和 Logback，如果 SLF4J 不可用，则默认记录到控制台 在生产环境中使用，应该注意配置底层日志记录框架以使用其最异步和非阻塞的方法——例如，Logback 中的 AsyncAppender 或 Log4j 2 中的 AsyncLogge 12345678910@Testpublic void test3() &#123; Mono&lt;String&gt; source = Flux.just(&quot;hello&quot;, &quot;world&quot;) .log() .filter(item -&gt; null != item) .map(item -&gt; item.toLowerCase()) .single(); Disposable subscribe = source.subscribe();&#125; 12345678910111213Test class reactor.core.publisher.MonoBlockTest00:08:19.834 [Test worker] INFO reactor.Flux.Array.1 - | onSubscribe([Synchronous Fuseable] FluxArray.ArrayConditionalSubscription)00:08:19.846 [Test worker] INFO reactor.Flux.Array.1 - | request(unbounded)00:08:19.847 [Test worker] INFO reactor.Flux.Array.1 - | onNext(hello)00:08:19.848 [Test worker] INFO reactor.Flux.Array.1 - | onNext(world)00:08:19.848 [Test worker] INFO reactor.Flux.Array.1 - | cancel() // 在获取第二个元素后流被取消00:08:19.860 [Test worker] ERROR reactor.core.publisher.Operators - Operator called default onErrorDroppedreactor.core.Exceptions$ErrorCallbackNotImplemented: java.lang.IndexOutOfBoundsException: Source emitted more than one itemCaused by: java.lang.IndexOutOfBoundsException: Source emitted more than one item at reactor.core.publisher.MonoSingle$SingleSubscriber.onNext(MonoSingle.java:134) at reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onNext(FluxMapFuseable.java:129) at reactor.core.publisher.FluxFilterFuseable$FilterFuseableSubscriber.onNext(FluxFilterFuseable.java:118) at reactor.core.publisher.FluxPeekFuseable$PeekFuseableConditionalSubscriber.onNext(FluxPeekFuseable.java:503) 通过log日志定位后，我们能够定位到当在获取第二个元素时流被取消，结合异常信息和源码，我们知道在调用single()方法时Flux存在多个元素 性能指标监控 TODO（依赖未发布到Maven仓库） 高级特性 高级特性 上下文传播 3.5.0版本之后内置了Context-Propagation第三方上下文传播库，能够实现ThreadLocal-&gt;Context的数据传播 数据库篇 R2DBC 响应式的关系型数据库驱动规范 全称是Reactive Relational Database Connectivity 响应式关系型数据库连接 和JDBC一样，R2DBC是使用响应式编程实现的关系型数据库驱动规范，是非阻塞型的。而JDBC则是传统编程实现的阻塞型驱动规范 注意：如果将传统项目升级为响应式项目，那么最大的阻力就是驱动切换，只有将Spring MVC 升级为webflux并且将JDBC启动替换为R2DBC才是一个响应式项目。如果单升级webflux的关系型数据库项目将毫无意义，不能发挥响应式编程的特性 已经实现R2DBC驱动的数据库 已经实现或在调研中的ORM客户端 spring-data-r2dbc spring-data-r2dbc是spring-data项目的一部分（大名顶顶的spring-data项目几乎支持了市面上主流的数据库），实现了R2DBC和JPA协议，是一个响应式的关系型数据库的ORM框架，可以很轻松使用JPA规范去编写响应式数据库操作（JPA规范比较有名的实现是Hibernate） reactive-mybatis 基于mybatis-r2dbc改造实现的响应式版mybatis,实现了ORM框架基本功能，目前无法使用Mybatis插件功能 实践篇 WebFlux 官方参考文档地址 国人翻译wiki 国人翻译 背景 项目 ruoyi-webflux-r2dbc-vue3 若依是一套全部开源的快速开发平台，毫无保留给个人及企业免费使用。 这是 RuoYi WebFlux + R2DBC + Vue3 的实现。 前端技术栈: Vue3 + Element Plus + Vite 是分支于 https://github.com/yangzongzhuan/RuoYi-Vue3 (版本: 3.8.2)。 后端采用 Spring Boot、Spring Security、Redis &amp; Jwt。 后端采用 Spring Boot WebFlux、R2DBC、MyBatis、MyBatis-R2DBC、Mysql。 权限认证使用 Jwt，支持多终端认证系统。 支持加载动态权限菜单，多方式轻松权限控制。 高效率开发，使用代码生成器可以一键生成前后端代码。 内置功能 用户管理：用户是系统操作者，该功能主要完成系统用户配置。 部门管理：配置系统组织机构（公司、部门、小组），树结构展现支持数据权限。 岗位管理：配置系统用户所属担任职务。 菜单管理：配置系统菜单，操作权限，按钮权限标识等。 角色管理：角色菜单权限分配、设置角色按机构进行数据范围权限划分。 字典管理：对系统中经常使用的一些较为固定的数据进行维护。 参数管理：对系统动态配置常用参数。 通知公告：系统通知公告信息发布维护。 操作日志：系统正常操作日志记录和查询；系统异常信息日志记录和查询。 登录日志：系统登录日志记录查询包含登录异常。 在线用户：当前系统中活跃用户状态监控。 定时任务：在线（添加、修改、删除)任务调度包含执行结果日志。 代码生成：前后端代码的生成（java、html、xml、sql）支持 CRUD 下载 。 系统接口：根据业务代码自动生成相关的 api 接口文档。 服务监控：监视当前系统 CPU、内存、磁盘、堆栈等相关信息。 缓存监控：对系统的缓存信息查询，命令统计等。 在线构建器：拖动表单元素生成相应的 HTML 代码。 连接池监视：不支持此功能。 微服务 Reactive Feign","categories":[],"tags":[{"name":"文章已过时","slug":"文章已过时","permalink":"https://wugengfeng.cn/tags/%E6%96%87%E7%AB%A0%E5%B7%B2%E8%BF%87%E6%97%B6/"}]},{"title":"分布式算法理论","slug":"分布式算法理论","date":"2022-10-10T09:27:36.000Z","updated":"2023-11-14T02:41:32.818Z","comments":true,"path":"2022/10/10/分布式算法理论/","link":"","permalink":"https://wugengfeng.cn/2022/10/10/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95%E7%90%86%E8%AE%BA/","excerpt":"","text":"分布式理论 CAP定理 不可能三角 CAP不可能同时得到满足 CAP定理，也称为布鲁尔定理（Brewer’s Theorem），是由加州大学伯克利分校的计算机科学家Eric Brewer在2000年提出的。这个定理是分布式计算领域的一个基本原则，它描述了分布式系统在设计时需要在一致性（Consistency）、可用性（Availability）和分区容忍性（Partition tolerance）三个方面做出权衡。 C：Consistency（一致性）A：Availability（可用性）P：Partition tolerance（分区容忍性） CAP定理的核心观点是，在任何给定的时刻，一个分布式系统只能同时满足上述三个要素中的两个。换句话说，如果系统保证了分区容忍性，它就必须在一致性和可用性之间做出选择。这个权衡的存在是因为在发生网络分区时，系统不可能同时保证数据的绝对一致性和高可用性。 Partition Tolerance 分区容忍性 网络波动 通讯故障 分区容忍性是指系统能够持续提供服务，即使出现了网络分区，也就是说，一些节点之间的通信可能因为网络故障而中断。在现实世界中，网络分区是不可避免的，因此分区容忍性在分布式系统中是必须要考虑的。 解决方案：增强分区容忍性，通常会在多个节点间复制数据，以确保即使在某些节点之间的网络连接失败时，系统的其他部分仍然可以继续运行。这种策略的关键在于，数据的多个副本分布在不同的网络分区中，可以独立地接受和处理请求。 AC悖论 如果选择一致性和分区容忍性（CP），系统在网络分区发生时会牺牲可用性来保证数据的一致性（强一致性，任何节点宕机都不允许进行写操作）。这意味着某些操作可能会因为无法保证数据的一致性而不被处理。 如果选择可用性和分区容忍性（AP），系统在网络分区发生时会牺牲一致性来保证服务的可用性。在这种情况下，用户可能会读到过时的数据。 Consistency 一致性 一致性意味着所有节点在同一时间看到的数据是一样的。换句话说，如果一个数据项在分布式系统的一个节点上被更新，那么所有的其他节点都应该立即知道这个更新。这是一个强一致性模型，类似于关系数据库中的事务，它保证了数据的一致性视图。 Availability 可用性 可用性是指系统的每个请求都能在有限的时间内收到一个响应，不管是成功还是失败的响应。在分布式系统中，即使某些节点宕机，系统仍然需要对外提供服务。 由于实现强一致性的代价很高，可能会显著影响写入性能，因此许多开源的分布式系统选择在CAP定理的CP模型中实现最终一致性或线性一致性，这些是相对较弱的一致性模型。这种在可用性（Availability）和一致性（Consistency）之间的权衡催生了BASE理论，它强调在分布式系统中基本可用性、软状态和最终一致性的重要性。 BASE理论 BASE理论是在CAP定理的背景下发展起来的，它是对一致性（Consistency）和可用性（Availability）之间权衡的结果。该理论的核心思想是，虽然在分布式系统中难以实现强一致性，但是每个应用都可以根据自己的业务需求，采用适当的策略来确保系统最终达到一致性（Eventual Consistency）。 在CAP定理指出一致性（C）和可用性（A）难以同时满足的情况下，BASE理论提出了一种替代方案。这种方案通过放宽对强一致性的要求，来提高系统的可用性和容错性。这样的设计允许在分布式系统中实现更高的性能和可伸缩性。 Basically Available（基本可用）基本可用指的是分布式系统在遇到故障时，保证核心功能可用。这意味着在发生故障时，系统可能会提供服务降级，但核心功能仍然是可用的。例如，一个在线商店在高流量期间可能会暂时关闭评论功能，以确保核心的购物车和结账功能仍然可以使用。 Soft state（软状态）软状态意味着系统的状态不需要时刻保持一致，而是可以有一段时间的不一致。这允许系统在不同步的情况下继续运行，而不是强制每次操作都立即同步。 Eventually consistent（最终一致性）最终一致性是指，系统中的所有数据副本，在经过一段时间的同步之后，最终会达到一个一致的状态。这是对强一致性要求的放松，允许系统在一定时间内存在不一致的状态。 案例说明假设有一个大型在线商店，它需要处理成千上万的用户请求，包括浏览商品、添加到购物车、结账等操作。在这种高流量的环境中，采用BASE理论可以帮助商店在保持高可用性的同时，处理数据一致性问题。Basically Available（基本可用）：在促销或高流量期间，为了保证核心交易系统的可用性，商店可能会暂时降级某些非关键功能，比如推荐算法或用户评论。这意味着即使在高负载下，用户仍然可以浏览商品、添加到购物车并完成购买，尽管某些附加功能可能暂时不可用。Soft state（软状态）：商品的库存信息可能不会实时更新。例如，当一个用户将商品添加到购物车时，系统不会立即从库存中扣除。相反，库存的准确计算可能会延迟进行，以减少对数据库的即时写入压力。这意味着在高并发情况下，系统的状态（如库存量）是“软的”，可能不会立即反映最新情况。Eventually consistent（最终一致性）：用户完成订单支付后，用户积分不会马上赠送，但在经过一段时间后，最终会将积分回馈给用户。通过这种方式，在线商店能够在保持高可用性和良好用户体验的同时，处理大规模数据和请求。虽然这可能导致短暂的数据不一致，但这种不一致是可控的，并且不会严重影响用户的购物体验。 AKF拆分原则 随着项目规模的不断扩大，传统的单体架构往往无法有效地满足日益增长的性能和可伸缩性需求。在这种背景下，转向分布式系统成为了一种必要的选择。分布式系统的设计和管理带来了一系列挑战，尤其是在可伸缩性（Scalability）方面。为了应对这些挑战，需要一种系统化的方法论来指导分布式系统的设计和扩展。AKF立方体模型正是为此而提出的，它提供了一种系统化的框架来指导技术架构和组织结构的扩展。 AKF 把系统扩展分为以下三个维度 X轴 水平扩展X轴扩展涉及在水平方向上增加相同类型的资源（如服务器、数据库实例）来处理更多的工作负载。这种扩展方式通常用于处理增加的用户数量或请求量。例如，一个Web应用可以通过增加更多的Web服务器来分散流量，从而提高整体的处理能力。 Y轴 服务分解Y轴扩展涉及将应用程序或服务分解为更小、更专注的部分，这通常是通过微服务架构来实现的。每个微服务负责应用程序的一个特定功能或业务领域。这种方法不仅有助于提高可伸缩性，还有助于提高团队的敏捷性，因为每个团队可以独立开发和部署其负责的服务。 Z轴 数据分片Z轴扩展涉及将数据分割成多个分片，每个分片可以独立存储和处理。这种方式常用于数据库，可以通过数据的关键属性（如用户ID）来分割数据。每个分片可以部署在不同的服务器上，从而分散负载和减少单点故障的风险。 假设对一个单体应用的电子商务平台进行AKF立体模型扩展。主要解决原有架构的可伸缩性和性能问题。水平扩展提高了处理高流量的能力，服务分解提高了开发效率和系统的灵活性，数据分片则解决了数据库的性能瓶颈。这使得平台能够有效地支持用户增长和业务扩展，同时保持高效的运营和快速的市场响应。 分布式算法 一致性 什么是一致性 在分布式系统中，一致性（Consistency）是指多副本（Replications）问题中的数据一致性。可以分为强一致性、顺序一致性与弱一致性 一致性的种类 事务一致性 数据一致性 强一致性（Strict Consistency） 也称为： 原子一致性（Atomic Consistency） 线性一致性（Linearizable Consistency） 两个要求： 任何一次读都能读到某个数据的最近一次写的数据。 系统中的所有进程，看到的操作顺序，都和全局时钟下的顺序一致。 简言之，在任意时刻，所有节点中的数据是一样的。 例如，对于关系型数据库，要求更新过的数据能被后续的访问都能看到（整个集群的更改过程是同步的），这是强一致性。 顺序一致性（Sequential Consistency） 任何执行的结果都是一样的，就好像所有处理器的操作都按某种顺序执行一样，并且每个单独处理器的操作按其程序指定的顺序出现在这个顺序中 两个要求： 任何一次读都能读到某个数据的最近一次写的数据。 系统的所有进程的顺序一致，而且是合理的。即不需要和全局时钟下的顺序一致，错的话一起错，对的话一起对。 弱一致性 数据更新后，如果能容忍后续的访问只能访问到部分或者全部访问不到，则是弱一致性 最终一致性 最终一致性属于弱一致性 不保证在任意时刻任意节点上的同一份数据都是相同的，但是在一段时间后，节点间的数据会最终达到一致状态 一致性hash算法 hash 算法 MD系列(MD5)、SHA系列(SHA-1)、CRC，甚至JDK hashCode()也是哈希算法的一种。可以将他们分成三代： 第一代：SHA-1（1993），MD5（1992），CRC（1975），Lookup3（2006） 第二代：MurmurHash（2008） 第三代：CityHash， SpookyHash（2011） 分类可分为加密型、非加密型： 加密型：MD系列(MD5)、SHA系列(SHA-1) 非加密型：CRC、MurmurHash 使用场景： MD5：消息摘要算法，常见的场景就是下载文件时进行MD5值验证 SHA：安全散列算法，该算法的思想是接收一段明文，然后以一种不可逆的方式将它转换成一段密文，常用于明文密码加密 CRC：循环冗余校验码，简称循环码，是一种常用的、具有检错、纠错能力的校验码，在早期的通信中运用广泛 murmurhash：MurmurHash 是一种非加密型哈希函数，适用于一般的哈希检索操作 murmurhash 高运算性能 低碰撞率 Redis，Memcached，Cassandra，Hadoop，HBase，Lucene，spark，nginx，常见的大数据库底层，都使用了这个算法作为底层的存储算法 hash环 分布式缓存 负载均衡算法 ​ 一致性Hash是一种特殊的Hash算法，由于其均衡性、持久性的映射特点，被广泛的应用于负载均衡领域，如nginx和memcached都采用了一致性Hash来作为集群负载均衡的方案 传统hash ​ 要了解一致性哈希，首先我们必须了解传统的哈希及其在大规模分布式系统中的局限性。简单地说，哈希就是一个键值对存储，在给定键的情况下，可以非常高效地找到所关联的值。假设我们要根据其邮政编码查找城市中的街道名称。一种最简单的实现方式是将此信息以哈希字典的形式进行存储 &lt;Zip Code，Street Name&gt; ​ 当数据太大而无法存储在一个节点或机器上时，问题变得更加有趣，系统中需要多个这样的节点或机器来存储它。比如，Elasticsearch的分片路由方案。那如何确定哪个 key 存储在哪个节点上？针对该问题，最简单的解决方案是使用哈希取模来确定。 给定一个 key，先对 key 进行哈希运算，将其除以系统中的节点数，然后将该 key 放入该节点。同样，在获取 key 时，对 key 进行哈希运算，再除以节点数，然后转到该节点并获取值。上述过程对应的哈希算法定义如下 routing = hash(key) % len # 其中 len 为节点数 缺点 ​ 其实通过Elasticsearch的分片策略就可以得出传统hash的局限性，当Elasticsearch的主分片数量设置后，是不能修改的，因为数据通过 hash(key) % len 计算后就散列在某个主分片上了，如果修改主分片数量（修改 len）则key的路由会失败。所以传统hash是不具备动态扩容 缩容能力的 一致性哈希算法 ​ 为了解决传统hash上的缺陷，一致性哈希算法应运而生 ​ 一致性哈希算法在 1997 年由麻省理工学院提出，是一种特殊的哈希算法，在移除或者添加一个服务器时，能够尽可能小地改变已存在的服务请求与处理请求服务器之间的映射关系。一致性哈希解决了简单哈希算法在分布式哈希表（Distributed Hash Table，DHT）中存在的动态伸缩等问题 一致性hash算法的特性 平衡性：尽可能让数据尽可能分散到所有节点上，避免造成极其不均匀 单调性：要求在新增或者减少节点的时候，原有的结果绝大部分不受影响，而新增的数据尽可能分配到新加的节点 分散性：好的算法在不同终端，针对相同的数据的计算，得到的结果应该是一样的，一致性要很强 负载：针对相同的节点，避免被不同终端映射不同的内容 平滑性：对于增加节点或者减少节点，应该能够平滑过渡 一致性哈希算法原理 一致性哈希算法通过一个叫作一致性哈希环的数据结构实现。这个环的起点是 0，终点是 2^32 - 1，并且起点与终点连接，故这个环的整数分布范围是 [0, 2^32-1] 算法原理 虚拟一个环的概念，在环上构造一个0～2^32-1个点 将N台服务器节点计算Hash值，映射到这个环上 可以用节点的IP或者主机名来计算hash值，得到一个Hash环 将数据用相同的Hash算法计算的值，映射到这个环上 然后顺时针寻找，找到的第一个服务器节点就是目标要保存的节点，如果超过2^32-1，就放到第一个节点 key1、key2 被映射到NodeB key3、key4、key5 被映射到NodeB key6、key7、key8 被映射到NodeA 动态扩容 ​ 假设随着业务的扩大，整个集群需要扩展一个服务节点Node D,经过同样的 hash 运算，该服务器最终落于 Node A 和 Node C 服务器之间，具体如下图所示： ​ 对于上述的情况，只有 Node A 和 Node C之间的key需要重新分配。在以上示例中key6、key7 需要重新分配，即它被重新到 Node D 。在前面我们已经分析过，如果使用简单的取模方法，当新添加服务器时可能会导致大部分缓存失效，而使用一致性哈希算法后，这种情况得到了较大的改善，因为只有少部分对象需要重新分配 故障转移（动态缩容） ​ 假设某个服务节点宕机，只要hash环上还存在可用节点，那么服务整体还是可用的 ​ 假设Node B宕机，那么key1、key2将跳过故障节点Node B，继续顺时针找到第一个可用节点Node C，由Node C继续提供服务，实现故障转移 数据倾斜（虚拟节点） ​ 假设根据hash计算后，服务节点在hash环上散列不均匀。那么就会导致数据倾斜，某个服务节点就会承受较高的服务压力，整个集群变得不稳定。解决方法就是在环上增加虚拟节点，尽量让服务节点在环上分布均匀，从而实现负载均衡 不带虚拟节点的hash一致性算法 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465import java.util.SortedMap;import java.util.TreeMap;public class ConsistentHashingWithoutVirtualNode &#123; //待添加入Hash环的服务器列表 private static String[] servers = &#123;&quot;192.168.0.1:8888&quot;, &quot;192.168.0.2:8888&quot;, &quot;192.168.0.3:8888&quot;&#125;; //key表示服务器的hash值，value表示服务器 private static SortedMap&lt;Integer, String&gt; sortedMap = new TreeMap&lt;Integer, String&gt;(); //程序初始化，将所有的服务器放入sortedMap中 static &#123; for (int i = 0; i &lt; servers.length; i++) &#123; int hash = getHash(servers[i]); System.out.println(&quot;[&quot; + servers[i] + &quot;]加入集合中, 其Hash值为&quot; + hash); sortedMap.put(hash, servers[i]); &#125; &#125; //得到应当路由到的结点 private static String getServer(String key) &#123; //得到该key的hash值 int hash = getHash(key); //得到大于该Hash值的所有Map SortedMap&lt;Integer, String&gt; subMap = sortedMap.tailMap(hash); if (subMap.isEmpty()) &#123; //如果没有比该key的hash值大的，则从第一个node开始 Integer i = sortedMap.firstKey(); //返回对应的服务器 return sortedMap.get(i); &#125; else &#123; //第一个Key就是顺时针过去离node最近的那个结点 Integer i = subMap.firstKey(); //返回对应的服务器 return subMap.get(i); &#125; &#125; //使用FNV1_32_HASH算法计算服务器的Hash值 private static int getHash(String str) &#123; final int p = 16777619; int hash = (int) 2166136261L; for (int i = 0; i &lt; str.length(); i++) hash = (hash ^ str.charAt(i)) * p; hash += hash &lt;&lt; 13; hash ^= hash &gt;&gt; 7; hash += hash &lt;&lt; 3; hash ^= hash &gt;&gt; 17; hash += hash &lt;&lt; 5; // 如果算出来的值为负数则取其绝对值 if (hash &lt; 0) hash = Math.abs(hash); return hash; &#125; public static void main(String[] args) &#123; String[] keys = &#123;&quot;semlinker&quot;, &quot;kakuqo&quot;, &quot;fer&quot;&#125;; for (int i = 0; i &lt; keys.length; i++) System.out.println(&quot;[&quot; + keys[i] + &quot;]的hash值为&quot; + getHash(keys[i]) + &quot;, 被路由到结点[&quot; + getServer(keys[i]) + &quot;]&quot;); &#125;&#125; 参考文章：https://zhuanlan.zhihu.com/p/146011745 hash槽 共识算法 拜占庭将军问题 想象拜占庭帝国的军队在攻击一个城市。军队被分散到城市周围的几个营地中，每个营地由一位将军指挥。为了攻占城市，将军们必须达成共识决定他们是同时进攻还是撤退。将军们通过信使来发送和接收消息，因为他们相隔很远，不能直接通讯。问题在于，一些将军可能是叛徒，他们可能会故意发送错误的攻击计划。例如，忠诚的将军可能决定明天早上进攻，但是叛徒将军可能告诉一些将军明天进攻，告诉其他将军撤退。由于信使可能也是叛徒，消息在传递过程中可能被篡改。 问题场景： 有若干个将军，他们需要就是否进攻或撤退达成一致决策。 这些将军分散在不同位置，仅能通过传递消息进行通信。 至少一个将军可能是叛徒，他可能会发送错误的消息或篡改消息。 问题复杂性： 叛徒可能不仅仅是发送错误的信息，他们可能会根据其他将军的行为来决定自己的行为，以最大化混乱和误导。 叛徒的行为模式可能非常复杂，包括但不限于假装忠诚、完全沉默或不一致的消息传递。 问题本质： ​ 分布式系统如何在部分组件可能发生故障或表现出恶意行为的情况下，仍然能够达成一致的决策。 解决方案要求： 所有忠诚的将军必须达成相同的决策。 如果所有忠诚的将军都决定进攻，那么他们应该进攻；如果一个忠诚的将军认为应该撤退，那么他们都应该撤退（以避免部分军队进攻导致的灾难性后果）。 解决方案挑战： 忠诚将军之间的通信可能被叛徒所干扰。 忠诚将军需要一种方法来确定哪些消息是可信的。 实际应用： 在实际的计算机网络和分布式系统中，拜占庭将军问题等同于如何在可能存在恶意节点的系统中达成共识。这个问题的解决方案形成了拜占庭容错（Byzantine Fault Tolerance, BFT）算法的基础，这类算法能够确保系统即使在一些节点表现出任意或恶意行为时仍然能够正常工作。 拜占庭容错性在现代分布式系统中非常重要，特别是在需要高安全性的系统中，如金融服务、航空交通控制系统和区块链技术。例如，比特币的区块链网络使用了工作量证明（Proof of Work）机制来实现拜占庭容错，并确保整个网络达成共识。 Paxos共识算法 具有高度容错的分布式共识算法` 解决分布式系统就某个值（提议）达成一致 动态演示地址 背景 Paxos算法由Leslie Lamport于1989年提出，并在1998年详细描述和发表。Lamport是分布式系统理论的先驱，他通过Paxos算法提供了一个基础解决方案，该方案能够保证即使在某些节点失效的情况下，系统也能达成一致的决策。Paxos算法以古希腊岛屿Paxos命名，Lamport用它来描述一个虚构的议会系统，以帮助理解这一抽象的问题。 前提条件：非拜占庭将军问题，保证军队没有叛徒（即通信是保证可靠的不会被传改的），但是可以存在丢失延迟等问题 角色介绍 提议者（Proposer）： 负责发起提案（proposal），每个提案包含一个唯一的编号（proposal number）和一个建议值（value）。 提议者需要确保其提案编号是唯一的，并且在其生命周期中递增。 提议者的目标是让其提案获得足够多的接受者（Acceptor）的接受。 接受者（Acceptor）： 在Paxos算法中起决定性作用，它们对提议者的提案给予响应。 接受者可以决定是否对收到的提案做出承诺（promise）不接受任何较低编号的提案，以及是否接受（accept）提议者的提案。 一个提案如果得到了集群中超过半数的接受者的接受，就被认为是被“选择”（chosen）了。 学习者（Learner）： 学习者的作用是了解被多数接受者接受的提案的值。 学习者不参与提案的接受或拒绝过程，但它们需要知道哪个提案被最终确认，以便系统可以按照这个值进行操作。 Proposal : 提议 提议编号n和内容value 算法最终目标：每个Proposer, Acceptor, Learner都认为同一个Proposal中的value被选中。 Paxos共识算法追求的是线性一致性。 参考文章：https://juejin.cn/post/6844903817297788942 参考B站视频：https://www.bilibili.com/medialist/play/ml1801382893/BV1kA411G7cK?oid=333101519&amp;otype=2 Basic Paxos 单提议的分布式共识算法,只是理论,没有被实现 一个或多个提议进程 (Proposer) 可以发起提案 (Proposal)，Paxos算法使所有提案中的某一个提案，在所有进程中达成一致。系统中的多数派同时认可该提案，即达成了一致。最多只针对一个确定的提案达成一致。 两阶段式提交 Prepare阶段 提议者（Proposer）生成一个提案编号，并向所有接受者（Acceptors）发送一个包含该编号的Prepare请求。 接受者（Acceptor）收到Prepare请求后，如果提案编号大于它之前承诺过的任何编号，它会向提议者发出承诺，不再接受编号小于该提案编号的任何提案，并且回复它所接受的最大编号的提案（如果有的话）。 Accept阶段 如果提议者从多数接受者处收到了承诺回应，它将向所有接受者发送Accept请求，该请求包含提案编号和提案值。 当接受者收到Accept请求时，如果它没有对更高编号的提案做出过承诺，则接受该提案。 活锁 Basic Paxos中产生活锁（Livelock）的原因主要是因为它允许多个提议者（Proposers）并发地尝试推动他们的提案，而这些提案可能会相互冲突。活锁的情况通常出现在一个高度竞争的环境中，其中每个提议者都试图将其提案编号（Proposal Number）设置为最高，以便它们的提案被接受者（Acceptors）接受。 以下是产生活锁的几个具体原因： 竞争条件： 多个提议者可能同时发起提案，每个提议者都试图使用比其他人更高的提案编号。 由于网络延迟，不同提议者的提案可能几乎同时到达接受者。 多数接受者承诺： 接受者承诺接受最高编号的提案，如果不同提议者的提案编号不断增加，接受者可能不断改变其承诺。 不断的提案编号增加： 提议者收到拒绝其提案的消息时，通常会选择一个更高的编号重试。 如果提议者不断地观察到其他提议者使用更高的编号，它们也会相应地增加自己的编号，并重新发起提案。 Multi Paxos Multi Paxos 做出的优化 Leader 选举：在Multi Paxos中，系统会选举出一个稳定的 Leader，这个 Leader 负责协调提案的过程。这减少了因提议者间的竞争而引起的活锁问题（只有Leader 能发起提案）。 减少通信轮次：Leader 在它的任期内，只需完成一次Prepare阶段，之后可以直接发起Accept请求，无需每次都进行Prepare请求。 减少状态转换：由于有了固定的 Leader，接受者的状态转换次数减少，因为它们只需要响应来自该领导者的消息。 Multi Paxos 流程 前提条件：从Proposers中选出一个Leader，选取Leader过程可以是 领导者选举： 在开始之前，集群通过某种机制选举出一个领导者。 领导者的准备阶段（Prepare ）： 一旦选举出领导者，它会开始一轮新的准备阶段，发送Prepare消息给所有接受者，消息中包含了一个新的提案编号。 接受者收到Prepare消息后，如果提案编号高于它们之前承诺过的任何编号，则会向领导者发出承诺，不再接受低于该编号的提案，并告知领导者它们已接受的最高编号提案的详情。 领导者的接受阶段（Accept）： 领导者收到多数接受者的承诺后，会开始接受阶段，发送带有提案编号和值的Accept消息给所有接受者。 如果接受者没有对更高编号的提案做出承诺，它们将接受该提案。 提案被接受： 一旦提案被多数接受者接受，该提案即被认为选定。 学习者（Learners）被告知哪个提案被选定，并据此更新其状态。 连续提案： 一旦领导者完成了初次的准备阶段并得到了多数接受者的承诺，它就可以连续地发起多个提案，而无需每次都发送Prepare消息。 领导者可以利用之前获得的授权来发起新的提案，每个提案都有唯一递增的编号。 客户端响应： 领导者也负责将决策结果通知给客户端，客户端随后可以基于这些结果执行相应的操作。 Raft算法 动画演示 consul Nacos Raft是一种用于替代Paxos的共识算法。相比于Paxos，Raft的目标是提供更清晰的逻辑分工使得算法本身能被更好地理解，同时它安全性更高，并能提供一些额外的特性 分布式存储系统通常通过维护多个副本来进行容错，提高系统的可用性。要实现此目标，就必须要解决分布式存储系统的最核心问题：维护多个副本的一致性 Raft 是一种更为简单方便易于理解的分布式算法，主要解决了分布式中的一致性问题。相比传统的 Paxos 算法，Raft 将大量的计算问题分解成为了一些简单的相对独立的子问题，并有着和 Multi-Paxos 同样的性能，下面我们通过动图，以后还原 Raft 内部原理 角色（状态） Raft协议的每个副本都会处于三种状态之一：Leader、Follower、Candidate Leader（领导）：所有请求的处理者，Leader副本接受client的更新请求，本地处理后再同步至多个其他副本； Follower（跟随者）：请求的被动更新者，从Leader接受更新请求，然后写入本地日志文件 Candidate（候选人）：如果Follower副本在一段时间内没有收到Leader副本的心跳，则判断Leader可能已经故障，此时启动选主过程，此时副本会变成Candidate状态，直到选主结束 Leader选举 （Leader election） Term（任期）：它其实是个单独递增的连续数字，每一次任期就会重新发起一次领导人选举 Election Timeout（选举超时）：就是一个超时时间，Follower众超时未收到Leader的心跳时，会重新进行选举 选举流程 集群启动时，刚开始所有节点身份都是Follower 成为候选人：每个节点都有自己的超时时间，因为是随机的，区间值为150~300ms，所以出现相同随机时间的概率比较小，当某个Flollower超过超时时间没收到Leader的心跳，它自己可以成为候选人，首选为自己投票（假设a最先超时） 候选人a向其他节点发送 Request Vote请求投票信息；如果接收节点再这个选举任期中还没有投票，那就将票投给候选人（Term=1） 如果候选人a获得超过半数以上节点则当选新的Leader；Leader开始向其他Follower发送Append Entries（追加条目或日志），这些消息以heartbeat timeout心跳超时指定的时间间隔发送，Follower响应每个Append Entries 追加条目消息 如果Leader宕机，超过heartbeat timeout 时长 Follower没有接收来自Leader的心跳则重新触发选举流程，选取新的Leader（假设节点B当选） 网络分区（脑裂）修复 假设AB和CDE发生了网络分区，那么CDE由于不存在Leader则会重新选举一个新Leader（节点E，任期=3） 现在整个集群就产生了脑裂问题，有两个Leader存在 客户端就会将更改信息发送到两个Leader中处理 发送到节点B，由于更改只收到节点A的响应，没有集群节点过半的响应，更改不能提交 发送到节点E，它能够得到集群过半节点的响应，此分区集群达成共识，更改提交 网络分区恢复 节点B通过心跳发现节点E的任期代号比自己高，则节点B退化为Follower 节点A和B都将回滚其未提交的日志并重新同步新Leader的日志 现在，所有日志在集群中是一致的，分区数据完成修复 复制状态机 状态机基础上增加复制多个服务来实现来实现分布式系统的容错。通常而言，一个支持F个故障的系统，必须至少包含2F+1个副本。复制状态机通常都是基于复制日志实现的。每一个服务器存储一个包含一系列指令的日志，并且按照日志的顺序进行执行。每一个日志都按照相同的顺序包含相同的指令，所以每一个服务器都执行相同的指令序列。因为每个状态机都是确定的，每一次执行操作都产生相同的状态和同样的序列。保证复制日志相同就是一致性算法的工作了。服务器集群看起来形成一个高可靠的状态机。 日志复制过程 经过Leader选举后，所有的写操作都交给Leader负责 分布式系统现在要对某个值进行更新，只能通过Leader操作 Leader会把客户端发起的更改请求记录到日志中，并未提交 然后将待提交的日志和日志的nxetIndex（下一条日志写入点，保证各节点日志的顺序性）到所有的Follower中，Follower接受到Rpc请求后将请求写入各自的日志后响应Leader Leader接收到过半节点的响应后，就将自身的值修改 然后Leader再发出一个commit请求让Follower提交刚刚写入的日志。现在，整个集群系统就达成了共识，修改生效 响应客户端 日志复制原理 日志格式 在 Raft 算法中，需要实现分布式一致性的数据被称作日志，我们 Java 后端绝大部分人谈到日志，一般会联想到项目通过 log4j 等日志框架输出的信息，而 Raft 算法中的数据提交记录，他们会按照时间顺序进行追加，Raft 也是严格按照时间顺序并以一定的格式写入日志文件中： 如上图所示，Raft 的日志以日志项（LogEntry）的形式来组织，每个日志项包含一条命令、任期信息、日志项在日志中的位置信息（索引值 LogIndex）。 指令：由客户端请求发送的执行指令，有点绕口，我觉得理解成客户端需要存储的日志数据即可。 索引值：日志项在日志中的位置，需要注意索引值是一个连续并且单调递增的整数。 任期编号：创建这条日志项的领导者的任期编号。 如何保证日志的一致性？ Follower获取上一个日志项做比对 Leader递减日志索引值重新复制日志给落后的Follower,直到完成一致 假设原来的Leader宕机，选取新的Leader如何保证日志的一致性 领导者会通过强制覆盖的方式让跟随者复制自己的日志来解决日志不一致的问题 领导者在追加 RPC 请求过程中会附带需要复制的日志以及前一个日志项相关信息，如果跟随者匹配不到前一个日志项，那么他就会拒绝接收新的日志条目 接着领导者会继续递减要复制的日志项索引值，直至找到相同索引和任期号的日志项，最后就直接覆盖跟随者之后的日志项 可认为两个条目拥有相同的索引和任期号，那么他们之前的所有日志条目也全部相同 分布式协议 Gossip协议 带冗余容错的最终一致性协议 去中心化分布式协议 要求非拜占庭将军问题 ​ Gossip protocol 也叫 Epidemic Protocol （流行病协议），是基于流行病传播方式的节点或者进程之间信息交换的协议。Gossip protocol在1987年8月由施乐公司帕洛阿尔托研究中心研究员艾伦·德默斯（Alan Demers）发表在ACM上的论文《Epidemic Algorithms for Replicated Database Maintenance》中被提出 ​ Gossip协议在计算机系统通常以随机的对等选择形式实现：以给定的频率，每台计算机随机选择另一台计算机，并共享任何消息。定义十分简单，所以实现方式非常多，可能有几百种Gossip协议变种。因为每个使用场景都可能根据组织的特定需求进行定制 Gossip协议执行过程 种子节点周期性的散播消息 （假定把周期限定为 1 秒） 被感染节点随机选择N个邻接节点散播消息（假定fan-out(扇出)设置为6，每次最多往6个节点散播） 节点只接收消息不反馈结果 每次散播消息都选择尚未发送过的节点进行散播 收到消息的节点不回传散播：A -&gt; B，那么B进行散播的时候，不再发给 A Goosip 协议的信息传播和扩散通常需要由种子节点发起。整个传播过程可能需要一定的时间，由于不能保证某个时刻所有节点都收到消息，但是理论上最终所有节点都会收到消息，因此它是一个最终一致性协议 ​ Gossip协议是一个多主协议，所有写操作可以由不同节点发起，并且同步给其他副本。Gossip内组成的网络节点都是对等节点，是非结构化网 -传播方式- 反熵传播 使用simple epidemics(SI model)（简单流行病）的方式：以固定的概率传播所有的数据。所有参与节点只有两种状态 Suspective(病原)：处于 susceptible 状态的节点代表其并没有收到来自其他节点的更新 Infective(感染)：处于 infective 状态的节点代表其有数据更新，并且会将这个数据分享给其他节点 反熵传播过程是每个节点周期性地随机选择其他节点，然后通过互相交换自己的所有数据来消除两者之间的差异 反熵传播方法每次节点两两交换自己的所有数据会带来非常大的通信负担，因此不会频繁使用，通常只用于新加入节点的数据初始化 谣言传播 complex epidemics(SIR model)（复杂流行病）的方式:以固定的概率仅传播新到达的数据。所有参与节点有三种状态：Suspective(病原)、Infective(感染)、Removed(愈除) Suspective(病原)：处于 susceptible 状态的节点代表其并没有收到来自其他节点的更新 Infective(感染)：处于 infective 状态的节点代表其有数据更新，并且会将这个数据分享给其他节点 Removed(愈除)：其已经接收到来自其他节点的更新，但是其并不会将这个更新分享给其他节点 谣言传播过程是消息只包含最新 update，谣言消息在某个时间点之后会被标记为removed，并且不再被传播。缺点是系统有一定的概率会不一致，通常用于节点间数据增量同步 一般来说，为了在通信代价和可靠性之间取得折中，需要将这两种方法结合使用 通信方式 Gossip 协议最终目的是将数据分发到网络中的每一个节点。根据不同的具体应用场景，网络中两个节点之间存在三种通信方式 Push（推送模式）: 节点 A 将数据 (key,value,version) 及对应的版本号推送给 节点B，节点B 更新 节点A 中比自己新的数据 Pull（拉取模式）：节点A 仅将数据 key, version 推送给节点 B，节点B 将本地比 节点A 新的数据（Key, value, version）推送给节点A，节点A 更新本地 Push&amp;Pull（推送&amp;拉取模式）：发起信息交换的 节点A 向选择的 节点B 发送信息，同时从对方获取数据，用于更新自己的本地数据 如果把两个节点数据同步一次定义为一个周期，则在一个周期内，Push 需通信 1 次，Pull 需 2 次，Push&amp;Pull 则需 3 次。虽然消息数增加了，但从效果上来讲，Push&amp;Pull 最好，理论上一个周期内可以使两个节点完全一致。直观上，Push&amp;Pull 的收敛速度也是最快的 总结 Gossip是一种去中心化的分布式协议，数据通过节点像病毒一样逐个传播。因为是指数级传播，整体传播速度非常快，很像现在流感病毒一样。它具备以下优势 可扩展性（Scalable）：允许节点的任意增加和减少，新增节点的状态最终会与其他节点一致。 容错（Fault-tolerance）：网络中任何节点的重启或者宕机都不会影响 gossip 协议的运行，具有天然的分布式系统容错特性。 健壮性（Robust）：gossip 协议是去中心化的协议，所以集群中的所有节点都是对等的，没有特殊的节点，所以任何节点出现问题都不会阻止其他节点继续发送消息。任何节点都可以随时加入或离开，而不会影响系统的整体服务质量。 最终一致性（Convergent consistency）：谣言传播可以是指数级的快速传播，因此新信息传播时，消息可以快速地发送到全局节点，在有限的时间内能够做到所有节点都拥有最新的数据。 简单 同样也存在以下缺点： 消息延迟：节点随机向少数几个节点发送消息，消息最终是通过多个轮次的散播而到达全网，不可避免的造成消息延迟。 消息冗余：节点定期随机选择周围节点发送消息，而收到消息的节点也会重复该步骤，因此不可避免地引起同一节点多次接收同一消息，增加消息处理的压力。一次通信会对网路带宽、CUP资源造成很大的负载，而这些负载又受限于 通信频率，该频率又影响着算法收敛的速度。 拜占庭问题：如果有一个恶意传播消息的节点，Gossip协议的分布式系统就会出问题。 上述优缺点的本质是因为Gossip是一个带冗余的容错算法，是一个最终一致性算法，虽然无法保证在某个时刻所有节点状态一致，但可以保证在最终所有节点一致，最终的时间是一个理论无法明确的时间点。所以适合于AP场景的数据一致性处理，常见应用有：Apache Cassandra、Redis Cluster ，还有Apache Gossip框架的开源实现供Gossip协议的学习 文章出处：https://cloud.tencent.com/developer/article/1662426 Zab协议 Zab简介 ZAB（ZooKeeper Atomic Broadcast）协议是专为ZooKeeper设计的共识协议，确保了分布式系统在面对节点故障时的数据一致性和系统的可靠恢复。ZAB协议在概念上受到了 Multi-Paxos 算法的启发，但针对ZooKeeper的特定场景进行了优化和调整。 ZAB协议的核心特点包括： 原子广播：ZAB确保所有的更新（如配置更改或状态变更）以相同的顺序被所有活跃的集群成员处理，这是通过原子广播机制实现的。 崩溃恢复：当系统检测到Leader节点故障时，ZAB协议会触发恢复模式，这包括了一个新的Leader选举过程。在新的Leader被选举出来之前，系统不会对外提供写服务，以避免可能的不一致状态。 数据同步：新选举出的Leader节点负责将其状态与其他Follower节点同步，确保整个集群达到一致的状态，从而实现所谓的“线性一致性”。这保证了任何已经被ZooKeeper确认的更新都不会丢失，并且客户端总是看到最新的状态。 领导者选举：ZAB通过一个快速的领导者选举机制，最小化了由于Leader崩溃导致的系统不可用时间。 消息广播效率：在Leader稳定之后，ZAB通过减少消息的往返次数优化了消息广播的效率，从而提高了整个系统的性能。 ZAB协议中节点存在四种状态 Leading 节点作为领导者（Leader）运行，负责处理所有的事务请求，并协调整个集群的事务顺序。领导者节点确保所有的更新以相同的顺序被所有活跃的集群成员处理。 Following 节点作为跟随者（Follower）运行，遵循领导者的指导并参与事务的执行。跟随者节点接受领导者的提案，并在本地复制状态变更。 Looking 节点处于锁定状态，表明集群当前没有活跃的领导者，节点正在参与选举过程，以选出新的领导者。在这个状态下，集群不会对外提供写服务。 Observing 节点作为观察者（Observer）运行，它与跟随者类似，保持与集群状态的同步，并提供读服务，但不参与领导者的选举过程也不参与到事务的投票中。观察者的主要作用是提高读取性能并扩展集群的规模，而不会影响写操作的一致性和性能。 ZAB协议中的两种工作模式 原子广播：在集群稳定运行时，领导者（Leader）使用原子广播机制来确保所有跟随者（Followers）节点的状态保持一致。 崩溃恢复：集群启动或 Leader 崩溃时系统进入恢复模式，选举 Leader 并将集群中各节点的数据同步到最新状态。 ZXID 组成 高32位：代表领导者（Leader）的纪元（epoch），每当一个新的领导者被选举出来时，这个数字会递增。纪元用于区分不同领导者的领导期，确保即使事务编号从0重新开始，事务ID也仍然是全局唯一和递增的。 低32位：在特定领导者的领导期内，每发生一个新事务，这个数字就递增。这确保了在同一个领导期内，所有的事务都能被正确排序。 作用 ZXID是ZooKeeper集群中用于确保所有事务全局有序执行的唯一标识符。每个ZXID包含一个纪元号和一个事务计数器，纪元号代表领导者的任期，事务计数器则是在该任期内事务的递增序号。 ZXID中的纪元号帮助集群在领导者崩溃和随后的选举过程中维护一致性。新的领导者开始一个新纪元，确保其领导期内的事务与之前的任期明确区分，从而避免执行无效或过时的事务。 跟随者使用ZXID中的纪元号来验证自己的数据是否最新，或者是否需要通过与领导者同步日志来更新自己的状态。 原子广播 类似分布式事务2PC提交 顺序一致性保证 原子广播是协议中确保所有更新按相同顺序应用到所有服务器上的机制。以下是ZAB协议原子广播的详细流程： 事务请求 客户端可以将事务请求（如创建、更新或删除ZNode）发送给任何一个ZooKeeper服务器节点，无论是领导者还是跟随者。 请求转发 如果跟随者接收到了事务请求，它会将该请求转发给领导者。这是因为只有领导者才能决定事务的顺序。 领导者提案 领导者收到事务请求后，创建一个提案（Proposal），其中包括了ZXID（事务ID），并将此提案发送给所有跟随者。 提案广播 领导者将提案广播给所有跟随者。每个提案都包括一个全局唯一的递增ZXID，用于保证顺序一致性。 跟随者确认提案 跟随者（Followers）接收到来自领导者（Leader）的提案广播时，它们首先将这些提案按照接收顺序放入预处理队列。这个队列的作用是保证事务能够根据其ZXID（事务ID）的顺序进行处理，即使在网络传输过程中事务的顺序发生了变化。 跟随者接着将这些提案写入它们的事务日志，但尚未进行实际的提交。 提案提交 一旦领导者从大多数跟随者那里得到了足够的投票，它将决定提交提案，并向所有跟随者发送提交（Commit）消息。 应用提案 接收到提交消息的跟随者将提案应用到它们的状态机，并向客户端发送响应，表明事务已经被处理。 如果Commit 的事务ID 和本地未提交的事务ID不一致，放弃请求。重新向 Leader 同步数据，保证数据线性一致。 连续提案处理 在一个稳定的领导者任期内，领导者可以连续地处理多个事务请求，而不需要为每个提案都进行新的选举或准备（Prepare）阶段。 领导者根据请求的到达顺序分配ZXID，并连续地广播提案。这样做提高了效率并减少了延迟。 状态同步 如果某个跟随者落后或者有新跟随者加入集群，领导者会与这些跟随者进行状态同步，确保它们的数据状态是最新的。 为什么是半数ACK则commit容错性：ZAB协议旨在确保即使在发生节点故障的情况下，集群仍能继续正常工作。通过只要求多数节点（超过半数）的确认，协议可以容忍少数节点的失败，而不影响整个集群的可用性和一致性。写性能优化：等待所有节点的确认可能会导致系统性能显著下降，特别是在分布式环境中节点数量较多或节点之间网络延迟较大的情况下。通过只需要多数节点的确认，ZAB协议提高了事务的处理速度和系统的吞吐量。避免不必要的等待：如果集群中某个节点因为网络问题或其他原因响应缓慢，等待它的确认会延迟整个集群的事务处理。多数投票机制确保了不会因为单个或少数节点的问题而影响整个集群的操作。 崩溃恢复 在ZooKeeper集群中，若现任领导者（Leader）遇到故障，或在集群启动时，集群会进入恢复模式。这个模式首先触发新的领导者选举，并同步所有节点以确保整个集群反映了最新且正确的状态。领导者崩溃的判定基于其与大多数跟随者（Followers）失去通信的能力。 超过半数Follower丢失和Leader心跳，判断Leader宕机。 Leader丢失了超过半数的Follower心跳，自动退出Leader。 为了确保一致性，在崩溃恢复的过程中，集群必须坚持以下原则： 事务持久性：任何由先前领导者提交并广播的事务必须在所有活跃的跟随者节点上得到持久化，确保这些事务不会因领导者的崩溃而丢失。 清理未提交事务：在领导者故障时还未提交的事务应被新的领导者视为无效，丢弃这个未提交事务。 集群初始化选举 遇强选强，myid大有投票优势 集群初始化选举时，选票内容为（myid，zXid）,由于集群初始化的zXid均为0，下文则忽略zXid 集群初始化选举流程： 启动 集群中的每个节点在启动时都会进入“查找”（LOOKING）状态，表明它们正在寻找一个领导者。 投票 每个节点都会投出一票，起初，每个节点都会为自己投票。 投票包含两个信息：所选节点的标识符（通常是服务器ID）和该节点已知的最高事务ID（ZXID）。ZXID越高，代表该节点的数据越新。 收集选票 每个节点将自己的选票发送给其他所有节点，并同时收集来自其他节点的选票。 更新投票 当一个节点收到来自其他节点的选票时，如果发现另一个节点有更高的ZXID，或者在ZXID相同的情况下服务器ID更高，它会更新自己的投票并将此新投票广播出去。 确定领导者 一旦一个节点收到超过半数的相同投票，它会宣布那个节点为领导者。 确定领导者后，该节点将自己的状态更改为“领导”（LEADING），其他所有节点将自己的状态更改为“跟随”（FOLLOWING）或“观察”（OBSERVING），观察者节点是一种特殊的节点，它接收更新但不参与选举过程。 同步状态 新选出的领导者会与所有的跟随者进行状态同步，确保所有节点的数据都是最新的。 开始服务 一旦状态同步完成，领导者就开始处理客户端的请求，并将更新广播给所有跟随者。 数据同步过程Follower 发送一个同步请求，该请求包含了 Follower 最后已知的数据状态（最大ZXID）。Leader 收到 Follower 的同步请求后，通过事务日志比对ZXID，确定同步起点。开始同步如果数据差距不大，直接进行事务日志同步。数据差距过大，直接发送数据快照，然后再同步后面的事务日志。Follower完成同步，向外提供读取功能。 运行期间选举 在ZooKeeper集群的运行期间，领导者（Leader）的宕机将触发一个选举新领导者的流程，以保证集群的高可用性和一致性。 Epoch（选举周期）Epoch是每次领导者选举时的逻辑时钟值，用于标识Leader的任期。在没有领导者的情况下，同一轮选举中所有节点的Epoch值是相同的。每完成一次投票，节点的Epoch值就会递增，这样可以防止选举过程中接收到陈旧的投票信息。ZXID（事务ID）ZXID是一个全局单调递增的序列号，用于标识集群状态变更的每一次事务。不同节点上的ZXID值可能不同，这是因为ZooKeeper集群中的每个服务器可能以不同的顺序处理来自客户端的更新请求。ZXID的不一致性不会影响系统的最终一致性，因为选举机制会确保选出的新领导者具有最新的数据状态。SID（服务器ID）SID是用来唯一标识ZooKeeper集群中的每台机器的标识符。这个ID在集群中每台机器上的配置文件中被设置为myid，并且每台机器的myid都是唯一的。在选举过程中，如果Epoch和ZXID都相同，那么SID（myid）将作为最后的决策因素。 选举过程 Leader 检测 领导者会周期性地向跟随者（Followers）发送心跳信号。 如果跟随者在预定的时间间隔内没有收到心跳，它会认定领导者已宕机，关闭当前的会话，并转入Looking状态。 触发选举 当过半数的节点进入Looking状态时，选举过程被触发，因为这意味着领导者与过半数的跟随者失去了联系。 发起投票 每个Looking状态的节点将发起一轮投票，每轮投票都有一个唯一的Epoch值，该值随着每次投票而递增，以防止来自之前选举轮次的过期投票干扰。 投票过程 在发起投票时，每个节点会向其他所有节点发送包含自己Epoch、ZXID和SID的投票。 Epoch优先 节点首先比较Epoch值，因为一个更高的Epoch表示节点在更近的领导者任期内活跃，优先选择Epoch较大的节点作为领导者是为了保证信息的时效性和减少可能的数据回滚。 ZXID比较 如果Epoch相同，则比较ZXID，因为更大的ZXID表示节点拥有最新的数据变更。 SID决断 如果Epoch和ZXID都相同，则使用SID，即myid来决定。这通常意味着节点启动的先后顺序，但在实践中几乎不会依赖于此因素来决定领导者。 宣布新领导者 当一个节点接收到超过半数的节点对同一个候选者的支持时，它将宣布该候选者为新领导者，并通知所有节点。 状态同步 新领导者接管后，会与每个跟随者进行状态同步，以确保所有节点反映了最新且一致的集群状态。 集群恢复 一旦同步完成，新的领导者开始处理客户端请求，集群继续其正常操作。 分布式事务解决方案 参考文章 分布式事务最经典的七种解决方案：https://segmentfault.com/a/1190000040321750 七种分布式事务的解决方案：https://cloud.tencent.com/developer/article/1806989 柔性事务：https://www.modb.pro/db/69276 Saga seata模式详解：https://seata.io/zh-cn/blog/seata-at-tcc-saga.html AT Saga DTP模型 DTP（Distributed Transaction Processing）分布式事务模型 AP：应用程序，例如订单服务、库存服务 RM：资源管理器可以把理解为一个数据库（mysql）AP通过RM对资源进行控制。资源必须实现XA定义的接口。 TM：事务管理器负责分配事务唯一标识，监控事务的执行进度，并负责事务的提交，回滚等。 两阶段提交（XA） 2PC多用于数据库层面 ​ 熟悉mysql底层对两阶段提交应该颇为熟悉，mysql的事务就是通过日志系统来完成两阶段提交的 ​ 二阶段提交（2PC）是XA分布式事务协议的一种实现。其实在XA协议定义的函数中，通过xa_prepare，xa_commit已经能发现XA完整提交分准备和提交两个阶段 ​ XA是由X/Open组织提出的分布式事务的规范，XA规范主要定义了(全局)事务管理器TM和(局部)资源管理器RM之间的接口。本地的数据库如mysql在XA中扮演的是RM角色 ​ XA规范一共分为两阶段 第一阶段（prepare）：即所有的参与者RM准备执行事务并锁住需要的资源。参与者ready时，向TM报告已准备就绪 第二阶段 (commit/rollback)：当事务管理者™确认所有参与者(RM)都ready后，向所有参与者发送commit命令 目前主流的数据库基本都支持XA事务，包括mysql、oracle、sqlserver、postgreSql XA 事务由一个或多个资源管理器（RM）、一个事务管理器（TM）和一个应用程序（ApplicationProgram）组成 一阶段：PrePare阶段此时协调者会向所有的参与者发送Prepare请求，参与者收到后开始执行事务操作，并将Undo和Redo信息记录到事务日志中（此时属于未提交事务的状态，向协调者节点反馈Ack消息 二阶段： commit阶段：在阶段二中如果所有的参与者节点都可以进行PrePare提交，那么协调者就会从预提交状态转变为提交状态。然后向所有的参与者节点发送Commit请求，参与者节点在收到提交请求后就会各自执行事务提交操作 rollback阶段：如果有一个参与者节点未完成PrePrepare的反馈或者反馈超时，那么协调者都会向所有的参与者节点发送rollback请求，从而中断事务 缺点 单点故障：一旦事务管理器出现故障，整个系统不可用 数据不一致：如果事务管理器只发送了部分 commit 消息，此时网络发生异常，那么只有部分参与者接收到 commit 消息，也就是说只有部分参与者提交了事务，使得系统数据不一致 整个消息链路是串行的，要等待响应结果，不适合高并发的场景 三阶段提交（3PC） CanCommit阶段保证RM有提交事务能力 二阶段超时自动Commit ​ 三阶段提交又称3PC，相对于2PC来说增加了CanCommit阶段和超时机制。如果短时间内没有收到协调者的commit请求，那么就会自动进行commit，解决了2PC单点故障的问题 第一阶段：CanCommit阶段这个阶段所做的事很简单，就是协调者询问事务参与者，你是否有能力完成此次事务（提前预防某个RM故障而无法完成全局事务场景） 如果都返回yes，则进入第二阶段 有一个返回no或等待响应超时，则中断事务，并向所有参与者发送事务中断 第二阶段：PreCommit阶段此时协调者会向所有的参与者发送PreCommit请求，参与者收到后开始执行事务操作（或者超时自动commit），并将Undo和Redo信息记录到事务日志中。参与者执行完事务操作后（此时属于未提交事务的状态），就会向协调者反馈Ack表示我已经准备好提交了，并等待协调者的下一步指令 第三阶段：DoCommit阶段在阶段二中如果所有的参与者节点都可以进行PreCommit提交，那么协调者就会从预提交状态转变为提交状态。然后向所有的参与者节点发送doCommit请求，参与者节点在收到提交请求后就会各自执行事务提交操作，并向协调者节点反馈Ack消息，协调者收到所有参与者的Ack消息后完成事务。相反，如果有一个参与者节点未完成PreCommit的反馈或者反馈超时，那么协调者都会向所有的参与者节点发送abort请求，从而中断事务 补偿事务（TCC） 关于 TCC（Try-Confirm-Cancel）的概念，最早是由 Pat Helland 于 2007 年发表的一篇名为《Life beyond Distributed Transactions:an Apostate’s Opinion》的论文提出。 TCC分为3个阶段 Try 阶段：尝试执行，完成所有业务检查（一致性）, 预留必须业务资源（准隔离性） Confirm 阶段：确认执行真正执行业务，不作任何业务检查，只使用 Try 阶段预留的业务资源，Confirm 操作要求具备幂等设计，Confirm 失败后需要进行重试。 Cancel 阶段：取消执行，释放 Try 阶段预留的业务资源。Cancel 阶段的异常和 Confirm 阶段异常处理方案基本上一致，要求满足幂等设计 两阶段模式和TCC模式的区别 2PC是偏数据库层面的 ，是XA分布式事务协议的一种实现 而TCC是纯业务层面 ，要求RM必须提供Try Confirm Cancel三个接口 资源锁定粒度不同，TCC根据具体业务来实现控制资源锁的粒度变小，不会锁定整个资源 本地消息表 本地事务保证消息表原子性 通过消息表和定时任务保证消息送达，失败重试 ​ 本地消息表这个方案最初是 ebay 架构师 Dan Pritchett 在 2008 年发表给 ACM 的文章。设计核心是将需要分布式处理的任务通过消息的方式来异步确保执行。 执行流程 消息生产方，需要额外建一个消息表，并记录消息发送状态。消息表和业务数据要在一个事务里提交，也就是说他们要在一个数据库里面。然后消息会经过MQ发送到消息的消费方。 如果消息发送失败，会进行重试发送。 消息消费方，需要处理这个消息，并完成自己的业务逻辑 如果是业务上面的失败，可以给生产方发送一个业务补偿消息，通知生产方进行回滚等操作 此时如果本地事务处理成功，表明已经处理成功了，修改消息表状态（或者删除消息表数据） 如果处理失败，那么就会重试执行 生产方和消费方定时扫描本地消息表，把还没处理完成的消息或者失败的消息再发送一遍 事务消息 RocketMQ 事务消息 适用于可异步执行的业务，且后续操作无需回滚的业务 在上述的本地消息表方案中，生产者需要额外创建消息表，还需要对本地消息表进行轮询，业务负担较重。阿里开源的RocketMQ 4.3之后的版本正式支持事务消息，该事务消息本质上是把本地消息表放到RocketMQ上，解决生产端的消息发送与本地事务执行的原子性问题 执行流程 正常流程 RocketMQ发送半消息到Broker Borker Ack,半消息发送成功 执行本地事务 本地事务Commit或Rollback通知Broker Broker定时回查本地事务状态 Broker确定本地事务提交，将消息投递给消费者 补偿流程 对没有Commit/Rollback的事务消息（pending状态的消息），从服务端发起一次回查 Producer收到回查消息，返回消息对应的本地事务的状态，为Commit或者Rollback 如果本地事务回滚，则不需要处理补偿业务，半消息定期自动清除 最大努力通知 ​ 最大努力通知型( Best-effort delivery)是最简单的一种柔性事务，适用于一些最终一致性时间敏感度低的业务，且被动方处理结果不影响主动方的处理结果。典型的使用场景：如银行通知、商户通知等。最大努力通知型的实现方案，一般符合以下特点： 不可靠消息：业务活动主动方，在完成业务处理之后，向业务活动的被动方发送消息，直到通知N次后不再通知，允许消息丢失(不可靠消息) 定期校对：业务活动的被动方，根据定时策略，向业务活动主动方查询(主动方提供查询接口)，恢复丢失的业务消息 最大努力通知适用于业务通知类型，例如支付宝交易的结果，就是通过最大努力通知方式通知各个商户，既有多次支付回调通知，也有交易查询接口 Saga ​ 1987年普林斯顿大学的Hector Garcia-Molina和Kenneth Salem发表了一篇Paper Sagas，讲述的是如何处理long lived transaction（长活事务）。Saga是一个长活事务可被分解成可以交错运行的子事务集合。其中每个子事务都是一个保持数据库一致性的真实事务。 ​ Saga模型是把一个分布式事务拆分为多个本地事务，每个本地事务都有相应的执行模块和补偿模块（对应TCC中的Confirm和Cancel），当Saga事务中任意一个本地事务出错时，可以通过调用相关的补偿方法恢复之前的事务，达到事务最终一致性。 Saga 模型由三部分组成 LLT（Long Live Transaction）：由一个个本地事务组成的事务链。 本地事务：事务链由一个个子事务（本地事务）组成，LLT = T1+T2+T3+…+Ti。 补偿：每个本地事务 Ti 有对应的补偿 Ci。 Saga 的执行顺序 正常情况：T1,T2,T3,…,Ti 异常情况：T1,T2,T3,…Ti,Ci,…C3,C2,C1 Saga 两种恢复策略 向后恢复（Backward Recovery）：撤销掉之前所有成功子事务。如果任意本地子事务失败，则补偿已完成的事务。如异常情况的执行顺序T1,T2,T3,…Ti,Ci,…C3,C2,C1。 向前恢复（Forward Recovery）：即重试失败的事务，适用于必须要成功的场景，该情况下不需要Ci。执行顺序：T1,T2,…,Tj（失败）,Tj（重试）,…,Ti。 Saga 模型可以满足事务的三个特性ACD 原子性：Saga 协调器协调事务链中的本地事务要么全部提交，要么全部回滚。 一致性：Saga 事务可以实现最终一致性。 持久性：基于本地事务，所以这个特性可以很好实现。 Saga缺乏隔离性会带来脏读，幻读，不可重复读的问题。由于Saga 事务和 TCC 事务一样，都是强依靠业务改造，因此需要在业务设计上去解决这个问题： 在应⽤层⾯加⼊逻辑锁的逻辑。 Session 层⾯隔离来保证串⾏化操作。 业务层⾯采⽤预先冻结数据的方式隔离此部分数据。 业务操作过程中通过及时读取当前状态的⽅式获取更新。 实现Saga的注意事项 Ti和Ci必须是幂等的。如向后恢复和向前恢复时候如果不是幂等操作会导致数据不一致。 Ci必须是能够成功的，如果无法成功则需要人工介入。 Ti-&gt;Ci和Ci-&gt;Ti的执行结果必须是一样的。 Saga对比TCC Saga和TCC都是补偿型事务，他们的区别为： 劣势 无法保证隔离性； 优势 一阶段提交本地事务，无锁，高性能； 事件驱动模式，参与者可异步执行，高吞吐； Saga 对业务侵入较小，只需要提供一个逆向操作的Cancel即可；而TCC需要对业务进行全局性的流程改造；","categories":[],"tags":[]},{"title":"MAT中文文档","slug":"MAT中文文档","date":"2022-09-25T16:07:00.000Z","updated":"2023-07-14T06:49:14.788Z","comments":true,"path":"2022/09/26/MAT中文文档/","link":"","permalink":"https://wugengfeng.cn/2022/09/26/MAT%E4%B8%AD%E6%96%87%E6%96%87%E6%A1%A3/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"zookeeper","slug":"zookeeper","date":"2022-08-18T02:46:56.000Z","updated":"2023-11-13T07:48:24.021Z","comments":true,"path":"2022/08/18/zookeeper/","link":"","permalink":"https://wugengfeng.cn/2022/08/18/zookeeper/","excerpt":"","text":"思维导图 选型的一些思考 在进行技术选型时，当考虑使用ZooKeeper作为分布式协调服务的解决方案时，首先需要明确ZooKeeper是否能够满足特定的应用场景需求。ZooKeeper作为一种分布式系统中的协调工具，虽然功能强大且通用，但它并不一定适合所有场景。在分布式协调服务的细分领域中，存在多种替代方案。这些方案针对不同的需求和约束条件，可能提供更优的性能、更高的可扩展性或更简单的使用方式。因此，在最终决定采用ZooKeeper之前，应该对其功能特性进行全面评估，并与其他可用的替代方案进行比较，以确保选择最适合当前业务需求的解决方案。 服务发现早期，Dubbo选择ZooKeeper作为其服务发现组件，这在一定程度上受到当时的技术环境影响。当时，微服务架构并未像现在这样广泛流行，服务发现的解决方案相对有限。重要的是要注意，ZooKeeper主要遵循CP（一致性和分区容错性）模型。然而，对于服务发现组件而言，是否真的需要强调一致性？个人认为，对于服务发现组件来说，最重要的应该是高可用性。在微服务架构中，服务发现组件的任何故障都不应导致整个系统的服务瘫痪，这种情况是不可接受的。理想情况下，服务发现组件应始终保持可用，即使数据不一致导致服务不可用也应通过负载均衡策略或服务降级等方案来确保系统的正常运行。随着技术的发展，现在已经出现了一些更适合微服务架构的服务发现组件，它们主要遵循AP（可用性和分区容错性）模型，以确保在网络分区和其他故障情况下的高可用性。例如，Eureka（Spring Cloud 1.x）和Nacos，它们提供了更灵活和健壮的服务发现机制，更适合于微服务架构的服务发现需求。 配置中心在对配置中心进行技术选型时，ZooKeeper的CP（一致性和分区容错性）模型确实非常适合用元数据配置服务。ZooKeeper的节点监听功能可以方便地实现配置的动态更新，这对于基本的元数据配置服务来说是一个不错的选择，早期的Kafka等相关服务也采用其作为解决方案。它的一致性保证了配置信息的准确性和可靠性。然而，如果将ZooKeeper作为一个功能更全面的配置中心，也存在一些局限性。首先，它缺乏一个灵活且易于使用的配置UI界面，这对于管理和维护大量配置项可能会带来一定的挑战。其次，ZooKeeper本身并不提供数据解析逻辑，这意味着用户需要自行实现配置数据的解析和处理逻辑，这增加了使用的复杂性。鉴于这些局限性，不如使用针对性更强的服务，如Apollo或Nacos。这些工具不仅提供了更友好的配置界面和数据管理功能，还内置了数据解析和处理逻辑，减少了开发者的工作量。此外，它们还可能提供更多高级功能，如配置版本管理、环境隔离、权限控制等，这些都是在复杂的应用环境中不可或缺的特性。 分布式锁ZooKeeper构建的分布式锁不是一个适用于所有场景的通用解决方案。这主要是由于ZooKeeper底层的ZAB（ZooKeeper Atomic Broadcast）协议的工作机制。在ZAB协议中，事务的提交需要集群中大多数节点的同意，这意味着随着集群节点数量的增加，达成共识所需的时间会增加，从而导致事务操作的延迟性增高。因此，如果考虑使用ZooKeeper实现分布式锁，特别是在分布式事务场景中，就需要考量业务是否能够容忍这种写操作的性能瓶颈。如果业务场景中对分布式锁的要求是高可用性，并且需要在服务异常时锁具有自动恢复的能力，那么ZooKeeper是一个非常合适的选择。可以使用临时节点，服务异常时自动释放锁。然而，如果分布式锁应用于高并发的场景，ZooKeeper就不是一个理想的选择。在高并发场景下，ZooKeeper的性能瓶颈会严重影响业务，导致锁操作的响应时间变长，从而影响整体系统的性能。在这种情况下，可以考虑其他专门为高并发设计的分布式锁解决方案，例如基于Redis的RedLock算法等，这些方案能够提供更快的响应时间和更好的扩展性，更适合于高并发环境。 Leader选举分布式服务中的Leader选举可以通过ZooKeeper来实现，优点是减少了开发工作量，因为ZooKeeper提供了一套现成的、可靠的机制来处理选举的复杂性。在整个方案中，各个服务节点尝试在ZooKeeper中创建同一个zNode（节点）。由于ZooKeeper保证同一个节点名下只能成功创建一个临时zNode，因此第一个成功创建该zNode的服务节点将成为Leader。这个临时zNode的特性是关键：它确保了当Leader节点宕机或失去与ZooKeeper集群的连接时，该zNode会被自动删除。其他的服务节点（Follower）在ZooKeeper上对这个zNode进行监听。当它们检测到这个zNode被删除的事件时，意味着原Leader已不再可用，随即开始新一轮的Leader选举。这种选举方案适合在Leader选举时，没有额外业务逻辑处理的选举场景。 入门 概述 文章涉及源码 Apache ZooKeeper是一个开源的分布式协调服务，主要用于构建分布式应用程序。它为分布式应用程序提供一套简单、可靠的协调和管理功能，帮助应用程序处理分布式环境中的各种服务，如 配置管理、同步 和 服务发现 等。 工作机制 ZooKeeper, 从设计模式的角度来看，可以被认为是一个实现了 观察者模式 的分布式协调服务框架。它主要负责存储和管理分布式系统中重要的配置信息和命名数据。当这些数据状态发生变化时，ZooKeeper会通知已经向其注册的观察者，从而使这些观察者能够做出相应的反应。 Zookeeper = 文件系统 + 通知机制 特点 CAP定理 可靠性：即使部分节点发生故障，整个系统仍将继续运行。 顺序一致性：Zookeeper 数据操作按照请求的先后顺序排队进行。集群可能存在短暂的数据不一致窗口，但ZooKeeper保证最终一致性。 原子性：数据更新操作是原子的，要么完全成功，要么完全失败，不会有中间或部分完成的状态。 可扩展性：集群架构支持节点水平扩展。 监视和通知：可以在 znode 上设置监视。此机制允许客户端接收有关特定 znode 更改的通知，而无需轮询更新。 广义上满足CP定理ZK是一个满足CP理论的分布式应用程序协调服务，A（可用性）不满足是因为ZK集群Leader宕机恢复选举过程中，整个集群处于不可用状态，Zab协议 保证了集群间数据的最终一致性。 数据结构 ZooKeeper 数据模型的结构与 Unix 文件系统很类似，整体上可以看作是一棵树，每个节点称做一个 ZNode。每一个 ZNode 默认能够存储 1MB 的数据，每个ZNode都有一个与之关联的路径，这个路径为其在ZooKeeper中提供了唯一的标识。 节点类型 持久性 持久节点（Persistent）：无论客户端与服务端的会话是否失效，该节点都会持续存在，除非被显式删除。 临时 （Ephemeral）：当客户端与服务端的会话失效时，该节点会自动被删除。 顺序性 普通节点：节点的名称与创建时客户端指定的名称完全相同。 顺序节点：在节点的名称后会自动附加一个唯一的递增序列号。 节点类型 说明 持久节点 在Zookeeper中，持久节点会持续存在，直到被显式删除。 持久顺序节点 与持久节点特性相同，但每次创建时，节点名称后会自动添加由其父节点维护的递增整型数字。 临时节点 临时节点与客户端会话相关联。客户端会话失效时，其创建的所有临时节点都会被删除。 临时顺序节点 与临时节点特性相同，但在节点名称后会自动添加由其父节点维护的递增整型数字。 临时节点 注意：临时节点不能添加子节点节点元数据 ephemeralOwner 就是临时节点客户端的 session id 使用场景 服务注册与发现 分布式锁（临时顺序节点） Container节点 1create -c /test Container 节点是ZooKeeper 3.5.0及其以后版本中引入的一种特殊类型的znode。 自动清理： 当 Container 节点下的所有子节点都被删除后，Container 节点会在将来某个时间点被ZooKeeper自动删除。 这个特性使得 Container 节点很适合用于那些只需要短暂地作为容器存在的场景。 无法直接删除： 如果你尝试直接删除一个还包含子节点的 Container 节点，这个操作会失败。只有当其下没有子节点时，Container 节点才会被自动删除。 TTL节点 3.5.3版本新增，需要配置系统变量zookeeper.extendedTypesEnabled=true，3.6.3版本后默认启用 1create -t 10 /test 十秒后自动删除 自动删除：一旦TTL时间过去，节点将被自动删除。 递归删除：如果一个TTL节点是一个父节点，当其到期被删除时，其子节点也会被删除。 不可更改的TTL：一旦设置了TTL值，该值是不可以被修改的。 TTL的时间单位：在ZooKeeper中，TTL的单位是毫秒。 节点数据信息 zookeeper 中的所有存储的数据是由 znode 组成的，节点也称为 znode，并以 key/value 形式存储数据 说明 描述 data 存储在znode中的数据。 acl 定义了用户的权限以及他们能够对znode执行的操作。c: 允许创建子节点w: 允许更新节点数据r: 允许读取节点数据和获取子节点列表d: 允许删除子节点a: 允许设置节点的acl权限 stat 包含znode的元数据，如创建和修改的时间戳、版本信息、大小等。 child 列出当前znode的直接子节点。 节点元数据 字段 说明 czxid 创建节点时的事务ID。事务ID（zxid）表示ZooKeeper状态的每次修改。每个zxid都是唯一的，且按修改的顺序连续生成。如果zxid1小于zxid2，则zxid1在zxid2之前发生。 mzxid 节点最后修改时的事务ID。 pZxid 当前节点的子节点列表最后一次修改的事务ID。只有当子节点列表变动（例如，添加或删除子节点）时，此ID才会变更。修改子节点的数据内容不会影响此ID。 ctime 创建节点时的时间戳（毫秒为单位，从1970年1月1日开始）。 mtime 节点最后修改的时间戳（毫秒为单位，从1970年1月1日开始）。 cversion 子节点版本号。每当子节点列表变动时，此版本号递增。 dataversion 数据版本号。每当节点数据变动时，此版本号递增。 aclVersion ACL（访问控制列表）版本号。每当节点的ACL变动时，此版本号递增。 ephemeralOwner 如果是临时节点，此字段表示znode拥有者的session id。如果是持久节点，则此字段为0。 dataLength 节点数据的长度（以字节为单位）。 numChildren 当前节点的直接子节点数量。 集群角色 事务请求： 在ZooKeeper的集群（称为ensemble）中，为了保持数据一致性，修改操作都被视为事务请求。 客户端发起的事务请求首先到达Leader。 Leader创建提议并发起投票，请求Follower节点的意见。 Follower根据其状态回应投票结果。 若过半数Follower同意，Leader确认并执行事务。 由于需过半同意，集群大小确实对写性能有所影响：节点越多，达到多数的延迟越高。 事务请求转发： Follower收到客户端写请求后，会转发给Leader，因为仅Leader可提议更改。 Leader提议更改并等待Follower的回应。 一旦得到过半Follower的确认，Leader提交更改，并同步确保所有Follower与其状态一致。 写请求需要所有活跃的节点参与进来保证数据的一致性，因此它们被视为事务请求。 角色 说明 leader 负责发起投票和做出决策，更新系统状态，以及处理事务请求。 follower (跟随者) 参与投票，接收并处理客户端的非事务请求并返回结果，同时将事务请求转发给leader进行处理。 observer (观察者) 虽然不参与投票，但同步leader的状态来扩展系统并提高读取性能。它还可以接收客户端请求，处理非事务请求并返回结果，同时将事务请求转发给leader。 observer 观察者 在 Leader 选举过程中，我们通常不讨论 Observer，这是因为 Observer 不具备投票权。尽管如此，Observer 与 Follower 在功能上极为相似，主要区别是其不参与投票和过半机制。Observer 可以接受客户端连接，并能够从 Leader 同步数据。 Observer 的设计目的之一是为了支持扩容。它可以帮助实现数据的动态迁移和扩容。最重要的是，Observer在不影响集群写性能的前提下增强了读取性能，使其成为异地数据中心数据同步的理想选择。 集群搭建 zookeeper 下载 创建节点文件夹（伪集群） 1234567891011121314mkdir /data/zookeeper/zk1mkdir /data/zookeeper/zk1/datamkdir /data/zookeeper/zk1/logsecho 1 &gt; /data/zookeeper/zk1/data/myidmkdir /data/zookeeper/zk2mkdir /data/zookeeper/zk2/datamkdir /data/zookeeper/zk2/logsecho 2 &gt; /data/zookeeper/zk2/data/myidmkdir /data/zookeeper/zk3mkdir /data/zookeeper/zk3/datamkdir /data/zookeeper/zk3/logsecho 3 &gt; /data/zookeeper/zk3/data/myid 将下载好的zk分别解压到创建的三个zk目录中 在zookeeper conf目录下分别添加zoo.cfg zookeeper 完整配置详解 1234567891011121314151617181920#心跳时间2秒 tickTime=2000#Follower跟随者服务器与Leader领导者服务器之间初始化连接时能容忍的最多心跳数10*tickTime initLimit=10#集群中Leader与Follower之间的最大响应时间单位5*tickTime syncLimit=5#存储快照文件 snapshot 的目录。默认情况下，事务日志也会存储在这里。建议同时配置参数dataLogDir, 事务日志的写性能直接影响zk性能dataDir=/data/zookeeper/zk1/data#事务日志输出目录。尽量给事务日志的输出配置单独的磁盘或是挂载点，这将极大的提升ZK性能dataLogDir=/data/zookeeper/zk1/logs#zookeeper端口 clientPort=2181#单个客户端与单台服务器之间的连接数的限制，是ip级别的，默认是60，如果设置为0，那么表明不作任何限制maxClientCnxns=60#server.1代表一台服务器的编号，第一个端口为集群通讯端口，第二个端口代表Leader选举的端口 server.1=127.0.0.1:2881:3881server.2=127.0.0.1:2882:3882server.3=127.0.0.1:2883:3883#指定观察者#server.3=127.0.0.1:2883:3883:observer 123456789101112131415161718#心跳时间2秒 tickTime=2000#Follower跟随者服务器与Leader领导者服务器之间初始化连接时能容忍的最多心跳数10*tickTime initLimit=10#集群中Leader与Follower之间的最大响应时间单位5*tickTime syncLimit=5#存储快照文件 snapshot 的目录。默认情况下，事务日志也会存储在这里。建议同时配置参数dataLogDir, 事务日志的写性能直接影响zk性能dataDir=/data/zookeeper/zk2/data#事务日志输出目录。尽量给事务日志的输出配置单独的磁盘或是挂载点，这将极大的提升ZK性能dataLogDir=/data/zookeeper/zk2/logs#zookeeper端口 clientPort=2182#单个客户端与单台服务器之间的连接数的限制，是ip级别的，默认是60，如果设置为0，那么表明不作任何限制maxClientCnxns=60#server.1代表一台服务器的编号，第一个端口为集群通讯端口，第二个端口代表Leader选举的端口 server.1=127.0.0.1:2881:3881server.2=127.0.0.1:2882:3882server.3=127.0.0.1:2883:3883 123456789101112131415161718#心跳时间2秒 tickTime=2000#Follower跟随者服务器与Leader领导者服务器之间初始化连接时能容忍的最多心跳数10*tickTime initLimit=10#集群中Leader与Follower之间的最大响应时间单位5*tickTime syncLimit=5#存储快照文件 snapshot 的目录。默认情况下，事务日志也会存储在这里。建议同时配置参数dataLogDir, 事务日志的写性能直接影响zk性能dataDir=/data/zookeeper/zk3/data#事务日志输出目录。尽量给事务日志的输出配置单独的磁盘或是挂载点，这将极大的提升ZK性能dataLogDir=/data/zookeeper/zk3/logs#zookeeper端口 clientPort=2183#单个客户端与单台服务器之间的连接数的限制，是ip级别的，默认是60，如果设置为0，那么表明不作任何限制maxClientCnxns=60#server.1代表一台服务器的编号，第一个端口为集群通讯端口，第二个端口代表Leader选举的端口 server.1=127.0.0.1:2881:3881server.2=127.0.0.1:2882:3882server.3=127.0.0.1:2883:3883 分别启动zk 12345cd bin目录./bin/zkServer.sh start# 查看状态./bin/zkServer.sh status 客户端命令行 命令行语法 命令基本语法 功能描述 help 显示ZooKeeper支持的所有命令及其用法。 ls &lt;path&gt; [watch] 列出指定path下的子节点。可选参数watch设置监听指定节点的子节点变化。-R：递归列出所有子节点。 ls2 &lt;path&gt; [watch] 类似于ls，但同时显示节点的状态信息，如数据版本和ACLs。 create &lt;path&gt; [data] [acl] 在指定的路径创建一个新节点。-s：创建一个带序列号的节点。-e：创建一个临时节点，会话结束或超时后消失。-c：创建一个容器节点，会自动删除没有子节点的容器。-t &lt;time&gt;：创建一个具有TTL（存活时间）的节点，未修改且无子节点时将被自动删除。 get &lt;path&gt; [watch] 获取指定路径节点的数据内容。watch参数用于设置对节点数据变化的监听。 set &lt;path&gt; &lt;data&gt; [version] 更新指定路径节点的数据。-w：更新时设置对节点数据变化的监听。-s：更新数据同时设置附加信息。-v &lt;version&gt;：指定节点的版本进行乐观锁控制。 stat &lt;path&gt; 显示指定节点的元数据信息，如版本号和子节点数。 delete &lt;path&gt; [version] 删除指定路径的节点。-v &lt;version&gt;：用于乐观锁控制，只有版本匹配时才能删除。 deleteall &lt;path&gt; 递归删除指定路径及其所有子节点。 rmr &lt;path&gt; 递归删除节点，与deleteall功能相同，但rmr已被弃用。 操作节点 查看指定路径下的子节点 1ls / 查看子节点和元数据 1ls2 / 查看节点元数据 1stat / 创建永久节点 1create /yongjiu 永久节点 创建永久顺序节点 1create -s /yongjiu/shunxu 永久顺序节点 创建成功后名称会发生变化 shunxu0000000000 后面会加上序列号 创建临时节点 1create -e /linshi 临时节点 创建临时顺序节点 1create -e -s /linshi 临时顺序节点 临时顺序节点的父节点不能是临时节点 设置节点的值 1set /yongjiu 永久节点2 获取节点的值 1get /yongjiu 删除节点 普通删除 1delete /yongjiu 乐观锁删除 1delete -v 1 /youjin 递归删除节点，包括其子节点 1deleteall /yongjiu 1rmr /yongjiu ACL权限操作 注册当前会话的账号和密码 1addauth digest test:123456 创建节点并设置权限（指定该节点的用户，以及用户所拥有的权限） 1create /test-node test auth:test:123456:cdwra 如果其他会话不注册当前会话的账号密码，则没有权限操作/test-node节点 客户端操作 ZooKeeper原生Java API的不足之处： 在连接zk超时的时候，不支持自动重连，需要手动操作 Watch注册一次就会失效，需要反复注册 不支持递归创建节点 Apache curator 解决Watch注册一次就会失效的问题 支持直接创建多级结点 提供的 API 更加简单易用 提供更多解决方案并且实现简单，例如：分布式锁 提供常用的ZooKeeper工具类 编程风格更舒服 zkClient 节点操作 curator 节点操作 Curator使用手册 节点监听原理 在ZooKeeper中，节点监听（watch）是一种机制，允许客户端在指定的znode上注册一个watch，以便在该节点上发生特定类型的事件时得到通知。ZooKeeper的watch机制是轻量级的，它被设计为一次性触发器，即一旦被触发就会被移除，如果需要持续监听，则需要在每次接收到通知后重新设置watch。 节点监听流程 客户端初始化： 当主线程中创建一个ZooKeeper客户端实例时，客户端会初始化其内部组件和线程。 线程启动： 客户端启动两个关键的后台线程： 发送线程（Send Thread）： 负责与ZooKeeper集群建立和维持网络连接，发送请求和接收响应。 事件线程（Event Thread）： 负责处理来自服务器的事件通知，并触发注册的Watcher回调。 监听器注册： 客户端通过发送线程向ZooKeeper服务器注册Watcher，同时指定感兴趣的事件类型。 服务器处理： ZooKeeper服务器接收到Watcher注册信息后，会把Watcher对象与指定的节点关联起来，并保存在内部的监听器映射表中。 事件触发： 一旦监视的节点发生了客户端注册的事件类型变化，ZooKeeper服务器会将此变化封装为一个事件通知，并将其加入到待处理的事件队列中。 事件通知： 事件线程从队列中获取通知，并通知相关客户端的Watcher进行处理。 监听器一次性特性： 在ZooKeeper中，Watcher是一次性的，即一旦触发，就不再有效。如果客户端需要持续监听某个事件，它需要在每次处理完事件通知后再次注册相同的Watcher。 注意事项： 为了保证线程安全，事件处理逻辑应避免直接在process()方法中执行长时间运行的操作或网络调用。如果必须进行这些操作，应在process()方法中将任务委托给其他线程执行。 客户端的主线程通常用于发起ZooKeeper操作和Watcher注册，并不直接参与网络通信或事件处理。这些职责由发送线程和事件线程分别承担。 监听事件 事件 说明 触发条件 None 连接状态事件 客户端的连接状态改变时，包括以下KeeperState事件：- Expired：会话过期- Disconnected：断开连接- SyncConnected：同步连接已建立- AuthFailed：认证失败 NodeCreated 节点创建事件 对特定节点设置exists监听后，该节点被成功创建时触发 NodeDeleted 节点删除事件 被监听的节点被删除时触发 NodeDataChanged 节点数据变化事件 被监听的节点的数据发生变化时触发 NodeChildrenChanged 子节点列表变化事件 被监听节点的直接子节点列表发生变化时触发，如子节点的添加或删除 客户端实现 Watch Curator事件监听 Curator 实现Watch zkClient 实现Watch Curator提供了五种监听方式 Curator框架为ZooKeeper客户端操作提供了强化的监听器功能，包括以下几种类型： Watcher监听：类似于ZooKeeper原生API的Watcher，用于对节点的变更进行一次性监听。它与ZooKeeper的原生实现在使用上差别不大，但Curator提供了更易用的接口。 CuratorListener监听：专门用于处理后台操作的通知，如使用inBackground方法提交的异步任务。这类监听关注的是任务执行的结果及错误通知，而非节点内容的改变。 NodeCache监听：针对单个节点的监听器，它会监控节点本身的创建、更新和删除事件。一旦注册，无需再次设置监听器，Curator会自动管理监听的生命周期。 PathChildrenCache监听：用于监听一个节点的所有子节点的状态变化，如子节点的增加、更新和删除。与NodeCache一样，它提供了自动再监听机制，简化了开发者的操作。 TreeCache监听：结合了NodeCache和PathChildrenCache的特性，监听指定的起始节点及其所有子节点的变化。这意味着无论是节点本身还是其任意层次的子节点发生变化，TreeCache都能捕捉到事件，并自动重新注册监听，方便用户追踪整个树形结构的状态。 节点的值变化监听 12客户端1get -w /yongjiu 12客户端2set /yongjiu 永久节点2 1234客户端1WATCHER::WatchedEvent state:SyncConnected type:NodeDataChanged path:/yongjiu 节点的子节点变化监听 只对一级子节点有效 当监听节点的子节点发生变化就会触发（新增和删除） 12客户端1ls -w /yongjiu 12客户端2create -s /yongjiu/shunxu 1234客户端1WATCHER::WatchedEvent state:SyncConnected type:NodeChildrenChanged path:/yonjiu 节点的后辈节点变化监听 12客户端1ls -R -w /test 12客户端2create /test/test2/test3/test4 1234客户端1WATCHER::WatchedEvent state:SyncConnected type:NodeChildrenChanged path:/test/test2/test3 判断Znode是否存在 1exists /test 存在则返回节点信息，不存在则返回null 事务操作 zkClient 使用事务 curator 使用事务 在分布式系统中，我们经常需要确保一组操作要么全部成功执行，要么全部不执行，以此来保持系统的一致性。这是事务的基本特性，也称为原子性。在多线程或者分布式环境中，操作原子性尤为重要，因为它可以避免由于操作部分完成而引起的数据不一致问题。 从版本3.4.0起，ZooKeeper引入了 multi 操作，它允许客户端原子性地执行一批操作。这意味着这批操作要么都成功，要么都不会对ZooKeeper的状态产生任何影响。在Java客户端中，这一特性被封装在 Transaction 类中，提供了一种便捷的方法来组合多个操作并一次性提交。如果事务中的任何操作失败，整个事务会回滚，保证数据的一致性。 multiop 删除 /a /b /c 节点 12345678910111213@Test@SneakyThrowspublic void multiOpTest() &#123; List&lt;OpResult&gt; results = zooKeeper.multi(Arrays.asList( Op.delete(&quot;/a&quot;, -1), Op.delete(&quot;/b&quot;, -1), Op.delete(&quot;/c&quot;, -1) )); for (OpResult result : results) &#123; System.out.println(result.getType()); &#125;&#125; 假如只存在 /a /b，执行时会抛出 NoNodeException 因为不存在 /c，执行完之后发生 /a /b 还在 异步处理 跟 ZooKeeper 的其他节点操作一样，multiop也提供了异步的版本，通过返回码判断执行结果，不用去捕获处理异常 123456789101112131415161718192021222324252627282930313233/** * multi 事务异步操作 */@Testpublic void asyncMultiOpTest() throws InterruptedException &#123; // 回调函数 // ctx 是multi 方法传入的 AsyncCallback.MultiCallback callback = (rc, path, ctx, opResults) -&gt; &#123; switch (KeeperException.Code.get(rc)) &#123; case OK: System.out.println(String.format(&quot;节点%s删除成功&quot;, path)); case CONNECTIONLOSS: System.out.println(&quot;连接丢失&quot;); break; case NONODE: System.out.println(&quot;NoNode Error!&quot;); break; default: System.out.println(&quot;Error when trying to delete node: &quot; + KeeperException.create(KeeperException.Code.get(rc), path)); &#125; &#125;; zkClient.multi(Arrays.asList( Op.delete(&quot;/a&quot;, -1), Op.delete(&quot;/b&quot;, -1), Op.delete(&quot;/c&quot;, -1), ), callback, null ); TimeUnit.SECONDS.sleep(2);&#125; Transaction ZooKeeper 的 Transaction 功能建立在 multi 操作之上，提供了一种灵活的方式来组织一组操作。在这个机制中，你可以在事务提交前随时添加新的操作到一个原子操作序列中。此外，你可以在不同的方法中构建事务，不受限于单一代码块。当准备好提交时，Transaction 支持同步（commit）和异步（commitAsync）两种提交方式，确保事务的一致性执行。 123456789101112131415@Test@SneakyThrowspublic void transactionTest() &#123; Transaction t = zooKeeper.transaction(); t.create(&quot;/t1&quot;, &quot;t1&quot;.getBytes(StandardCharsets.UTF_8), ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT); t.create(&quot;/t2&quot;, &quot;t2&quot;.getBytes(StandardCharsets.UTF_8), ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT); t.create(&quot;/t3&quot;, &quot;t3&quot;.getBytes(StandardCharsets.UTF_8), ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT); List&lt;OpResult&gt; results = t.commit(); for (OpResult result : results) &#123; if (result instanceof OpResult.CreateResult) &#123; System.out.println(&quot;create success&quot;); &#125; &#125;&#125; 异步处理 123456789101112131415161718192021222324252627@Test@SneakyThrowspublic void asyncTransactionTest() &#123; // 回调函数 AsyncCallback.MultiCallback callback = (rc, path, ctx, opResults) -&gt; &#123; switch (KeeperException.Code.get(rc)) &#123; case OK: System.out.println(&quot;事务执行成功&quot;); case CONNECTIONLOSS: System.out.println(&quot;连接丢失&quot;); break; case NONODE: System.out.println(&quot;NoNode Error!&quot;); break; default: System.out.println(&quot;Error when trying to delete node: &quot; + KeeperException.create(KeeperException.Code.get(rc), path)); &#125; &#125;; Transaction t = zooKeeper.transaction(); t.delete(&quot;/t1&quot;, -1) .delete(&quot;/t2&quot;, -1) .delete(&quot;/t3&quot;, -1); t.commit(callback, null); TimeUnit.SECONDS.sleep(1);&#125; 服务上下线 在分布式系统中，一个服务的节点可以有多台（比如Dubbo），可以动态上下线，任意一台客户端都能实时感知到服务节点的上下线 服务端实现 首先建立一个服务的永久节点 当节点上线则在服务节点下创建一个临时节点，节点的值存储服务器连接信息 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556@Slf4jpublic class ProviderTest &#123; // 集群地址 private static final String ROOT = &quot;/server&quot;; private String connectString = &quot;127.0.0.1:2181&quot;; private int sessionTimeout = 2000; private ZooKeeper zkClient; @SneakyThrows @BeforeEach public void init() throws IOException &#123; zkClient = new ZooKeeper(connectString, 2000, new Watcher() &#123; @Override public void process(WatchedEvent event) &#123; &#125; &#125;); // 创建 /server 节点 Stat exists = zkClient.exists(ROOT, false); if (exists == null) &#123; try &#123; zkClient.create(ROOT, null, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT); &#125; catch (Exception e) &#123; &#125; &#125; // 启动注册服务 this.register(&quot;provider&quot;, &quot;127.0.0.1&quot;, 8080); &#125; /** * 注册服务 * @param serverName * @param ip * @param port */ @SneakyThrows private void register(String serverName, String ip, int port) &#123; String path = String.format(&quot;%s/%s&quot;, ROOT, serverName); String value = String.format(&quot;%s:%s&quot;, ip, port); String create = zkClient.create(path, value.getBytes(StandardCharsets.UTF_8), ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL); log.info(&quot;server &#123;&#125; is online&quot;, serverName); &#125; @Test public void test() throws InterruptedException &#123; log.info(&quot;启动服务...&quot;); TimeUnit.SECONDS.sleep(1000); &#125;&#125; 服务端实现 获取服务节点下的所有临时节点，并保存到本地服务列表 监听服务节点子节点变化，当服务节点的子节点发生变化时刷新本地服务列表 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071@Slf4jpublic class ConsumerTest &#123; // 集群地址 private static final String ROOT = &quot;/server&quot;; private String connectString = &quot;127.0.0.1:2181&quot;; private int sessionTimeout = 2000; private ZooKeeper zkClient; private List&lt;String&gt; serverList = null; private AtomicLong sequence = new AtomicLong(0); @BeforeEach public void init() throws IOException &#123; zkClient = new ZooKeeper(connectString, 2000, new Watcher() &#123; @Override public void process(WatchedEvent event) &#123; log.info(&quot;path: &#123;&#125;&quot;, event.getPath()); // 服务变更再次获取服务列表 getServerList(); &#125; &#125;); // 服务启动获取服务列表 getServerList(); &#125; /** * 获取远程服务 */ @SneakyThrows private void getServerList() &#123; // 1 获取服务器子节点信息，并且对父节点进行监听 List&lt;String&gt; children = zkClient.getChildren(ROOT, true); // 2 存储服务器信息列表 serverList = new CopyOnWriteArrayList&lt;&gt;(children); log.info(&quot;serverList: &#123;&#125;&quot;, serverList); &#125; /** * 获取服务 * @return */ private String getServer() &#123; if (CollectionUtils.isEmpty(serverList)) &#123; throw new RuntimeException(&quot;没有可用的服务&quot;); &#125; int len = serverList.size(); return serverList.get((int) (sequence.getAndIncrement() % len)); &#125; @Test public void test() throws InterruptedException &#123; for (;;) &#123; try &#123; TimeUnit.SECONDS.sleep(10); log.info(&quot;执行业务&quot;); String server = getServer(); log.info(&quot;跨服务调用：调用服务：&#123;&#125;&quot;, server); log.info(&quot;服务调用结束...&quot;); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 分布式锁实现 ZooKeeper 利用其临时顺序节点实现分布式锁，这种机制的优势在于其高可靠性：一旦持锁的服务意外宕机，ZooKeeper会自动删除那些与会话关联的临时节点，因此锁会被迅速释放。相比之下，基于Redis的分布式锁通常依赖于键的过期时间（TTL）来释放锁，如果服务在锁未释放时宕机，则需要等待TTL到期，这可能导致锁被持有超过必要的时间。 然而，ZooKeeper并不总是适用于高并发场景下的分布式锁。因为ZooKeeper的写操作需要经过集群中唯一的领导者（Leader）进行处理和复制到其他跟随者（Follower）节点，这限制了其在高写负载条件下的性能。如果写请求过于频繁，可能会成为系统性能的瓶颈。 分布式锁实现 创建根节点：在ZooKeeper中创建一个持久的根节点/locks，作为所有分布式锁的父节点。 请求锁：当一个客户端希望获得锁时，它在/locks节点下创建一个临时顺序节点，例如/locks/lock_。 节点排序：客户端获取/locks下所有子节点，并按照节点编号排序。 检查排名：客户端检查自己创建的临时顺序节点在所有子节点中的排序位置： 如果是最小的节点，那么客户端获得锁。 如果不是，客户端找到比自己小的最近的一个节点，并在这个节点上设置监听。 锁等待：如果没有获得锁，客户端等待监听的节点变更（例如被删除），在变更发生时重新进行排名判断。 锁的释放：一旦客户端完成其业务逻辑，它会删除自己的临时顺序节点，从而释放锁。 监听触发：其他客户端的监听器会在它们监听的节点被删除时收到通知，然后这些客户端将重复步骤4和5来尝试获取锁。 原生 Zookeeper 实现 12345678/** * 分布式锁接口 */public interface DistributedLock &#123; void lock(); void unLock();&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134@Slf4jpublic class ZookeeperLock implements DistributedLock &#123; // 根节点 private static final String ROOT = &quot;/lock&quot;; // 使用ThreadLocal绑定当前线程与其所创建的节点信息 private ThreadLocal&lt;String&gt; nodeInfo = new ThreadLocal&lt;&gt;(); // 连接地址 private String connectString = &quot;127.0.0.1:2181&quot;; // 过期事件 private int sessionTimeout = 2000; // 当前线程锁的路径 private String lockPath; // zk客户端独享 private ZooKeeper zkClient; private ZookeeperLock(String lockName) &#123; this.lockPath = String.format(&quot;%s/%s&quot;, ROOT, lockName); try &#123; // 用于同步等待zk客户端连接服务端 CountDownLatch connectedSignal = new CountDownLatch(1); zkClient = new ZooKeeper(connectString, sessionTimeout, (Watcher) watchedEvent -&gt; &#123; // 连接事件 if (watchedEvent.getState() == Watcher.Event.KeeperState.SyncConnected) &#123; connectedSignal.countDown(); &#125; &#125;); // 确保连接建立 connectedSignal.await(); // 是否需要创建根节点 Stat stat = zkClient.exists(ROOT, false); if (stat == null) &#123; // 创建持久节点 zkClient.create(ROOT, null, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; @Override public void lock() &#123; try &#123; // 创建临时顺序节点 String nodeName = zkClient.create(lockPath, null, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL_SEQUENTIAL); // 取出所有子节点 List&lt;String&gt; subNodeList = zkClient.getChildren(ROOT, false); TreeSet&lt;String&gt; subNodeSet = new TreeSet&lt;&gt;(); subNodeList.forEach(node -&gt; subNodeSet.add(String.format(&quot;%s/%s&quot;, ROOT, node))); // 获取最小节点 String smallNode = subNodeSet.first(); // 获取当前节点的上一个节点 String preNode = subNodeSet.lower(nodeName); // 如果当前线程节点为最小节点，则表示获取锁 if (nodeName.equals(smallNode)) &#123; nodeInfo.set(nodeName); return; &#125; CountDownLatch waitLockLatch = new CountDownLatch(1); // 注册当前节点的上个节点删除事件监听 Stat stat = zkClient.exists(preNode, watchedEvent -&gt; &#123; if (watchedEvent.getType() == Watcher.Event.EventType.NodeDeleted) &#123; waitLockLatch.countDown(); &#125; &#125;); // 判断比当前节点小的节点是否存在，不存在则获得锁 if (stat != null) &#123; // 阻塞主线程，等待上一个节点删除后唤醒线程 waitLockLatch.await(); nodeInfo.set(nodeName); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; @Override public void unLock() &#123; String nodeName = nodeInfo.get(); try &#123; // 判断当前线程是否已上锁（在等待锁时不能解锁） if (nodeName != null) &#123; zkClient.delete(nodeName, -1); nodeInfo.remove(); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; static int NUM = 0; public static void main(String[] args) throws InterruptedException &#123; Executor executor = Executors.newFixedThreadPool(100); ZookeeperLock zookeeperLock = new ZookeeperLock(&quot;count&quot;); CountDownLatch latch = new CountDownLatch(1000); for (int i = 0; i &lt; 1000; i++) &#123; executor.execute(() -&gt; &#123; try &#123; zookeeperLock.lock(); NUM++; &#125; finally &#123; zookeeperLock.unLock(); latch.countDown(); &#125; &#125;); &#125; latch.await(); log.info(&quot;NUM = &#123;&#125;&quot;, NUM); &#125;&#125; Curator 框架实现 Curator使用手册 Curator是Netflix公司开源的一套zookeeper客户端框架，Curator是对Zookeeper支持最好的客户端框架。Curator封装了大部分Zookeeper的功能，比如Leader选举、分布式锁等，减少了技术人员在使用Zookeeper时的底层细节开发工作 使用原生API存在的问题 会话连接是异步的，需要自己去处理。比如使用 CountDownLatch Watch 需要重复注册，不然就不能生效 开发的复杂性比较高 不支持多节点删除和创建。需要自己去递归 Curator主要实现了下面四种锁 InterProcessMutex：分布式可重入排它锁 InterProcessSemaphoreMutex：分布式排它锁 InterProcessReadWriteLock：分布式读写锁 InterProcessMultiLock：将多个锁作为单个实体管理的容器 分布式可重入排他锁 123456789101112131415161718192021222324252627282930313233343536373839@Slf4j@SpringBootTestpublic class InterProcessMutexTest &#123; @Autowired private CuratorFramework curatorFramework; @Test @SneakyThrows public void test() &#123; CountDownLatch countDownLatch = new CountDownLatch(100); ExecutorService executorService = Executors.newFixedThreadPool(100); String lockNode = &quot;/lock&quot;; InterProcessMutex lock = new InterProcessMutex(curatorFramework, lockNode); // 多线程竞争锁 for (int i = 0; i &lt; 100; i++) &#123; executorService.execute(() -&gt; &#123; try &#123; // 获取锁 lock.acquire(); log.info(&quot;线程：&#123;&#125; 获得分布式锁&quot;, Thread.currentThread().getName()); TimeUnit.MILLISECONDS.sleep(100); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; // 释放锁 try &#123; lock.release(); countDownLatch.countDown(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125;); &#125; countDownLatch.await(); &#125;&#125; 分布式排他锁 用法和 InterProcessMutex 一样 123456789101112131415161718192021222324252627282930313233343536373839@Slf4j@SpringBootTestpublic class InterProcessSemaphoreMutexTest &#123; @Autowired private CuratorFramework curatorFramework; @Test @SneakyThrows public void test() &#123; CountDownLatch countDownLatch = new CountDownLatch(100); ExecutorService executorService = Executors.newFixedThreadPool(100); String lockNode = &quot;/lock&quot;; InterProcessSemaphoreMutex lock = new InterProcessSemaphoreMutex(curatorFramework, lockNode); // 多线程竞争锁 for (int i = 0; i &lt; 100; i++) &#123; executorService.execute(() -&gt; &#123; try &#123; // 获取锁 lock.acquire(); log.info(&quot;线程：&#123;&#125; 获得分布式锁&quot;, Thread.currentThread().getName()); // TimeUnit.MILLISECONDS.sleep(100); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; // 释放锁 try &#123; lock.release(); countDownLatch.countDown(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125;); &#125; countDownLatch.await(); &#125;&#125; 分布式读写锁 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647@Slf4j@SpringBootTestpublic class InterProcessReadWriteLockTest &#123; @Autowired private CuratorFramework curatorFramework; @Test @SneakyThrows public void test() &#123; CountDownLatch countDownLatch = new CountDownLatch(22); ExecutorService executorService = Executors.newFixedThreadPool(30); String lockNode = &quot;/lock&quot;; InterProcessReadWriteLock lock = new InterProcessReadWriteLock(curatorFramework, lockNode); InterProcessMutex readLock = lock.readLock(); InterProcessMutex writeLock = lock.writeLock(); for (int i = 0; i &lt; 10; i++) &#123; executorService.execute(() -&gt; tryLock(readLock, &quot;获取读锁&quot;, countDownLatch)); &#125; for (int i = 0; i &lt; 2; i++) &#123; executorService.execute(() -&gt; tryLock(writeLock, &quot;获取写锁&quot;, countDownLatch)); &#125; for (int i = 0; i &lt; 10; i++) &#123; executorService.execute(() -&gt; tryLock(readLock, &quot;获取读锁&quot;, countDownLatch)); &#125; countDownLatch.await(); &#125; public void tryLock(InterProcessMutex lock, String msg, CountDownLatch countDownLatch) &#123; try &#123; lock.acquire(); log.info(&quot;线程：&#123;&#125; 获取&#123;&#125;&quot;, Thread.currentThread().getName(), msg); TimeUnit.SECONDS.sleep(3); &#125; catch (Exception e) &#123; log.error(e.getMessage()); &#125; finally &#123; try &#123; lock.release(); &#125; catch (Exception e) &#123; &#125; &#125; &#125;&#125; 分布式锁容器 curator实现了一个类似容器的锁InterProcessMultiLock，它可以把多个锁包含起来像一个锁一样进行操作，简单来说就是对多个锁进行一组操作。当acquire的时候就获得多个锁资源，否则失败。当release时候释放所有锁资源，不过如果其中一把锁释放失败将会被忽略 123456789101112131415161718192021222324252627282930313233343536373839404142@Slf4j@SpringBootTestpublic class InterProcessMultiLockTest &#123; @Autowired private CuratorFramework curatorFramework; @Test @SneakyThrows public void test() &#123; CountDownLatch countDownLatch = new CountDownLatch(10); ExecutorService executorService = Executors.newFixedThreadPool(5); String lockNode1 = &quot;/lock1&quot;; String lockNode2 = &quot;/lock2&quot;; InterProcessMutex lock1 = new InterProcessMutex(curatorFramework, lockNode1); InterProcessSemaphoreMutex lock2 = new InterProcessSemaphoreMutex(curatorFramework, lockNode2); // 锁容器，合并多个锁 InterProcessMultiLock multiLock = new InterProcessMultiLock(Arrays.asList(lock1, lock2)); for (int i = 0; i &lt; 10; i++) &#123; executorService.execute(() -&gt; &#123; try &#123; multiLock.acquire(); log.info(&quot;线程：&#123;&#125; 获取锁&quot;, Thread.currentThread().getName()); TimeUnit.SECONDS.sleep(1); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; try &#123; multiLock.release(); countDownLatch.countDown(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125;); &#125; countDownLatch.await(); &#125;&#125; 底层原理 持久化机制 ZooKeeper将所有的数据存储在一个内存中的数据结构称为 Data Tree 中。这允许快速的读取操作，因为所有的数据都在内存中。但为了持久化和恢复，ZooKeeper使用了以下机制来保存数据到磁盘。 事务日志 每当ZooKeeper的数据发生变化时（例如，创建、删除或更新znodes），这些变化都会作为一个事务写入到事务日志中。事务日志是ZooKeeper保持数据一致性的关键，允许它在崩溃后重新构建数据状态。 数据快照 为了减少磁盘I/O，ZooKeeper周期性地将整个Data Tree的状态保存到磁盘上的快照文件中。快照包含了在特定时间点上所有znodes的数据和状态信息。 zk通过两种形式的持久化，在恢复时先全量恢复快照文件中的数据到内存中，再用日志文件中的数据做增量恢复，这样的恢复速度更快。 读流程 集群中任意节点都是可读的 遵循 BASE 理论 请求发送：客户端可以向集群中的任何服务器（ Leader、Follower 或 Observer ）发送读取请求。 本地读取：接收请求的ZooKeeper服务器将直接在本地数据副本上执行读取操作。因为ZooKeeper集群中的数据是一致的，没有分片，每个服务器都包含完整的数据集。 最终一致性：尽管每个服务器都保存有全量数据，但由于Zab协议的事务处理方式，集群可能存在一个时间窗口的数据不一致，但经过一段时间数据同步或，保证最终一致性。 Zab协议 ZooKeeper 是一个为分布式应用提供协调服务的关键系统，通常部署多节点集群以确保高可用性。集群中的节点可以扮演领导者（Leader）、跟随者（Follower）或观察者（Observer）的角色。领导者处理所有的写请求，确保数据的一致性，而跟随者和观察者则参与读请求和状态的复制。 ZooKeeper 通过实现 ZAB（ZooKeeper Atomic Broadcast）协议来维护集群数据的一致性。ZAB 协议是特别为ZooKeeper设计的，以确保即使在领导者更换或系统崩溃后也能够保持数据的一致性。ZAB协议负责： 消息广播：ZAB 保证所有的写操作，如创建、更新或删除数据节点等事务性请求，都按照全局一致的顺序被应用到每一个服务器上，从而保持整个集群的一致性。 状态恢复：当一个新的服务器节点加入集群，或者一个已有的服务器节点在崩溃后重新加入集群时，ZAB 保证这个节点能够与当前领导者同步，获得最新的系统状态。 Zab 协议简介 原子广播 崩溃恢复 集群初始化选举 运行期间选举 为什么集群节点推荐奇数 最大化容错能力 奇数节点集群能够在不增加冗余节点的同时，最大化容错能力。例如，有3个节点的集群可以容忍1个节点失败。一个有4个节点的集群也只能容忍1个节点失败。 避免脑裂 脑裂是指集群在网络分区发生时分成两个独立的部分，每部分都可能独自做出决策。奇数节点集群在分区发生时，不可能有两个等大的部分，这样就总有一个部分能够维持多数派，从而保持集群的正常运作。 决策效率 在进行领导者选举或其他需要多数派同意的操作时，奇数节点集群能更快地达成决策，因为不会出现票数相等的情况，从而避免了决策的延迟。 ZK中的NIO ZK追求的模型 遵循 BASE理论 ZooKeeper选择优先保证一致性（C）和分区容错性（P），在网络分区发生时，它会牺牲一部分可用性以保持一致性。这里的一致性是指线性一致性或顺序一致性，意味着系统保证一旦更新操作完成，所有后续的读操作都将返回该更新的值，形成一个全局一致的操作顺序。","categories":[],"tags":[]},{"title":"dubbo3","slug":"dubbo3","date":"2022-07-13T09:45:57.000Z","updated":"2023-07-14T06:49:15.573Z","comments":true,"path":"2022/07/13/dubbo3/","link":"","permalink":"https://wugengfeng.cn/2022/07/13/dubbo3/","excerpt":"","text":"文章参考博客：风祈的时光录 前期准备 dubbo3 源码 官方文档 源码构建 12下载源码 git clone https://github.com/apache/dubbo.git编译打包 mvn clean source:jar install -Dmaven.test.skip dubbo sample 源码 dubbo admin 安装 前提条件是需要先安装 zookeeper 123docker pull zookeeperdocker run -d -e TZ=&quot;Asia/Shanghai&quot; -p 2181:2181 --name zookeeper zookeeper 123获取docker容器：docker pull docker.io/apache/dubbo-admin:0.4.0运行：docker run -d --name dubbo-admin -p 8080:8080 -e admin.registry.address=zookeeper://ip:2181 -e admin.config-center=zookeeper://ip:2181 -e admin.metadata-report.address=zookeeper://ip:2181 docker.io/apache/dubbo-admin:0.4.0 整体设计 官方文档 框架设计 基本介绍 什么是RPC 分布式计算中，远程过程调用（英语：Remote Procedure Call，RPC）是一个计算机通信协议。该协议允许运行于一台计算机的程序调用另一个地址空间（通常为一个开放网络的一台计算机）的子程序，而程序员就像调用本地程序一样，无需额外地为这个交互作用编程（无需关注细节）。RPC是一种服务器-客户端（Client/Server）模式，经典实现是一个通过发送请求-接受回应进行信息交互的系统。 如果涉及的软件采用面向对象编程，那么远程过程调用亦可称作远程调用或远程方法调用，例：Java RMI。 RPC是一种进程间通信的模式，程序分布在不同的地址空间里。如果在同一主机里，RPC可以通过不同的虚拟地址空间（即便使用相同的物理地址）进行通讯，而在不同的主机间，则通过不同的物理地址进行交互。许多技术（通常是不兼容）都是基于这种概念而实现的 –来至自维基百科 远程方法调用和本地方法调用是相对的两个概念，本地方法调用指的是进程内部的方法调用，而远程方法调用指的是两个进程内的方法相互调用 如果实现远程方法调用，基本的就是通过网络，通过传输数据来进行调用 所以就有了： RPC over Http：基于Http协议来传输数据 PRC over Tcp：基于Tcp协议来传输数据 对于所传输的数据，可以交由RPC的双方来协商定义，但基本都会包括： 调用的是哪个类或接口 调用的是哪个方法，方法名和方法参数类型（考虑方法重载） 调用方法的入参 所以，我们其实可以看到RPC的自定义性是很高的，各个公司内部都可以实现自己的一套RPC框架，而Dubbo就是阿里所开源出来的一套RPC框架，根据上面所描述的条件很容易联想到将类名，方法名称，参数，参数列表通过网络传输到服务提供方，通过反射等方式执行服务提供方代码逻辑后将返回值原路返回，这就是整个Dubbo实现RPC调用的基本原理 什么是Dubbo 官网地址：http://dubbo.apache.org/zh/ 目前，官网上是这么介绍的：Apache Dubbo 是一款高性能、轻量级的开源 Java服务框架 在几个月前，官网的介绍是：Apache Dubbo 是一款高性能、轻量级的开源 JavaRPC框架 为什么会将RPC改为服务？ Dubbo一开始的定位就是RPC，专注于两个服务之间的调用。但随着微服务的盛行，除开服务调用之外，Dubbo也在逐步的涉猎 服务治理、服务监控、服务网关 等等，所以现在的Dubbo目标已经不止是RPC框架了，而是和Spring Cloud类似想成为了一个服务框架 dubbo RPC主要用于两个dubbo系统之间作远程调用，特别适合高并发、小数据的互联网场景 Dubbo网关参考：https://github.com/apache/dubbo-proxy（社区不是很活跃） 基本原理 相关文档 文档 服务发现 社区 应用级发现 社区 URL同一建模 值得注意的是，dubbo3服务注册粒度由接口级改为应用级 开源RPC框架对比 功能 Hessian Montan rpcx gRPC Thrift Dubbo Dubbox Spring Cloud 开发语言 跨语言 Java Go 跨语言 跨语言 Java Java Java 分布式(服务治理) × √ √ × × √ √ √ 多序列化框架支持 hessian √(支持Hessian2、Json,可扩展) √ × 只支持(protobuf) ×(thrift格式) √ √ √ 多种注册中心 × √ √ × × √ √ √ 管理中心 × √ √ × × √ √ √ 跨编程语言 √ ×(支持php client和C server) × √ √ × × × 支持REST × × × × × × √ √ 关注度 低 中 低 中 中 中 高 中 上手难度 低 低 中 中 中 低 低 中 运维成本 低 中 中 中 低 中 中 中 开源机构 Caucho Weibo Apache Google Apache Alibaba Dangdang Apache 基本与高级应用 启动时检查 使用文档 启动时检查 在消费端启动时检查依赖的服务是否可用，不可用时会抛出异常，阻止 Spring 初始化完成，默认开启 服务级别配置 1@DubboReference(check = true) 全局配置 123dubbo: consumer: check: true XML 配置参考官方文档 线程模型 使用文档 线程模型 配置 Dubbo 中的线程模型 如果事件处理的逻辑能迅速完成，并且不会发起新的 IO 请求，比如只是在内存中记个标识，则直接在 IO 线程上处理更快，因为减少了线程池调度。 但如果事件处理逻辑较慢，或者需要发起新的 IO 请求，比如需要查询数据库，则必须派发到线程池，否则 IO 线程阻塞，将导致不能接收其它请求。 如果用 IO 线程处理事件，又在事件处理过程中发起新的 IO 请求，比如在连接事件中发起登录请求，会报“可能引发死锁”异常，但不会真死锁。 XML 配置 （缺省配置） 1&lt;dubbo:protocol name=&quot;dubbo&quot; dispatcher=&quot;all&quot; threadpool=&quot;fixed&quot; threads=&quot;200&quot; /&gt; yml 配置 12345dubbo: protocol: dispatcher: all threadpool: fixed threads: 200 Dispatcher all 所有消息都派发到线程池，包括请求，响应，连接事件，断开事件，心跳等。(默认配置) direct 所有消息都不派发到线程池，全部在 IO 线程上直接执行。 message 只有请求响应消息派发到线程池，其它连接断开事件，心跳等消息，直接在 IO 线程上执行。 execution 只有请求消息派发到线程池，不含响应，响应和其它连接断开事件，心跳等消息，直接在 IO 线程上执行。 connection 在 IO 线程上，将连接断开事件放入队列，有序逐个执行，其它消息派发到线程池。 ThreadPool fixed 固定大小线程池，启动时建立线程，不关闭，一直持有。(默认配置) cached 缓存线程池，空闲一分钟自动删除，需要时重建。 limited 可伸缩线程池，但池中的线程数只会增长不会收缩。只增长不收缩的目的是为了避免收缩时突然来了大流量引起的性能问题。 eager 优先创建Worker线程池。在任务数量大于corePoolSize但是小于maximumPoolSize时，优先创建Worker来处理任务。当任务数量大于maximumPoolSize时，将任务放入阻塞队列中。阻塞队列充满时抛出RejectedExecutionException。(相比于cached:cached在任务数量超过maximumPoolSize时直接抛出异常而不是将任务放入阻塞队列) 消费端线程模型 官方介绍 直接提供者 使用文档 直连提供者 Dubbo 中点对点的直连方式 在开发及测试环境下，经常需要绕过注册中心，只测试指定服务提供者，这时候可能需要点对点直连，点对点直连方式，将以服务接口为单位，忽略注册中心的提供者列表，A 接口配置点对点，不影响 B 接口从注册中心获取列表 主要用于开发阶段本地调试 XML 1&lt;dubbo:reference id=&quot;xxxService&quot; interface=&quot;com.alibaba.xxx.XxxService&quot; url=&quot;dubbo://localhost:20890&quot; /&gt; -D 参数 1java -Dcom.alibaba.xxx.XxxService=dubbo://localhost:20890 注解 1@DubboReference(url = &quot;dubbo://localhost:20880&quot;) 只订阅 使用文档 只订阅 只订阅不注册 为方便开发测试，经常会在线下共用一个所有服务可用的注册中心，这时，如果一个正在开发中的服务提供者注册，可能会影响消费者不能正常运行 可以让服务提供者开发方，只订阅服务(开发的服务可能依赖其它服务)，而不注册正在开发的服务，通过直连测试正在开发的服务 主要用于开发阶段本地调试 XML 123&lt;dubbo:registry address=&quot;10.20.153.10:9090&quot; register=&quot;false&quot; /&gt;或者&lt;dubbo:registry address=&quot;10.20.153.10:9090?register=false&quot; /&gt; yml 1234dubbo: registry: address: zookeeper://my-server:2181 register: false 多注册中心 使用文档 多注册中心 扩展文档 注册中心扩展 在 Dubbo 中把同一个服务注册到多个注册中心上 Dubbo 支持同一服务向多注册中心同时注册，或者不同服务分别注册到不同的注册中心上去，甚至可以同时引用注册在不同注册中心上的同名服务。另外，注册中心是支持自定义扩展的 在没有配置 default=false 的注册中心都是默认的注册中心(默认值是 true)，不管服务怎么指定注册中心都一定会注册到配置 default=false 的注册中心 多注册中心 比如：中文站有些服务来不及在青岛部署，只在杭州部署，而青岛的其它应用需要引用此服务，就可以将服务同时注册到两个注册中心 注意：3.0.9的 default=false 有bug, 默认还是会向 default=false 的注册中心注册 XML 123456789101112&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:dubbo=&quot;http://dubbo.apache.org/schema/dubbo&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-4.3.xsd http://dubbo.apache.org/schema/dubbo http://dubbo.apache.org/schema/dubbo/dubbo.xsd&quot;&gt; &lt;dubbo:application name=&quot;world&quot; /&gt; &lt;!-- 多注册中心配置 --&gt; &lt;dubbo:registry id=&quot;hangzhouRegistry&quot; address=&quot;10.20.141.150:9090&quot; /&gt; &lt;dubbo:registry id=&quot;qingdaoRegistry&quot; address=&quot;10.20.141.151:9010&quot; default=&quot;false&quot; /&gt; &lt;!-- 向多个注册中心注册 --&gt; &lt;dubbo:service interface=&quot;com.alibaba.hello.api.HelloService&quot; version=&quot;1.0.0&quot; ref=&quot;helloService&quot; registry=&quot;hangzhouRegistry,qingdaoRegistry&quot; /&gt;&lt;/beans&gt; 注解 12345678# 多注册中心配置dubbo: registries: hangzhouRegistry: address: zookeeper://10.20.141.150:9090:9090 qingdaoRegistry: default: false #默认不向这个注册中心注册，除非指定 address: zookeeper://10.20.141.151:9090 1@DubboService(registry = &#123;&quot;hangzhouRegistry&quot;,&quot;qingdaoRegistry&quot;&#125;) 不同服务使用不同注册中心 比如：CRM 有些服务是专门为国际站设计的，有些服务是专门为中文站设计的 XML 1234567891011121314&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:dubbo=&quot;http://dubbo.apache.org/schema/dubbo&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-4.3.xsd http://dubbo.apache.org/schema/dubbo http://dubbo.apache.org/schema/dubbo/dubbo.xsd&quot;&gt; &lt;dubbo:application name=&quot;world&quot; /&gt; &lt;!-- 多注册中心配置 --&gt; &lt;dubbo:registry id=&quot;chinaRegistry&quot; address=&quot;10.20.141.150:9090&quot; /&gt; &lt;dubbo:registry id=&quot;intlRegistry&quot; address=&quot;10.20.154.177:9010&quot; default=&quot;false&quot; /&gt; &lt;!-- 向中文站注册中心注册 --&gt; &lt;dubbo:service interface=&quot;com.alibaba.hello.api.HelloService&quot; version=&quot;1.0.0&quot; ref=&quot;helloService&quot; registry=&quot;chinaRegistry&quot; /&gt; &lt;!-- 向国际站注册中心注册 --&gt; &lt;dubbo:service interface=&quot;com.alibaba.hello.api.DemoService&quot; version=&quot;1.0.0&quot; ref=&quot;demoService&quot; registry=&quot;intlRegistry&quot; /&gt;&lt;/beans&gt; 注解 yml 需要先配置多注册中心 12345// 这个服务会注册到 intlRegistry 和 chinaRegistry@DubboService(registry = &quot;chinaRegistry&quot;)public class HelloServiceImpl implements HelloService &#123; ...&#125; 1234@DubboService(registry = &quot;intlRegistry&quot;)public class DemoServiceImpl implements DemoService &#123; ...&#125; 多注册中心引用 比如：CRM 需同时调用中文站和国际站的 PC2 服务，PC2 在中文站和国际站均有部署，接口及版本号都一样，但连的数据库不一样 XML 1234567891011121314&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:dubbo=&quot;http://dubbo.apache.org/schema/dubbo&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-4.3.xsd http://dubbo.apache.org/schema/dubbo http://dubbo.apache.org/schema/dubbo/dubbo.xsd&quot;&gt; &lt;dubbo:application name=&quot;world&quot; /&gt; &lt;!-- 多注册中心配置 --&gt; &lt;dubbo:registry id=&quot;chinaRegistry&quot; address=&quot;10.20.141.150:9090&quot; /&gt; &lt;dubbo:registry id=&quot;intlRegistry&quot; address=&quot;10.20.154.177:9010&quot; default=&quot;false&quot; /&gt; &lt;!-- 引用中文站服务 --&gt; &lt;dubbo:reference id=&quot;chinaHelloService&quot; interface=&quot;com.alibaba.hello.api.HelloService&quot; version=&quot;1.0.0&quot; registry=&quot;chinaRegistry&quot; /&gt; &lt;!-- 引用国际站站服务 --&gt; &lt;dubbo:reference id=&quot;intlHelloService&quot; interface=&quot;com.alibaba.hello.api.HelloService&quot; version=&quot;1.0.0&quot; registry=&quot;intlRegistry&quot; /&gt;&lt;/beans&gt; 注解 12@DubboReference(registry = &quot;chinaHelloService&quot;)HelloService helloService; 如果只是测试环境临时需要连接两个不同注册中心，使用竖号分隔多个不同注册中心地址： 1234567891011&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:dubbo=&quot;http://dubbo.apache.org/schema/dubbo&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-4.3.xsd http://dubbo.apache.org/schema/dubbo http://dubbo.apache.org/schema/dubbo/dubbo.xsd&quot;&gt; &lt;dubbo:application name=&quot;world&quot; /&gt; &lt;!-- 多注册中心配置，竖号分隔表示同时连接多个不同注册中心，同一注册中心的多个集群地址用逗号分隔 --&gt; &lt;dubbo:registry address=&quot;10.20.141.150:9090|10.20.154.177:9010&quot; /&gt; &lt;!-- 引用服务 --&gt; &lt;dubbo:reference id=&quot;helloService&quot; interface=&quot;com.alibaba.hello.api.HelloService&quot; version=&quot;1.0.0&quot; /&gt;&lt;/beans&gt; 服务分组 使用文档 服务分组 当一个接口有多种实现时，可以用 group 区分 XML 服务 12&lt;dubbo:service group=&quot;feedback&quot; interface=&quot;com.xxx.IndexService&quot; /&gt;&lt;dubbo:service group=&quot;member&quot; interface=&quot;com.xxx.IndexService&quot; /&gt; 引用 12&lt;dubbo:reference id=&quot;feedbackIndexService&quot; group=&quot;feedback&quot; interface=&quot;com.xxx.IndexService&quot; /&gt;&lt;dubbo:reference id=&quot;memberIndexService&quot; group=&quot;member&quot; interface=&quot;com.xxx.IndexService&quot; /&gt; 任意组 1&lt;dubbo:reference id=&quot;barService&quot; interface=&quot;com.foo.BarService&quot; group=&quot;*&quot; /&gt; 注解 实现一个服务，服务端使用服务分组实现加法和减法两种实现，客户端根据需要引用 接口项目 123public interface GroupService &#123; int calculate(int a, int b);&#125; 服务端 1234567@DubboService(group = &quot;add&quot;)public class GroupAddServiceImpl implements GroupService &#123; @Override public int calculate(int a, int b) &#123; return a + b; &#125;&#125; 1234567@DubboService(group = &quot;sub&quot;)public class GroupSubServiceImpl implements GroupService &#123; @Override public int calculate(int a, int b) &#123; return a - b; &#125;&#125; 消费端 1234567891011121314@SpringBootTestpublic class GroupTest &#123; @DubboReference(group = &quot;sub&quot;) GroupService groupService; /* @DubboReference(group = &quot;add&quot;) GroupService groupService;*/ @Test public void test() &#123; // 根据引入不同的分组实现不同的计算逻辑 System.out.println(groupService.calculate(10, 5)); &#125;&#125; 静态服务 使用文档 静态服务 将 Dubbo 服务标识为非动态管理模式 有时候希望人工管理服务提供者的上线和下线，此时需将注册中心标识为非动态管理模式，模块的上下线 XML 1&lt;dubbo:registry address=&quot;10.20.141.150:9090&quot; dynamic=&quot;false&quot; /&gt; 或者 1&lt;dubbo:registry address=&quot;10.20.141.150:9090?dynamic=false&quot; /&gt; 注解 1@DubboService(dynamic = false) 多版本 使用文档 多版本 在 Dubbo 中为同一个服务配置多个版本 当一个接口实现，出现不兼容升级时，可以用版本号过渡，版本号不同的服务相互间不引用 可以按照以下的步骤进行版本迁移： 在低压力时间段，先升级一半提供者为新版本 再将所有消费者升级为新版本 然后将剩下的一半提供者升级为新版本 老版本服务提供者配置 1234567@DubboService(version = &quot;1.0.0&quot;)public class VersionServerImpl implements VersionServer &#123; @Override public String version() &#123; return &quot;1.0.0&quot;; &#125;&#125; 新版本服务提供者配置 1234567@DubboService(version = &quot;2.0.0&quot;)public class VersionServerImpl implements VersionServer &#123; @Override public String version() &#123; return &quot;2.0.0&quot;; &#125;&#125; 老版本服务消费者配置 12@DubboReference(version = &quot;1.0.0&quot;)VersionServer versionServer; 新版本服务消费者配置 12@DubboReference(version = &quot;2.0.0&quot;)VersionServer versionServer; 如果不需要区分版本，可以按照以下的方式配置 12@DubboReference(version = &quot;*&quot;)VersionServer versionServer; 分组聚合 使用文档 分组聚合 实现原理 通过分组对结果进行聚合并返回聚合后的结果 分组合并可以指定 服务级别 方法级别 指定方法合并 指定方法不合并，需要合并的方法返回值必须是个 集合 通过分组对结果进行聚合并返回聚合后的结果，比如菜单服务，用group区分同一接口的多种实现，现在消费方需从每种group中调用一次并返回结果，对结果进行合并之后返回，这样就可以实现聚合菜单项 接口项目 1234567public interface GroupMergerService &#123; // A模块菜单 List&lt;String&gt; menuA(); // B模块菜单 String menuB();&#125; 服务端 123456789101112131415@DubboService(group = &quot;user&quot;)public class GroupMergerServiceImpl implements GroupMergerService &#123; @Override public List&lt;String&gt; menuA() &#123; List&lt;String&gt; menus = new ArrayList&lt;&gt;(); menus.add(&quot;user.1&quot;); menus.add(&quot;user.2&quot;); return menus; &#125; @Override public String menuB() &#123; return &quot;user&quot;; &#125;&#125; 123456789101112131415@DubboService(group = &quot;order&quot;)public class GroupMergerServiceImpl2 implements GroupMergerService &#123; @Override public List&lt;String&gt; menuA() &#123; List&lt;String&gt; menus = new ArrayList&lt;&gt;(); menus.add(&quot;order.1&quot;); menus.add(&quot;order.2&quot;); return menus; &#125; @Override public String menuB() &#123; return &quot;order&quot;; &#125;&#125; 消费端 1234567891011121314151617181920@Slf4j@SpringBootTestpublic class GroupMergerTest &#123; @DubboReference(group = &quot;*&quot;, merger = &quot;true&quot;) GroupMergerService groupMergerService; @Test public void test() &#123; // user.1 // user.2 // order.1 // order.2 List&lt;String&gt; menuList = groupMergerService.menuA(); menuList.forEach(line -&gt; log.info(line)); // 报错 There is no merger to merge result (返回值需要集合) String s = groupMergerService.menuB(); System.out.println(s); &#125;&#125; 参数验证 使用文档 参数验证 扩展文档 调用拦截扩展 扩展文档 验证扩展 在 Dubbo 中进行参数验证 参数验证功能是基于 JSR303 实现的，用户只需标识 JSR303 标准的验证 annotation，并通过声明 filter 来实现验证, 验证逻辑建议放在消费端，这能能够在验证异常后不进行RPC调用 接口项目 添加 maven 依赖 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-validation&lt;/artifactId&gt;&lt;/dependency&gt; 通用返回实体 1234567@Data@Accessors(chain = true)public class R&lt;T&gt; implements Serializable &#123; private Integer code; private T data; private String message;&#125; 实体对象 12345678910@Data@Accessors(chain = true)public class User implements Serializable &#123; @Min(value = 0, message = &quot;最小值要大于0&quot;) @Max(value = 1000, message = &quot;最大值不能超过1000&quot;) private Integer id; @NotNull(message = &quot;name 不能为空&quot;) private String name;&#125; 服务接口 12345public interface ValidationService &#123; R&lt;Void&gt; addUser(User user); R&lt;Void&gt; select(@NotNull(message = &quot;id 不能为空&quot;) Integer id);&#125; 服务端 123456789101112@DubboServicepublic class ValidationServiceImpl implements ValidationService &#123; @Override public R&lt;Void&gt; addUser(User user) &#123; return new R().setCode(200); &#125; @Override public R&lt;Void&gt; select(Integer id) &#123; return new R().setCode(200); &#125;&#125; 消费端 验证扩展 参数校验器 1234567891011121314/** * @author: ken 😃 * @date: 2022-07-22 * @description: 参数校验器 * * 首先编写自定义的校验器。实现Validation接口。再编写自己的校验者 **/public class ParamValidation extends JValidation &#123; @Override public Validator getValidator(URL url) &#123; // 自定义校验者 return new ParamValidator(url); &#125;&#125; 参数校验者 12345678910111213141516171819/** * @author: ken 😃 * @date: 2022-07-22 * @description: 参数校验者 拷贝 org.apache.dubbo.validation.support.jvalidation.JValidator **/public class ParamValidator implements Validator &#123; public void validate(String methodName, Class&lt;?&gt;[] parameterTypes, Object[] arguments) throws Exception &#123; ... &#125; catch (Exception e) &#123; // 修改点 // 如果是 ConstraintViolationException 直接返回，3.0.9会重新封装成 ValidationException if (e instanceof ConstraintViolationException) &#123; throw e; &#125; throw new ValidationException(e.getMessage()); &#125; &#125;&#125; 扩展验证拦截器 1234567891011121314151617181920212223242526272829303132333435363738/** * @author: ken 😃 * @date: 2022-07-22 * @description: 参考 org.apache.dubbo.validation.filter.ValidationFilter **/@Activate(group = &#123;CONSUMER, PROVIDER&#125;, value = VALIDATION_KEY, order = 10000)public class DubboValidationFilter implements Filter &#123; private Validation validation; public void setValidation(Validation validation) &#123; this.validation = validation; &#125; @Override public Result invoke(Invoker&lt;?&gt; invoker, Invocation invocation) throws RpcException &#123; if (validation != null &amp;&amp; !invocation.getMethodName().startsWith(&quot;$&quot;) &amp;&amp; ConfigUtils.isNotEmpty(invoker.getUrl().getMethodParameter(invocation.getMethodName(), VALIDATION_KEY))) &#123; try &#123; Validator validator = validation.getValidator(invoker.getUrl()); if (validator != null) &#123; validator.validate(invocation.getMethodName(), invocation.getParameterTypes(), invocation.getArguments()); &#125; &#125; catch (ConstraintViolationException e) &#123; // 处理参数校验异常 返回通用 VO ConstraintViolation&lt;?&gt; constraintViolation = e.getConstraintViolations().stream().findFirst().get(); R result = new R() .setCode(1000) .setMessage(constraintViolation.getMessage()); return AsyncRpcResult.newDefaultAsyncResult(result, invocation); &#125; catch (RpcException e) &#123; throw e; &#125; catch (Throwable t) &#123; return AsyncRpcResult.newDefaultAsyncResult(t, invocation); &#125; &#125; return invoker.invoke(invocation); &#125;&#125; 添加SPI配置 Filter 1234|-resources |-META-INF |-dubbo |-org.apache.dubbo.rpc.Filter org.apache.dubbo.rpc.Filter 1dubboValidation=com.wgf.springboot.filter.DubboValidationFilter Validation 1234|-resources |-META-INF |-dubbo |-org.apache.dubbo.validation.Validation org.apache.dubbo.validation.Validation 1paramValidation=com.wgf.springboot.validation.ParamValidation 修改 yml 配置 1234dubbo: consumer: validation: &#x27;paramValidation&#x27; # 启用自定义验证 filter: &#x27;dubboValidation, -validation&#x27; # 启用自定义参数校验拦截器，禁用默认的参数校验拦截器 测试 12345678910111213141516171819@SpringBootTestpublic class ValidationTest &#123; @DubboReference ValidationService validationService; @Test public void addTest() &#123; User user = new User() .setId(10); R&lt;Void&gt; r = validationService.addUser(user); Assertions.assertEquals(r.getCode(), 200); &#125; @Test public void selectTest() &#123; R&lt;Void&gt; r = validationService.select(null); Assertions.assertEquals(r.getCode(), 200); &#125;&#125; 3.0.9 版本如果需要实现验证异常后返回自定义的VO, 需要扩展 JValidation 和 JValidator, 因为 ConstraintViolationException 异常被 JValidator 捕获重新抛出 ValidationException , 并且需要重写 （取消默认实现用 -）ValidationFilter, 验证异常后返回自定义的VO 负载均衡 相关文档 使用文档 负载均衡 源码文档 负载均衡 参考文档 负载扩展 LoadBalance 中文意思为负载均衡，它的职责是将网络请求，或者其他形式的负载“均摊”到不同的机器上。避免集群中部分服务器压力过大，而另一些服务器比较空闲的情况。通过负载均衡，可以让每台服务器获取到适合自己处理能力的负载。在为高负载服务器分流的同时，还可以避免资源浪费，一举两得 Random LoadBalance 随机，按权重设置随机概率。 在一个截面上碰撞的概率高，但调用量越大分布越均匀，而且按概率使用权重后也比较均匀，有利于动态调整提供者权重。 RoundRobin LoadBalance 轮询，按公约后的权重设置轮询比率。 存在慢的提供者累积请求的问题，比如：第二台机器很慢，但没挂，当请求调到第二台时就卡在那，久而久之，所有请求都卡在调到第二台上。 LeastActive LoadBalance 最少活跃调用数，相同活跃数的随机，活跃数指调用前后计数差。 使慢的提供者收到更少请求，因为越慢的提供者的调用前后计数差会越大 最少活跃数应该是在服务提供者端进行统计的，服务提供者统计有多少个请求正在执行中。但在Dubbo中，就是不讲道理，它是在消费端进行统计的，为什么能在消费端进行统计？ 每个消费者都会从注册中心（常用的是Zookeeper）缓存所调用服务的所有提供者信息到本地，比如记为p1、p2、p3三个服务提供者，每个提供者内都有一个属性记为active，默认位0 消费者在调用服务时，如果负载均衡策略是leastactive 消费者端会判断缓存的所有服务提供者的active，选择最小的，如果都相同，则随机 选出某一个服务提供者后，假设是p2，Dubbo就会对p2.active+1 然后真正发出请求调用该服务 消费端收到响应结果后，对p2.active-1 这样就完成了对某个服务提供者当前活跃调用数进行了统计，并且并不影响服务调用的性能 如果由服务提供者来统计调用数反而不好统计，因为服务提供者有多个，你无法确定是哪个服务提供者统计调用数，除非你放到zookeeper这种分布式共享的数据中心，但是这样的话，每个消费者都要请求zookeeper找到需要调用的那一台服务提供者机器然后加1，在调用结束后，还要在zookeeper上进行减1操作，zookeeper明显扛不住。 由服务消费者来统计调用数的话，虽然每个消费者都有自己的一套调用数数据，调用数数据可能不一样，但是经过长时间的调用后，每个消费者自己本地存的调用数据还是能够有差不多的趋势（这里的趋势不是指数据相等）。比如说p2响应很慢，堆积了很多请求，那么每个消费者在请求多次后，短时间内都不会再请求p2 总结：如果由服务提供者来统计调用次数，那么就需要共享每个服务接口的调用次数，不管是使用那种共享中间件都会有额外的开销。最节省性能的算法是在服务消费者端的jvm内存维护一份本地的服务调用次数表，再选择空闲服务调用，这样可以避免数据共享的复杂性 ConsistentHash LoadBalance 一致性 Hash，相同参数的请求总是发到同一提供者。 当某一台提供者挂时，原本发往该提供者的请求，基于虚拟节点，平摊到其它提供者，不会引起剧烈变动。 算法参见：http://en.wikipedia.org/wiki/Consistent_hashing 缺省只对第一个参数 Hash，如果要修改，请配置 &lt;dubbo:parameter key=&quot;hash.arguments&quot; value=&quot;0,1&quot; /&gt; 缺省用 160 份虚拟节点，如果要修改，请配置 &lt;dubbo:parameter key=&quot;hash.nodes&quot; value=&quot;320&quot; /&gt; 配置 服务端 XML 服务级别 1&lt;dubbo:service interface=&quot;...&quot; loadbalance=&quot;roundrobin&quot; /&gt; 方法级别 123&lt;dubbo:service interface=&quot;...&quot;&gt; &lt;dubbo:method name=&quot;hello&quot; loadbalance=&quot;roundrobin&quot;/&gt;&lt;/dubbo:service&gt; 注解 服务级别 1@DubboService(loadbalance = &quot;roundrobin&quot;) 方法级别 123@DubboService(methods = &#123; @Method(name = &quot;...&quot;, loadbalance = &quot;roundrobin&quot;)&#125;) 客户端 XML 服务级别 1&lt;dubbo:reference interface=&quot;...&quot; loadbalance=&quot;roundrobin&quot; /&gt; 方法级别 123&lt;dubbo:reference interface=&quot;...&quot;&gt; &lt;dubbo:method name=&quot;...&quot; loadbalance=&quot;roundrobin&quot;/&gt;&lt;/dubbo:reference&gt; 注解 服务级别 1@DubboReference(loadbalance = &quot;roundrobin&quot;) 方法级别 123@DubboReference(methods = &#123; @Method(name = &quot;...&quot;, loadbalance = &quot;roundrobin&quot;)&#125;) yml 123dubbo: provider: loadbalance: roundrobin 官方已提供扩展 org.apache.dubbo.rpc.cluster.loadbalance.RandomLoadBalance org.apache.dubbo.rpc.cluster.loadbalance.RoundRobinLoadBalance org.apache.dubbo.rpc.cluster.loadbalance.LeastActiveLoadBalance org.apache.dubbo.rpc.cluster.loadbalance.ConsistentHashLoadBalance 可取值： random roundrobin leastactive consistenthash 在集群负载均衡时，Dubbo 提供了多种均衡策略，缺省为 random 随机调用 如果在消费端和服务端都配置了负载均衡策略，以消费端为准 如果在服务级别和方法级别都配置了负载均衡策略，以方法级别为准，遵循配置就近原则 服务超时 默认超时时间：1000ms 超时会触发重试机制：默认重试两次，不包含第一次 在服务提供者和服务消费者上都可以配置服务超时时间，这两者是不一样的 服务端 服务级别 xml &lt;dubbo:service interface=“…” timeout=“4000” /&gt; 方法级别 xml &lt;dubbo:service interface=“…”&gt; &lt;dubbo:method name=“…” loadbalance=“roundrobin”/&gt;&lt;/dubbo:reference&gt; 服务级别 注解 @DubboService(timeout = 3000) 方法级别 注解 @DubboService(methods = { @Method(name = “methodTimeout”, timeout = 5000) }) 消费端 服务级别 xml &lt;dubbo:reference interface=“…” timeout=“4000” /&gt; 方法级别 xml &lt;dubbo:reference interface=“…”&gt; &lt;dubbo:method name=“…” loadbalance=“roundrobin”/&gt;&lt;/dubbo:reference&gt; 服务级别 注解 @DubboReference(timeout = 3000) 方法级别 注解 @DubboReference(methods = { @Method(name = “xxx”, timeout = 5000) }) 全局配置 xml &lt;dubbo:provider timeout=“5000”/&gt; yml 12345678910# 按服务配置dubbo: application: xxx: timeout: 3000 # 全局配置dubbo: provider: timeout: 3000 消费者调用一个服务，分为三步： sequenceDiagram 消费端 ->> 服务端 : 1. 消费者发送请求（网络传输） 服务端 ->> 服务端 : 2. 执行逻辑 服务端 ->> 消费端 : 3. 服务端返回响应（网络请求） 如果在服务端和消费端只在其中一方配置了 timeout，那么没有歧义，表示消费端调用服务的超时时间，消费端如果超过时间还没有收到响应结果，则消费端会抛超时异常。但服务端不会抛异常，服务端在执行服务后，会检查执行该服务的时间，如果超过timeout，则会打印一个超时日志。服务会正常的执行完 如果在服务端和消费端各配了一个timeout，那就比较复杂了，假设 服务执行为2s 消费端timeout=1s 服务端timeout=3s 123456789// 服务端@DubboService(timeout = 3000)public class TimeOutServiceImpl implements TimeOutService &#123;...&#125;// 消费端@DubboReference(timeout = 1000)TimeOutService timeOutService; 那么消费端调用服务时，消费端会收到超时异常（因为消费端超时了），服务端一切正常（服务端没有超时），实际服务端正常执行业务流程 无论何种情况，服务端的timeout配置的作用是：如果服务执行时间超过这个timeout，仅仅只是打印一个超时日志 provider超时打断 Dubbo provider执行超时释放执行线程 支持provider根据超时时间进行业务打断 适用场景：对于一个provider，如果某个操作执行超时，则打断(释放)该执行线程，而不是仅仅打印超时日志 提示 支持版本：2.7.12 之后版本，3.0.x 暂不支持 核心实现类 org.apache.dubbo.remoting.transport.dispatcher.all2.AllChannelHandler2 接口项目 123public interface TimeoutReleaseService &#123; void hello();&#125; 服务端 yml配置 123dubbo: protocol: dispatcher: all2 1234567891011121314151617@Slf4j@DubboService(timeout = 1000, retries = 0)public class TimeoutReleaseServiceImpl implements TimeoutReleaseService &#123; @Override public void hello() &#123; try &#123; for (int i = 0; i &lt; 10; i++) &#123; TimeUnit.MILLISECONDS.sleep(200); log.info(i + &quot;&quot;); &#125; &#125;catch (Exception e) &#123; log.error(&quot;线程被打断&quot;); &#125; &#125;&#125; 消费端 12345678910@SpringBootTestpublic class TimeoutReleaseTest &#123; @DubboReference TimeoutReleaseService timeoutReleaseService; @Test public void test() &#123; timeoutReleaseService.hello(); &#125;&#125; 运行结果，服务端超时 1234562022-07-23 16:21:56.977 INFO 3344 --- [:20880-thread-2] c.w.s.p.TimeoutReleaseServiceImpl : 02022-07-23 16:21:57.180 INFO 3344 --- [:20880-thread-2] c.w.s.p.TimeoutReleaseServiceImpl : 12022-07-23 16:21:57.383 INFO 3344 --- [:20880-thread-2] c.w.s.p.TimeoutReleaseServiceImpl : 22022-07-23 16:21:57.586 INFO 3344 --- [:20880-thread-2] c.w.s.p.TimeoutReleaseServiceImpl : 32022-07-23 16:21:57.790 INFO 3344 --- [:20880-thread-2] c.w.s.p.TimeoutReleaseServiceImpl : 42022-07-23 16:21:57.868 ERROR 3344 --- [:20880-thread-2] c.w.s.p.TimeoutReleaseServiceImpl : 线程被打断 服务重试 使用文档 重试次数配置 超时和重试一般情况下是成对配置的 Dubbo 服务在尝试调用一次之后，如出现非业务异常(服务突然不可用、超时等)，Dubbo 默认会进行额外的最多2次重试 服务端 目前 3.0.9 版本使用注解 @DubboService(timeout = 3000, retries = 1) 重试次数不生效，只有客户端配置才生效，原因是服务端注册时注册URL没有加上注解的重试配置 服务级别 xml &lt;dubbo:service interface=“…” timeout=“4000” retries=“2”/&gt; 方法级别 xml &lt;dubbo:service interface=“…”&gt; &lt;dubbo:method name=&quot;…&quot;timeout=“4000” retries=“2”/&gt;&lt;/dubbo:service&gt; 服务级别 注解 @DubboService(timeout = 3000, retries = 2) 3.0.9 不生效 方法级别 注解 @DubboService(methods = { @Method(name = “globalTimeout”, timeout = 3000, retries = 0) }) 3.0.9 不生效 客户端 服务级别 xml &lt;dubbo:reference interface=“…” timeout=“4000” retries=“2”/&gt; 方法级别 xml &lt;dubbo:reference interface=“…”&gt; &lt;dubbo:method name=&quot;…&quot;timeout=“4000” retries=“2”/&gt;&lt;/dubbo:reference&gt; 服务级别 注解 @DubboReference(timeout = 2000, retries = 2) 方法级别 注解 @DubboReference(methods = { @Method(name = “…”, timeout = 2000, retries = 2)} ) yml 1234# 客户端全局配置dubbo: consumer: retries: 0 集群容错 使用文档 集群容错 扩展文档 集群扩展 目前 3.0.9 版本在服务端配置不生效，消费端正常 集群容错表示：服务消费者在调用某个服务时，这个服务有多个服务提供者，在经过负载均衡后选出其中一个服务提供者之后进行调用，但调用报错后，Dubbo所采取的后续处理策略。 集群容错模式 Failover Cluster (默认) 取值：failover 失败自动切换，当出现失败，重试其它服务器。通常用于读操作，但重试会带来更长延迟。可通过 retries=&quot;2&quot; 来设置重试次数(不含第一次) Failfast Cluster 取值：failfast 快速失败，只发起一次调用，失败立即报错。通常用于非幂等性的写操作，比如新增记录 Failsafe Cluster 取值：failsafe 失败安全，出现异常时，直接忽略。通常用于写入审计日志等操作 Failback Cluster 取值：failback 失败自动恢复，后台记录失败请求，定时重发。通常用于消息通知操作 Forking Cluster 取值：forking 并行调用多个服务器，只要一个成功即返回。通常用于实时性要求较高的读操作，但需要浪费更多服务资源。可通过 forks=&quot;2&quot; 来设置最大并行数 Broadcast Cluster 取值：broadcast 广播调用所有提供者，逐个调用，任意一台报错则报错。通常用于通知所有提供者更新缓存或日志等本地资源信息 现在广播调用中，可以通过 broadcast.fail.percent 配置节点调用失败的比例，当达到这个比例后，BroadcastClusterInvoker 将不再调用其他节点，直接抛出异常。 broadcast.fail.percent 取值在 0～100 范围内。默认情况下当全部调用失败后，才会抛出异常。 broadcast.fail.percent 只是控制的当失败后是否继续调用其他节点，并不改变结果(任意一台报错则报错)。broadcast.fail.percent 参数 在 dubbo2.7.10 及以上版本生效 Broadcast Cluster 配置 broadcast.fail.percent broadcast.fail.percent=20 代表了当 20% 的节点调用失败就抛出异常，不再调用其他节点 1@DubboReference(cluster = &quot;broadcast&quot;, parameters = &#123;&quot;broadcast.fail.percent&quot;, &quot;20&quot;&#125;) yml 1234# 客户端全局配置dubbo: consumer: cluster: failfast 客户端的服务级别配置 1@DubboReference(cluster = &quot;forking&quot;) 服务降级 使用文档 服务降级 更复杂的服务降级参考 本地伪装 服务降级表示：服务消费者在调用某个服务提供者时，如果该服务提供者报错了，所采取的备选措施 集群容错和服务降级的区别在于： 集群容错是整个集群范围内的容错 服务降级是单个服务提供者的自身容错 用法 mock=force:return+null 表示消费方对该服务的方法调用都直接返回 null 值，不发起远程调用。用来屏蔽不重要服务不可用时对调用方的影响 mock=fail:return+null 表示消费方对该服务的方法调用在失败后，再返回 null 值，不抛异常。用来容忍不重要服务不稳定时对调用方的影响 1@DubboReference(mock = &quot;force:return 123&quot;) 不发起远程调用，直接返回 mork 的值 1@DubboReference(timeout = 1000, mock = &quot;fail: return 123&quot;) 尝试远程调用，如果超时则返回 mork 的值 注意 如果服务调用超时会返回 mork 的值，但是服务端默认会被调用三次，因为默认的 集群容错 是 failover 如果服务端抛出异常会将异常传递到消费端，而不是返回 mork 的值 指定IP 使用文档 指定IP 提示 支持版本：2.7.12 之后, 3.0.x 暂不支持 对于Provider集群中注册的多个实例，指定Ip:Port进行调用 当多个Provider注册到注册中心时，可以通过在RpcContext中动态的指定其中一个实例的Ip，Port进行Dubbo调用 接口项目 123public interface SpecifiedIpService &#123; String hello();&#125; 服务 1 ip 192.168.1.106 port 20880 1234567@DubboServicepublic class SpecifiedIpServiceImpl implements SpecifiedIpService &#123; @Override public String hello() &#123; return &quot;i am provider&quot;; &#125;&#125; 服务 2 ip 192.168.1.106 port 20881 1234567@DubboServicepublic class SpecifiedIpServiceImpl implements SpecifiedIpService &#123; @Override public String hello() &#123; return &quot;i am provider2&quot;; &#125;&#125; 消费端 123456789101112131415161718192021@Slf4j@SpringBootTestpublic class SpecifiedIpTest &#123; @DubboReference(parameters = &#123;&quot;router&quot;,&quot;address&quot;&#125;) SpecifiedIpService specifiedIpService; @Test public void test() &#123; // 根据provider的ip,port创建Address实例 Address address = new Address(&quot;192.168.1.106&quot;, 20880); RpcContext.getContext().setObjectAttachment(&quot;address&quot;, address); log.info(specifiedIpService.hello()); address = new Address(&quot;192.168.1.106&quot;, 20881); RpcContext.getContext().setObjectAttachment(&quot;address&quot;, address); log.info(specifiedIpService.hello()); &#125;&#125;2022-07-23 16:46:14.352 INFO 3988 --- [ main] com.wgf.springboot.SpecifiedIpTest : i am provider2022-07-23 16:46:14.431 INFO 3988 --- [ main] com.wgf.springboot.SpecifiedIpTest : i am provider2 收集广播响应 Dubbo broadcast2 广播模式收集所有服务提供者的接口响应 适用场景：对于一个dubbo消费者，广播调用多个dubbo 提供者，该消费者可以收集所有服务提供者的响应结果 提示 支持版本：2.7.12 后，3.0.x 无效 接口项目 123public interface BroadcastService &#123; String hello();&#125; 服务端 1 1234567@DubboServicepublic class BroadcastServiceImpl implements BroadcastService &#123; @Override public String hello() &#123; return &quot;hello&quot;; &#125;&#125; 服务端 2 12345678@DubboServicepublic class BroadcastServiceImpl implements BroadcastService &#123; @Override public String hello() &#123; int a = 10 / 0; return &quot;hello&quot;; &#125;&#125; 消费端 1234567891011121314151617181920@Slf4j@SpringBootTestpublic class BroadcastTest &#123; @DubboReference(cluster = &quot;broadcast2&quot;) BroadcastService broadcastService; @Test public void test() &#123; try&#123; broadcastService.hello(); &#125;catch (Exception e)&#123; Map&lt;String, String&gt; m = RpcContext.getServerContext().getAttachments(); log.info(m.toString()+&quot;|&quot;+&quot;fail&quot;); &#125; Map&lt;String, String&gt; m = RpcContext.getServerContext().getAttachments(); log.info(m.toString()+&quot;|&quot;+&quot;success&quot;); &#125;&#125;2022-07-23 23:19:38.571 INFO 712 --- [ main] com.wgf.springboot.BroadcastTest : &#123;broadcast.results=[&#123;&quot;ip&quot;:&quot;192.168.1.106&quot;,&quot;port&quot;:20880,&quot;exceptionMsg&quot;:&quot;/ by zero&quot;&#125;,&#123;&quot;ip&quot;:&quot;192.168.1.106&quot;,&quot;port&quot;:20881,&quot;exceptionMsg&quot;:&quot;/ by zero&quot;&#125;]&#125;|success 结果缓存 Dubbo broadcast2 广播模式收集所有服务提供者的接口响应 结果缓存，用于加速热门数据的访问速度，Dubbo 提供声明式缓存，以减少用户加缓存的工作量 提示 2.1.0 以上版本支持 支持 服务级别 方法级别 缓存类型 lru 基于最近最少使用原则删除多余缓存，保持最热的数据被缓存 threadlocal 当前线程缓存，比如一个页面渲染，用到很多 portal，每个 portal 都要去查用户信息，通过线程缓存，可以减少这种多余访问 jcache 与 JSR107 集成，可以桥接各种缓存实现 接口项目 123public interface ResultCacheService &#123; int increment();&#125; 服务端 123456789@DubboServicepublic class ResultCacheServiceImpl implements ResultCacheService &#123; private AtomicInteger seq = new AtomicInteger(0); @Override public int increment() &#123; return seq.incrementAndGet(); &#125;&#125; 消费端 12345678910111213141516171819202122@Slf4j@SpringBootTestpublic class ResultCacheTest &#123; @DubboReference(cache = &quot;lru&quot;) ResultCacheService resultCacheService; @Test public void test() &#123; IntStream.range(0, 10).forEach(line -&gt; log.info(resultCacheService.increment() + &quot;&quot;)); &#125;&#125;INFO 11740 --- [ main] com.wgf.springboot.ResultCacheTest : 1INFO 11740 --- [ main] com.wgf.springboot.ResultCacheTest : 2INFO 11740 --- [ main] com.wgf.springboot.ResultCacheTest : 2INFO 11740 --- [ main] com.wgf.springboot.ResultCacheTest : 2INFO 11740 --- [ main] com.wgf.springboot.ResultCacheTest : 2INFO 11740 --- [ main] com.wgf.springboot.ResultCacheTest : 2INFO 11740 --- [ main] com.wgf.springboot.ResultCacheTest : 2INFO 11740 --- [ main] com.wgf.springboot.ResultCacheTest : 2INFO 11740 --- [ main] com.wgf.springboot.ResultCacheTest : 2INFO 11740 --- [ main] com.wgf.springboot.ResultCacheTest : 2 事件通知 使用文档 事件通知 在调用之前、调用之后、出现异常时的事件通知 在调用之前、调用之后、出现异常时，会触发 oninvoke、onreturn、onthrow 三个事件，可以配置当事件发生时，通知哪个类的哪个方法 提示 支持版本：2.0.7 之后 接口项目 123public interface EventsNotifyService &#123; String hello(String name);&#125; 服务端 1234567891011@DubboServicepublic class EventsNotifyServiceImpl implements EventsNotifyService &#123; @Override public String hello(String name) &#123; if (&quot;test&quot;.equals(name)) &#123; throw new RuntimeException(&quot;调用异常&quot;); &#125; return &quot;hello !!&quot;; &#125;&#125; 消费端 定义事件通知接口 12345public interface EventsNotify &#123; void onReturn(String result, String name); void onThrow(Throwable ex, String name);&#125; 实现事件通知接口并注册到Spring 容器 123456789101112131415@Slf4j@Service(&quot;eventsNotify&quot;)public class EventsNotifyImpl implements EventsNotify &#123; @Override public void onReturn(String result, String name) &#123; log.info(&quot;result : &#123;&#125;&quot;, result); log.info(&quot;name : &#123;&#125;&quot;, name); &#125; @Override public void onThrow(Throwable ex, String name) &#123; log.error(ex.getMessage()); log.info(&quot;name : &#123;&#125;&quot;, name); &#125;&#125; 使用 123456789101112131415161718192021@SpringBootTestpublic class EventsNotifyTest &#123; @DubboReference(methods = &#123; @Method(name = &quot;hello&quot;, async = true, onreturn = &quot;eventsNotify.onReturn&quot;, onthrow = &quot;eventsNotify.onThrow&quot; ) &#125;) EventsNotifyService eventsNotifyService; @Test public void test() &#123; eventsNotifyService.hello(&quot;wgf&quot;); eventsNotifyService.hello(&quot;test&quot;); &#125;&#125; //INFO 16480 --- [andler-thread-1] c.w.springboot.notify.EventsNotifyImpl : result : hello !! //INFO 16480 --- [andler-thread-1] c.w.springboot.notify.EventsNotifyImpl : name : wgf //ERROR 16480 --- [andler-thread-1] c.w.springboot.notify.EventsNotifyImpl : 调用异常 //INFO 16480 --- [andler-thread-1] c.w.springboot.notify.EventsNotifyImpl : name : test 本地伪装 使用文档 本地伪装 注意：服务降级 和 本地伪装 只能处理Dubbo框架自身的 RpcException，业务异常熔断需要集成 其他中间件 Mock 是 Stub 的一个子集，便于服务提供方在客户端执行容错逻辑 本地伪装就是Mock，Dubbo中Mock的功能相对于本地存根更简单一点，Mock其实就是Dubbo中的 服务降级，不同的名词罢了 接口项目 123public interface LocalMockService &#123; String hello();&#125; mock实现 123456public class LocalMockServiceMock implements LocalMockService &#123; @Override public String hello() &#123; return &quot;服务降级数据&quot;; &#125;&#125; 服务端 1234567891011121314@DubboServicepublic class LocalMockServiceImpl implements LocalMockService &#123; @Override public String hello() &#123; try &#123; TimeUnit.SECONDS.sleep(1100); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; return &quot;hello&quot;; &#125;&#125; 消费端 123456789101112@Slf4j@SpringBootTestpublic class LocalMockTest &#123; // 本地伪装只能处理Dubbo框架自身调用异常，无法处理业务异常 @DubboReference(mock = &quot;com.wgf.springboot.connector.mock.LocalMockServiceMock&quot;, timeout = 1000) LocalMockService localMockService; @Test public void test() &#123; log.info(localMockService.hello()); &#125;&#125; 本地存根 使用文档 本地存根 在 Dubbo 中利用本地存根在客户端执行部分逻辑 本地存根，名字很抽象，但实际上不难理解，本地存根就是一段逻辑，这段逻辑是在服务消费端执行的，这段逻辑一般都是由服务提供者提供，服务提供者可以利用这种机制在服务消费者远程调用服务提供者之前或之后再做一些其他事情，比如结果缓存，请求参数验证等等 接口项目 123public interface LocalStubService &#123; String hello();&#125; 同包下本地存根实现 123456789101112131415161718192021222324/** * @author: ken 😃 * @date: 2022-07-19 * @description: 本地存根实现 * 本地存根其实就是使用装饰者模式在本地增强一些功能 **/public class LocalStubServiceStub implements LocalStubService &#123; // 真正的远端调用对象 private LocalStubService localStubService; public LocalStubServiceStub(LocalStubService localStubService) &#123; this.localStubService = localStubService; &#125; @Override public String hello() &#123; try &#123; String hello = this.localStubService.hello(); return &quot;hi &quot; + hello; &#125; catch (Exception e) &#123; return &quot;&quot;; &#125; &#125;&#125; 服务端 1234567@DubboServicepublic class LocalStubServiceImpl implements LocalStubService &#123; @Override public String hello() &#123; return &quot;wgf&quot;; &#125;&#125; 消费端 123456789101112@Slf4j@SpringBootTestpublic class LocalStubTest &#123; // stub=true的是个默认配置,默认用接口的全限定类名+Stub去调用 @DubboReference(stub = &quot;true&quot;) LocalStubService localStubService; @Test public void test() &#123; log.info(localStubService.hello()); &#125;&#125; stub=true的是个默认配置，默认用接口的全限定类名+Stub去调用 也可以用 stub=权限定类名 进行调用 stub实现必须传入一个远程的调用对象，也就是暴露的服务接口 stub实现必须要有一个传入远程调用对象的构造参数 说白了就是使用装饰者模式在消费端本地进行功能增强 延迟暴露 使用文档 延迟暴露 延迟暴露 Dubbo 服务 如果你的服务需要预热时间，比如初始化缓存，等待相关资源就位等，可以使用 delay 进行延迟暴露。我们在 Dubbo 2.6.5 版本中对服务延迟暴露逻辑进行了细微的调整，将需要延迟暴露（delay &gt; 0）服务的倒计时动作推迟到了 Spring 初始化完成后进行。你在使用 Dubbo 的过程中，并不会感知到此变化，因此请放心使用 如果依赖服务开启启动时检查则如果服务同时重启依赖服务可能会启动失败 1234567@DubboService(delay = 60000)public class DelayPublishServiceImpl implements DelayPublishService &#123; @Override public String hello() &#123; return &quot;hello&quot;; &#125;&#125; 延迟连接 使用文档 延迟连接 在 Dubbo 中配置延迟连接 提示 该配置只对使用长连接的 dubbo 协议生效 延迟连接用于减少长连接数。当有调用发起时，再创建长连接 123// 默认值就是 false@DubboReference(lazy = false)xxxService xxxService; 参数回调 使用文档 参数回调 参数回调方式与调用本地 callback 或 listener 相同，只需要在 Spring 的配置文件中声明哪个参数是 callback 类型即可。Dubbo 将基于长连接生成反向代理，这样就可以从服务器端调用客户端逻辑 接口项目 12345678/** * @author: ken.😊 * @create: 2022-07-19 22:28 * @description: 定义回调参数接口 **/public interface CallbackListener &#123; String getFirstName();&#125; 123456789public interface CallbackService &#123; /** * 第二个参数是回调参数 * @param lastName * @param callbackListener * @return */ String hello(String lastName, CallbackListener callbackListener);&#125; 服务端 12345678910111213141516171819202122/** * @author: ken.😊 * @create: 2022-07-19 22:33 * @description: 参数回调 * * 这个回调对象CallbackListener是Dubbo给我们生成的代理对象 * 通过 @Argument 注解指定哪个参数是回调参数 * callbacks = 3 支持3个回调，数目超过了会报错 **/@DubboService(methods = &#123; @Method(name = &quot;hello&quot;, arguments = &#123; @Argument(index = 1, callback = true) &#125;)&#125;, callbacks = 3)public class CallbackServiceImpl implements CallbackService &#123; @Override public String hello(String lastName, CallbackListener callbackListener) &#123; // 回调客户端逻辑 String firstName = callbackListener.getFirstName(); return String.format(&quot;hello %s %s&quot;, firstName, lastName); &#125;&#125; 消费端 123456789// 回调接口实现@Slf4jpublic class MyCallback implements CallbackListener &#123; @Override public String getFirstName() &#123; log.info(&quot;参数回调&quot;); return &quot;wu&quot;; &#125;&#125; 1234567891011121314151617@Slf4j@SpringBootTestpublic class CallbackTest &#123; @DubboReference CallbackService callbackService; @Test public void test() &#123; // 创建回调参数 CallbackListener callbackListener = new MyCallback(); log.info(callbackService.hello(&quot;gengfeng&quot;, callbackListener)); &#125;&#125;// 输出//2022-07-19 22:51:45.791 INFO 1885 --- [andler-thread-2] com.wgf.springboot.callback.MyCallback : 参数回调//2022-07-19 22:51:45.799 INFO 1885 --- [ main] com.wgf.springboot.CallbackTest : hello wu gengfeng 异步执行 使用文档 异步执行 Dubbo 服务提供方的异步执行 Provider端异步执行将阻塞的业务从Dubbo内部线程池切换到业务自定义线程，避免Dubbo线程池的过度占用，有助于避免不同服务间的互相影响。异步执行无益于节省资源或提升RPC响应性能，因为如果业务执行需要阻塞，则始终还是要有线程来负责执行 注意 Provider 端 异步执行 和 Consumer 端 异步调用 是相互独立的，你可以任意正交组合两端配置 Consumer同步 - Provider同步 Consumer异步 - Provider同步 Consumer同步 - Provider异步 Consumer异步 - Provider异步 用法一 参考 异步调用 用的 AsyncCallServiceImpl 用法二 使用AsyncContext Dubbo 提供了一个类似 Serverlet 3.0 的异步接口AsyncContext，在没有 CompletableFuture 签名接口的情况下，也可以实现 Provider 端的异步执行 接口项目 123public interface AsyncExecuteService &#123; String hello();&#125; 服务端 1234567891011121314151617181920212223242526@Slf4j@DubboServicepublic class AsyncExecuteServiceImpl implements AsyncExecuteService &#123; @Override public String hello() &#123; log.info(Thread.currentThread().getName()); final AsyncContext asyncContext = RpcContext.startAsync(); new Thread(() -&gt; &#123; // 如果要使用上下文，则必须要放在第一句执行 asyncContext.signalContextSwitch(); log.info(Thread.currentThread().getName()); try &#123; Thread.sleep(500); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; // 写回响应 asyncContext.write(&quot;Hello&quot;); &#125;).start(); return null; &#125;&#125;//INFO 9340 --- [:20880-thread-5] c.w.s.provider.AsyncExecuteServiceImpl : DubboServerHandler-192.168.1.106:20880-thread-5//INFO 9340 --- [ Thread-17] c.w.s.provider.AsyncExecuteServiceImpl : Thread-17 客户端 1234567891011@Slf4j@SpringBootTestpublic class AsyncExecuteTest &#123; @DubboReference AsyncExecuteService asyncExecuteService; @Test public void test() &#123; Assertions.assertEquals(asyncExecuteService.hello(), &quot;Hello&quot;); &#125;&#125; 异步调用 使用文档 异步调用 其他异步调用方式 理解起来比较容易，主要要理解 CompletableFuture。只是说dubbo也可以支持java的CompletableFuture sent 属性 sent=&quot;true&quot; 等待消息发出，消息发送失败将抛出异常。 sent=&quot;false&quot; 不等待消息发出，将消息放入 IO 队列，即刻返回。 如果你只是想异步，完全忽略返回值，可以配置 return=&quot;false&quot;，以减少 Future 对象的创建和管理成本 123@DubboReference(methods = &#123; @Method(name = &quot;hello&quot;, async = true, sent = false)&#125;) 接口项目 1234public interface AsyncCallService &#123; // 有多种扩展模式, 默认方法扩展、RpcContext CompletableFuture&lt;String&gt; hello();&#125; 服务端 123456789101112131415161718192021222324252627@Slf4j@DubboServicepublic class AsyncCallServiceImpl implements AsyncCallService &#123; @Override public CompletableFuture&lt;String&gt; hello() &#123; log.info(&quot;hello&quot;); // 建议为supplyAsync提供自定义线程池，避免使用JDK公用线程池 return CompletableFuture.supplyAsync(() -&gt; &#123; // 将同步逻辑包装成异步 return getName(); &#125;); &#125; private String getName() &#123; try &#123; TimeUnit.MILLISECONDS.sleep(500); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; log.info(&quot;getName&quot;); return &quot;wgf&quot;; &#125;&#125;// 输出// 2022-07-19 23:47:30.242 INFO 2216 --- [:20880-thread-5] c.w.s.provider.AsyncCallServiceImpl : hello// 2022-07-19 23:47:30.749 INFO 2216 --- [onPool-worker-1] c.w.s.provider.AsyncCallServiceImpl : getName 消费端 12345678910111213141516171819202122232425262728293031@Slf4j@SpringBootTestpublic class AsyncCallTest &#123; @DubboReference(methods = &#123; @Method(name = &quot;hello&quot;, async = true) &#125;) AsyncCallService asyncCallService; @Test public void test() throws InterruptedException &#123; CompletableFuture&lt;String&gt; future = asyncCallService.hello(); // future 可以使用其他 Api 等待结果返回 future.whenComplete((result, exception) -&gt; &#123; if (exception == null) &#123; log.info(&quot;异步调用返回值：&#123;&#125;&quot;, result); &#125; else &#123; exception.printStackTrace(); &#125; &#125;); log.info(&quot;测试执行完毕&quot;); // 等待异步返回 TimeUnit.SECONDS.sleep(1); &#125;&#125;// 输出// 2022-07-19 23:42:47.311 INFO 2194 --- [ main] com.wgf.springboot.AsyncCallTest : 测试执行完毕// 2022-07-19 23:42:48.320 INFO 2194 --- [andler-thread-1] com.wgf.springboot.AsyncCallTest : 异步调用返回值：wgf 并发控制 使用文档 并发控制 相关博客 Dubbo 中的并发控制，可控制 服务级别 方法级别 服务端 消费端 用法一 服务级别 限制服务器端并发执行线程数 1234567891011121314@DubboService(executes = 1)public class ConcurrencyServiceImpl implements ConcurrencyService &#123; @Override public String hello() &#123; try &#123; TimeUnit.MILLISECONDS.sleep(200); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; return &quot;hello&quot;; &#125;&#125;// 并发超过会引发Rpc异常 用法二 方法级别 限制服务器端并发执行线程数 1234567891011121314@DubboService(methods = &#123; @Method(name = &quot;hello&quot;, executes = 1)&#125;)public class ConcurrencyServiceImpl implements ConcurrencyService &#123; @Override public String hello() &#123; try &#123; TimeUnit.MILLISECONDS.sleep(200); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; return &quot;hello&quot;; &#125;&#125; 用法三 服务级别 限制每客户端并发执行请求数 说明：2.7.x 版本生效，3.0.x版本不生效 12@DubboReference(actives = 1)ConcurrencyService concurrencyService; 用法四 方法级别 限制每客户端并发执行请求数 说明：2.7.x 版本生效，3.0.x版本不生效 1234@DubboReference(methods = &#123; @Method(name = &quot;hello&quot;, actives = 1)&#125;)ConcurrencyService concurrencyService; 连接控制 使用文档 连接控制 Dubbo 中服务端和客户端的连接控制 服务端连接控制 限制服务器端接受的连接不能超过 N 个 1234dubbo: provider: protocol: dubbo accepts: 10 或者 1234dubbo: protocol: name: dubbo accepts: 10 或者 1234@DubboService(connections = 1)public class ConnectionsServiceImpl implements ConnectionsService &#123; ...&#125; 或者 12@DubboReference(connections = 1)ConnectionsService connectionsService; 粘滞连接 使用文档 粘滞连接 为有状态服务配置粘滞连接 粘滞连接用于有状态服务(当有多个服务提供者)，尽可能让客户端总是向同一提供者发起调用，除非该提供者挂了，再连另一台。 粘滞连接将自动开启延迟连接，以减少长连接数。 支持消费端 服务级别 方法级别 配置 12@DubboReference(sticky = true)StickyService stickyService; 泛化调用 使用文档 使用泛化调用 泛化接口调用方式主要用于客户端没有 API 接口及模型类元的情况，参数及返回值中的所有 POJO 均用 Map 表示，通常用于框架集成，比如：实现一个通用的服务测试框架，可通过 GenericService 调用所有服务实现，GenericService 是 Dubbo 提供的 消费端 12345678910111213141516@SpringBootTestpublic class GenericTest &#123; // 泛化 LoadBalanceService @DubboReference(interfaceName = &quot;com.wgf.springboot.connector.LoadBalanceService&quot;, generic = true) GenericService genericService; @Test public void test() &#123; // 方法名称 // 参数类型 // 参数 Object hello = genericService.$invoke(&quot;hello&quot;, new String[]&#123;&quot;java.lang.String&quot;&#125;, new Object[]&#123;&quot;wgf&quot;&#125;); System.out.println(hello); &#125;&#125; json泛化调用 使用文档 json泛化调用 提示 支持版本：2.7.12之后， 3.0.x 暂时不支持 对于Dubbo泛化调用，提供一种新的方式：直接传递字符串来完成一次调用。即用户可以直接传递参数对象的json字符串来完成一次Dubbo泛化调用 接口项目 123public interface GenericJsonService &#123; User add(User user);&#125; 服务端 123456789@Slf4j@DubboServicepublic class GenericJsonServiceImpl implements GenericJsonService &#123; @Override public User add(User user) &#123; log.info(user.toString()); return user; &#125;&#125; 消费端 123456789101112131415161718@Slf4j@SpringBootTestpublic class GenericJsonTest &#123; @DubboReference(interfaceName = &quot;com.wgf.springboot.connector.GenericJsonService&quot;, generic = true) GenericService genericService; @Test public void test() &#123; String json = &quot;&#123;&#x27;id&#x27;:10,&#x27;name&#x27;:&#x27;wgf&#x27;&#125;&quot;; // 3.0.x 版本不支持 2.7.12 之后支持 // RpcContext中设置generic=gson RpcContext.getContext().setAttachment(&quot;generic&quot;,&quot;gson&quot;); Object result = genericService.$invoke(&quot;add&quot;, new String[]&#123;&quot;com.wgf.springboot.entity.User&quot;&#125;, new Object[]&#123;json&#125;); log.info(result.toString()); &#125;&#125; 泛化服务 使用文档 实现泛化调用 泛化接口实现方式主要用于服务器端没有 API 接口及模型类元的情况，参数及返回值中的所有 POJO 均用 Map 表示 实现一个通用的远程服务 Mock 框架，可通过实现 GenericService 接口处理所有服务请求 服务端 1234567891011121314151617181920/** * @author: ken 😃 * @date: 2022-07-20 * @description: * 泛化 LoadBalanceService 服务 实现一个mock服务，需要通过version和原版本区分 **/@DubboService(interfaceName = &quot;com.wgf.springboot.connector.LoadBalanceService&quot;, version = &quot;mock&quot;)public class GenericServiceImpl implements GenericService &#123; /** * @param method 方法名字 * @param parameterTypes 参数类型数组 * @param args 参数值数组 */ @Override public Object $invoke(String method, String[] parameterTypes, Object[] args) throws GenericException &#123; return String.format(&quot;调用 %s 方法&quot;, method); &#125;&#125; 消费端 12345678910111213@Slf4j@SpringBootTestpublic class GenericServiceTest &#123; // 指定为泛化版本 @DubboReference(version = &quot;mock&quot;) LoadBalanceService loadBalanceService; @Test public void test() &#123; log.info(loadBalanceService.hello(&quot;test&quot;)); &#125;&#125; 服务端会暴露原版和泛化的 LoadBalanceService 服务 如果没有指定 mock 版本，默认执行 LoadBalanceService 实现原逻辑 指定 mock 版本，则会进入泛化服务的 $invoke 方法中 REST支持 使用文档 REST 支持 使用文档 多协议 注意Dubbo的REST也是Dubbo所支持的一种协议 当我们用Dubbo提供了一个服务后，如果消费者没有使用Dubbo也想调用服务，那么这个时候我们就可以让我们的服务支持REST协议，这样消费者就可以通过REST形式调用我们的服务了 注意：如果某个服务只有REST协议可用，那么该服务必须用@Path注解定义访问路径 接口项目 123public interface RestService &#123; Map&lt;Integer, Object&gt; selectUser(Integer id);&#125; 服务端 添加依赖 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.dubbo&lt;/groupId&gt; &lt;artifactId&gt;dubbo-rpc-rest&lt;/artifactId&gt; &lt;version&gt;3.0.9&lt;/version&gt;&lt;/dependency&gt; 12345678910111213141516171819/** * @author: ken 😃 * @date: 2022-07-20 * @description: REST 应用，服务可以同时支持多个协议 **/@Path(&quot;test&quot;)@DubboService(protocol = &#123;&quot;rest&quot;, &quot;dubbo&quot;&#125;)public class RestServiceImpl implements RestService &#123; @GET @Path(&quot;selectUser&quot;) @Produces(&#123;ContentType.APPLICATION_JSON_UTF_8&#125;) @Override public Map&lt;Integer, Object&gt; selectUser(@QueryParam(&quot;id&quot;) Integer id) &#123; Map&lt;Integer, Object&gt; map = new HashMap&lt;&gt;(); map.put(id, &quot;test&quot;); return map; &#125;&#125; 多协议配置 123456789101112131415161718dubbo: application: name: dubbo-springboot-provider protocols: dubbo: name: dubbo port: -1 rest: name: rest port: 8080 server: jetty registry: id: zk-registry address: zookeeper://my-server:2181 config-center: address: zookeeper://my-server:2181 metadata-report: address: zookeeper://my-server:2181 客户端 123456789101112@Slf4j@SpringBootTestpublic class RestTest &#123; // 使用dubbo 协议 @DubboReference RestService restService; @Test public void test() &#123; Assertions.assertEquals(restService.selectUser(1).get(1), &quot;test&quot;); &#125;&#125; 浏览器访问 ：http://localhost:8080/test/selectUser?id=2 msgpack序列化 使用文档 msgpack序列化 msgpack 官网 msgpack 用法 MessagePack 是一种高效的二进制序列化格式。它允许您在 JSON 等多种语言之间交换数据。但它更快更小。小整数被编码为一个字节，典型的短字符串除了字符串本身之外只需要一个额外的字节 需要注意的是解包顺序必须与打包顺序一致（字段顺序），否则会出错。也就是说协议格式的维护要靠两端手写代码进行保证，而这是很不安全的 提示 支持版本：2.7.12 之后 回声测试 使用文档 回声测试 通过回声测试检测 Dubbo 服务是否可用 回声测试用于检测服务是否可用，回声测试按照正常请求流程执行，能够测试整个调用是否通畅，可用于监控。 所有服务自动实现 EchoService 接口，只需将任意服务引用强制转型为 EchoService，即可使用。 接口项目 123public interface TestEchoService &#123; String hello();&#125; 服务端 1234567@DubboServicepublic class TestEchoServiceImpl implements TestEchoService &#123; @Override public String hello() &#123; return &quot;HELLO&quot;; &#125;&#125; 消费端 1234567891011121314@Slf4j@SpringBootTestpublic class EchoTest &#123; @DubboReference TestEchoService testEchoService; @Test public void test() &#123; EchoService echoService = (EchoService) testEchoService; log.info(echoService.$echo(&quot;ok&quot;).toString()); &#125;&#125;INFO 3448 --- [ main] com.wgf.springboot.EchoTest : ok 上下文信息 使用文档 上下文信息 RpcContext (Dubbo 2.5.4 API) (javadoc.io) 通过上下文存放当前调用过程中所需的环境信息 上下文中存放的是当前调用过程中所需的环境信息。所有配置信息都将转换为 URL 的参数 RpcContext 是一个 ThreadLocal 的临时状态记录器，当接收到 RPC 请求，或发起 RPC 请求时，RpcContext 的状态都会变化。比如：A 调 B，B 再调 C，则 B 机器上，在 B 调 C 之前，RpcContext 记录的是 A 调 B 的信息，在 B 调 C 之后，RpcContext 记录的是 B 调 C 的信息。 服务端 123456789@DubboServicepublic class RpcContextServiceImpl implements RpcContextService &#123; @Override public void hello() &#123; final RpcContext context = RpcContext.getContext(); // 反射输出 RpcContextService.print(context); &#125;&#125; 消费端 12345678910111213@SpringBootTestpublic class RpcContextTest &#123; @DubboReference RpcContextService rpcContextService; @Test public void test() &#123; rpcContextService.hello(); // 注意 调用后获取上下文 final RpcContext context = RpcContext.getContext(); RpcContextService.print(context); &#125;&#125; 隐式参数 使用文档 隐式参数 通过 Dubbo 中的 Attachment 在服务消费方和提供方之间隐式传递参数 可以通过 RpcContext 上的 setAttachment 和 getAttachment 在服务消费方和提供方之间进行参数的隐式传递 注意 path, group, version, dubbo, token, timeout 几个 key 是保留字段，请使用其它值 接口项目 123public interface AttachmentService &#123; void hello();&#125; 服务端 1234567891011@Slf4j@DubboServicepublic class AttachmentServiceImpl implements AttachmentService &#123; @Override public void hello() &#123; // 隐式参数获取 用于框架集成，不建议常规业务使用 log.info(RpcContext.getContext().getAttachment(&quot;param&quot;)); &#125;&#125;INFO 11172 --- [:20880-thread-5] c.w.s.provider.AttachmentServiceImpl : test 消费端 12345678910111213@SpringBootTestpublic class AttachmentTest &#123; @DubboReference AttachmentService attachmentService; @Test public void test() &#123; // 传递隐式参数 RpcContext.getContext().setAttachment(&quot;param&quot;, &quot;test&quot;); attachmentService.hello(); &#125;&#125; TLS 使用文档 TLS 传输安全协议 TLS SSL 证书配置博客 通过 TLS 保证传输安全 2.7.5 版本在传输链路的安全性上做了很多工作，对于内置的 Dubbo Netty Server 和新引入的 gRPC 协议都提供了基于 TLS 的安全链路传输机制 TLS 的配置都有统一的入口，如下所示： Provider 端 12345678910SslConfig sslConfig = new SslConfig();sslConfig.setServerKeyCertChainPath(&quot;path to cert&quot;);sslConfig.setServerPrivateKeyPath(args[1]);// 如果开启双向 cert 认证if (mutualTls) &#123; sslConfig.setServerTrustCertCollectionPath(args[2]);&#125;ProtocolConfig protocolConfig = new ProtocolConfig(&quot;dubbo/grpc&quot;);protocolConfig.setSslEnabled(true); Consumer 端 1234567if (!mutualTls) &#123;&#125; sslConfig.setClientTrustCertCollectionPath(args[0]);&#125; else &#123; sslConfig.setClientTrustCertCollectionPath(args[0]); sslConfig.setClientKeyCertChainPath(args[1]); sslConfig.setClientPrivateKeyPath(args[2]);&#125; 令牌验证 使用文档 令牌验证 通过令牌验证在注册中心控制权限 通过令牌验证在注册中心控制权限，以决定要不要下发令牌给消费者，可以防止消费者绕过注册中心访问提供者，另外通过注册中心可灵活改变授权方式，而不需修改或升级提供者 服务端 全局配置 123dubbo: provider: token: &#x27;true&#x27; # 随机token令牌，使用UUID生成 或 123dubbo: provider: token: &#x27;123456&#x27; # 固定token令牌，相当于密码 服务端 服务级别 配置 1234@DubboService(token = &quot;true&quot;)public class xxxServiceImpl implements xxxService&#123;...&#125; 或 1234@DubboService(token = &quot;123456&quot;)public class xxxServiceImpl implements xxxService&#123;...&#125; 测试 本地直连会抛出异常 12345678910111213@SpringBootTestpublic class LoadBalanceTest &#123; // 绕过注册中心 本地直连方式 @DubboReference(url = &quot;dubbo://localhost:20880&quot;) LoadBalanceService loadBalanceService; @Test public void test() &#123; IntStream.range(0, 20). forEach(line -&gt; log.info(loadBalanceService.hello(String.valueOf(line)))); &#125;&#125;// org.apache.dubbo.rpc.RpcException: Invalid token! Forbid invoke remote service in\\terface ... 路由规则 使用文档 路由规则 通过 Dubbo 中的路由规则做服务治理 路由规则在发起一次RPC调用前起到过滤目标服务器地址的作用，过滤后的地址列表，将作为消费端最终发起RPC调用的备选地址 条件路由。支持以服务或 Consumer 应用为粒度配置路由规则。 标签路由。以 Provider 应用为粒度配置路由规则。 条件路由 您可以随时在服务治理控制台 Dubbo-Admin 写入路由规则 admin 控制台具体配置参考使用文档 配置规则 使用文档 规则配置 在 Dubbo 中配置应用级治理规则和服务级治理规则 提示 本文描述的是新版本规则配置，而不是老版本配置规则 覆盖规则是 Dubbo 设计的在无需重启应用的情况下，动态调整 RPC 调用行为的一种能力。2.7.0 版本开始，支持从服务和应用两个粒度来调整动态配置 请在服务治理控制台查看或修改覆盖规则 「动态配置」 使用请查看使用文档 优雅停机 使用文档 优雅停机 让 Dubbo 服务完成优雅停机 Dubbo 是通过 JDK 的 ShutdownHook 来完成优雅停机的，所以如果用户使用 kill -9 PID 等强制关闭指令，是不会执行优雅停机的，只有通过 kill PID 时，才会执行 原理 服务提供方 停止时，先标记为不接收新请求，新请求过来时直接报错，让客户端重试其它机器（默认的集群容错 Failover） 然后，检测线程池中的线程是否正在运行，如果有，等待所有线程执行完成，除非超时，则强制关闭 服务消费方 停止时，不再发起新的调用请求，所有新的调用在客户端即报错 然后，检测有没有请求的响应还没有返回，等待响应返回，除非超时，则强制关闭 设置方式 设置优雅停机超时时间，缺省超时时间是 10 秒，如果超时则强制关闭 123dubbo: application: shutwait: 15000 如果 ShutdownHook 不能生效，可以自行调用 1DubboShutdownHook.destroyAll(); 建议 使用 tomcat 等容器部署的场景，建议通过扩展 ContextListener 等自行调用以下代码实现优雅停机 主机绑定 使用文档 主机绑定 在 Dubbo 中绑定主机名 查找顺序 缺省主机 IP 查找顺序： 通过 LocalHost.getLocalHost() 获取本机地址 如果是 127.* 等 loopback 地址，则扫描各网卡，获取网卡 IP 主机配置 注册的地址如果获取不正确，比如需要注册公网地址，可以： 可以在 /etc/hosts 中加入：机器名 公网 IP，比如： 1test1 205.182.23.201 在 dubbo.xml 中加入主机地址的配置： 1&lt;dubbo:protocol host=&quot;205.182.23.201&quot;&gt; Yml 文件: 123dubbo: protocol: host: &#x27;205.182.23.201&#x27; 或在 dubbo.properties 中加入主机地址的配置: 1dubbo.protocol.host=205.182.23.201 端口配置 缺省主机端口与协议相关： 协议 端口 dubbo 20880 rmi 1099 http 80 hessian 80 webservice 80 memcached 11211 redis 6379 可以按照下面的方式配置端口： 在 dubbo.xml 中加入主机地址的配置： 1&lt;dubbo:protocol name=&quot;dubbo&quot; port=&quot;20880&quot;&gt; 在 dubbo.properties 中加入主机地址的配置： 1dubbo.protocol.dubbo.port=20880 yml 配置文件: 123dubbo: protocol: port: 20880 主机配置 使用文档 主机配置 自定义 Dubbo 服务对外暴露的主机地址 服务地址默认取值：InetAddress.getLocalHost().getHostAddress() 获取默认 host 主要解决服务提供者暴露服务的IP地址，可以指定 内网地址 或 公网地址 或者是 一个域名 同时也可以解决 docker 映射 具体使用参考使用文档 &lt;&lt;&lt;&lt;&lt;&lt;&lt; Updated upstream 序列化框架 ======= 序列化漫谈 Stashed changes 官方文档 序列化漫谈 框架 优点 缺点 Kryo 速度快，序列化后体积小 跨语言支持较复杂，字段增、减，序列化和反序列化时无法兼容 Hessian 默认支持跨语言 较慢，字段增、减，序列化和反序列化时可以兼容 Protostuff 速度快，基于protobuf 需静态编译，只能在末尾添加新字段 Protostuff-Runtime 无需静态编译，但序列化前需预先传入schema 不支持无默认构造函数的类，反序列化时需用户自己初始化序列化后的对象，其只负责将该对象进行赋值 Java 使用方便，可序列化所有类 速度慢，占空间 源码 **源码部分只提供 DEMO 及运行结果，源码的解析请看最权威的官方文档 ** Dubbo SPI 官方文档 Dubbo SPI SPI 全称为 Service Provider Interface，是一种服务发现机制。SPI 的本质是将接口实现类的全限定名配置在文件中，并由服务加载器读取配置文件，加载实现类。这样可以在运行时，动态为接口替换实现类。正因此特性，我们可以很容易的通过 SPI 机制为我们的程序提供拓展功能。SPI 机制在第三方框架中也有所应用，比如 Dubbo 就是通过 SPI 机制加载所有的组件。不过，Dubbo 并未使用 Java 原生的 SPI 机制，而是对其进行了增强，使其能够更好的满足需求。在 Dubbo 中，SPI 是一个非常重要的模块。基于 SPI，我们可以很容易的对 Dubbo 进行拓展。如果大家想要学习 Dubbo 的源码，SPI 机制务必弄懂。接下来，我们先来了解一下 Java SPI 与 Dubbo SPI 的用法，然后再来分析 Dubbo SPI 的源码 Java SPI 定义 SPI 接口 123456/** * SPI 接口 */public interface Animal &#123; void name();&#125; 接口实现 123456public class Cat implements Animal&#123; @Override public void name() &#123; System.out.println(&quot;cat&quot;); &#125;&#125; 123456public class Dog implements Animal&#123; @Override public void name() &#123; System.out.println(&quot;dog&quot;); &#125;&#125; resources META-INF\\services 添加接口全限定名配置文件 com.wgf.springboot.spi.java.Animal 12com.wgf.springboot.spi.java.Catcom.wgf.springboot.spi.java.Dog 使用 ServiceLoader 加载配置的实现类 1234567891011121314151617@Slf4jpublic class SpiTest &#123; /** * java spi 机制 */ @Test public void spiTest() &#123; // 加载 META-INF/services/com.wgf.springboot.spi.java.Animal 文件下配置的SPI接口实现类 ServiceLoader&lt;Animal&gt; serviceLoader = ServiceLoader.load(Animal.class); log.info(&quot;Java SPI&quot;); serviceLoader.forEach( line -&gt; line.name()); &#125;&#125;// cat// dog Dubbo 扩展SPI Dubbo 并未使用 Java SPI，而是重新实现了一套功能更强的 SPI 机制。Dubbo SPI 的相关逻辑被封装在了 ExtensionLoader 类中，通过 ExtensionLoader，我们可以加载指定的实现类。Dubbo SPI 所需的配置文件需放置在 META-INF/dubbo 路径下 Java SPI 默认会加载配置文件下的所有配置类，在实际场景中很多 SPI 可能都不会被使用，造成资源浪费 与 Java SPI 实现类配置不同，Dubbo SPI 是通过键值对的方式进行配置，这样我们可以按需加载指定的实现类。另外，在测试 Dubbo SPI 时，需要在 Robot 接口上标注 @SPI 注解 定义 SPI 接口 1234@SPIpublic interface Robot &#123; void sayHello();&#125; SPI 接口实现类 注意：SPI 接口的实现类必须有无参构造函数，因为 ExtensionLoader 底层是通过 Class.getDeclaredConstructor() 获取构造函数反射创建 SPI 实现类的 123456public class OptimusPrime implements Robot &#123; @Override public void sayHello() &#123; System.out.println(&quot;Hello, I am Optimus Prime&quot;); &#125;&#125; 123456public class Bumblebee implements Robot &#123; @Override public void sayHello() &#123; System.out.println(&quot;Hello, I am Bumblebee&quot;); &#125;&#125; SPI 文件配置 resources\\META-INF\\dubbo\\com.wgf.springboot.spi.dubbo.Robot 12optimusPrime = com.wgf.springboot.spi.dubbo.OptimusPrimebumblebee = com.wgf.springboot.spi.dubbo.Bumblebee 使用 1234567891011121314151617181920public class SpiTest &#123; @Test public void test() throws InterruptedException &#123; // 扩展加载器， 类似 java spi 的 ServiceLoader ExtensionLoader&lt;Robot&gt; extensionLoader = ExtensionLoader.getExtensionLoader(Robot.class); // 需要用到哪个 SPI 的实现类用对应的 key 进行加载 // 第一次调用 extensionLoader 方法时，默认会去解析 resources\\META-INF\\dubbo\\com.wgf.springboot.spi.dubbo.Robot // 将所有 SPI 配置类加载到 cachedNames 中缓存 Robot optimusPrime = extensionLoader.getExtension(&quot;optimusPrime&quot;); optimusPrime.sayHello(); Robot bumblebee = extensionLoader.getExtension(&quot;bumblebee&quot;); bumblebee.sayHello(); &#125;&#125;//Hello, I am Optimus Prime//Hello, I am Bumblebee Dubbo SPI实现AOP AOP 具体实现类 123456789101112131415161718/** * dubbo SPI 的 AOP 使用 装饰者模式实现 */@Slf4jpublic class RobotWrapper implements Robot &#123; private Robot robot; public RobotWrapper(Robot robot) &#123; this.robot = robot; &#125; @Override public void sayHello() &#123; log.info(&quot;包装方法开始&quot;); robot.sayHello(); log.info(&quot;包装方法结束&quot;); &#125;&#125; SPI 文件配置 resources\\META-INF\\dubbo\\com.wgf.springboot.spi.dubbo.Robot 添加AOP实现类全限定类名 123optimusPrime = com.wgf.springboot.spi.dubbo.OptimusPrimebumblebee = com.wgf.springboot.spi.dubbo.Bumblebeecom.wgf.springboot.spi.dubbo.RobotWrapper 测试 1234567891011121314151617181920212223public class SpiTest &#123; @Test public void test() throws InterruptedException &#123; // 扩展加载器， 类似 java spi 的 ServiceLoader ExtensionLoader&lt;Robot&gt; extensionLoader = ExtensionLoader.getExtensionLoader(Robot.class); // 需要用到哪个 SPI 的实现类用对应的 key 进行加载 // 第一次调用 extensionLoader 方法时，默认会去解析 resources\\META-INF\\dubbo\\com.wgf.springboot.spi.dubbo.Robot // 将所有 SPI 配置类加载到 cachedNames 中缓存 Robot optimusPrime = extensionLoader.getExtension(&quot;optimusPrime&quot;); optimusPrime.sayHello(); Robot bumblebee = extensionLoader.getExtension(&quot;bumblebee&quot;); bumblebee.sayHello(); &#125;&#125;//17:10:18.053 [main] INFO com.wgf.springboot.spi.dubbo.RobotWrapper - 包装方法开始//Hello, I am Optimus Prime//17:10:18.053 [main] INFO com.wgf.springboot.spi.dubbo.RobotWrapper - 包装方法结束//17:10:18.053 [main] INFO com.wgf.springboot.spi.dubbo.RobotWrapper - 包装方法开始//Hello, I am Bumblebee//17:10:18.053 [main] INFO com.wgf.springboot.spi.dubbo.RobotWrapper - 包装方法结束 Dubbo SPI实现IOC 官方文档 SPI 自适应拓展 对应于Adaptive机制，Dubbo提供了一个注解@Adaptive，该注解可以用于接口的某个子类上，也可以用于接口方法上。如果用在接口的子类上，则表示Adaptive机制的实现会按照该子类的方式进行自定义实现；如果用在方法上，则表示Dubbo会为该接口自动生成一个子类，并且按照一定的格式重写该方法，而其余没有标注@Adaptive注解的方法将会默认抛出异常 SPI 接口 12345678910@SPIpublic interface LoadBalance &#123; /** * 自适应扩展，value用于指定 URL 的属性，通过解析URL 参数(loadbalance) 对应的 value 完成SPI注入 * @param url */ @Adaptive(&quot;loadbalance&quot;) void balance(URL url);&#125; SPI 实现类 1234567891011121314151617181920212223/** * 装饰者模式 * IOC 注入实现类，注入只能是注入SPI 实现类 */@Slf4jpublic class CommonLoadBalance implements LoadBalance &#123; private LoadBalance loadBalance; @Override public void balance(URL url) &#123; log.info(&quot;IOC 调用前&quot;); loadBalance.balance(url); log.info(&quot;IOC 调用后&quot;); &#125; /** * 提供注入 set 方法 * @param loadBalance */ public void setLoadBalance(LoadBalance loadBalance) &#123; this.loadBalance = loadBalance; &#125;&#125; 123456public class RandomLoadBalance implements LoadBalance &#123; @Override public void balance(URL url) &#123; System.out.println(&quot;随机负载均衡策略&quot;); &#125;&#125; 123456public class WeightsLoadBalance implements LoadBalance&#123; @Override public void balance(URL url) &#123; System.out.println(&quot;权重负载均衡策略&quot;); &#125;&#125; **SPI 文件配置 resources\\META-INF\\dubbo\\SPI 文件配置 resources\\META-INF\\dubbo\\com.wgf.springboot.spi.dubbo.Robot 添加AOP实现类全限定类名 ** 123randomLoadBalance=com.wgf.springboot.spi.dubbo.ioc.RandomLoadBalanceweightsLoadBalance=com.wgf.springboot.spi.dubbo.ioc.WeightsLoadBalancecommonLoadBalance=com.wgf.springboot.spi.dubbo.ioc.CommonLoadBalance 测试 123456789101112131415161718public class IocTest &#123; @Test public void test() &#123; ExtensionLoader&lt;LoadBalance&gt; extensionLoader = ExtensionLoader.getExtensionLoader(LoadBalance.class); LoadBalance loadBalance = extensionLoader.getExtension(&quot;commonLoadBalance&quot;); URL url = URL.valueOf(&quot;test://localhost/test?loadbalance=randomLoadBalance&quot;); loadBalance.balance(url); url = URL.valueOf(&quot;test://localhost/test?loadbalance=weightsLoadBalance&quot;); loadBalance.balance(url); &#125;&#125;//18:05:21.306 [main] INFO com.wgf.springboot.spi.dubbo.ioc.CommonLoadBalance - IOC 调用前//随机负载均衡策略//18:05:21.321 [main] INFO com.wgf.springboot.spi.dubbo.ioc.CommonLoadBalance - IOC 调用后//18:05:21.321 [main] INFO com.wgf.springboot.spi.dubbo.ioc.CommonLoadBalance - IOC 调用前//权重负载均衡策略//18:05:21.321 [main] INFO com.wgf.springboot.spi.dubbo.ioc.CommonLoadBalance - IOC 调用后","categories":[],"tags":[]},{"title":"netty核心原理","slug":"netty核心原理","date":"2022-06-15T10:57:33.000Z","updated":"2023-07-14T06:49:17.567Z","comments":true,"path":"2022/06/15/netty核心原理/","link":"","permalink":"https://wugengfeng.cn/2022/06/15/netty%E6%A0%B8%E5%BF%83%E5%8E%9F%E7%90%86/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"Linux","slug":"Linux","date":"2022-05-29T15:57:33.000Z","updated":"2023-11-01T09:35:49.534Z","comments":true,"path":"2022/05/29/Linux/","link":"","permalink":"https://wugengfeng.cn/2022/05/29/Linux/","excerpt":"","text":"零拷贝 零拷贝 是一种技术，它允许数据在内存中的移动或操作，而无需进行应用程序和内核之间的数据拷贝，也就是零次多余的拷贝。 什么是零拷贝 零拷贝字面上的意思包括两个，“零” 和 “拷贝”： 拷贝：指的是数据从一个存储位置移至另一个存储位置的过程。 零 ：在此上下文中，代表零次多余的拷贝。 零拷贝 是在执行I/O操作时，避免CPU从一个存储区域向另一个存储区域复制数据的技术(应用程序和内核)。这样做可以降低系统的上下文切换次数，减少CPU的负载，并避免不必要的内存使用，从而提高I/O性能。 传统的IO执行流程 12while((n = read(diskfd, buf, BUF_SIZE)) &gt; 0) write(sockfd, buf , n); 文件描述符: diskfd 是一个文件描述符，指代在文件系统上的一个文件。 sockfd 是一个文件描述符，指代一个已连接的socket。 read系统调用: 在用户空间: 当应用程序执行read函数时，它会通过系统调用请求OS执行数据读取的操作。 在内核空间: 从磁盘读取数据到内核缓冲区：这个操作是在内核空间中进行的。内核负责管理硬件资源，所以从磁盘读取数据的实际操作是在内核空间完成的。 从内核缓冲区拷贝到用户缓冲区：这也是在内核空间中完成的。系统需要将内核缓冲区中的数据拷贝到用户空间的缓冲区。 write系统调用: 在用户空间: 当应用程序执行write函数时，它会通过系统调用请求OS执行数据写入的操作。 在内核空间: 从用户缓冲区拷贝到内核的socket缓冲区：这个操作是在内核空间完成的。系统需要将用户缓冲区中的数据拷贝到内核空间的socket缓冲区。 从socket缓冲区写入到网卡设备：这也是在内核空间完成的。内核通过网络协议栈处理数据，然后将数据发送到网卡。 注意事项: 该方法是阻塞的：这意味着，如果 read 或 write 系统调用不能立即完成（例如，磁盘很慢或网络有延迟），那么整个进程会被阻塞，直到操作完成。 在现实场景中，为了提高性能和响应时间，通常会使用非阻塞IO、多线程或异步IO。 用户应用进程调用read函数，向操作系统发起IO调用(系统调用)，上下文从用户态转为内核态（切换1）。 DMA控制器把数据从磁盘读取到内核缓冲区。 CPU把内核缓冲区数据拷贝到用户应用缓冲区，上下文从内核态转为用户态（切换2），read函数返回。 用户应用进程通过write函数发起IO调用(系统调用)，上下文从用户态转为内核态（切换3）。 CPU将用户缓冲区中的数据拷贝到socket缓冲区。 DMA控制器将数据从socket缓冲区传输到网卡设备，上下文从内核态切换回用户态（切换4），write函数返回。 总结：传统的IO流程确实会经历四次上下文切换和四次数据拷贝。 CPU上下文切换 一般我们说的上下文切换，是指操作系统在 CPU 上切换进程或线程的状态，包括用户态和内核态之间的转换，以及内核态下不同进程或线程之间的切换。进程从用户态切换到内核态，通常是通过系统调用来完成的。在系统调用的过程中，会涉及到 CPU 上下文的切换。这个过程包括保存当前进程或线程的 CPU 寄存器状态，然后加载新的进程或线程的 CPU 寄存器状态，以便新的进程或线程可以接着之前的状态继续运行。具体来说，CPU 寄存器里原来的用户态指令位置需要被保存起来，然后 CPU 寄存器需要更新为内核态指令的新位置，最后 CPU 才能跳转到内核态运行 内核任务。 用户空间和内核空间 什么是系统内核 内核是操作系统的核心组成部分，它作为应用程序和硬件之间的桥梁，负责管理系统的进程、内存、设备驱动程序、文件系统和网络功能等。内核的设计和性能直接影响整个系统的性能和稳定性。 内核空间：系统内核运行的空间 用户空间：应用程序运行的空间 为什么要区分内核空间和用户空间： 为了确保操作系统的稳定性和可用性，应用程序被限制在用户空间运行，而不能直接访问硬件或执行某些关键的系统操作。这些底层的任务，如读写磁盘文件、分配和回收内存或从网络接口读写数据，都必须通过内核提供的接口来完成。 具体实现：比如 Intel 的 CPU 将特权等级分为 4 个级别：Ring0~Ring3 当进程运行在 Ring3 级别时被称为运行在用户态 而运行在 Ring0 级别时被称为运行在内核态 DMA拷贝 DMA，全称为Direct Memory Access，即直接内存访问。DMA是一种硬件级的数据传输技术，通常由主板上的独立DMA控制器芯片实现。它允许外设设备和内存存储器之间直接进行IO数据传输，而不需要CPU的参与。 DMA的主要任务是协助CPU处理IO请求和数据拷贝。 DMA使硬件设备能够直接与内存存储器进行数据IO传输，提高了数据传输的效率。 虚拟内存 现代操作系统使用虚拟内存技术，其中系统为应用程序提供虚拟地址，而不是直接的物理地址。使用虚拟内存有以下好处： 虚拟内存空间通常远大于物理内存空间。这是因为虚拟内存不仅仅是物理内存，还包括使用磁盘上的一部分作为&quot;交换空间&quot;或&quot;页面文件&quot;。 多个虚拟地址可以映射到同一个物理地址。这使得多个进程或线程可以共享同一物理内存地址，从而实现内存共享。 正是因为多个虚拟地址可以映射到同一个物理地址，内核空间和用户空间的虚拟地址可以映射到同一个物理地址。这种方式可以减少在数据传输时的数据拷贝次数。 虚拟地址到物理地址的转换过程称为地址翻译。 通过多个虚拟地址共同指向同一个物理地址，可以在内核空间与用户空间或应用程序之间实现内存共享，从而减少数据复制。 实现 零拷贝的核心是减少无用的数据复制次数，优化IO。其实现是通过系统调用(调用系统内核函数) mmap 使用 mmap 方法代替传统的IO方式，核心是利用虚拟内存的映射特性。这使得用户空间和内核空间可以共享相同的Read Buffer，因此数据不再需要从内核空间复制到用户空间，从而避免了一次数据复制。 数据移动次数：三次 上下文切换次数：四次 用户进程通过mmap方法向操作系统内核发起IO调用，此时上下文从用户态切换为内核态。 利用DMA控制器，数据从硬盘被传输到内核缓冲区。 上下文从内核态切换回用户态，mmap方法返回。 用户进程通过write方法向操作系统内核发起IO调用，此时上下文从用户态切换为内核态。 数据在内核缓冲区中被准备好并通过DMA直接从socket缓冲区传输到网卡。 上下文从内核态切换回用户态，write调用返回。 优点：操作文件的速度与操作内存相当，特别适合处理较大的文件，如在NIO和RocketMQ中的应用。 缺点： 对于非常小的文件（例如小于4KB），可能会浪费内存，因为内存页面的最小单位通常为4KB。 如果系统频繁使用mmap操作，且每次映射的大小都不同，可能导致内存碎片化，从而缺乏连续的内存空间。 适用场景 对数据读取后需要进行加工处理的，比如NIO,RocketMQ sendfile sendfile是Linux2.1内核版本后引入的一个系统调用函数，API如下： 1ssize_t sendfile(int out_fd, int in_fd, off_t *offset, size_t count); out_fd:为待写入内容的文件描述符，一个socket描述符。， in_fd:为待读出内容的文件描述符，必须是真实的文件，不能是socket和管道。 offset：指定从读入文件的哪个位置开始读，如果为NULL，表示文件的默认起始位置。 count：指定在fdout和fdin之间传输的字节数。 sendfile 是一种专门用于在两个文件描述符之间传输数据的系统调用。它直接在内核中进行数据传输，避免了数据在内核空间与用户空间之间的中间拷贝，从而实现真正的零拷贝传输。这种优化减少了不必要的上下文切换和数据复制，从而提高了文件传输的效率。 sendfile实现的零拷贝流程如下： 数据移动次数：三次 上下文切换次数：两次，使用sendfile()系统调用替换Read和Write系统调用 用户进程发起sendfile系统调用，此时上下文从用户态切换到内核态。 通过DMA控制器，数据从硬盘直接拷贝到内核的页缓存。 数据从页缓存被转移到目标socket的内核缓冲区。 通过DMA控制器，数据从内核的socket缓冲区传输到网卡。 上下文从内核态切换回用户态，sendfile调用返回。 Linux 2.4内核优化SG-DMA拷贝 使用scatter/gather DMA技术，允许数据直接从硬盘拷贝到网卡，无需中间内存间拷贝。 数据移动次数：两次 （都是DMA拷贝） 上下文切换次数：两次 优点： 2次上下文切换，0次CPU拷贝，2次DMA拷贝 — 实现了真正意义上的零拷贝。 非常适合大文件传输。 缺点： 需要硬件支持DMA技术。 不能进行数据的中间处理或修改。 适用场景： 对于应用层不需要对数据进行处理的场景，例如，直接将硬盘上的文件发送到网卡。如kafka的高吞吐量实现。 需要注意的是，sendfile存在限制。特别是在某些版本的Linux内核中，数据源不能是socket，而数据的目标必须是socket。这意味着sendfile主要用于文件到网络的传输，限制了其使用范围 splice 鉴于 Sendfile 的缺点，在 Linux2.6.17 中引入了 Splice，它在读缓冲区和网络操作缓冲区之间建立管道避免 CPU 拷贝：先将文件读入到内核缓冲区，然后再与内核网络缓冲区建立管道。它的函数原型 1ssize_t splice(int fd_in, loff_t *off_in, int fd_out, loff_t *off_out, size_t len, unsigned int flags); 用户进程调用 splice()，此时从用户态切换到内核态。 如果源是一个普通文件，DMA（Direct Memory Access）将文件数据从存储设备读入内核缓冲区。 splice() 将数据从源文件描述符直接移动到目标文件描述符，避免了数据复制到用户空间的额外步骤。 如果目标是网络套接字，数据会被传输到相关的网络缓冲区（通过修改数据的指针），并最终通过 DMA 被发送到网络接口。 管道设备 管道设备文件，也称为 FIFO (First-In, First-Out) 文件，数据流入一端并从另一端流出。虽然通常用于进程间通信，管道也能够配合 splice 等系统调用，实现高效的数据传输。 局限性 splice也有一些局限，它的两个文件描述符参数中有一个必须是管道设备 tee tee类似splice但是两个fd都必须是管道，而且tee不消耗输入fd的数据","categories":[],"tags":[]},{"title":"netty","slug":"netty","date":"2022-05-25T03:48:28.000Z","updated":"2023-07-14T06:49:16.905Z","comments":true,"path":"2022/05/25/netty/","link":"","permalink":"https://wugengfeng.cn/2022/05/25/netty/","excerpt":"","text":"IO模型 什么是IO？ I/O（Input/Outpu） 即输入／输出 BIO 什么是BIO？ Blocking I/O，属于同步阻塞 IO 模型 ​ 同步并阻塞(传统阻塞型)，服务器实现模式为一个连接一个线程，即客户端有连接请求时服务器端就需要启动一个线程进行处理，如果这个连接不做任何事情会造成不必要的线程开销 ​ 同步阻塞 IO 模型中，应用程序发起 read 调用后，会一直阻塞，直到内核把数据拷贝到用户空间 缺点： 如果客户端很多就必须创建更多的工作线程服务于客户端，线程的创建会消耗服务器资源，影响效率 Client和Server建立连接后，不一定会一直进行读写操作，有可能什么都不干，但是还是占用一个工作线程，浪费资源 BIO一个服务端的工作线程对应一个客户端请求无法满足高并发场景 适用于连接数目比较小且固定的架构，这种方式对服务器资源要求比较高， 并发局限于应用中，JDK1.4以前的唯一选择，但程序简单易理解 NIO 背景：在客户端连接数量不高的情况下，使用BIO是没问题的。但是，当面对十万甚至百万级连接的时候，传统的 BIO 模型是无能为力的。因此，我们需要一种更高效的 I/O 处理模型来应对更高的并发量 NIO： ​ Java 中的 NIO 于 Java 1.4 中引入，对应 java.nio 包，提供了 Channel , Selector，Buffer 等抽象。NIO 中的 N 可以理解为 Non-blocking，不单纯是 New。它是支持面向缓冲的，基于通道的 I/O 操作方法。 对于高负载、高并发的（网络）应用，应使用 NIO Non-blocking I/O （同步非阻塞IO） 同步非阻塞，服务器实现模式为一个线程处理多个请求(连接)，即客户端发送的连接请求都会注册到多路复用器上，多路复用器轮询到连接有I/O请求就进行处理 同步非阻塞 同步非阻塞 IO 模型中，应用程序会一直发起 read 调用，等待数据从内核空间拷贝到用户空间的这段时间里，线程依然是阻塞的，直到在内核把数据拷贝到用户空间。 相比于同步阻塞 IO 模型，同步非阻塞 IO 模型确实有了很大改进。通过轮询操作，避免了一直阻塞。 但是，这种 IO 模型同样存在问题：应用程序不断进行 I/O 系统调用轮询数据是否已经准备好的过程是十分消耗 CPU 资源的。 这个时候，I/O 多路复用模型 就上场了 IO多路复用 IO 多路复用模型，通过减少无效的系统调用，减少了对 CPU 资源的消耗 Java 中的 NIO ，有一个非常重要的选择器 ( Selector ) 的概念，也可以被称为 多路复用器。通过它，只需要一个线程便可以管理多个客户端连接。当客户端数据到了之后，才会为其服务 IO 多路复用模型中，线程首先发起 select 调用，询问内核数据是否准备就绪，等内核把数据准备好了，用户线程再发起 read 调用。read 调用的过程（数据从内核空间 -&gt; 用户空间）还是阻塞的 目前支持 IO 多路复用的系统调用，有 select，epoll 等等。select 系统调用，目前几乎在所有的操作系统上都有支持 select 调用 ：内核提供的系统调用，它支持一次查询多个系统调用的可用状态。几乎所有的操作系统都支持 epoll 调用 ：linux 2.6 内核，属于 select 调用的增强版本，优化了 IO 的执行效率 内核空间与用户空间 户空间的程序不能直接去磁盘空间(网卡)中读取数据，必须由经由内核空间通过DMA来获取 一般用户空间的内存分页与磁盘空间不会对齐，因此需要由内核空间在中间做一层处理 适用于连接数目多且连接比较短（轻操作）的架构，比如聊天服务器，弹幕系统，服务器间通讯等。编程比较复杂，JDK1.4开始支持 AIO Asynchronous I/O 异步非阻塞IO AIO 也就是 NIO 2。Java 7 中引入了 NIO 的改进版 NIO 2,它是异步 IO 模型 异步 IO 是基于事件和回调机制实现的，也就是应用操作之后会直接返回，不会堵塞在那里，当后台处理完成，操作系统会通知相应的线程进行后续的操作 目前来说 AIO 的应用还不是很广泛。Netty 之前也尝试使用过 AIO，不过又放弃了。这是因为，Netty 使用了 AIO 之后，在 Linux 系统上的性能并没有多少提升 使用于连接数目多且连接比较长（重操作）的架构，比如相册服务器，充分调用OS参与并发操作，编程比较复杂，JDK7开始支持 NIO NIO的基本介绍 Java NIO 全称 java non-blocking IO，是指 JDK 提供的新 API。从 JDK1.4 开始，Java 提供了一系列改进的输入/输出 的新特性，被统称为 NIO(即 New IO)，是同步非阻塞的 NIO 相关类都被放在 java.nio 包及子包下，并且对原 java.io 包中的很多类进行改写 NIO 有三大核心部分：Channel(通道)，Buffer(缓冲区), Selector(选择器) NIO是面向缓冲区 ，或者面向块编程的。数据读取到一个它稍后处理的缓冲区(Buffer)，需要时可在缓冲区中前后移动，这就 增加了处理过程中的灵活性，使用它可以提供非阻塞式的高伸缩性网络 Java NIO的非阻塞模式，使一个线程从某通道发送请求或者读取数据，但是它仅能得到目前可用的数据，如果目前没有数据可用时，就什么都不会获取，而不是保持线程阻塞，所以直至数据变的可以读取之前，该线程可以继续做其他的事情。 非阻塞写也是如此，一个线程请求写入一些数据到某通道，但不需要等待它完全写入，这个线程同时可以去做别的事情 通俗理解：NIO是可以做到用一个线程来处理多个操作的。假设有10000个请求过来, 根据实际情况，可以分配50或者100个线程来处理。不像之前的阻塞IO那样，非得分配10000个 HTTP2.0使用了多路复用的技术，做到同一个连接并发处理多个请求，而且并发请求 的数量比HTTP1.1大了好几个数量级 NIO三大核心 Buffer java.nio 定义了以下几个 Buffer 的实现 一个 Buffer本质上是内存中的一块，我们可以将数据写入这块内存，之后从这块内存获取数据 其实核心是最后的 ByteBuffer，前面的一大串类只是包装了一下它而已，我们使用最多的通常也是 ByteBuffer 我们应该将 Buffer 理解为一个数组，IntBuffer、CharBuffer、DoubleBuffer 等分别对应 int[]、char[]、double[] 等 MappedByteBuffer 用于实现内存映射文件 操作 Buffer 和操作数组、类集差不多，只不过大部分时候我们都把它放到了 NIO 的场景里面来使用而已。下面介绍 Buffer 中的几个重要属性和几个重要方法 position、limit、capacity 就像数组有数组容量，每次访问元素要指定下标，Buffer 中也有几个重要属性：position、limit、capacity capacity：它代表这个缓冲区的容量，一旦设定就不可以更改。比如 capacity 为 1024 的 IntBuffer，代表其一次可以存放 1024 个 int 类型的值。一旦 Buffer 的容量达到 capacity，需要清空 Buffer，才能重新写入值 position：它代表这个缓冲区的数据定位，从写操作模式到读操作模式切换的时候（flip），position 都会归零，这样就可以从头开始读写了 写模式：每往 Buffer 中写入一个值，position 就自动加 1，代表下一次的写入位置 读模式：每读一个值，position 就自动加 1，指向下一次要读取的位置 limit：它代表这个缓冲区的数据限制 写模式：limit 代表的是最大能写入的数据，这个时候 limit 等于 capacity 读模式：此时的 limit 等于 Buffer 中实际的数据大小，因为 Buffer 不一定被写满 初始化Buffer 每个 Buffer 实现类都提供了一个静态方法 allocate(int capacity) 帮助我们快速实例化一个 Buffer 123ByteBuffer byteBuf = ByteBuffer.allocate(1024);IntBuffer intBuf = IntBuffer.allocate(1024);LongBuffer longBuf = LongBuffer.allocate(1024); 另外，我们经常使用 wrap 方法来初始化一个 Buffer 123ByteBuffer byteBuffer = ByteBuffer.wrap(new byte[1024]);IntBuffer intBuffer = IntBuffer.wrap(new int[1024]);LongBuffer longBuffer = LongBuffer.wrap(new long[1024]); 填充Buffer (写操作) 1234567891011// 填充一个 byte 值public abstract ByteBuffer put(byte b);// 在指定位置填充一个 byte 值public abstract ByteBuffer put(int index, byte b);// 将一个数组中的值填充进去public final ByteBuffer put(byte[] src);// 截取数组中的一部分填充进去public ByteBuffer put(byte[] src, int offset, int length) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788public class BufferTest &#123; @Test public void bufferExample() &#123; // 分配一个长度为5的IntBuffer //IntBuffer buffer = IntBuffer.allocate(5); IntBuffer buffer = IntBuffer.wrap(new int[5]); for (int i = 0; i &lt; buffer.capacity(); i++) &#123; buffer.put(i); &#125; // 转换，如果当前是写模式则转换为读模式 buffer.flip(); while (buffer.hasRemaining()) &#123; // get 有指针会向前移动 System.out.println(buffer.get()); &#125; &#125; @Test public void put() &#123; IntBuffer intBuffer = IntBuffer.allocate(5); // HeapIntBuffer 底层pos自动 + 1， flip 后 lim=pos， pos=1 intBuffer.put(1); intBuffer.flip(); // 输出 1 while (intBuffer.hasRemaining()) &#123; System.out.println(intBuffer.get()); &#125; &#125; @Test public void put2() &#123; IntBuffer intBuffer = IntBuffer.allocate(5); // 这里put底层，pos不会自动加1,导致flip后 lim = pos = 0 intBuffer.put(1, 2); intBuffer.flip(); // 重新设置限制，默认lim为0 intBuffer.limit(2); // lim和pos都为0 while (intBuffer.hasRemaining()) &#123; System.out.println(intBuffer.get()); &#125; // [0, 2, 0, 0, 0] System.out.println(Arrays.toString(intBuffer.array())); // 不设置intBuffer.limit(2) 则数组下标越界（lim为0） System.out.println(intBuffer.get(1)); &#125; @Test public void put3() &#123; IntBuffer intBuffer = IntBuffer.allocate(5); int[] bytes = &#123;1,2,3,4,5&#125;; intBuffer.put(bytes); intBuffer.flip(); // 输出 1，2，3，4，5 while (intBuffer.hasRemaining()) &#123; System.out.println(intBuffer.get()); &#125; &#125; @Test public void put4() &#123; IntBuffer intBuffer = IntBuffer.allocate(10); int[] array = &#123;1,2,3&#125;; // 第二个参数offse指的是数组的偏移量。第三个参数length 指的是取偏移量后几位 intBuffer.put(array, 1, 2); intBuffer.flip(); // 输出 2，3 while (intBuffer.hasRemaining()) &#123; System.out.println(intBuffer.get()); &#125; &#125;&#125; 上述这些方法需要自己控制 Buffer 大小，不能超过 capacity，超过会抛 java.nio.BufferOverflowException异常 读操作：对于 Buffer 来说，另一个常见的操作中就是，我们要将来自 Channel 的数据填充到 Buffer 中，在系统层面上，这个操作我们称为读操作，因为数据是从外部（文件或网络等）读到内存中 1int num = channel.read(buf); 上述方法会返回从 Channel 中读入到 Buffer 的数据大小 提取 Buffer 中的值 （读操作） 前面介绍了写操作，每写入一个值，position 的值都需要加 1，所以 position 最后会指向最后一次写入的位置的后面一个，如果 Buffer 写满了，那么 position 等于 capacity（position 从 0 开始） 如果要读 Buffer 中的值，需要切换模式，从写入模式切换到读出模式。注意，通常在说 NIO 的读操作的时候，我们说的是从 Channel 中读数据到 Buffer 中，对应的是对 Buffer 的写入操作，初学者需要理清楚这个 调用 Buffer 的 flip() 方法，可以从写入模式切换到读取模式。其实这个方法也就是设置了一下 position 和 limit 值罢了 123456public final Buffer flip() &#123; limit = position; // 将 limit 设置为实际写入的数据数量 position = 0; // 重置 position 为 0 mark = -1; // mark 之后再说 return this;&#125; 对应写入操作的一系列 put 方法，读操作提供了一系列的 get 方法 123456// 根据 position 来获取数据public abstract byte get();// 获取指定位置的数据public abstract byte get(int index);// 将 Buffer 中的数据写入到数组中public ByteBuffer get(byte[] dst) 123456789101112131415161718192021222324252627282930313233343536373839404142@Testpublic void get() &#123; IntBuffer intBuffer = IntBuffer.allocate(5); int[] bytes = &#123;1,2,3,4,5&#125;; intBuffer.put(bytes); intBuffer.flip(); // 1,2,3,4,5 while (intBuffer.hasRemaining()) &#123; // 根据 position 来获取数据, 每读取一次 position + 1 System.out.println(intBuffer.get()); &#125;&#125;@Testpublic void get2() &#123; IntBuffer intBuffer = IntBuffer.allocate(5); int[] bytes = &#123;1,2,3,4,5&#125;; intBuffer.put(bytes); intBuffer.flip(); // 3 System.out.println(intBuffer.get(2));&#125;@Testpublic void get3() &#123; IntBuffer intBuffer = IntBuffer.allocate(5); int[] bytes = &#123;1,2,3,4,5&#125;; intBuffer.put(bytes); intBuffer.flip(); int[] arr = new int[3]; // 将Buffer中的数据写入到数组中 intBuffer.get(arr); // 1,2,3 System.out.println(Arrays.toString(arr));&#125; 一个ByteBuffer经常使用的方法 1new String(buffer.array()).trim(); 当然了，除了将数据从 Buffer 取出来使用，更常见的操作是将我们写入的数据传输到 Channel 中，如通过 FileChannel 将数据写入到文件中，通过 SocketChannel 将数据写入网络发送到远程机器等。对应的，这种操作，我们称之为写操作 1int num = channel.write(buf); mark() &amp; reset() mark: 用于临时保存 position 的值，每次调用 mark() 方法都会将 mark 设值为当前的 position，便于后续需要的时候使用 reset: 把position设置成mark的值，相当于之前做过一个标记，现在要退回到之前标记(mark)的地方 1234public final Buffer mark() &#123; mark = position; return this;&#125; 那到底什么时候用呢？考虑以下场景，我们在 position 为 5 的时候，先 mark() 一下，然后继续往下读，读到第 10 的时候，我想重新回到 position 为 5 的地方重新来一遍，那只要调一下 reset() 方法，position 就回到 5 了 1234567public final Buffer reset() &#123; int m = mark; if (m &lt; 0) throw new InvalidMarkException(); position = m; return this;&#125; rewind() &amp; clear() &amp; compact() rewind：会重置 position 为 0，通常用于重新从头读写 Buffer,类似磁带倒带 场景：在写模式下重写数据到缓冲区 12345public final Buffer rewind() &#123; position = 0; mark = -1; return this;&#125; clear：有点重置 Buffer 的意思，相当于重新实例化了一样 场景：写 -&gt; 读 -&gt; clear -&gt; 继续写。通常，我们会先填充 Buffer，然后从 Buffer 读取数据，之后我们再重新往里填充新的数据，我们一般在重新填充之前先调用 clear() 123456public final Buffer clear() &#123; position = 0; limit = capacity; mark = -1; return this;&#125; compact: 和 clear() 一样的是，它们都是在准备往 Buffer 填充新的数据之前调用，相当于压缩 场景：Buffer数据还没完全读完就要继续写，那么就将未读取的数据重新压缩到Buffer头部方便下次去读 前面说的 clear() 方法会重置几个属性，但是我们要看到，clear() 方法并不会将 Buffer 中的数据清空，只不过后续的写入会覆盖掉原来的数据，也就相当于清空了数据了。 而 compact() 方法有点不一样，调用这个方法以后，会先处理还没有读取的数据，也就是 position 到 limit 之间的数据（还没有读过的数据），先将这些数据移到左边，然后在这个基础上再开始写入。很明显，此时 limit 还是等于 capacity，position 指向原来数据的右边 类型化的Buffer ByteBuffer 支持类型化的put 和 get, put 放入的是什么数据类型， get就应该使用相应的数据类型来取出， 否则可能有 BufferUnderflowException 异常 123456789101112131415161718@Testpublic void type() &#123; ByteBuffer buffer = ByteBuffer.allocate(64); //类型化方式放入数据 buffer.putInt(100); buffer.putLong(9); buffer.putChar(&#x27;A&#x27;); buffer.putShort((short) 4); buffer.flip(); // 如果取出的数据类型不匹配，则可能会抛出异常， 比如最后一行调用 buffer.getShort() 缓冲会溢出 // 如果取出的顺序不一致，则读取的数据会错乱 System.out.println(buffer.getInt()); System.out.println(buffer.getLong()); System.out.println(buffer.getChar()); System.out.println(buffer.getShort());&#125; 只读Buffer 可以将一个普通Buffer 转成只读Buffer,如果对 readOnlyBuffer进行 put 操作就会抛出异常 1234567891011121314151617181920212223@Testpublic void readTest() &#123; //创建一个buffer ByteBuffer buffer = ByteBuffer.allocate(3); for(int i = 0; i &lt; 3; i++) &#123; buffer.put((byte)i); &#125; //准备读取 buffer.flip(); //得到一个只读的Buffer HeapByteBufferR ByteBuffer readOnlyBuffer = buffer.asReadOnlyBuffer(); System.out.println(readOnlyBuffer.getClass()); while (readOnlyBuffer.hasRemaining()) &#123; System.out.println(readOnlyBuffer.get()); &#125; //ReadOnlyBufferException readOnlyBuffer.put((byte)100);&#125; MappedByteBuffer NIO 还提供了 MappedByteBuffer， 可以让文件直接在内存（堆外的内存） 中进行修改， 而如何同步到文件由NIO 来完成 这里底层其实就是使用到了零拷贝的 MMAP 技术 12345678910111213141516171819202122232425@Testpublic void mappedByteBufferTest() throws IOException &#123; RandomAccessFile randomAccessFile = new RandomAccessFile(&quot;/Volumes/app/test.txt&quot;, &quot;rw&quot;); //获取对应的通道 FileChannel channel = randomAccessFile.getChannel(); /** * 这里使用内存映射文件，可以提高内存的读写效率.实际上就是使用零拷贝的 MMAP * 参数1: FileChannel.MapMode.READ_WRITE 使用的读写模式 * 参数2： 0 ： 可以直接修改的起始位置 * 参数3: 5: 是映射到内存的大小(不是索引位置) ,即将 test.txt 的多少个字节映射到内存 * 可以直接修改的范围就是 0-5 * 实际类型 DirectByteBuffer */ MappedByteBuffer mappedByteBuffer = channel.map(FileChannel.MapMode.READ_WRITE, 0, 5); // MMAP 映射到内存的数据是可以直接修改的, 可以直接修改的范围是 0-5 数据在内核空间映射到用户空间 mappedByteBuffer.put(0, (byte) &#x27;H&#x27;); mappedByteBuffer.put(3, (byte) &#x27;9&#x27;); mappedByteBuffer.put(4, (byte) &#x27;Y&#x27;); // mappedByteBuffer.put(5, (byte) &#x27;Z&#x27;); //IndexOutOfBoundsException randomAccessFile.close(); System.out.println(&quot;修改成功~~&quot;); &#125; 这里的5是映射到内存的大小，不是索引位置，如果在操作第5个位置就会抛出异常 Buffer的分散和聚集 前面我们讲的读写操作， 都是通过一个Buffer 完成的， NIO 还支持 通过多个Buffer (即 Buffer 数组) 完成读写操作， 即 Scattering 和 Gathering Buffer 的分散和聚集 Scattering：将数据写入到buffer时，可以采用buffer数组，依次写入 [分散] 将Channel中的数据读取到多个Buffer 分离读取的时候，Channel写入buffer的数据是按顺序的，Scatter操作并不适合动态长度的数据传输，也就意味着传输数据的每一部分都是固定长度时，Scatter才能发挥它的作用 Gathering: 从buffer读取数据时，可以采用buffer数组，依次读 [聚集] Gather操作将多个buffer的数据写入到同一个Channel channel的write()方法可以接受buffer数据作为参数，write()方法会按照顺序将多个buffer中的数据依次写入channel。需要注意的是，write()操作只会写入buffer中已写入的数据，即position到limit之间的数据；例如一个buffer的容量为128字节，但buffer中只写入了28字节的数据，只有这28个字节会写入channel中，因此Gather操作非常适合动态长度数据写入 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657// 客户端使用名称 telnet localhost 8080 @Test public void test() throws IOException &#123; ServerSocketChannel serverSocketChannel = ServerSocketChannel.open(); // 监听 8080 端口 serverSocketChannel.socket().bind(new InetSocketAddress(8080)); //创建buffer数组 ByteBuffer[] byteBuffers = new ByteBuffer[2]; byteBuffers[0] = ByteBuffer.allocate(5); byteBuffers[1] = ByteBuffer.allocate(3); //等待客户端连接（telnet） SocketChannel socketChannel = serverSocketChannel.accept(); //假定从客户端接受8个字节 int messageLength = 8; //循环的读取 while (true) &#123; int byteRead = 0; while (byteRead &lt; messageLength) &#123; // 使用scattering方式读取 将数据读取到buffer数组中 long r = socketChannel.read(byteBuffers); System.out.println(&quot;r = &quot; + r); //累计读取的字节数 byteRead += 1; System.out.println(&quot;byteRead = &quot; + byteRead); //使用流打印, 看看当前的这个buffer的position 和 limit Arrays.asList(byteBuffers) .stream() .map(byteBuffer -&gt; &quot;position=&quot; + byteBuffer.position() + &quot;, limit=&quot; + byteBuffer.limit()) .forEach(System.out::println); &#125; //将所有的buffer进行flip Arrays.asList(byteBuffers).forEach(byteBuffer -&gt; byteBuffer.flip()); //将数据读出显示到客户端 int byteWrite = 0; while (byteWrite &lt; messageLength) &#123; // 使用gathering方式读取 将buffer数组中的数据写出到客户端 long w = socketChannel.write(byteBuffers); byteWrite += 1; System.out.println(&quot;byteWrite = &quot; + byteWrite); &#125; //将所有的buffer 进行clear Arrays.asList(byteBuffers).forEach(buffer -&gt; &#123; buffer.clear(); &#125;); System.out.println(&quot;byteRead:=&quot; + byteRead + &quot; byteWrite=&quot; + byteWrite + &quot;, messagelength=&quot; + messageLength); &#125; &#125; 总结 重要属性 属性 说明 Capacity 容量，即可以容纳的最大数据量；在缓冲区创建时被设定并且不能改变 Limit 表示缓冲区的当前终点，不能对缓冲区超过极限的位置进行读写操作。且极限是可以修改的 Position 位置，下一个要被读或写的元素的索引，每次读写缓冲区数据时都会改变改值，为下次读写作准备 Mark 标记，调用mark()来设置mark=position，再调用reset()可以让position恢复到标记的位置 实例化 方法 说明 allocate(int capacity) 从堆空间中分配一个容量大小为capacity的byte数组作为缓冲区的byte数据存储器 allocateDirect(int capacity) 是不使用JVM堆栈而是通过操作系统来创建内存块用作缓冲区，它与当前操作系统能够更好的耦合，因此能进一步提高I/O操作速度。但是分配直接缓冲区的系统开销很大，因此只有在缓冲区较大并长期存在，或者需要经常重用时，才使用这种缓冲区 wrap(byte[] array) 这个缓冲区的数据会存放在byte数组中，bytes数组或buff缓冲区任何一方中数据的改动都会影响另一方。其实ByteBuffer底层本来就有一个bytes数组负责来保存buffer缓冲区中的数据，通过allocate方法系统会帮你构造一个byte数组 wrap(byte[] array, int offset, int length) 在上一个方法的基础上可以指定偏移量和长度，这个offset也就是包装后byteBuffer的position，而length呢就是limit-position的大小，从而我们可以得到limit的位置为length+position(offset) 重要方法 方法 说明 limit(), limit(10)等 其中读取和设置这4个属性的方法的命名和jQuery中的val(),val(10)类似，一个负责get，一个负责set reset() 把position设置成mark的值，相当于之前做过一个标记，现在要退回到之前标记的地方 clear() position = 0;limit = capacity;mark = -1; 有点初始化的味道，但是并不影响底层byte数组的内容 flip() limit = position;position = 0;mark = -1; 翻转，也就是让flip之后的position到limit这块区域变成之前的0到position这块，翻转就是将一个处于存数据状态的缓冲区变为一个处于准备取数据的状态 rewind() 把position设为0，mark设为-1，不改变limit的值 remaining() return limit - position;返回limit和position之间相对位置差 hasRemaining() return position &lt; limit返回是否还有未读内容 compact() 把从position到limit中的内容移到0到limit-position的区域内，position和limit的取值也分别变成limit-position、capacity。如果先将positon设置到limit，再compact，那么相当于clear() get() 相对读，从position位置读取一个byte，并将position+1，为下次读写作准备 get(int index) 绝对读，读取byteBuffer底层的bytes中下标为index的byte，不改变position get(byte[]） 将 Buffer 中的数据写入到数组中 get(byte[] dst, int offset, int length) 从position位置开始相对读，读length个byte，并写入dst下标从offset到offset+length的区域 put(byte b) 相对写，向position的位置写入一个byte，并将postion+1，为下次读写作准备 put(int index, byte b) 绝对写，向byteBuffer底层的bytes中下标为index的位置插入byte b，不改变position put(ByteBuffer src) 用相对写，把src中可读的部分（也就是position到limit）写入此byteBuffer put(byte[] src, int offset, int length) 从src数组中的offset到offset+length区域读取数据并使用相对写写入此byteBuffer 相对读：position+1 绝对读：不改变position 相对写：postion+1 绝对写：不改变position 非直接缓冲区（直接在堆内存中开辟空间，也就是数组）： 竖左边的是操作系统（OS），右边是Java虚拟机(JVM),应用程序无论是读操作还是写操作都必须在OS和JVM之间进行复制 12通过allocate()方法分配缓冲区，将缓冲区建立在JVM的内存中ByteBuffer byteBuffer = ByteBuffer.allocate(1020); 直接缓冲区 解释：在NIO中，直接开辟物理内存映射文件，应用程序直接操作物理内存映射文件，这样就少了中间的copy过程，可以极大得提高读写效率。虽然直接缓冲可以进行高效的I/O操作，但它使用的内存是操作系统分配的，绕过了JVM堆栈，建立和销毁比堆栈上的缓冲区要更大的开销 12分配直接缓冲区ByteBuffer byteBuffer = ByteBuffer.allocateDirect(1020); 类型 优点 缺点 直接缓冲区 在虚拟机内存外，开辟的内存，IO操作直接进行,没有再次复制 创建和销毁开销大 非直接缓冲区 在虚拟机内存中创建，易回收 但占用虚拟机内存开销，处理中有复制过程。 内存映射文件MMAP 多个进程可以允许并发地内存映射同一文件，以便允许数据共享。任何一个进程的写入会修改虚拟内存的数据，并且其他映射同一文件部分的进程都可看到 内存映射的共享是实现：每个共享进程的虚拟内存映射指向物理内存的同一页面，而该页面有磁盘块的复制 总而言之，内存映射就是将每个磁盘块映射到内存中，一个进程或多个进程把对磁盘块的访问转化为对内存页的访问 内存映射文件的目的是实现内存共享，MMAP是零拷贝的一种实现 很多时候，共享内存实际上是通过内存映射来实现的。在这种情况下，进程可以通过共享内存来通信，而共享内存是通过映射同样文件到通信进程的虚拟地址空间来实现的。内存映射文件充当通信进程之间的共享内存区域 虚拟内存 虚拟内存是计算机系统内存管理的一种技术。它使得应用程序认为它拥有连续可用的内存（一个连续完整的地址空间），而实际上物理内存通常被分隔成多个内存碎片，还有部分暂时存储在外部磁盘存储器上，在需要时进行数据交换。与没有使用虚拟内存技术的系统相比，使用这种技术使得大型程序的编写变得更容易，对真正的物理内存（例如RAM）的使用也更有效率。此外，虚拟内存技术可以使多个进程共享同一个运行库，并通过分割不同进程的内存空间来提高系统的安全性 相关知识理论 什么是系统内核 内核是一个操作系统的核心。它负责管理系统的进程、内存、设备驱动程序、文件和网络系统等等，决定着系统的性能和稳定性。是连接应用程序和硬件的桥梁 内核空间和用户空间 原因： 保证系统运行的稳定性及可用性 应用程序无法直接完成读写磁盘文件，分配回收内存，从网络接口读写数据等底层硬件操作，必须由内核提供统一的接口完成 具体实现：比如 Intel 的 CPU 将特权等级分为 4 个级别：Ring0~Ring3 当进程运行在 Ring3 级别时被称为运行在用户态 而运行在 Ring0 级别时被称为运行在内核态 零拷贝技术 Channel 所有的 NIO 操作始于通道，通道是数据来源或数据写入的目的地，主要地，我们将关心 java.nio 包中实现的以下几个 Channel FileChannel：文件通道，用于文件的读和写 DatagramChannel：用于 UDP 连接的接收和发送 SocketChannel：把它理解为 TCP 连接通道，简单理解就是 TCP 客户端 ServerSocketChannel：TCP 对应的服务端，用于监听某个端口进来的请求 Channel 经常翻译为通道，类似 IO 中的流，用于读取和写入。它与前面介绍的 Buffer 打交道，读操作的时候将 Channel 中的数据填充到 Buffer 中，而写操作时将 Buffer 中的数据写入到 Channel 中 NIO的通道类似于流，但有些区别如下： 通道可以同时进行读写，而流只能读或者只能写 通道可以实现异步读写数据 通道可以从缓冲读数据，也可以写数据到缓冲 FileChannel 文件操作对于大家来说应该是最熟悉的，不过我们在说 NIO 的时候，其实 FileChannel 并不是关注的重点。而且后面我们说非阻塞的时候会看到，FileChannel 是不支持非阻塞的 FileChannel主要用来对本地文件进行 IO 操作，常见的方法有 1234public int read(ByteBuffer dst) ，从通道读取数据并放到缓冲区中public int write(ByteBuffer src) ，把缓冲区的数据写到通道中public long transferFrom(ReadableByteChannel src, long position, long count)，从目标通道 中复制数据到当前通道public long transferTo(long position, long count, WritableByteChannel target)，把数据从当 前通道复制给目标通道 流只能读或者写 1234567891011121314151617181920212223242526/** * 流只能读或者只能写 * @throws IOException */ @Test public void test() throws IOException &#123; // 写入数据 输出流 FileOutputStream fileOutputStream = new FileOutputStream(&quot;/Volumes/app/data.txt&quot;); FileChannel fileChannel = fileOutputStream.getChannel(); ByteBuffer byteBuffer = ByteBuffer.allocate(18); byteBuffer.put(&quot;写入测试数据&quot;.getBytes()); // 切换读模式 byteBuffer.flip(); fileChannel.write(byteBuffer); // 读取数据 byteBuffer.clear(); // 读取数据 输入流 FileInputStream fileInputStream = new FileInputStream(&quot;/Volumes/app/data.txt&quot;); fileChannel = fileInputStream.getChannel(); fileChannel.read(byteBuffer); System.out.println(new String(byteBuffer.array())); &#125; 通道可以从缓冲读数据，也可以写数据到缓冲 123456789101112131415161718192021@Testpublic void test2() throws IOException &#123;RandomAccessFile randomAccessFile = new RandomAccessFile(&quot;/Volumes/app/data.txt&quot;, &quot;rw&quot;);FileChannel fileChannel = randomAccessFile.getChannel();// 写入数据， utf-8 一个汉字三个字节ByteBuffer byteBuffer = ByteBuffer.allocate(18);byteBuffer.put(&quot;写入测试数据&quot;.getBytes());// 切换读模式byteBuffer.flip();// 读取Buffer的数据到channelwhile (byteBuffer.hasRemaining()) &#123; fileChannel.write(byteBuffer);&#125;byteBuffer.clear();fileChannel.read(byteBuffer);System.out.println(new String(byteBuffer.array()));&#125; 实现文件复制 12345678910111213141516171819202122232425262728@Test public void copy() throws IOException &#123; FileInputStream inputStream = new FileInputStream(&quot;/Volumes/app/data.txt&quot;); FileChannel inChannel = inputStream.getChannel(); FileOutputStream outputStream = new FileOutputStream(&quot;/Volumes/app/data2.txt&quot;); FileChannel outChannel = outputStream.getChannel(); ByteBuffer buffer = ByteBuffer.allocate(10); for (;;) &#123; // 这里一定要清理buffer.否则limit 和 position相等会读取不到数据 num一直等于0（buffer刚被读完状态） buffer.clear(); int num = inChannel.read(buffer); if (num == -1) &#123; break; &#125; buffer.flip(); outChannel.write(buffer); &#125; outChannel.close(); inChannel.close(); outputStream.close(); inputStream.close(); &#125; sendFile 零拷贝 1234567891011121314151617181920// 文件拷贝使用sendFile实现零拷贝@Testpublic void transferFrom() throws IOException &#123; //创建相关流 FileInputStream fileInputStream = new FileInputStream(&quot;/Volumes/app/data.txt&quot;); FileOutputStream fileOutputStream = new FileOutputStream(&quot;/Volumes/app/data2.txt&quot;); //获取各个流对应的filechannel FileChannel sourceCh = fileInputStream.getChannel(); FileChannel destCh = fileOutputStream.getChannel(); //使用transferForm完成拷贝 destCh.transferFrom(sourceCh, 0,sourceCh.size()); //关闭资源 sourceCh.close(); sourceCh.close(); fileInputStream.close(); fileOutputStream.close();&#125; 123456789101112131415161718192021222324252627282930313233// 网络传输文件实现零拷贝@Testpublic void transferTo() throws IOException &#123; File file = new File(&quot;/Volumes/app/data.txt&quot;); Long size = file.length(); try &#123; // 1.将test.txt文件内容读取到arr中 RandomAccessFile raFile = new RandomAccessFile(file, &quot;rwd&quot;); FileChannel channel = raFile.getChannel(); // 2.提供对外服务 ServerSocketChannel serverSocketChannel = ServerSocketChannel.open(); serverSocketChannel.socket().bind(new InetSocketAddress(8080)); serverSocketChannel.configureBlocking(false); while(true)&#123; SocketChannel socketChannel = serverSocketChannel.accept(); if(socketChannel != null)&#123; // 3.使用transferTo方法将文件数据传输到客户端 channel.transferTo(0, size, socketChannel); &#125; &#125; &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125;&#125; ServerSocketChannel SocketChannel可以看作是 TCP 客户端，那么 ServerSocketChannel 就是对应的服务端 123456789101112@Testpublic void test() throws IOException &#123;// 实例化ServerSocketChannel serverSocketChannel = ServerSocketChannel.open();// 监听 8080 端口serverSocketChannel.socket().bind(new InetSocketAddress(8080));while (true) &#123; // 一旦有一个 TCP 连接进来，就对应创建一个 SocketChannel 进行处理 SocketChannel socketChannel = serverSocketChannel.accept();&#125;&#125; 这里我们可以看到 SocketChannel 的另一个实例化方式 SocketChannel 它不仅仅是 TCP 客户端，它代表的是一个网络通道，可读可写 ServerSocketChannel 不和 Buffer 打交道了，因为它并不实际处理数据，它一旦接收到请求后，实例化 SocketChannel，之后在这个连接通道上的数据传递它就不管了，因为它需要继续监听端口，等待下一个连接 SocketChannel 可以将 SocketChannel 理解成一个 TCP 客户端或者一个可读可写的网络通道 NIO中的 ServerSocketChannel功能类似ServerSocket，SocketChannel功能类 似Socket 12345678public void test() throws IOException &#123;// 打开一个通道// SocketChannel socketChannel = SocketChannel.open();// 发起连接// socketChannel.connect(new InetSocketAddress(&quot;http://localhost&quot;, 8080));SocketChannel socketChannel = SocketChannel.open(new InetSocketAddress(&quot;localhost&quot;, 8080));&#125; SocketChannel 的读写和 FileChannel 没什么区别，就是操作缓冲区 1234567// 读取数据socketChannel.read(buffer);// 写入数据到网络连接中while(buffer.hasRemaining()) &#123;socketChannel.write(buffer); &#125; DatagramChannel UDP 和 TCP 不一样，DatagramChannel 一个类处理了服务端和客户端 UDP 是面向无连接的，不需要和对方握手，不需要通知对方，就可以直接将数据包投出去，至于能不能送达，它是不知道的 12345678910111213141516171819@Testpublic void test() throws IOException &#123; // 服务端 DatagramChannel server = DatagramChannel.open(); server.socket().bind(new InetSocketAddress(8080)); ByteBuffer buf = ByteBuffer.allocate(35); // 客户端发送数据 String newData = &quot;datagramChannel test &quot; + System.currentTimeMillis(); buf.put(newData.getBytes()); buf.flip(); int bytesSent = server.send(buf, new InetSocketAddress(&quot;localhost&quot;, 8080)); // 服务端监听 server.receive(buf); System.out.println(new String(buf.array())); &#125; 总结 通道类似于流，但是通道可以进行读和写 读和写 通道的read操作是将通道中的数据写到缓冲的过程 通道的writer操作是将缓冲的数据写到通道的过程 一个通道可以对应多个缓冲，能够对多个缓冲进行读写 Selector 介绍 Java 的 NIO，用非阻塞的 IO 方式。可以用一个线程，处理多个的客户端连接，就会使用到Selector(选择器或叫多路复用器) Selector 能够检测多个注册的通道上是否有事件发生(注意:多个Channel以事件的方式可以注册到同一个Selector)，如果有事件发生，便获取事件然后针对每个事件进行相应的处理。这样就可以只用一个单线程去管理多个通道，也就是管理多个连接和请求 只有在 通道(连接) 真正有读写事件发生时，才会进行读写，就大大地减少了系统开销，并且不必为每个连接都创建一个线程，不用去维护多个线程 避免了多线程之间的上下文切换导致的开销 说明 Netty 的 IO 线程 NioEventLoop 聚合了 Selector(选择器， 也叫多路复用器)，可以同时并发处理成百上千个客户端连接 当线程从某客户端 Socket 通道进行读写数据时，若没有数据可用时，该线程可以进行其他任务 线程通常将非阻塞 IO 的空闲时间用于在其他通道上 执行 IO 操作，所以单独的线程可以管理多个输入和 输出通道 由于读写操作都是非阻塞的，这就可以充分提升 IO 线程的运行效率，避免由于频繁 I/O 阻塞导致的线程挂起（BIO数据没就绪就会阻塞线程） 一个 I/O 线程可以并发处理 N 个客户端连接和读写操作，这从根本上解决了传统同步阻塞 I/O 一连接一线程模型，架构的性能、弹性伸缩能力和可靠性都得到 了极大的提升 Selector常用方法 Selector是一个抽象类 源码 1234567891011public abstract class Selector implements Closeable &#123; public static Selector open();//得到一个选择器对象 public int select(long timeout);//监控所有注册的通道，当其中有 IO 操作可以进行时，将对应的 SelectionKey 加入到内部集合中并返回，参数用来设置超时时间 public abstract Set&lt;SelectionKey&gt; keys();//获取当前channel注册在Selector上所有的key public abstract Set&lt;SelectionKey&gt; selectedKeys();//从内部集合中得到所有的 SelectionKey public abstract int selectNow() throws IOException;//不阻塞，立马返还 public abstract int select(long timeout)throws IOException;//阻塞1000毫秒，在1000毫秒后返回 public abstract int select() throws IOException;//阻塞,监控所有注册的通,至少有一个已注册的Channel发生了事件才返回 public abstract Selector wakeup();//唤醒 public abstract void close() throws IOException;//关闭&#125; SelectionKey SelectionKey，表示 Selector 和Channel的注册关系 当我们调用**channel.register(selector, SelectionKey.OP_READ, buffer);方法时，会将通道channel注册到选择器selector上并监听指定事件，同时会返回一个SelectionKey选择键对象，这个键对象标识了通道和选择器之间的注册关系。选择键会记住您关心的通道。它们也会追踪对应的通道是否已经就绪**。当您调用一个选择器对象的select( )方法时，相关的键建会被更新，用来检查所有被注册到该选择器的通道。您可以获取一个就绪键的集合，从而找到当时已经就绪的通道。通过遍历这些键，您可以选择出每个从上次您调用select( )开始直到现在，已经就绪的通道 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960public abstract class SelectionKey &#123; //以下四个常量是定义的通道的四种操作 // 代表读操作 public static final int OP_READ = 1; // 代表写操作 public static final int OP_WRITE = 4; // 代表连接已经建立 public static final int OP_CONNECT = 8; //有新的网络连接可以 accept public static final int OP_ACCEPT = 16; //可以针对指定的通道附加一个对象 private volatile Object attachment = null; // 获得通道对象 public abstract SelectableChannel channel(); // 获得通道所注册的选择器 public abstract Selector selector(); // 验证通道和选择器之间的注册关系是否还生效 public abstract boolean isValid(); //将通道放入选择器的已注销集合中 public abstract void cancel(); //获得该通道所感兴趣的操作 public abstract int interestOps(); //设置通道感兴趣的操作 public abstract SelectionKey interestOps(int paramInt); //获得通道已准备好的操作 public abstract int readyOps(); //是否可以读 public final boolean isReadable() &#123; return ((readyOps() &amp; 0x1) != 0); &#125; //是否可以写 public final boolean isWritable() &#123; return ((readyOps() &amp; 0x4) != 0); &#125; // 是否可以 accept public final boolean isConnectable() &#123; return ((readyOps() &amp; 0x8) != 0); &#125; //是否可以 accept public final boolean isAcceptable() &#123; return ((readyOps() &amp; 0x10) != 0); &#125; //为SelectionKey绑定附加对象 public final Object attach(Object paramObject) &#123; Object localObject = this.attachment; this.attachment = paramObject; return localObject; &#125; //绑定之后，可通过对应的SelectionKey取出该对象 public final Object attachment() &#123; return this.attachment; &#125;&#125; 当客户端连接时，会通过ServletSocketChannel得到对应的StoketChannel Selector进行事件监听（select方法），返回有事件发生的通道个数 将SocketChannel注册到Selector上，一个Selector可以注册多个Channel（比如：SelectableChannel:register(Selector sel, int ops) 其中ops为事件类型） 注册后返回一个SelectionKey,会和该Selector关联（Selector维护一个集合） 进一步得到SelectionKey(有事件发生) 通过SelectionKey反向获得SocketChannel(比如：SelectionKey:channel()) 通过得到的Channel配合Buffer完成读写操作 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778public class SelectorTest &#123; @Test public void server() throws IOException &#123; // 1.打开一个服务端通道 ServerSocketChannel serverSocketChannel = ServerSocketChannel.open(); // 2.绑定端口号 serverSocketChannel.socket().bind(new InetSocketAddress(8080)); // 3.通道默认是阻塞的,设置为非阻塞 serverSocketChannel.configureBlocking(false); // 4.创建一个选择器 // 真实类型 // Windows平台: WindowsSelectorImpl // Linux平台: EpollSelectorImpl // Mac平台: KQueueSelectorImpl Selector selector = Selector.open(); // 5.将通道注册到选择器上, 并且指定监听事件 OP_ACCEPT serverSocketChannel.register(selector, SelectionKey.OP_ACCEPT); // 6.轮询式的获取选择器上已经准备就绪的事件 while (true) &#123; // 7.轮询获取已经就绪的事件 if (selector.select(1000) == 0) &#123; // 8.如果3秒钟内没有获取到已经就绪的事件, 则打印一个警告信息 System.out.println(&quot;没有获取到已经就绪的事件&quot;); continue; &#125; // 8.获取已经就绪的事件(可能有多个就绪的事件,所以是集合) Set&lt;SelectionKey&gt; selectionKeys = selector.selectedKeys(); Iterator&lt;SelectionKey&gt; iterator = selectionKeys.iterator(); while (iterator.hasNext()) &#123; SelectionKey selectionKey = iterator.next(); // 9.判断就绪时间类型, 如果是OP_ACCEPT, 则表示有新的客户端连接到来 if (selectionKey.isAcceptable()) &#123; // 10.获取到客户端通道 SocketChannel socketChannel = serverSocketChannel.accept(); // 11.设置为非阻塞 socketChannel.configureBlocking(false); // 12.将客户端通道注册到选择器上, 并且指定监听事件 OP_READ, 再添加一个绑定对象 socketChannel.register(selector, SelectionKey.OP_READ, ByteBuffer.allocate(1024)); &#125; // 13.如果是OP_READ, 则表示有客户端数据发送过来 if (selectionKey.isReadable()) &#123; // 14.通过selectionKey获取到客户端通道 SocketChannel socketChannel = (SocketChannel) selectionKey.channel(); // 15. 获取绑定对象 ByteBuffer byteBuffer = (ByteBuffer) selectionKey.attachment(); // 16.读取数据 int read = socketChannel.read(byteBuffer); System.out.println(new String(byteBuffer.array(), 0, read, StandardCharsets.UTF_8)); // 17.向客户端发送数据 socketChannel.write(ByteBuffer.wrap(&quot;hello client&quot;.getBytes(StandardCharsets.UTF_8))); // 通道一旦关闭就不能被再次打开 socketChannel.close(); &#125; // 18.清空已处理的事件 iterator.remove(); &#125; &#125; &#125; @Test public void client() throws IOException, InterruptedException &#123; // 1.打开一个客户端通道 SocketChannel socketChannel = SocketChannel.open(new InetSocketAddress(&quot;localhost&quot;, 8080)); // 2.设置为非阻塞 socketChannel.configureBlocking(false); // 3.发送数据 socketChannel.write(ByteBuffer.wrap(&quot;hello server&quot;.getBytes(StandardCharsets.UTF_8))); // 4.客户端读取服务端发送过来的数据 ByteBuffer byteBuffer = ByteBuffer.allocate(1024); int read = socketChannel.read(byteBuffer); System.out.println(new String(byteBuffer.array(), 0, read, StandardCharsets.UTF_8)); &#125;&#125; 总结 Channel注册到Selector中必须设置需要关注的事件 Channel注册到Selector会返回一个SelectionKey,可以通过这个Key反向获取到和客户端连接的Channel 一个Selector可以注册多个Channel 当触发了注册的事件后,数据准备就绪就会将SelectionKey放到Selector的selectedKeys中,可以遍历这个Set去处理就绪事件 NIO的非阻塞特性需要手动设置SelectableChannel.configureBlocking(false) NIO 网络编程 要求 内核角度看NIO 聊聊Netty那些事儿之从内核角度看IO模型 Reactor线程模型 Reactor模式 是一种「事件驱动」模式 「Reactor线程模型」就是通过单个线程使用Java NIO包中的Selector的select()方法，进行监听。当获取到事件(如accept、read等)后，就会分配(dispatch)事件进行相应的事件处理(handle) 如果要给 Reactor线程模型 下一个更明确的定义，应该是 Reactor线程模式 = Reactor(I/O多路复用)+ 线程池 其中Reactor负责监听和分配事件，线程池负责处理事件 单Reactor单线程 说明: Select 是前面 I/O 复用模型介绍的标准网络编程API，可以实现应用程序通过一个阻塞对象监听多路连接请求(IO多路复用) Reactor 对象通过 Select 监控客户端请求事件，收到事件后通过 Dispatch 进行分发 如果是建立连接请求事件，则由 Acceptor 通过 accept 处理连接请求，然后创建一个 Handler 对象处理连接完成后的后续业务处理 如果不是建立连接事件，则 Reactor 会分发调用连接对应的 Handler 来响应 Handler 会完成 Read→业务处理→Send 的完整业务流程.服务器端用一个线程通过多路复用搞定所有的 IO 操作（包括连接，读、写 等），编码简单，清晰明了，但是如果客户端连接数量较多，将无法支撑 优点: 模型简单，没有多线程、进程通信、竞争的问题，全部都在一个线程中完成 缺点: 性能问题，只有一个线程，无法完全发挥多核 CPU 的性能。Handler 在处理某 个连接上的业务时，整个进程无法处理其他连接事件，很容易导致性能瓶颈 可靠性问题，线程意外终止，或者进入死循环，会导致整个系统通信模块不 可用，不能接收和处理外部消息，造成节点故障 使用场景: 客户端的数量有限，业务处理非常快速，比如 Redis在业务处理的时间复 杂度 O(1) 的情况 单Reactor多线程 说明: Reactor 对象通过 select 监控客户端请求事件, 收到事件后，通过 dispatch 进行分发 如果建立连接请求, 则由 Acceptor 通过 accept 处理连接请求, 然后创建一个Handler对象处理完成连接后的各种事件 如果不是连接请求，则由 Reactor 分发调用连接对 应的Handler 来处理 Handler 只负责响应事件，不做具体的业务处理, 通过 read 读取数据后，会分发给后面的 Worker 线程池的某个线程处理业务 Worker 线程池会分配独立线程完成真正的业务， 并将结果返回给 Handler Handler 收到响应后，通过 send 将结果返回给 Client 优点: 可以充分的利用多核cpu 的处理能力 缺点: 多线程数据共享和访问比较复杂,单 Reactor 处理所有的事件的监听和响应，在单线程运行， 在高并发场景容易出现性能瓶颈 主从Reactor多线程 说明: Reactor 主线程 MainReactor 对象通过 select 监听连接事件, 收到事件后，通过 Acceptor 处理连接事件(主 Reactor 只处理连接事件) 当 Acceptor 处理连接事件后，MainReactor 将连接分配给 SubReactor SubReactor 将连接加入到连接队列进行监听,并创建 Handler 进行各种事件处理 当有新事件发生时， SubReactor 就会调用对应的 Handler处理，Handler 通过 read 读取数据，分发给后面的 （Worker 线程池）处理 （Worker 线程池）分配独立的 （Worker 线程）进行业务处理，并返 回结果 Handler 收到响应的结果后，再通过 send 将结果返回给 Client 一个 MainReactor 可以关联多个 SubReactor 优点: 父线程与子线程的数据交互简单职责明确，父线程只需要接收新连接，子线程完成后续的业务处理 父线程与子线程的数据交互简单，Reactor 主线程只需要把新连接传给子线程，子线程无需返回数据 缺点: 编程复杂度较高 结合实例: 这种模型在许多项目中广泛使用，包括 Nginx 主从 Reactor 多进程模型， Memcached 主从多线程，Netty 主从多线程模型的支持 小结 单 Reactor 单线程，前台接待员和服务员是同一个人，全程为顾客服 单 Reactor 多线程，1 个前台接待员，多个服务员，接待员只负责接待 主从 Reactor 多线程，多个前台接待员，多个服务员 Reactor 模式具有如下的优点： 响应快，不必为单个同步事件所阻塞，虽然 Reactor 本身依然是同步的 可以最大程度的避免复杂的多线程及同步问题，并且避免了多线程/进程 的切换开销 扩展性好，可以方便的通过增加 Reactor 实例个数来充分利用 CPU 资源 复用性好，Reactor 模型本身与具体事件处理逻辑无关，具有很高的复用性 Netty模型 简单版 BossGroup线程维护Selector, 只关注Accecpt（只处理连接事件） 当接收到Accept事件，获取到对应的 SocketChannel, 封装成 NIOScoketChannel并注册到 Worker 线程(事件循环), 并进行维护 当Worker线程监听到 Selector 中通道发生自己感兴趣的事件后（监听事件），就进行处理(就由 Handler 处理)， 注意 Handler 已经加入到通道 进阶版 Netty主要基于主从Reactor多线程模型做了一定的改进，其中主从Reactor多线程模型有多个Reactor 每个Reactor都可以看作是一个NIOEventLoop 最终版 Netty 抽象出两组线程池 ，BossGroup 专门负责接收客户端的连接，WorkerGroup 专门负责网络的读写 BossGroup 和 WorkerGroup 类型都是 NioEventLoopGroup (NIO事件循环组) NioEventLoopGroup 相当于一个事件循环组，这个组中含有多个事件循环，每一个事件循环是 NioEventLoop NioEventLoop 表示一个不断循环的执行处理任务的线程(selector监听绑定事件是否发生，因为是非阻塞的所以需要不断循环)，每个 NioEventLoop 都有一个 Selector，用于监听绑定在其上的 socket 的网络通讯，比如NioServerSocketChannel绑定在服务器boosgroup的NioEventLoop的selector上，NioSocketChannel绑定在客户端的NioEventLoop的selector上，然后各自的selector就不断循环监听相关事件 NioEventLoopGroup 可以有多个线程，即可以含有多个 NioEventLoop 每个 BossGroup下面的NioEventLoop 循环执行的步骤有 3 步 轮询 accept 事件 处理 accept 事件，与 client 建立连接，生成 NioScocketChannel，并将其注册到某个 workerGroup NIOEventLoop 上的 Selector 继续处理任务队列的任务，即 runAllTasks（同一时间可能会有多个连接，就绪的任务就放在TaskQueue中） 每个 WorkerGroup NIOEventLoop 循环执行的步骤 轮询 read，write 事件 处理 I/O 事件，即 read，write 事件，在对应 NioScocketChannel 处理 处理任务队列的任务，即 runAllTasks 每个 Worker NIOEventLoop 处理业务时，会使用 pipeline（管道），pipeline 中包含了 channel（通道），即通过 pipeline 可以获取到对应通道，管道中维护了很多的Handler NioEventLoop 内部采用串行化设计，从消息的 读取-&gt;解码-&gt;处理-&gt;编码-&gt;发送，始终由 IO 线程 NioEventLoop 负责 NioEventLoopGroup 下包含多个 NioEventLoop 每个 NioEventLoop 中包含有一个 Selector，一个 taskQueue 每个 NioEventLoop 的 Selector 上可以注册监听多个 NioChannel 每个 NioChannel 只会绑定在唯一的 NioEventLoop 上 每个 NioChannel 都绑定有一个自己的 ChannelPipeline NioChannel可以获取对应的ChannelPipeline，ChannelPipeline也可以获取对应的NioChannel ChannelPipeline 管道：比如水管，水管是由多个组件组成的，比如，弯头，开关，变径等。这些组件就好比是Handler, 当水（NioChannel）流过管道（ChannelPipeline）就会经过经过管道的组件，水就会被管道组件（Handler）所处理 Netty TCP服务例子 使用Netty开发，当客户端连接上服务端后发送消息，服务端接收消息后相应客户端 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677/** * Netty 服务端 */public class NettyServer &#123; private final static int PORT = 6666; public static void main(String[] args) throws Exception &#123; //创建BossGroup 和 WorkerGroup //说明 //1. 创建两个线程组 bossGroup 和 workerGroup //2. bossGroup 只是处理连接请求 , 真正的和客户端业务处理，会交给 workerGroup完成 //3. 两个都是无限循环 //4. bossGroup 和 workerGroup 含有的子线程(NioEventLoop)的个数 // 默认实际 cpu核数 * 2 EventLoopGroup bossGroup = new NioEventLoopGroup(1); EventLoopGroup workerGroup = new NioEventLoopGroup(); //ServerBootstrap作为一个启动辅助类，通过他可以很方便的创建一个Netty服务端。从EchoServer可以看出，实现一个Server大概分为这样几个步骤： // //1.设置bossGroup和workGroup。这里可以先理解为两个线程池，bossGroup设置一个线程，用于处理连接请求和建立连接， // 而workGroup线程池大小默认值2*CPU核数，在连接建立之后处理IO请求。 // EventLoopGroup体现了Netty对线程模型的抽象设计 //2.指定使用NioServerSocketChannel来处理连接请求。 // channel(NioServerSocketChannel.class)这段代码实际上在设置channelFactory，而ServerBootstrap会通过channelFactory.newChannel来生产channel。 // 这里channelFactory是一个ReflectiveChannelFactory，顾名思义这个工厂类以反射的方式来构建channel实例，而实例的类型就是我们指定的NioServerSocketChannel。 // 3.配置TCP参数。 // 4.配置handler和childHandler，数据处理器。 // 5.ServerBootstrap启动服务器。 // 真正的启动过程由ChannelFuture f = b.bind(PORT).sync();开始触发。调用链路：ServerBootstrap.bind → AbstractBootstrap.bind → AbstractBootstrap.doBind try &#123; //创建服务器端的启动对象，配置参数 ServerBootstrap bootstrap = new ServerBootstrap(); //配置 //使用链式编程来进行设置 bootstrap.group(bossGroup, workerGroup) // 设置两个线程组 .channel(NioServerSocketChannel.class) //使用NioServerSocketChannel 作为服务器的通道实现 .option(ChannelOption.SO_BACKLOG, 128) // 设置线程taskQueue的长度 .childOption(ChannelOption.SO_KEEPALIVE, true) // 设置保持活动连接状态 //.handler(null) // 该 handler对应 bossGroup , childHandler 对应 workerGroup .childHandler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; //创建一个通道初始化对象(匿名对象) //给pipeline 设置处理器 @Override protected void initChannel(SocketChannel socketChannel) throws Exception &#123; System.out.println(&quot;client socketChannel hashCode = &quot; + socketChannel.hashCode()); socketChannel.pipeline().addLast(new NettyServerHandler()); &#125; &#125;); System.out.println(&quot;server is ready&quot;); //绑定一个端口并且同步, 生成了一个 ChannelFuture 对象 //启动服务器(并绑定端口) ChannelFuture channelFuture = bootstrap.bind(PORT).sync(); //给channelFuture 注册监听器，监控我们关心的事件 channelFuture.addListener(new ChannelFutureListener() &#123; @Override public void operationComplete(ChannelFuture future) throws Exception &#123; if(future.isSuccess()) &#123; System.out.println(&quot;监听端口 6666 成功&quot;); &#125; else &#123; System.out.println(&quot;监听端口 6666 失败&quot;); &#125; &#125; &#125;); //对关闭通道进行监听 channelFuture.channel().closeFuture().sync(); &#125; finally &#123; // 优雅停机 bossGroup.shutdownGracefully(); workerGroup.shutdownGracefully(); &#125; &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142/*说明1. 我们自定义一个Handler 需要继续netty 规定好的某个HandlerAdapter(规范)2. 这时我们自定义一个Handler , 才能称为一个handler */public class NettyServerHandler extends ChannelInboundHandlerAdapter &#123; //读取数据实际(这里我们可以读取客户端发送的消息) /* 1. ChannelHandlerContext ctx:上下文对象, 含有 管道pipeline , 通道channel, 地址 2. Object msg: 就是客户端发送的数据 默认Object */ @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; System.out.println(&quot;服务器读取线程 &quot; + Thread.currentThread().getName() + &quot; channle =&quot; + ctx.channel()); System.out.println(&quot;server ctx =&quot; + ctx); System.out.println(&quot;看看channel 和 pipeline的关系&quot;); Channel channel = ctx.channel(); ChannelPipeline pipeline = ctx.pipeline(); //本质是一个双向链接, 出站入站 //将 msg 转成一个 ByteBuf //ByteBuf 是 Netty 提供的，不是 NIO 的 ByteBuffer. ByteBuf byteBuf = (ByteBuf) msg; System.out.println(&quot;客户端发送消息是:&quot; + byteBuf.toString(CharsetUtil.UTF_8)); System.out.println(&quot;客户端地址:&quot; + channel.remoteAddress()); &#125; // 数据读取完毕 @Override public void channelReadComplete(ChannelHandlerContext ctx) throws Exception &#123; //writeAndFlush 是 write + flush //将数据写入到缓存，并刷新 //一般讲，我们对这个发送的数据进行编码 ctx.writeAndFlush(Unpooled.copiedBuffer(&quot;hello client !!!&quot;, CharsetUtil.UTF_8)); &#125; //处理异常, 一般是需要关闭通道 @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; ctx.close(); &#125;&#125; 1234567891011121314151617181920212223242526272829303132/** * Netty客户端 */public class NettyClient &#123; public static void main(String[] args) throws Exception &#123; // 客户端需要一个时间循环组 EventLoopGroup eventLoopGroup = new NioEventLoopGroup(); try &#123; //创建客户端启动对象 //注意客户端使用的不是 ServerBootstrap 而是 Bootstrap Bootstrap bootstrap = new Bootstrap(); bootstrap.group(eventLoopGroup) .channel(NioSocketChannel.class) // 设置客户端通道的实现类(反射) .handler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override protected void initChannel(SocketChannel ch) throws Exception &#123; //给pipeline 设置处理器 ch.pipeline().addLast(new NettyClientHandler()); &#125; &#125;); ChannelFuture channelFuture = bootstrap.connect(&quot;localhost&quot;, 6666).sync(); // 对关闭通道进行监听 channelFuture.channel().closeFuture().sync(); &#125; finally &#123; // 优雅停机 eventLoopGroup.shutdownGracefully(); &#125; &#125;&#125; 12345678910111213141516171819202122232425262728/** * 客户端处理类 */public class NettyClientHandler extends ChannelInboundHandlerAdapter &#123; // 当通道准备就绪就会触发该方法 @Override public void channelActive(ChannelHandlerContext ctx) throws Exception &#123; // 发送消息给服务端 System.out.println(&quot;client &quot; + ctx); ctx.writeAndFlush(Unpooled.copiedBuffer(&quot;hello server !!&quot;, CharsetUtil.UTF_8)); &#125; // 当通道就就绪的读取时间时，会触发 @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; ByteBuf buf = (ByteBuf) msg; System.out.println(&quot;服务端回复的消息是：&quot; + buf.toString(CharsetUtil.UTF_8)); System.out.println(&quot;服务器地址是：&quot; + ctx.channel().remoteAddress()); &#125; // 异常处理 @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; cause.printStackTrace(); ctx.close(); &#125;&#125; 自定义任务 当需要执行比较耗时的任务时，为了避免长时间阻塞EventLoop,可以将任务提交给taskQueue,由任务队列异步执行 用户自定义普通任务，修改NettyServerHandler 1234567891011121314151617181920212223242526272829303132333435363738// 数据读取完毕 @Override public void channelReadComplete(ChannelHandlerContext ctx) throws Exception &#123; // 添加耗时长的自定义任务， 异步执行10秒, 如果添加多个则串行执行 ctx.channel().eventLoop().execute(() -&gt; &#123; try &#123; // 假设任务需要执行10 TimeUnit.SECONDS.sleep(10); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread().getName()); ctx.channel().writeAndFlush(Unpooled.copiedBuffer(&quot;自定义任务响应&quot;, CharsetUtil.UTF_8)); &#125;); //writeAndFlush 是 write + flush //将数据写入到缓存，并刷新 //一般讲，我们对这个发送的数据进行编码 System.out.println(Thread.currentThread().getName()); ctx.writeAndFlush(Unpooled.copiedBuffer(&quot;hello client !!!&quot;, CharsetUtil.UTF_8)); &#125;启动服务端和客户端后服务端输出：客户端发送消息是:hello server !!客户端地址:/127.0.0.1:57026nioEventLoopGroup-3-1nioEventLoopGroup-3-1 客户端输出：服务端回复的消息是：hello client !!!服务器地址是：localhost/127.0.0.1:6666服务端回复的消息是：自定义任务响应服务器地址是：localhost/127.0.0.1:6666 由此可见，用户自定义的任务不会阻塞正常的流程，并且taskQueue执行的线程是EventLoop这个线程 定时任务 定时任务，按照指定时间间隔触发，异步执行。任务是提交到scheduledTaskQueue 用户定义定时任务，修改NettyServerHandler 123456789101112131415161718192021222324252627282930313233343536373839404142434445 // 数据读取完毕 @Override public void channelReadComplete(ChannelHandlerContext ctx) throws Exception &#123; // 添加耗时长的自定义任务， 异步执行 ctx.channel().eventLoop().execute(() -&gt; &#123; try &#123; TimeUnit.SECONDS.sleep(10); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; log.info(Thread.currentThread().getName()); ctx.channel().writeAndFlush(Unpooled.copiedBuffer(&quot;自定义任务响应&quot;, CharsetUtil.UTF_8)); &#125;); // 添加定时任务, 10秒执行一次 ctx.channel().eventLoop().schedule(() -&gt; &#123; log.info(Thread.currentThread().getName()); ctx.channel().writeAndFlush(Unpooled.copiedBuffer(&quot;定时任务响应&quot;, CharsetUtil.UTF_8)); &#125;, 10, TimeUnit.SECONDS); //writeAndFlush 是 write + flush //将数据写入到缓存，并刷新 //一般讲，我们对这个发送的数据进行编码 log.info(Thread.currentThread().getName()); ctx.writeAndFlush(Unpooled.copiedBuffer(&quot;hello client !!!&quot;, CharsetUtil.UTF_8)); &#125;启动服务端和客户端后服务端输出：客户端发送消息是:hello server !!客户端地址:/127.0.0.1:5896419:17:19.889 [nioEventLoopGroup-3-1] INFO com.wgf.tcp.NettyServerHandler - nioEventLoopGroup-3-119:17:29.912 [nioEventLoopGroup-3-1] INFO com.wgf.tcp.NettyServerHandler - nioEventLoopGroup-3-119:17:29.912 [nioEventLoopGroup-3-1] INFO com.wgf.tcp.NettyServerHandler - nioEventLoopGroup-3-1 客户端输出：19:17:19.905 [nioEventLoopGroup-2-1] INFO com.wgf.tcp.NettyClientHandler - 服务端回复的消息是：hello client !!!19:17:19.905 [nioEventLoopGroup-2-1] INFO com.wgf.tcp.NettyClientHandler - 服务器地址是：localhost/127.0.0.1:666619:17:29.912 [nioEventLoopGroup-2-1] INFO com.wgf.tcp.NettyClientHandler - 服务端回复的消息是：自定义任务响应19:17:29.912 [nioEventLoopGroup-2-1] INFO com.wgf.tcp.NettyClientHandler - 服务器地址是：localhost/127.0.0.1:666619:17:29.912 [nioEventLoopGroup-2-1] INFO com.wgf.tcp.NettyClientHandler - 服务端回复的消息是：定时任务响应19:17:29.912 [nioEventLoopGroup-2-1] INFO com.wgf.tcp.NettyClientHandler - 服务器地址是：localhost/127.0.0.1:6666 scheduledTaskQueue的执行线程是EventLoop这个线程 非Reachor线程调用Channel 例如在推送系统的业务线程里面，根据用户的标识，找到对应用户的Channel引用，然后调用Channel的writer方法向用户推送消息 用户定义定时任务，修改NettyServerHandler 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778@Slf4jpublic class NettyServerHandler extends ChannelInboundHandlerAdapter &#123; // channel 用户列表 public static final Set&lt;Channel&gt; channelSet = new HashSet&lt;&gt;(); // 构造器执行定时任务 public NettyServerHandler() &#123; // 模拟推送业务 pushMsg(); &#125; //读取数据实际(这里我们可以读取客户端发送的消息) /* 1. ChannelHandlerContext ctx:上下文对象, 含有 管道pipeline , 通道channel, 地址 2. Object msg: 就是客户端发送的数据 默认Object */ @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; System.out.println(&quot;服务器读取线程 &quot; + Thread.currentThread().getName() + &quot; channle =&quot; + ctx.channel()); System.out.println(&quot;server ctx =&quot; + ctx); System.out.println(&quot;看看channel 和 pipeline的关系&quot;); Channel channel = ctx.channel(); ChannelPipeline pipeline = ctx.pipeline(); //本质是一个双向链接, 出站入站 //将 msg 转成一个 ByteBuf //ByteBuf 是 Netty 提供的，不是 NIO 的 ByteBuffer. ByteBuf byteBuf = (ByteBuf) msg; System.out.println(&quot;客户端发送消息是:&quot; + byteBuf.toString(CharsetUtil.UTF_8)); System.out.println(&quot;客户端地址:&quot; + channel.remoteAddress()); &#125; // 数据读取完毕 @Override public void channelReadComplete(ChannelHandlerContext ctx) throws Exception &#123; channelSet.add(ctx.channel()); //writeAndFlush 是 write + flush //将数据写入到缓存，并刷新 //一般讲，我们对这个发送的数据进行编码 log.info(Thread.currentThread().getName()); ctx.writeAndFlush(Unpooled.copiedBuffer(&quot;hello client !!!&quot;, CharsetUtil.UTF_8)); &#125; // 定时向客户端推送在线人数 private void pushMsg() &#123; // 非Reactor线程 ScheduledExecutorService scheduledExecutorService = Executors.newScheduledThreadPool(1); scheduledExecutorService.scheduleAtFixedRate(() -&gt; &#123; int count = channelSet.size(); if (count &gt; 0) &#123; Iterator&lt;Channel&gt; iterator = channelSet.iterator(); while (iterator.hasNext()) &#123; Channel next = iterator.next(); if (!next.isActive()) &#123; iterator.remove(); &#125; else &#123; next.pipeline().writeAndFlush(Unpooled.copiedBuffer(&quot;当前人数：&quot; + count, CharsetUtil.UTF_8)); &#125; &#125; &#125; &#125;, 5, 5, TimeUnit.SECONDS); &#125; //处理异常, 一般是需要关闭通道 @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; ctx.close(); &#125;&#125;客户端输出：19:49:46.322 [nioEventLoopGroup-2-1] INFO com.wgf.tcp.NettyClientHandler - 服务端回复的消息是：hello client !!!19:49:46.322 [nioEventLoopGroup-2-1] INFO com.wgf.tcp.NettyClientHandler - 服务器地址是：localhost/127.0.0.1:666619:49:51.287 [nioEventLoopGroup-2-1] INFO com.wgf.tcp.NettyClientHandler - 服务端回复的消息是：当前人数：119:49:51.287 [nioEventLoopGroup-2-1] INFO com.wgf.tcp.NettyClientHandler - 服务器地址是：localhost/127.0.0.1:666619:49:56.275 [nioEventLoopGroup-2-1] INFO com.wgf.tcp.NettyClientHandler - 服务端回复的消息是：当前人数：119:49:56.275 [nioEventLoopGroup-2-1] INFO com.wgf.tcp.NettyClientHandler - 服务器地址是：localhost/127.0.0.1:6666 Netty源码剖析 客户端Bootstrap启动过程 Netty 中 Bootstrap 类是客户端程序的启动引导类 使用官方Example EchoClient, 下面的源码剖析都根据这个Example进行 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public final class EchoClient &#123; static final boolean SSL = System.getProperty(&quot;ssl&quot;) != null; static final String HOST = System.getProperty(&quot;host&quot;, &quot;127.0.0.1&quot;); static final int PORT = Integer.parseInt(System.getProperty(&quot;port&quot;, &quot;8007&quot;)); static final int SIZE = Integer.parseInt(System.getProperty(&quot;size&quot;, &quot;256&quot;)); public static void main(String[] args) throws Exception &#123; // Configure SSL.git final SslContext sslCtx; if (SSL) &#123; sslCtx = SslContextBuilder.forClient() .trustManager(InsecureTrustManagerFactory.INSTANCE).build(); &#125; else &#123; sslCtx = null; &#125; // Configure the client. EventLoopGroup group = new NioEventLoopGroup(); try &#123; Bootstrap b = new Bootstrap(); b.group(group) .channel(NioSocketChannel.class) .option(ChannelOption.TCP_NODELAY, true) .handler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override public void initChannel(SocketChannel ch) throws Exception &#123; ChannelPipeline p = ch.pipeline(); if (sslCtx != null) &#123; p.addLast(sslCtx.newHandler(ch.alloc(), HOST, PORT)); &#125; //p.addLast(new LoggingHandler(LogLevel.INFO)); p.addLast(new EchoClientHandler()); &#125; &#125;); // Start the client. ChannelFuture f = b.connect(HOST, PORT).sync(); // Wait until the connection is closed. f.channel().closeFuture().sync(); &#125; finally &#123; // Shut down the event loop to terminate all threads. group.shutdownGracefully(); &#125; &#125;&#125; 使用官方Example EchoClientHandler 123456789101112131415161718192021222324252627282930313233343536public class EchoClientHandler extends ChannelInboundHandlerAdapter &#123; private final ByteBuf firstMessage; /** * Creates a client-side handler. */ public EchoClientHandler() &#123; firstMessage = Unpooled.buffer(EchoClient.SIZE); for (int i = 0; i &lt; firstMessage.capacity(); i ++) &#123; firstMessage.writeByte((byte) i); &#125; &#125; @Override public void channelActive(ChannelHandlerContext ctx) &#123; ctx.writeAndFlush(firstMessage); &#125; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) &#123; ctx.write(msg); &#125; @Override public void channelReadComplete(ChannelHandlerContext ctx) &#123; ctx.flush(); &#125; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) &#123; // Close the connection when an exception is raised. cause.printStackTrace(); ctx.close(); &#125;&#125; Bootstrap启动流程 EventLoopGroup的初始化 客户端源码在一开始就实例化了 一个 NioEventLoopGroup 对象因此就从它的构造器中追踪 EventLoop 的初始化过程。首先来看 NioEventLoopGroup 的类继承层次结构 NioEventLoopGroup 有几个重载的构造器，不过内容都没有太大的区别，最终都调用父类 MultithreadEventLoopGroup 的构造器 123protected MultithreadEventLoopGroup(int nThreads, Executor executor, Object... args) &#123; super(nThreads == 0 ? DEFAULT_EVENT_LOOP_THREADS : nThreads, executor, args);&#125; 参数nThreads 就是 NioEventLoopGroup 线程数量，其实就是 EventLoop 的数量，如果传入的参数为0则额外特殊处理, 在静态代码块中获取当前机器的CPU核心数量 * 2 1234567891011/** * Netty首先从系统属性中获取“io.netty.eventLoopThreads”的 值，如果我们没有设置，就返回默认值，即CPU核数×2 */static &#123; DEFAULT_EVENT_LOOP_THREADS = Math.max(1, SystemPropertyUtil.getInt( &quot;io.netty.eventLoopThreads&quot;, NettyRuntime.availableProcessors() * 2)); if (logger.isDebugEnabled()) &#123; logger.debug(&quot;-Dio.netty.eventLoopThreads: &#123;&#125;&quot;, DEFAULT_EVENT_LOOP_THREADS); &#125;&#125; ​ Netty首先从系统属性中获取“io.netty.eventLoopThreads”的值，如果我们没有设置，就返回默认值，即CPU核数 × 2。回到 MultithreadEventLoopGroup 构造器中会继续调用父类MultithreadEventExecutorGroup 的构造器 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586 。。。 // 这个就是 EventLoop 的容器 private final EventExecutor[] children;/** * 最终的创建实例调用的构造器 * * @param nThreads 该实例将使用的线程数 * @param executor 将要使用的executor, 默认为null * @param chooserFactory 将要使用的EventExecutorChooserFactory * @param args arguments which will passed to each &#123;@link #newChild(Executor, Object...)&#125; call */ protected MultithreadEventExecutorGroup(int nThreads, Executor executor, EventExecutorChooserFactory chooserFactory, Object... args) &#123; // 校验线程数量的合法性 checkPositive(nThreads, &quot;nThreads&quot;); // executor校验非空, 如果为空就创建ThreadPerTaskExecutor, 该类实现了 Executor接口(JUC) // 题外说明：Executor接口的execute方法在JUC线程池中是用于提交异步任务的 // 这个executor 是用来执行线程池中的所有的线程，也就是所有的NioEventLoop，其实从 // NioEventLoop构造器中也可以知道，NioEventLoop构造器中都传入了executor这个参数 if (executor == null) &#123; executor = new ThreadPerTaskExecutor(newDefaultThreadFactory()); &#125; // 这里的children数组， 其实就是线程池的核心实现，线程池中就是通过指定的线程数组来实现线程池； // 数组中每个元素其实就是一个NioEventLoop children = new EventExecutor[nThreads]; // for循环实例化children数组，NioEventLoop对象 for (int i = 0; i &lt; nThreads; i++) &#123; boolean success = false; try &#123; // newChild 这个抽象方法在 NioEventLoopGroup类 被实现，就是返回 NioEventLoop children[i] = newChild(executor, args); success = true; &#125; catch (Exception e) &#123; // TODO: Think about if this is a good exception type throw new IllegalStateException(&quot;failed to create a child event loop&quot;, e); &#125; finally &#123; // 如果创建失败就释放资源 if (!success) &#123; for (int j = 0; j &lt; i; j++) &#123; children[j].shutdownGracefully(); &#125; for (int j = 0; j &lt; i; j++) &#123; EventExecutor e = children[j]; try &#123; while (!e.isTerminated()) &#123; e.awaitTermination(Integer.MAX_VALUE, TimeUnit.SECONDS); &#125; &#125; catch (InterruptedException interrupted) &#123; // Let the caller handle the interruption. Thread.currentThread().interrupt(); break; &#125; &#125; &#125; &#125; &#125; // chooserFactory类型是 DefaultEventExecutorChooserFactory // chooser类型是 DefaultEventExecutorChooserFactory的两个内部类之一，选择器的作用就是当BossGroup创建连接后注册到WorkerGroup的EventLoop时 // 需要使用来选择数组中的EventLoop, 底层算法是通过 executors[(int) Math.abs(idx.getAndIncrement() % executors.length)] 实现的轮询算法 chooser = chooserFactory.newChooser(children); // 为每个EventLoop线程添加 线程终止监听器 final FutureListener&lt;Object&gt; terminationListener = new FutureListener&lt;Object&gt;() &#123; @Override public void operationComplete(Future&lt;Object&gt; future) throws Exception &#123; // 异常终止数量自增 if (terminatedChildren.incrementAndGet() == children.length) &#123; terminationFuture.setSuccess(null); &#125; &#125; &#125;; for (EventExecutor e : children) &#123; e.terminationFuture().addListener(terminationListener); &#125; // 将children 添加到对应的set集合中去重， 表示只可读 Set&lt;EventExecutor&gt; childrenSet = new LinkedHashSet&lt;EventExecutor&gt;(children.length); Collections.addAll(childrenSet, children); readonlyChildren = Collections.unmodifiableSet(childrenSet); &#125; EventLoop的创建过程 newChild(executor, args) 这是一个抽象方法，在 MultithreadEventLoopGroup 的构造函数中被调用，通过循环来创建EventLoop ，它的任务是实例化 EventLoop 对象。我们跟踪一下它的代码，可以发现，这个方法在 NioEventLoopGroup 类中有实现 NioEventLoopGroup 1234567891011121314151617181920212223@Override// 创建 并返回 NioEventLoop 对象protected EventLoop newChild(Executor executor, Object... args) throws Exception &#123; // 就 是 在 NioEventLoopGroup 构 造 器 中 ， 调 用 //SelectorProvider.provider()方法获取的SelectorProvider对象 SelectorProvider selectorProvider = (SelectorProvider) args[0]; SelectStrategyFactory selectStrategyFactory = (SelectStrategyFactory) args[1]; // 拒绝策略，实际上调用了RejectedExecutionHandlers.reject(), 拒绝策略是抛出异常 RejectedExecutionHandler rejectedExecutionHandler = (RejectedExecutionHandler) args[2]; EventLoopTaskQueueFactory taskQueueFactory = null; EventLoopTaskQueueFactory tailTaskQueueFactory = null; int argsLength = args.length; if (argsLength &gt; 3) &#123; taskQueueFactory = (EventLoopTaskQueueFactory) args[3]; &#125; if (argsLength &gt; 4) &#123; tailTaskQueueFactory = (EventLoopTaskQueueFactory) args[4]; &#125; return new NioEventLoop(this, executor, selectorProvider, selectStrategyFactory.newSelectStrategy(), rejectedExecutionHandler, taskQueueFactory, tailTaskQueueFactory);&#125; NioEventLoop 12345678910111213141516NioEventLoop(NioEventLoopGroup parent, Executor executor, SelectorProvider selectorProvider, SelectStrategy strategy, RejectedExecutionHandler rejectedExecutionHandler, EventLoopTaskQueueFactory taskQueueFactory, EventLoopTaskQueueFactory tailTaskQueueFactory) &#123; // newTaskQueue 创建任务队列，底层使用JCTools第三方非阻塞线程安全的队列 super(parent, executor, false, newTaskQueue(taskQueueFactory), newTaskQueue(tailTaskQueueFactory), rejectedExecutionHandler); // 就 是 在 NioEventLoopGroup 构 造 器 中 ， 调 用SelectorProvider.provider()方法获取的SelectorProvider对象 // SelectorProvider 使用java spi, 不同平台又不同实现 this.provider = ObjectUtil.checkNotNull(selectorProvider, &quot;selectorProvider&quot;); this.selectStrategy = ObjectUtil.checkNotNull(strategy, &quot;selectStrategy&quot;); // 获取原生 Selector， netty 默认对 Selector 有优化，将keys属性由Set改为数组，这样add操作的时间复杂度为O(1) final SelectorTuple selectorTuple = openSelector(); // provider.openSelector() 获取selector方法 this.selector = selectorTuple.selector; this.unwrappedSelector = selectorTuple.unwrappedSelector;&#125; EventExecutorChooser的创建过程 事件执行器选择器，用于在Channel注册时提供选择 EventLoop 功能chooserFactory.newChooser(children); 在 MultithreadEventLoopGroup 的构造函数中被调用 DefaultEventExecutorChooserFactory 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455@UnstableApipublic final class DefaultEventExecutorChooserFactory implements EventExecutorChooserFactory &#123; public static final DefaultEventExecutorChooserFactory INSTANCE = new DefaultEventExecutorChooserFactory(); private DefaultEventExecutorChooserFactory() &#123; &#125; @Override public EventExecutorChooser newChooser(EventExecutor[] executors) &#123; if (isPowerOfTwo(executors.length)) &#123; return new PowerOfTwoEventExecutorChooser(executors); &#125; else &#123; return new GenericEventExecutorChooser(executors); &#125; &#125; private static boolean isPowerOfTwo(int val) &#123; return (val &amp; -val) == val; &#125; // 如果nThreads是2的平方，则使用 PowerOfTwoEventExecutorChooser private static final class PowerOfTwoEventExecutorChooser implements EventExecutorChooser &#123; private final AtomicInteger idx = new AtomicInteger(); private final EventExecutor[] executors; PowerOfTwoEventExecutorChooser(EventExecutor[] executors) &#123; this.executors = executors; &#125; @Override public EventExecutor next() &#123; // 轮询算法，如果时2的幂次方，使用&amp;比使用%效率高，极限优化 return executors[idx.getAndIncrement() &amp; executors.length - 1]; &#125; &#125; // 如果nThreads不是2的平方，则使用 GenericEventExecutorChooser private static final class GenericEventExecutorChooser implements EventExecutorChooser &#123; // Use a &#x27;long&#x27; counter to avoid non-round-robin behaviour at the 32-bit overflow boundary. // The 64-bit long solves this by placing the overflow so far into the future, that no system // will encounter this in practice. private final AtomicLong idx = new AtomicLong(); private final EventExecutor[] executors; GenericEventExecutorChooser(EventExecutor[] executors) &#123; this.executors = executors; &#125; @Override public EventExecutor next() &#123; // 自增序列向长度取模就是轮询算法 return executors[(int) Math.abs(idx.getAndIncrement() % executors.length)]; &#125; &#125;&#125; 如果nThreads是2的平方，则使用 PowerOfTwoEventExecutorChooser ， 否则使用GenericEventExecutorChooser。这两个Chooser都重写next()方法。next() 方法的主要功能就是将数组索引循环位移，如下图所示 当索引移动到最后一个位置时，再调用next()方法就会将索引位 置重新指向0，如下图所示 选择器之所有两个内部类，是因为计算机底层的 &amp; 运算比 % 运算效率高，Netty会根据nThread是否为2次幂动态选择，属于一个优化手段，最终实现的都是一个 轮询算法 总结 NioEventLoopGroup 底层其实是 MultithreadEventExecutorGroup 内部维护好了一个 EventExecutor 的children数组 ,其大小是nThreads，这样就构成了一 个线程池 实例化 NioEventLoopGroup 时，如果指定线程池大小，则nThreads就是指定的值，反之是CPU核数×2 在 MultithreadEventExecutorGroup 中调用 newChild() 象方法来初始化children数组 newChild() 方法是在 NioEventLoopGroup 中实现的，它返回一个 NioEventLoop 实例 初始化NioEventLoop对象并给属性赋值，具体赋值的属性如下 provider ： 就是在 NioEventLoopGroup 构造器中 ， 调用 **SelectorProvider.provider()**方法获取的 SelectorProvider (java spi) 对象 selector ： 就是在 NioEventLoop 构造器中 ， 调用 provider.openSelector() (java nio) 方法获取的 Selector 对象 sequenceDiagram EventLoopGroup ->> MultithreadEventLoopGroup : 构造函数调用，nThreads取值：参数指定，构造函数指定，或cpu核心数 * 2 MultithreadEventLoopGroup ->> MultithreadEventExecutorGroup : 构造函数调用，确认EventLoop数量 MultithreadEventExecutorGroup ->> NioEventLoopGroup : 循环调用 newChild() 创建EventLoo NioEventLoopGroup ->> NioEventLoop : 构造函数 -> JCTolls 创建任务队列,openSelector()，默认创建优化的Selector MultithreadEventExecutorGroup ->> DefaultEventExecutorChooserFactory : newChooser(), 创建EventLoop选择器 MultithreadEventExecutorGroup ->> MultithreadEventExecutorGroup : 添加EventLoop终止监听 MultithreadEventExecutorGroup ->> MultithreadEventExecutorGroup : 将EventLoop数组转为只读Set Channel 在Netty中，Channel相当于一个Socket的抽象，它为用户提供了关于Socket状态（是连接还是断开）及对Socket的读、写等操作。每当Netty建立了一个连接，都创建一个与其对应的Channel实例 除了TCP，Netty还支持很多其他的连接协议，并且每种协议还有NIO（非阻塞I/O）和OIO（Old-I/O，即传统的阻塞I/O）版本的区别，不同协议不同阻塞类型的连接都有不同的Channel类型与之对应 通道类型 说明 NioSocketChannel 异步的客户端 TCP Socket 连接 NioServerSocketChannel 异步的服务器端 TCP Socket 连接 NioDatagramChannel 异步的 UDP 连接 NioSctpChannel 异步的客户端 Sctp 连接 NioSctpServerChannel 异步的 Sctp 服务器端连接，这些通道涵盖了 UDP 和 TCP 网络 IO 以及文件 IO OioSocketChannel 同步的客户端 TCP Socket 连接 OioServerSocketChannel 同步的服务器端 TCP Socket 连接 OioDatagramChannel 同步的 UDP 连接 OioSctpChannel 同步的 Sctp 服务器端连接 OioSctpServerChannel 同步的客户端 TCP Socket 连接 Channel的创建过程 客户端连接代码的初始化Bootstrap中，该方法调用了一个channel()方法，传入的参数是NioSocketChannel.class，在这个方法中其实就是初始化了一ReflectiveChannelFactory的对象，代码实现如下 ： AbstractBootstrap 1234567891011/** * 设置服务端Channel类型，返回ReflectiveChannelFactory 它是一个反射工厂， 通过反射创建Channel对象 * The &#123;@link Class&#125; which is used to create &#123;@link Channel&#125; instances from. * You either use this or &#123;@link #channelFactory(io.netty.channel.ChannelFactory)&#125; if your * &#123;@link Channel&#125; implementation has no no-args constructor. */ public B channel(Class&lt;? extends C&gt; channelClass) &#123; return channelFactory(new ReflectiveChannelFactory&lt;C&gt;( ObjectUtil.checkNotNull(channelClass, &quot;channelClass&quot;) )); &#125; 而 ReflectiveChannelFactory 实现了 ChannelFactory 接口，它提供了唯一的方法，即newChannel()方法。顾名思义，ChannelFactory 就是创建 Channel 的工厂类 123456789101112131415161718192021222324252627282930313233343536/** * 反射Channel工厂 * A &#123;@link ChannelFactory&#125; that instantiates a new &#123;@link Channel&#125; by invoking its default constructor reflectively. */public class ReflectiveChannelFactory&lt;T extends Channel&gt; implements ChannelFactory&lt;T&gt; &#123; // Channel 的构造函数 private final Constructor&lt;? extends T&gt; constructor; /** * 构造函数，根据传递进来的Channel类型获取其无参构造函数 */ public ReflectiveChannelFactory(Class&lt;? extends T&gt; clazz) &#123; ObjectUtil.checkNotNull(clazz, &quot;clazz&quot;); try &#123; this.constructor = clazz.getConstructor(); &#125; catch (NoSuchMethodException e) &#123; throw new IllegalArgumentException(&quot;Class &quot; + StringUtil.simpleClassName(clazz) + &quot; does not have a public non-arg constructor&quot;, e); &#125; &#125; /** * 通过无参构造函数调用Java反射Api生成一个新的Channel,在有客户端建立新的连接时使用 * @return */ @Override public T newChannel() &#123; try &#123; return constructor.newInstance(); &#125; catch (Throwable t) &#123; throw new ChannelException(&quot;Unable to create Channel from class &quot; + constructor.getDeclaringClass(), t); &#125; &#125; ...&#125; 结论: Bootstrap 中的 ChannelFactory 实现类是 ReflectiveChannelFactory 通过 channel() 方法创建的 Channel 具体类型是 NioSocketChannel Channel 的实例化过程其实就是调用 ChannelFactory 的newChannel() 方 法 ， 而实例化的 Channel 具体类型又和初始化Bootstrap时传入的channel()方法的参数相关。因此对于客户端的 Bootstrap 而言，创建的 Channel 实例就是 NioSocketChannel sequenceDiagram AbstractBootstrap ->> AbstractBootstrap : channel() -> channelFactory() 创建ChannelFactory AbstractBootstrap ->> ReflectiveChannelFactory : 创建一个通过反射创建Channel的ChannelFactory NioSocketChannel的创建 EchoClient 1234567... // 指定Channel类型 .channel(NioSocketChannel.class) ... // Start the client. ChannelFuture f = b.connect(HOST, PORT).sync();... 分析 Bootstrap 类的 connect() 方法调用链路图可知，当客户端发起连接操作的时候在 AbstractBootstrap 的 initAndRegister() 方 法 中 ， 调用 ChannelFactory 的 newChannel() 方法来创建一个 NioSocketChannel 的实例 AbstractBootstrap 1234567891011121314151617181920212223242526272829303132333435... // 初始化和注册Channel final ChannelFuture initAndRegister() &#123; Channel channel = null; try &#123; // 通过工厂生成Channel, 工厂在配置引导类调用 .channel(channelClass) 方法时被初始化 channel = channelFactory.newChannel(); // 本类的抽象方法，客户端由 Bootstrap 类实现，服务端由 ServerBootStrap 类实现 init(channel); &#125; catch (Throwable t) &#123; if (channel != null) &#123; // 异常强制关闭 channel, Netty里，channel就是对Socket的抽象封装 // channel can be null if newChannel crashed (eg SocketException(&quot;too many open files&quot;)) channel.unsafe().closeForcibly(); // as the Channel is not registered yet we need to force the usage of the GlobalEventExecutor return new DefaultChannelPromise(channel, GlobalEventExecutor.INSTANCE).setFailure(t); &#125; // as the Channel is not registered yet we need to force the usage of the GlobalEventExecutor return new DefaultChannelPromise(new FailedChannel(), GlobalEventExecutor.INSTANCE).setFailure(t); &#125; // 异步地向EventLoopGroup 注册当前的channel ChannelFuture regFuture = config().group().register(channel); // 处理重复注册和异常 if (regFuture.cause() != null) &#123; if (channel.isRegistered()) &#123; channel.close(); &#125; else &#123; channel.unsafe().closeForcibly(); &#125; &#125; return regFuture; &#125;... 客户端Channel的初始化 NioSocketChannel体系 在 ReflectiveChannelFactory. newChannel() 方法中，利用反射机制调用类对象的 newInstance() 方法来创建一个新的Channel实例，相当于调用NioSocketChannel的默认构造器 (参考 ReflectiveChannelFactory 工厂的实现) 利用反射机制调用类对象的 newInstance() 方法来创建一个新的Channel实例，相当于调用 NioSocketChannel 的默认构造器。NioSocketChannel的默认构造器代码 NioSocketChannel 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748public class NioSocketChannel extends AbstractNioByteChannel implements io.netty.channel.socket.SocketChannel &#123; private static final InternalLogger logger = InternalLoggerFactory.getInstance(NioSocketChannel.class); // JDK的 Selector提供者，JDK NIO 模块的 SPI机制, 主要兼容不同的平台 mac, win, linux private static final SelectorProvider DEFAULT_SELECTOR_PROVIDER = SelectorProvider.provider(); // 通过java SPI 机制获取不同平台上打开Socket的实现 private static final Method OPEN_SOCKET_CHANNEL_WITH_FAMILY = SelectorProviderUtil.findOpenMethod(&quot;openSocketChannel&quot;); // java nio Channel的相关配置，NioSocketChannel 持有 java nio Channel的相关配置这点可以体现Netty 对 java nio Channel 的高度封装 private final SocketChannelConfig config; // 2.创建一个新的Channel, 构造函数会调用这个方法 private static SocketChannel newChannel(SelectorProvider provider, InternetProtocolFamily family) &#123; try &#123; // 通过反射调用 不同平台实现的 openSocketChannel 方法获取一个 SocketChannel SocketChannel channel = SelectorProviderUtil.newChannel(OPEN_SOCKET_CHANNEL_WITH_FAMILY, provider, family); // 根据 SPI 打开一个新的 java Nio Channel return channel == null ? provider.openSocketChannel() : channel; &#125; catch (IOException e) &#123; throw new ChannelException(&quot;Failed to open a socket.&quot;, e); &#125; &#125; /** * 无参构造函数 * ReflectiveChannelFactory 工厂使用这个反射生成Channel * Create a new instance */ public NioSocketChannel() &#123; this(DEFAULT_SELECTOR_PROVIDER); &#125; /** * 支持自定义 Selector提供者 * Create a new instance using the given &#123;@link SelectorProvider&#125;. */ public NioSocketChannel(SelectorProvider provider) &#123; this(provider, null); &#125; /** * 构造器 * Create a new instance using the given &#123;@link SelectorProvider&#125; and protocol family (supported only since JDK 15). */ public NioSocketChannel(SelectorProvider provider, InternetProtocolFamily family) &#123; this(newChannel(provider, family)); &#125; 在这个构造器中首先会调用 newSocket() 方法来打开一个新的Java NIO的 SocketChannel 对象 (步骤 2) AbstractNioByteChannel 1234567891011/** * 构造函数，第一个channel 暂时为空，第二个ch 为 java nio SocketChannel * Create a new instance * * @param parent the parent &#123;@link Channel&#125; by which this instance was created. May be &#123;@code null&#125; * @param ch the underlying &#123;@link SelectableChannel&#125; on which it operates */ protected AbstractNioByteChannel(Channel parent, SelectableChannel ch) &#123; // 调用父类 AbstractNioChannel 的构造器，并传入一个读操作标识 super(parent, ch, SelectionKey.OP_READ); &#125; 最后会调用其父类的构造器，即AbstractNioByteChannel的构造器 （步骤 4），传入参数，parent的值默认为null，ch为之前调用 newSocket()方法创建的Java NIO 的 SocketChannel 对象，因此新创建的 NioSocketChannel 对象中的parent暂时是null AbstractNioByteChannel 1234567891011121314151617181920212223242526/** * Create a new instance * * @param parent the parent &#123;@link Channel&#125; by which this instance was created. May be &#123;@code null&#125; * @param ch the underlying &#123;@link SelectableChannel&#125; on which it operates * @param readInterestOp the ops to set to receive data from the &#123;@link SelectableChannel&#125; */ protected AbstractNioChannel(Channel parent, SelectableChannel ch, int readInterestOp) &#123; // AbstractChannel 父类的构造函数 super(parent); this.ch = ch; this.readInterestOp = readInterestOp; try &#123; // 设置 java nio channel 为非阻塞的 ch.configureBlocking(false); &#125; catch (IOException e) &#123; try &#123; ch.close(); &#125; catch (IOException e2) &#123; logger.warn( &quot;Failed to close a partially initialized socket.&quot;, e2); &#125; throw new ChannelException(&quot;Failed to enter non-blocking mode.&quot;, e); &#125; &#125; 接着会调用父类AbstractNioChannel的构造器，并传入实际参数readInterestOp=SelectionKey.OP_READ AbstractNioChannel 12345678910111213141516171819protected AbstractNioChannel(Channel parent, SelectableChannel ch, int readInterestOp) &#123; // AbstractChannel 父类的构造函数 super(parent); this.ch = ch; this.readInterestOp = readInterestOp; try &#123; // 设置 java nio channel 为非阻塞的 ch.configureBlocking(false); &#125; catch (IOException e) &#123; try &#123; ch.close(); &#125; catch (IOException e2) &#123; logger.warn( &quot;Failed to close a partially initialized socket.&quot;, e2); &#125; throw new ChannelException(&quot;Failed to enter non-blocking mode.&quot;, e); &#125; &#125; 最后会调用父类 AbstractChannel 的构造器 DefaultChannelId Channel$Unsafe DefaultChannelPipeline 123456789protected AbstractChannel(Channel parent) &#123; this.parent = parent; // 创建一个 DefaultChannelId，为Channel 绑定一个 ShortId 不唯一的值, 只有LongId才不会重复，通过mac 地址等基础数据计算 id = newId(); // Unsafe 是 Channel 的内部类，只允许IO线程调用，真正用于数据传输操作 unsafe = newUnsafe(); // 为当前的 Channel 创建一个 DefaultChannelPipeline 管道，所以 Channel 和 Pipeline 是一对一关系 pipeline = newChannelPipeline(); &#125; 至此，NioSocketChannel就完成了初始化, 注意：创建了一个Nio 的 Channel, 此时上面的parent 还是 null 总结： 调用 NioSocketChannel.newSocket(DEFAULT_SELECTOR_PROVIDER) -&gt; SelectorProvider.openSocketChannel() 打开一个新的Java Nio SocketChannel 向上不断调用其父类的构造函数直到初始化 AbstractChannel(Channel parent) 对象并给属性赋值，具体赋值的属性如下 id：每个Channel都会被分配一个id parent：属性值默认为null unsafe：通过调用newUnsafe()方法实例化一个Unsafe对象，它的类型是 AbstractNioByteChannel.NioByteUnsafe内部类，负责真正的I/O读写 pipeline：是通过调用new DefaultChannelPipeline(this)新创建的实例 AbstractNioByteChannel 中被赋值的属性如下 ch：被赋值为Java原生SocketChannel，即NioSocketChannel的newSocket()方法返回的Java NIO SocketChannel readInterestOp：被赋值为SelectionKey.OP_READ ch：被配置为非阻塞，即调用 ch.configureBlocking(false) 方法 NioSocketChannel 中被赋值的属性 ： config = newNioSocketChannelConfig(this，socket.socket()) sequenceDiagram Bootstrap ->> Bootstrap : connect() -> doResolveAndConnect() 客户端连接 Bootstrap ->> AbstractBootstrap : initAndRegister() 初始化和注册Channel AbstractBootstrap ->> ReflectiveChannelFactory : .newChannel() 通过工厂创建Channel ReflectiveChannelFactory ->> NioSocketChannel : 反射创建 NioSocketChannel NioSocketChannel ->> SelectorProvider : SPI机制调用 openSocketChannel SelectorProvider ->> SocketChannel : 获得nio原生Channel NioSocketChannel ->> AbstractNioChannel : Channel设置非阻塞 AbstractNioChannel ->> AbstractChannel : newId() newUnsafe() newChannelPipeline() Unsafe内部类 注意：这里的 Unsafe 并不是我们常说的Java自带的 sun.misc.Unsafe，而是 io.netty.channel.Channel#Unsafe。按源码注释上说法是 “Unsafe函数不允许被用户代码使用，这些函数是真正用于数据传输操作，必须被IO线程调用” ，也就是说真正依赖于底层协议/方案的实现是通过Unsafe包装出去的 ​ 在实例化 NioSocketChannel 的 过 程 中 ， Unsafe就特别关键 。 Unsafe 其实是对Java底层Socket操作的封装，因此，它实际上是沟通 Netty上层和Java底层的重要桥梁。下面我们看一下Unsafe接口所提供的方法 ​ Unsafe 的初始化时机在 AbstractChannel 的构造函数中调用了 newUnsafe() Unsafe 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107interface Unsafe &#123; /** * Return the assigned &#123;@link RecvByteBufAllocator.Handle&#125; which will be used to allocate &#123;@link ByteBuf&#125;&#x27;s when * receiving data. */ RecvByteBufAllocator.Handle recvBufAllocHandle(); /** * 返回本地绑定的Socket地址 * Return the &#123;@link SocketAddress&#125; to which is bound local or * &#123;@code null&#125; if none. */ SocketAddress localAddress(); /** * 返回通信对端的Socket地址 * Return the &#123;@link SocketAddress&#125; to which is bound remote or * &#123;@code null&#125; if none is bound yet. */ SocketAddress remoteAddress(); /** * 注册Channel到多路复用器上,一旦注册操作完成,通知ChannelFuture * Register the &#123;@link Channel&#125; of the &#123;@link ChannelPromise&#125; and notify * the &#123;@link ChannelFuture&#125; once the registration was complete. */ void register(EventLoop eventLoop, ChannelPromise promise); /** * 绑定指定的localAddress到当前的Channel上,完成后通知ChannelFuture * Bind the &#123;@link SocketAddress&#125; to the &#123;@link Channel&#125; of the &#123;@link ChannelPromise&#125; and notify * it once its done. */ void bind(SocketAddress localAddress, ChannelPromise promise); /** * 绑定完成之后,连接服务器,操作完成之后通知ChannelFuture * Connect the &#123;@link Channel&#125; of the given &#123;@link ChannelFuture&#125; with the given remote &#123;@link SocketAddress&#125;. * If a specific local &#123;@link SocketAddress&#125; should be used it need to be given as argument. Otherwise just * pass &#123;@code null&#125; to it. * * The &#123;@link ChannelPromise&#125; will get notified once the connect operation was complete. */ void connect(SocketAddress remoteAddress, SocketAddress localAddress, ChannelPromise promise); /** * 断开连接,完成后通知ChannelFuture * Disconnect the &#123;@link Channel&#125; of the &#123;@link ChannelFuture&#125; and notify the &#123;@link ChannelPromise&#125; once the * operation was complete. */ void disconnect(ChannelPromise promise); /** * 关闭Channel连接,完成后通知 * Close the &#123;@link Channel&#125; of the &#123;@link ChannelPromise&#125; and notify the &#123;@link ChannelPromise&#125; once the * operation was complete. */ void close(ChannelPromise promise); /** * 强制关闭连接 * Closes the &#123;@link Channel&#125; immediately without firing any events. Probably only useful * when registration attempt failed. */ void closeForcibly(); /** * 取消此Channel在多路复用器上的注册 * Deregister the &#123;@link Channel&#125; of the &#123;@link ChannelPromise&#125; from &#123;@link EventLoop&#125; and notify the * &#123;@link ChannelPromise&#125; once the operation was complete. */ void deregister(ChannelPromise promise); /** * 设置网络操作位为用于读取消息 * Schedules a read operation that fills the inbound buffer of the first &#123;@link ChannelInboundHandler&#125; in the * &#123;@link ChannelPipeline&#125;. If there&#x27;s already a pending read operation, this method does nothing. */ void beginRead(); /** * 发送消息,完成之后通知ChannelFuture * Schedules a write operation. */ void write(Object msg, ChannelPromise promise); /** * 将发送缓冲数组中的消息写入到Channel中 * Flush out all write operations scheduled via &#123;@link #write(Object, ChannelPromise)&#125;. */ void flush(); /** * 返回一个特殊的可重用和传递的ChannelPromise,它不用于操作成功或失败的通知器,仅仅作为容器使用 * Return a special ChannelPromise which can be reused and passed to the operations in &#123;@link Unsafe&#125;. * It will never be notified of a success or error and so is only a placeholder for operations * that take a &#123;@link ChannelPromise&#125; as argument but for which you not want to get notified. */ ChannelPromise voidPromise(); /** * 返回消息发送缓冲区 * Returns the &#123;@link ChannelOutboundBuffer&#125; of the &#123;@link Channel&#125; where the pending write requests are stored. */ ChannelOutboundBuffer outboundBuffer(); &#125; 从上述源码中可以看出，这些方法其实都是与Java底层的相关Socket的操作相对应的 AbstractChannel$AbstractUnsafe 基本抽象实现 AbstractChannel 的构造方法中 ， 在这里调用了 newUnsafe()方法获取一个新的Unsafe对象，而newUnsafe()方法在 NioSocketChannel中被重写了 123456789protected AbstractChannel(Channel parent) &#123; this.parent = parent; // 创建一个 DefaultChannelId，为Channel 绑定一个 ShortId 不唯一的值, 只有LongId才不会重复，通过mac 地址等基础数据计算 id = newId(); // Unsafe 是 Channel 的内部类，只允许IO线程调用，真正用于数据传输操作 unsafe = newUnsafe(); // 为当前的 Channel 创建一个 DefaultChannelPipeline 管道，所以 Channel 和 Pipeline 是一对一关系 pipeline = newChannelPipeline();&#125; NioSocketChannel 12345678// 重写 newUnsafe， 因为 NioSocketChannel 自己实现了Unsafe@Overrideprotected AbstractNioUnsafe newUnsafe() &#123; return new NioSocketChannelUnsafe();&#125;private final class NioSocketChannelUnsafe extends NioByteUnsafe &#123; ... NioSocketChannel 的 newUnsafe() 方 法 会 返 回 一 个 NioSocketChannelUnsafe实例。从这里我们就可以确定，在实例化的 NioSocketChannel中的 Unsafe 属性其实是一 个 NioSocketChannelUnsafe的实例 ChannelPipeline的初始化 AbstractChannel 123456789protected AbstractChannel(Channel parent) &#123; this.parent = parent; // 创建一个 DefaultChannelId，为Channel 绑定一个 ShortId 不唯一的值, 只有LongId才不会重复，通过mac 地址等基础数据计算 id = newId(); // Unsafe 是 Channel 的内部类，只允许IO线程调用，真正用于数据传输操作 unsafe = newUnsafe(); // 为当前的 Channel 创建一个 DefaultChannelPipeline 管道，所以 Channel 和 Pipeline 是一对一关系 pipeline = newChannelPipeline();&#125; 在创建完 AbstractChannel 构造函数中可以看到，在创建完 Unsafe 后紧接着又调用 newChannelPipeline() 创建 ChannelPipeline 在 ChannelPipeline 的注释说明中写道 “Each channel has its own pipeline and it is created automatically when a new channel is created” 。我们知道，在实例化一个Channel时，必然都要实例化一个 ChannelPipeline 。而我们确实在 AbstractChannel 的构造器中看到了ChannelPipeline 属性被初始化为DefaultChannelPipeline 的实例 12345678910111213ChannelPipeline 源码的注释/** * A list of &#123;@link ChannelHandler&#125;s which handles or intercepts inbound events and outbound operations of a * &#123;@link Channel&#125;. &#123;@link ChannelPipeline&#125; implements an advanced form of the * &lt;a href=&quot;https://www.oracle.com/technetwork/java/interceptingfilter-142169.html&quot;&gt;Intercepting Filter&lt;/a&gt; pattern * to give a user full control over how an event is handled and how the &#123;@link ChannelHandler&#125;s in a pipeline * interact with each other. * * &lt;h3&gt;Creation of a pipeline&lt;/h3&gt; * * Each channel has its own pipeline and it is created automatically when a new channel is created. * ... AbstractChannel.newChannelPipeline() 方法实现 1234567/** * 为 Channel 创建一个默认的管道 * Returns a new &#123;@link DefaultChannelPipeline&#125; instance. */protected DefaultChannelPipeline newChannelPipeline() &#123; return new DefaultChannelPipeline(this);&#125; DefaultChannelPipeline 12345678910111213141516171819 // handlerContext 链表头指针 final AbstractChannelHandlerContext head; // handlerContext 链表尾指针 final AbstractChannelHandlerContext tail;... // 唯一的构造函数，创建 DefaultChannelPipeline 必穿一个 netty 的 Channel 对象 protected DefaultChannelPipeline(Channel channel) &#123; this.channel = ObjectUtil.checkNotNull(channel, &quot;channel&quot;); succeededFuture = new SucceededChannelFuture(channel, null); voidPromise = new VoidChannelPromise(channel, true); // ChannelPipeline 维护了 ChannelHandler 的链表头部和尾部，在Pipeline里 // 用户配置的多个ChannelHandler 组成了一个双向链表 tail = new TailContext(this); head = new HeadContext(this); head.next = tail; tail.prev = head; &#125; DefaultChannelPipeline 的构造器需要传入一个 Channel，而这个 Channel 其实就是我们实例化的NioSocketChannel 对象，DefaultChannelPipeline 会将这个NioSocketChannel 对象保存在 Channel 属性中。DefaultChannelPipeline 中还有两个特殊的属性，即Head和Tail，这两个属性是双向链表的头和尾。其实在DefaultChannelPipeline 中维护了一个以AbstractChannelHandlerContext为节点元素的双向链表，这个链表是Netty实现Pipeline机制的关键 HandlerContext HeadContext 体系 TailContext 体系 12345678910final class HeadContext extends AbstractChannelHandlerContext implements ChannelOutboundHandler, ChannelInboundHandler &#123; private final Unsafe unsafe; HeadContext(DefaultChannelPipeline pipeline) &#123; super(pipeline, null, HEAD_NAME, HeadContext.class); unsafe = pipeline.channel().unsafe(); setAddComplete(); &#125; 123456final class TailContext extends AbstractChannelHandlerContext implements ChannelInboundHandler &#123; TailContext(DefaultChannelPipeline pipeline) &#123; super(pipeline, null, TAIL_NAME, TailContext.class); setAddComplete(); &#125; 获取出站和入站的HandleConetxt方法 AbstractChannelHandlerContext 12345678910111213141516171819// 获取处理入站的HandlerContextprivate AbstractChannelHandlerContext findContextInbound(int mask) &#123; AbstractChannelHandlerContext ctx = this; EventExecutor currentExecutor = executor(); do &#123; ctx = ctx.next; &#125; while (skipContext(ctx, currentExecutor, mask, MASK_ONLY_INBOUND)); return ctx;&#125;// 获取处理出站的HandlerContextprivate AbstractChannelHandlerContext findContextOutbound(int mask) &#123; AbstractChannelHandlerContext ctx = this; EventExecutor currentExecutor = executor(); do &#123; ctx = ctx.prev; &#125; while (skipContext(ctx, currentExecutor, mask, MASK_ONLY_OUTBOUND)); return ctx;&#125; sequenceDiagram AbstractChannel ->> AbstractChannel : 构造函数 -> newChannelPipeline() AbstractChannel ->> DefaultChannelPipeline : 创建默认的 ChannelPipeline, 传递Channel DefaultChannelPipeline ->> HeadContext : 创建Pipeline 链表头 DefaultChannelPipeline ->> TailContext : 创建Pipeline 链表尾 将Channel注册到Selector Bootstrap 最后会调用 connect(String inetHost, int inetPort)方法连接服务端，最终会调用 Bootstrap 的 doResolveAndConnect(final SocketAddress remoteAddress, final SocketAddress localAddress) 方法，Channel （java nio Channel） 会在 Bootstrap 的 initAndRegister() 中进行初始化，但是这个方法还会将初始化好的 Channe （java nio Channel） 注册到 NioEventLoop 的 Selector 中 Bootstrap 1234567891011121314151617181920212223242526272829303132333435363738/** * 处理远程连接 * @see #connect() */ private ChannelFuture doResolveAndConnect(final SocketAddress remoteAddress, final SocketAddress localAddress) &#123; // 初始化和注册Channel final ChannelFuture regFuture = initAndRegister(); final Channel channel = regFuture.channel(); if (regFuture.isDone()) &#123; if (!regFuture.isSuccess()) &#123; return regFuture; &#125; return doResolveAndConnect0(channel, remoteAddress, localAddress, channel.newPromise()); &#125; else &#123; // Registration future is almost always fulfilled already, but just in case it&#x27;s not. final PendingRegistrationPromise promise = new PendingRegistrationPromise(channel); regFuture.addListener(new ChannelFutureListener() &#123; @Override public void operationComplete(ChannelFuture future) throws Exception &#123; // Directly obtain the cause and do a null check so we only need one volatile read in case of a // failure. Throwable cause = future.cause(); if (cause != null) &#123; // Registration on the EventLoop failed so fail the ChannelPromise directly to not cause an // IllegalStateException once we try to access the EventLoop of the Channel. promise.setFailure(cause); &#125; else &#123; // Registration was successful, so set the correct executor to use. // See https://github.com/netty/netty/issues/2586 promise.registered(); doResolveAndConnect0(channel, remoteAddress, localAddress, promise); &#125; &#125; &#125;); return promise; &#125; &#125; Bootstrap 123456789101112131415161718192021222324252627282930313233343536373839404142// 初始化和注册Channelfinal ChannelFuture initAndRegister() &#123; Channel channel = null; try &#123; // 通过工厂生成Channel, 工厂在配置引导类调用 .channel(channelClass) 方法时被初始化 channel = channelFactory.newChannel(); // 本类的抽象方法，客户端由 Bootstrap 类实现，服务端由 ServerBootStrap 类实现 init(channel); &#125; catch (Throwable t) &#123; if (channel != null) &#123; // 异常强制关闭 channel, Netty里，channel就是对Socket的抽象封装 // channel can be null if newChannel crashed (eg SocketException(&quot;too many open files&quot;)) channel.unsafe().closeForcibly(); // as the Channel is not registered yet we need to force the usage of the GlobalEventExecutor return new DefaultChannelPromise(channel, GlobalEventExecutor.INSTANCE).setFailure(t); &#125; // as the Channel is not registered yet we need to force the usage of the GlobalEventExecutor return new DefaultChannelPromise(new FailedChannel(), GlobalEventExecutor.INSTANCE).setFailure(t); &#125; // 异步地向EventLoopGroup 注册当前的channel ChannelFuture regFuture = config().group().register(channel); // 处理重复注册和异常 if (regFuture.cause() != null) &#123; if (channel.isRegistered()) &#123; channel.close(); &#125; else &#123; channel.unsafe().closeForcibly(); &#125; &#125; // If we are here and the promise is not failed, it&#x27;s one of the following cases: // 1) If we attempted registration from the event loop, the registration has been completed at this point. // i.e. It&#x27;s safe to attempt bind() or connect() now because the channel has been registered. // 2) If we attempted registration from the other thread, the registration request has been successfully // added to the event loop&#x27;s task queue for later execution. // i.e. It&#x27;s safe to attempt bind() or connect() now: // because bind() or connect() will be executed *after* the scheduled registration task is executed // because register(), bind(), and connect() are all bound to the same thread. return regFuture;&#125; 当Channel初始化后，紧接着会调用 config().group().register(channel) 方法来向 Selector 注册 Channel 通过跟踪调用链 ， 我们最终发现在 AbstractBootstrap 的 initAndRegister() 方法中调用的是 Unsafe 的 register() 方法，接下来看一下 AbstractChannel$AbstractUnsafe.register() 方法的具体实现代码 sequenceDiagram Bootstrap ->> AbstractBootstrap : initAndRegister() AbstractBootstrap ->> MultithreadEventLoopGroup : register() MultithreadEventLoopGroup ->> SingleThreadEventLoop : register() SingleThreadEventLoop ->> AbstractChannel$AbstractUnsafe : register() 通过跟踪调用链，我们最终发现在 AbstractBootstrap 的 initAndRegister() 方法中调用的是Unsafe的register()方法，接下来看一下 AbstractChannel$AbstractUnsafe.register()方法的具体实现代码 AbstractChannel$AbstractUnsafe 123456789101112131415161718192021222324252627282930313233343536373839404142434445/** * 注册到事件循环 * 注册方法其实就是判断是否当前线程就是IO线程，是的话就直接执行，不是就包装成一个任务提交给IO线程，这样就避免多线程的问题，始终是单线程操作 * @param eventLoop * @param promise */@Overridepublic final void register(EventLoop eventLoop, final ChannelPromise promise) &#123; ObjectUtil.checkNotNull(eventLoop, &quot;eventLoop&quot;); // 是否已经注册到一个eventLoop if (isRegistered()) &#123; promise.setFailure(new IllegalStateException(&quot;registered to an event loop already&quot;)); return; &#125; // 是否是NioEventLoop类型 if (!isCompatible(eventLoop)) &#123; promise.setFailure( new IllegalStateException(&quot;incompatible event loop type: &quot; + eventLoop.getClass().getName())); return; &#125; // 给外部类的 eventLoop 属性赋值，传递事件循环 AbstractChannel.this.eventLoop = eventLoop; // 当前线程是eventLoop的线程才可以注册，防止多线程并发问题，所以即使多线程来操作，也是安全的，会按照一定顺序提交到任务队列里 if (eventLoop.inEventLoop()) &#123; register0(promise); &#125; else &#123; try &#123; //否则就当做任务提交给eventLoop的任务队列 eventLoop.execute(new Runnable() &#123; @Override public void run() &#123; register0(promise); &#125; &#125;); &#125; catch (Throwable t) &#123; logger.warn( &quot;Force-closing a channel whose registration task was not accepted by an event loop: &#123;&#125;&quot;, AbstractChannel.this, t); closeForcibly(); //强制关闭 closeFuture.setClosed(); //关闭回调 safeSetFailure(promise, t); //设置失败 &#125; &#125;&#125; 首先，将 EventLoop 赋值给 AbstractChannel 的 eventLoop 属性，我们知道 EventLoop 对象其实是通过 MultithreadEventLoopGroup的next() 方法 获取的，根据前面的分析，可以确定next()方法返回的EventLoop对象是 NioEventLoop 实例。register()方法接着调用了register0()方法，代码如下 AbstractChannel$AbstractUnsafe 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556/** * 注册前的处理 * 真正的注册逻辑在 doRegister 方法，其实就是将NIO Channel 注册到 Selector 上，然后进行处理器的待添加事件的处理, * 注册回调成功，管道传递注册事件，如果是第一次注册，管道传递通道激活事件，否则是设置自动读的话就注册读监听 * @param promise */private void register0(ChannelPromise promise) &#123; try &#123; // 确保 Channel 当前是打开着的 // check if the channel is still open as it could be closed in the mean time when the register // call was outside of the eventLoop if (!promise.setUncancellable() || !ensureOpen(promise)) &#123; return; &#125; // 设置注册标记 boolean firstRegistration = neverRegistered; // 真正的注册逻辑，参考 AbstractNioChannel，其实也就是拿到 java Nio Channel 往 EventLoop 注册 // EventLoop 的来源在上面 register(EventLoop eventLoop, final ChannelPromise promise) 方法中被赋值 doRegister(); // 标记 Channel 已经注册 neverRegistered = false; // AbstractChannel 外部类的注册标记，标记注册 registered = true; // 如果在注册前有处理器添加，还没进行HandlerAdded回调，注册成功后要回调 // Ensure we call handlerAdded(...) before we actually notify the promise. This is needed as the // user may already fire events through the pipeline in the ChannelFutureListener. pipeline.invokeHandlerAddedIfNeeded(); // 回调注册成功 safeSetSuccess(promise); // 通道注册事件传递 pipeline.fireChannelRegistered(); // Only fire a channelActive if the channel has never been registered. This prevents firing // multiple channel actives if the channel is deregistered and re-registered. // 判断当前通道是否激活，其实就是判断 java Nio Channel 当前是否打开和连接着 if (isActive()) &#123; // 第一次注册要进行激活事件传递 if (firstRegistration) &#123; pipeline.fireChannelActive(); &#125; else if (config().isAutoRead()) &#123; // This channel was registered before and autoRead() is set. This means we need to begin read // again so that we process inbound data. // // See https://github.com/netty/netty/issues/4805 // 如果设置乐自动读，就继续监听读事件，这里也就是将 Channel 注册返回的 SelectionKey 感兴趣事件设置为 读 beginRead(); &#125; &#125; &#125; catch (Throwable t) &#123; // Close the channel directly to avoid FD leak. closeForcibly(); //强制关闭 closeFuture.setClosed(); //关闭回调 safeSetFailure(promise, t); //设置失败 &#125;&#125; register0() 方法又调用了 AbstractNioChannel的doRegister()方法，代码如下 123456789101112131415161718192021@Overrideprotected void doRegister() throws Exception &#123; boolean selected = false; for (;;) &#123; try &#123; selectionKey = javaChannel().register(eventLoop().unwrappedSelector(), 0, this); return; &#125; catch (CancelledKeyException e) &#123; if (!selected) &#123; // Force the Selector to select now as the &quot;canceled&quot; SelectionKey may still be // cached and not removed because no Select.select(..) operation was called yet. eventLoop().selectNow(); selected = true; &#125; else &#123; // We forced a select operation on the selector before but the SelectionKey is still cached // for whatever reason. JDK bug ? throw e; &#125; &#125; &#125;&#125; 看到 javaChannel() 这个方法，我们在前面已经知道了，它返回的是一个Java NIO的SocketChannel对象，这里我们将SocketChannel注册到与eventLoop关联的Selector上 总结 在客户端引导类 Bootstrap 的连接操作方法 connect 中，最终户i调用到 AbstractBootstrap 的initAndRegister()方法 在 AbstractBootstrap 的 initAndRegister() 方法中，通过group().register(channel) 调用 MultithreadEventLoopGroup 的register() 方法 在 MultithreadEventLoopGroup 的 register() 方法中，调用 next() (通过前面的 EventExecutorChooser 从EventloopGroup选择一个EventGroup) 方法获取一个可用的 SingleThreadEventLoop，然后调用它的 register() 方法 在 SingleThreadEventLoop 的 register() 方法中 ， 调用 channel.unsafe().register(this ， promise) 方法获取 AbstractChannel 的 unsafe() 底层操作对象，然后调用 AbstractUnsafe 的register()方法 在 AbstractUnsafe 的register()方法中，调用register0() 方法注册Channel对象 在 AbstractUnsafe 的 register0() 方法中 ， 调用 AbstractNioChannel 的doRegister()方法 AbstractNioChannel 的 doRegister() 方法通过 javaChannel().register(eventLoop().selector ， 0 ， this) 将Channel 对应的 Java NIO 的 SocketChannel 注册到一个 EventLoop 的Selector中，并且将当前Channel作为Attachment与SocketChannel关联 ​ 总的来说，Channel的注册过程所做的工作就是将Channel与对应的EventLoop进行关联。因此，在Netty中，每个Channel都会关联一个特定的EventLoop，并且这个Channel中的所有I/O操作都是在这个EventLoop中执行的；当关联好Channel和EventLoop后，会继续调用底层Java NIO的SocketChannel对象的register()方法，将底层Java NIO的SocketChannel注册到指定的Selector中。通过这两步，就完成了Netty对Channel的注册过程 sequenceDiagram Bootstrap ->> Channel : 1.connetc() 通过Factory创建一个Channel Bootstrap ->> EventLoopGroup : 2.group() 通过配置得到 EventLoopGroup EventLoopGroup ->> EventLoop : 3.next() 得到EventLoop EventLoop ->> SingleThreadEventLoop : 4 register(Channel) SingleThreadEventLoop ->> AbstractNioUnsafe : 5. doRegister() 通过Channel获得Unsafe AbstractNioUnsafe --> AbstractNioUnsafe : 6. 获取EventLoop 对应的Selector AbstractNioUnsafe --> AbstractNioUnsafe : 7 javaChannel().register(selector) Handler的添加过程 ​ Netty有一个强大和灵活之处就是基于Pipeline的自定义Handler机制。基于此，我们可以像添加插件一样自由组合各种各样的Handler来完成业务逻辑。 例如我们需要处理 HTTP 数据 ， 那么就可以在Pipeline前添加一个针对HTTP编解码的Handler，然后添加我们自己的业务逻辑的Handler，这样网络上的数据流就像通过一个管道一样，从不同的Handler中流过并进行编解码，最终到达我们自定义的Handler中。 ​ 在Bootstrap引导类中就有关于Handler的配置，handler(ChannelHandler handler) 方法，可供用户自定义将 Handler 添加到 ChannelPipeline 中， 调用时机在 Channel 被注册到 EventLoop 后添加 ChannelHandler 调用时机 AbstractUnsafe 12345678910private void register0(ChannelPromise promise) &#123; try &#123; .... // 注册Channel到EventLoop doRegister(); .... // 如果在注册前有处理器添加，还没进行HandlerAdded回调，注册成功后要回调，这里会添加Bootstrap配置的ChannelHandler // Ensure we call handlerAdded(...) before we actually notify the promise. This is needed as the // user may already fire events through the pipeline in the ChannelFutureListener. pipeline.invokeHandlerAddedIfNeeded(); 初始化 ChannelPipeline的初始化 1234567891011.handler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override public void initChannel(SocketChannel ch) throws Exception &#123; ChannelPipeline p = ch.pipeline(); if (sslCtx != null) &#123; p.addLast(sslCtx.newHandler(ch.alloc(), HOST, PORT)); &#125; //p.addLast(new LoggingHandler(LogLevel.INFO)); p.addLast(new EchoClientHandler()); &#125;&#125;); 这个代码片段就实现了Handler的添加功能。我们看到，Bootstrap的handler()方法接收一个 ChannelHandler，而我们传入的参数是一个派生于抽象类 ChannelInitializer 的匿名类，它也实现了 ChannelHandler 接口。我们来看 ChannelInitializer 类中到底有什么玄机，代码如下 ChannelInitializer 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950@Sharablepublic abstract class ChannelInitializer&lt;C extends Channel&gt; extends ChannelInboundHandlerAdapter &#123; private static final InternalLogger logger = InternalLoggerFactory.getInstance(ChannelInitializer.class); private final Set&lt;ChannelHandlerContext&gt; initMap = Collections.newSetFromMap( new ConcurrentHashMap&lt;ChannelHandlerContext, Boolean&gt;()); protected abstract void initChannel(C ch) throws Exception; @Override @SuppressWarnings(&quot;unchecked&quot;) public final void channelRegistered(ChannelHandlerContext ctx) throws Exception &#123; // Normally this method will never be called as handlerAdded(...) should call initChannel(...) and remove // the handler. // 调用下面的私有方法，私有方法再调用 initChannel 抽象方法 if (initChannel(ctx)) &#123; // we called initChannel(...) so we need to call now pipeline.fireChannelRegistered() to ensure we not // miss an event. ctx.pipeline().fireChannelRegistered(); // We are done with init the Channel, removing all the state for the Channel now. removeState(ctx); &#125; else &#123; // Called initChannel(...) before which is the expected behavior, so just forward the event. ctx.fireChannelRegistered(); &#125; &#125; // 初始化通道 // 上面的 channelRegistered 方法 和 AbstractChannel#register0() 中代码 pipeline.invokeHandlerAddedIfNeeded(); 会调用这里 @SuppressWarnings(&quot;unchecked&quot;) private boolean initChannel(ChannelHandlerContext ctx) throws Exception &#123; if (initMap.add(ctx)) &#123; // Guard against re-entrance. try &#123; // 抽象方法调用，调用子类实现的方法, 这里就是调用我们在Bootstrap配置ChannelHandler自定义的代码 initChannel((C) ctx.channel()); &#125; catch (Throwable cause) &#123; // Explicitly call exceptionCaught(...) as we removed the handler before calling initChannel(...). // We do so to prevent multiple calls to initChannel(...). exceptionCaught(ctx, cause); &#125; finally &#123; // 作为配置类，将自身从管道 pipeline 中移除 if (!ctx.isRemoved()) &#123; ctx.pipeline().remove(this); &#125; &#125; return true; &#125; return false; &#125; ​ ChannelInitializer 是一个抽象类 ， 它有一个抽象的 initChannel() 方法，我们看到的匿名类正是实现了这个方法，并在这个方法中添加了自定义的Handler。那么 initChannel() 方法是在哪里被调用的呢？其实是在 ChannelInitializer 的 channelRegistered()方法中 ​ 接下来关注一下 channelRegistered() 方法。我们从上面的代码中可以看到，在 channelRegistered() 方法中，会调用 initChannel() 方法 ， 将自定 义的 Handler 添加到 ChannelPipeline 中 ， 然后调用 ctx.pipeline().remove(this) 方法将自己从 ChannelPipeline 中删除 ​ 一开始， ChannelPipeline 中只有三个Handler，分别是Head、Tail和我们添加的 ChannelInitializer 接着调用initChannel()方法，添加自定义的Handler，如下图所示 最后将 ChannelInitializer 删掉，它的作用只是在 Channel 进行注册的时候对自定义的 ChannelHandler 添加到 ChannelPipeline sequenceDiagram AbstractUnsafe ->> AbstractUnsafe : register0() 注册Channel AbstractUnsafe ->> ChannelInitializer : pipeline.invokeHandlerAddedIfNeeded() 添加Handler ..-> handlerAdded() ChannelInitializer ->> 子类实现 : 调用 initChannel方法实现 子类实现 ->> ChannelPipeline : 添加 Handler ChannelInitializer ->> ChannelInitializer : removeState(ctx) 方法，将自身在ChannelPipeline中删除 客户端发起连接请求 客户端通过调用 Bootstrap 的 connect() 方法进行连接 。 在 connect() 方法中进行一些参数检查，并调用 doConnect() 方法，其代码实现如下 Bootstrap 123456789101112131415161718192021// 连接最终调用的方法private static void doConnect( final SocketAddress remoteAddress, final SocketAddress localAddress, final ChannelPromise connectPromise) &#123; // This method is invoked before channelRegistered() is triggered. Give user handlers a chance to set up // the pipeline in its channelRegistered() implementation. // Bootstrap 指定的Channel类型 final Channel channel = connectPromise.channel(); // 提交异步任务 channel.eventLoop().execute(new Runnable() &#123; @Override public void run() &#123; if (localAddress == null) &#123; channel.connect(remoteAddress, connectPromise); &#125; else &#123; channel.connect(remoteAddress, localAddress, connectPromise); &#125; connectPromise.addListener(ChannelFutureListener.CLOSE_ON_FAILURE); &#125; &#125;);&#125; 在 doConnect() 方法中 ， eventLoop 线程会调用 Channel 的 connect() 法 ， 而这个 Channel 的具体类型实际就是NioSocketChannel，前面已经分析过。继续跟踪 channel.connect() 方法，我们发现它调用的是 DefaultChannelPipeline 的 connect() 方法，Pipeline的connect()方法的代码如下 1234@Overridepublic final ChannelFuture connect(SocketAddress remoteAddress, ChannelPromise promise) &#123; return tail.connect(remoteAddress, promise);&#125; 我们已经分析过，Tail 是一个 TailContext 的实例，而 TailContext 又是 AbstractChannelHandlerContext 的子类，并且没有实现 connect() 方法，因此这里调用的其实是 AbstractChannelHandlerContext的connect()方法，我们看一下这个方法的实现代码 AbstractChannelHandlerContext 12345678910111213141516171819202122232425@Overridepublic ChannelFuture connect( final SocketAddress remoteAddress, final SocketAddress localAddress, final ChannelPromise promise) &#123; ObjectUtil.checkNotNull(remoteAddress, &quot;remoteAddress&quot;); if (isNotValidPromise(promise, false)) &#123; // cancelled return promise; &#125; // 从 tail 开始向前找，找到第一个OutboundChannelHandler final AbstractChannelHandlerContext next = findContextOutbound(MASK_CONNECT); EventExecutor executor = next.executor(); if (executor.inEventLoop()) &#123; next.invokeConnect(remoteAddress, localAddress, promise); &#125; else &#123; safeExecute(executor, new Runnable() &#123; @Override public void run() &#123; next.invokeConnect(remoteAddress, localAddress, promise); &#125; &#125;, promise, null, false); &#125; return promise;&#125; 上面代码片段中有一句非常关键的代码，即 final AbstractChannelHandlerContextnext=findContextOutbound()，这里调用findContextOutbound()方法，从 DefaultChannelPipeline 内的双向链表的Tail开始，不断向前找到第一个处理Outbound的 AbstractChannelHandlerContext，然后调用它的invokeConnect()方法，代码如下 AbstractChannelHandlerContext 1234567891011121314151617181920// 获取处理出站的HandlerContextprivate AbstractChannelHandlerContext findContextOutbound(int mask) &#123; AbstractChannelHandlerContext ctx = this; EventExecutor currentExecutor = executor(); do &#123; ctx = ctx.prev; &#125; while (skipContext(ctx, currentExecutor, mask, MASK_ONLY_OUTBOUND)); return ctx;&#125;private static boolean skipContext( AbstractChannelHandlerContext ctx, EventExecutor currentExecutor, int mask, int onlyMask) &#123; // Ensure we correctly handle MASK_EXCEPTION_CAUGHT which is not included in the MASK_EXCEPTION_CAUGHT return (ctx.executionMask &amp; (onlyMask | mask)) == 0 || // We can only skip if the EventExecutor is the same as otherwise we need to ensure we offload // everything to preserve ordering. // // See https://github.com/netty/netty/issues/10067 (ctx.executor() == currentExecutor &amp;&amp; (ctx.executionMask &amp; mask) == 0);&#125; 12345678910111213// 执行连接private void invokeConnect(SocketAddress remoteAddress, SocketAddress localAddress, ChannelPromise promise) &#123; if (invokeHandler()) &#123; try &#123; // 这里能够强转是因为在调这个方法前已经调用 findContextOutbound 方法获取 ChannelOutboundHandler 了 ((ChannelOutboundHandler) handler()).connect(this, remoteAddress, localAddress, promise); &#125; catch (Throwable t) &#123; notifyOutboundHandlerException(t, promise); &#125; &#125; else &#123; connect(remoteAddress, localAddress, promise); &#125;&#125; 在 DefaultChannelPipeline 的构造器中，实例化了两个对象：Head 和 Tail，并形成了双向链表的头和尾。Head 是 HeadContext 的实例，它实现了ChannelOutboundHandler 接口。因此在 findContextOutbound() 方法中，找到的 AbstractChannelHandlerContext 对象其实就是Head ， 进而在invokeConnect() 方法中，我们向上转换为 ChannelOutboundHandler 就没问题了。而又因为 HeadContext 重写了 connect()方法，所以实际上调用的是 HeadContext 的 connect() 方法 。 接着跟踪 HeadContext 的connect()方法 HeadContext 12345678// Channel 连接操作@Overridepublic void connect( ChannelHandlerContext ctx, SocketAddress remoteAddress, SocketAddress localAddress, ChannelPromise promise) &#123; unsafe.connect(remoteAddress, localAddress, promise);&#125; 这个 connect() 方法很简单，只是调用了Unsafe的connect()方法。回顾一下 HeadContext 的构造器，我们发现这个Unsafe其实就是pipeline.channel().unsafe() 返回的 Channel 的 Unsafe 属性。到这里为止，我们应该已经知道，其实是 AbstractNioByteChannel.NioByteUnsafe 内部类转了一大圈。最后，我们找到创建 Socket 连接的关键代码继续跟踪，其实调用的就是 AbstractNioUnsafe的connect()方法 NioByteUnsafe 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455// Channel 连接 @Override public final void connect( final SocketAddress remoteAddress, final SocketAddress localAddress, final ChannelPromise promise) &#123; if (!promise.setUncancellable() || !ensureOpen(promise)) &#123; return; &#125; try &#123; if (connectPromise != null) &#123; // Already a connect in process. throw new ConnectionPendingException(); &#125; boolean wasActive = isActive(); if (doConnect(remoteAddress, localAddress)) &#123; fulfillConnectPromise(promise, wasActive); &#125; else &#123; connectPromise = promise; requestedRemoteAddress = remoteAddress; // Schedule connect timeout. int connectTimeoutMillis = config().getConnectTimeoutMillis(); if (connectTimeoutMillis &gt; 0) &#123; connectTimeoutFuture = eventLoop().schedule(new Runnable() &#123; @Override public void run() &#123; ChannelPromise connectPromise = AbstractNioChannel.this.connectPromise; if (connectPromise != null &amp;&amp; !connectPromise.isDone() &amp;&amp; connectPromise.tryFailure(new ConnectTimeoutException( &quot;connection timed out: &quot; + remoteAddress))) &#123; close(voidPromise()); &#125; &#125; &#125;, connectTimeoutMillis, TimeUnit.MILLISECONDS); &#125; promise.addListener(new ChannelFutureListener() &#123; @Override public void operationComplete(ChannelFuture future) throws Exception &#123; if (future.isCancelled()) &#123; if (connectTimeoutFuture != null) &#123; connectTimeoutFuture.cancel(false); &#125; connectPromise = null; close(voidPromise()); &#125; &#125; &#125;); &#125; &#125; catch (Throwable t) &#123; promise.tryFailure(annotateConnectException(t, remoteAddress)); closeIfClosed(); &#125; &#125; 在这个connect()方法中，又调用了doConnect()方法。注意：这个方法并不是AbstractNioUnsafe的方法，而是AbstractNioChannel的抽象方法。doConnect()方法是在NioSocketChannel中实现的，因此进入NioSocketChannel的doConnect()方法，代码如下 AbstractNioUnsafe 1234567891011121314151617181920@Overrideprotected boolean doConnect(SocketAddress remoteAddress, SocketAddress localAddress) throws Exception &#123; if (localAddress != null) &#123; doBind0(localAddress); &#125; boolean success = false; try &#123; boolean connected = SocketUtils.connect(javaChannel(), remoteAddress); if (!connected) &#123; selectionKey().interestOps(SelectionKey.OP_CONNECT); &#125; success = true; return connected; &#125; finally &#123; if (!success) &#123; doClose(); &#125; &#125;&#125; 上面代码的功能是，首先获取Java NIO的SocketChannel，然后获取NioSocketChannel的newSocket()方法返回的SocketChannel对象；再调SocketChannel的 connect() 方法完成Java NIO底层的Socket连接 服务端 BossGroup和WorkerGroup Netty中的BossGroup和WorkerGroup的实际类型是NioEventLoopGroup,通过类图能够发现是通过JUC线程池接口扩展而来的 线程池线程的个数（NioEventLoop个数）如果在构造函数不指定的话，默认是CPU核心数的2倍 NioEventLoopGroup 就是 NioEventLoop 组，负责管理 NioEventLoop，当有 Channel 需要注册的时候，NioEventLoopGroup 会轮询找到下一个 NioEventLoop 注册上去。在NioEventLoopGroup 上作出的配置最终都会作用到 NioEventLoop 上 源码解析 官方 Netty Example 源码 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849public final class EchoServer &#123; static final boolean SSL = System.getProperty(&quot;ssl&quot;) != null; static final int PORT = Integer.parseInt(System.getProperty(&quot;port&quot;, &quot;8007&quot;)); public static void main(String[] args) throws Exception &#123; // Configure SSL. final SslContext sslCtx; if (SSL) &#123; SelfSignedCertificate ssc = new SelfSignedCertificate(); sslCtx = SslContextBuilder.forServer(ssc.certificate(), ssc.privateKey()).build(); &#125; else &#123; sslCtx = null; &#125; // Configure the server. EventLoopGroup bossGroup = new NioEventLoopGroup(1); EventLoopGroup workerGroup = new NioEventLoopGroup(); final EchoServerHandler serverHandler = new EchoServerHandler(); try &#123; ServerBootstrap b = new ServerBootstrap(); b.group(bossGroup, workerGroup) .channel(NioServerSocketChannel.class) .option(ChannelOption.SO_BACKLOG, 100) .handler(new LoggingHandler(LogLevel.INFO)) .childHandler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override public void initChannel(SocketChannel ch) throws Exception &#123; ChannelPipeline p = ch.pipeline(); if (sslCtx != null) &#123; p.addLast(sslCtx.newHandler(ch.alloc())); &#125; //p.addLast(new LoggingHandler(LogLevel.INFO)); p.addLast(serverHandler); &#125; &#125;); // Start the server. ChannelFuture f = b.bind(PORT).sync(); // Wait until the server socket is closed. f.channel().closeFuture().sync(); &#125; finally &#123; // Shut down all event loops to terminate all threads. bossGroup.shutdownGracefully(); workerGroup.shutdownGracefully(); &#125; &#125;&#125; 先看启动类：main 方法中，首先创建了关于 SSL 的配置类 重点分析下 创建了两个 EventLoopGroup 对象： 12EventLoopGroup bossGroup = new NioEventLoopGroup(1);EventLoopGroup workerGroup = new NioEventLoopGroup(); 这两个对象是整个 Netty 的核心对象，可以说，整个 Netty 的运作都依赖于他们。bossGroup 用于接受Tcp 请求，他会将请求交给 workerGroup ，workerGroup 会获取到真正的连接，然后和连接进行通信，比如读写解码编码等操作 EventLoopGroup 是事件循环组（线程组） 含有多个 EventLoop，可以注册 channel ,用于在事件循环中去进行选择（和选择器相关） 在NioEventLoopGroup的父类MultithreadEventExecutorGroup类中包含属性private final EventExecutor[] children,NioEventLoop类是接口EventExecutor的实现类之一 new NioEventLoopGroup(1); 这个 1 表示 bossGroup 事件组有 1 个线程你可以指定，如果 new NioEventLoopGroup() 会含有默认个线程 cpu 核数 * 2, 即可以充分的利用多核的优势 123456789101112131415161718192021222324252627282930public abstract class MultithreadEventLoopGroup extends MultithreadEventExecutorGroup implements EventLoopGroup &#123; private static final InternalLogger logger = InternalLoggerFactory.getInstance(MultithreadEventLoopGroup.class); private static final int DEFAULT_EVENT_LOOP_THREADS; /** * NettyRuntime.availableProcessors() 会拿到当前计算机的Cpu的核心数 * 最终结果是 Cpu核心数 * 2 */ static &#123; DEFAULT_EVENT_LOOP_THREADS = Math.max(1, SystemPropertyUtil.getInt( &quot;io.netty.eventLoopThreads&quot;, NettyRuntime.availableProcessors() * 2)); if (logger.isDebugEnabled()) &#123; logger.debug(&quot;-Dio.netty.eventLoopThreads: &#123;&#125;&quot;, DEFAULT_EVENT_LOOP_THREADS); &#125; &#125; /** * 跟踪 new NioEventLoopGroup() 源码 * 在类 MultithreadEventLoopGroup 源码的构造方法中，如果传入的 nThreads 为0 （new EventLoopGroup() 构造函数传入） * 则使用默认值 DEFAULT_EVENT_LOOP_THREADS * @see MultithreadEventExecutorGroup#MultithreadEventExecutorGroup(int, Executor, Object...) */ protected MultithreadEventLoopGroup(int nThreads, Executor executor, Object... args) &#123; super(nThreads == 0 ? DEFAULT_EVENT_LOOP_THREADS : nThreads, executor, args); &#125;...&#125; NioEventLoopGroup初始化过程 MultithreadEventExecutorGroup 源码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102public abstract class MultithreadEventExecutorGroup extends AbstractEventExecutorGroup &#123; // 这个就是 EventLoop 的容器 private final EventExecutor[] children; // http://gitbook.net/java/util/collections_unmodifiableset.html // 只读的 EventLoop 集合，当children初始化完成就会拷贝过来，底层类是 UnmodifiableSet，只读的Set, 添加会抛异常 private final Set&lt;EventExecutor&gt; readonlyChildren; // 终止的 EventLoop 数量 private final AtomicInteger terminatedChildren = new AtomicInteger(); // Promise 在 Future(JUC) 接口的基础上扩展了，可写的接口，DefaultPromise 是通过 CAS 来进行更新执行的结果 result 字段的 private final Promise&lt;?&gt; terminationFuture = new DefaultPromise(GlobalEventExecutor.INSTANCE); // EventLoop 选择器，Channel 注册到 EventLoop 时使用 private final EventExecutorChooserFactory.EventExecutorChooser chooser; ... /** * 最终的创建实例调用的构造器 * * @param nThreads 该实例将使用的线程数 * @param executor 将要使用的executor, 默认为null * @param chooserFactory 将要使用的EventExecutorChooserFactory * @param args arguments which will passed to each &#123;@link #newChild(Executor, Object...)&#125; call */ protected MultithreadEventExecutorGroup(int nThreads, Executor executor, EventExecutorChooserFactory chooserFactory, Object... args) &#123; // 校验线程数量的合法性 checkPositive(nThreads, &quot;nThreads&quot;); // executor校验非空, 如果为空就创建ThreadPerTaskExecutor, 该类实现了 Executor接口(JUC) // 题外说明：Executor接口的execute方法在JUC线程池中是用于提交异步任务的 // 这个executor 是用来执行线程池中的所有的线程，也就是所有的NioEventLoop，其实从 // NioEventLoop构造器中也可以知道，NioEventLoop构造器中都传入了executor这个参数 if (executor == null) &#123; executor = new ThreadPerTaskExecutor(newDefaultThreadFactory()); &#125; // 这里的children数组， 其实就是线程池的核心实现，线程池中就是通过指定的线程数组来实现线程池； // 数组中每个元素其实就是一个NioEventLoop children = new EventExecutor[nThreads]; // for循环实例化children数组，NioEventLoop对象 for (int i = 0; i &lt; nThreads; i++) &#123; boolean success = false; try &#123; // newChild 这个抽象方法在 NioEventLoopGroup类 被实现，就是返回 NioEventLoop children[i] = newChild(executor, args); success = true; &#125; catch (Exception e) &#123; // TODO: Think about if this is a good exception type throw new IllegalStateException(&quot;failed to create a child event loop&quot;, e); &#125; finally &#123; // 如果创建失败就释放资源 if (!success) &#123; for (int j = 0; j &lt; i; j++) &#123; children[j].shutdownGracefully(); &#125; for (int j = 0; j &lt; i; j++) &#123; EventExecutor e = children[j]; try &#123; while (!e.isTerminated()) &#123; e.awaitTermination(Integer.MAX_VALUE, TimeUnit.SECONDS); &#125; &#125; catch (InterruptedException interrupted) &#123; // Let the caller handle the interruption. Thread.currentThread().interrupt(); break; &#125; &#125; &#125; &#125; &#125; // chooserFactory类型是 DefaultEventExecutorChooserFactory // chooser类型是 DefaultEventExecutorChooserFactory的两个内部类之一，选择器的作用就是当BossGroup创建连接后注册到WorkerGroup的EventLoop时 // 需要使用来选择数组中的EventLoop, 底层算法是通过 executors[(int) Math.abs(idx.getAndIncrement() % executors.length)] 实现的轮询算法 chooser = chooserFactory.newChooser(children); // 为每个EventLoop线程添加 线程终止监听器 final FutureListener&lt;Object&gt; terminationListener = new FutureListener&lt;Object&gt;() &#123; @Override public void operationComplete(Future&lt;Object&gt; future) throws Exception &#123; // 异常终止数量自增 if (terminatedChildren.incrementAndGet() == children.length) &#123; terminationFuture.setSuccess(null); &#125; &#125; &#125;; for (EventExecutor e : children) &#123; e.terminationFuture().addListener(terminationListener); &#125; // 将children 添加到对应的set集合中去重， 表示只可读 Set&lt;EventExecutor&gt; childrenSet = new LinkedHashSet&lt;EventExecutor&gt;(children.length); Collections.addAll(childrenSet, children); readonlyChildren = Collections.unmodifiableSet(childrenSet); &#125;...&#125; ServerBootstrap Bootstrap 意思是引导，一个 Netty 应用通常由一个 Bootstrap 开始 ，主要作用是配置整个 Netty 程序，串联各个组件, ServerBootstrap是服务端启动引导类 常见的方法有： 方法 说明 public ServerBootstrap group(EventLoopGroup parentGroup, EventLoopGroup childGroup) 该方法用于服务器端，用来设置两个 EventLoopGroup public B channel(Class&lt;? extends C&gt; channelClass) 该方法用来设置一个服务器端的通道实现 public ChannelFuture bind(int inetPort) 该方法用于服务器端，用来设置占用的端口号 public B option(ChannelOption option, T value) 用来给 ServerChannel 添加配置 public B handler(ChannelHandler handler) 用于给bossGroup设置业务处理类 public B group(EventLoopGroup group) 该方法用于客户端，用来设置一个 EventLoopGroup public ChannelFuture connect(String inetHost, int inetPort) 该方法用于客户端，用来连接服务器端 public ServerBootstrap childOption(ChannelOption childOption, T value) 用来给接收到的通道添加配置 public ServerBootstrap childHandler(ChannelHandler childHandler) 该方法用来设置业务处理类 （workerGroup 自定义的 handler） 服务端的启动类，扩展于NIO的Channel接口 用于Netty的启动配置，如通道类型，Handler等配置 BossGroup的配置在ServerBootstrap实例中，WorkerGroup的配置信息在AbstractBootstrap中 源码解析 源码地址 123456789101112131415161718192021222324252627282930313233// Configure the server. EventLoopGroup bossGroup = new NioEventLoopGroup(1); EventLoopGroup workerGroup = new NioEventLoopGroup(5); final EchoServerHandler serverHandler = new EchoServerHandler(); try &#123; ServerBootstrap b = new ServerBootstrap(); b.group(bossGroup, workerGroup) // 指定channel类型，通过反射创建 .channel(NioServerSocketChannel.class) .option(ChannelOption.SO_BACKLOG, 100) .handler(new LoggingHandler(LogLevel.INFO)) .childHandler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override public void initChannel(SocketChannel ch) throws Exception &#123; ChannelPipeline p = ch.pipeline(); if (sslCtx != null) &#123; p.addLast(sslCtx.newHandler(ch.alloc())); &#125; //p.addLast(new LoggingHandler(LogLevel.INFO)); p.addLast(serverHandler); &#125; &#125;); // Start the server. ChannelFuture f = b.bind(PORT).sync(); // Wait until the server socket is closed. f.channel().closeFuture().sync(); &#125; finally &#123; // Shut down all event loops to terminate all threads. bossGroup.shutdownGracefully(); workerGroup.shutdownGracefully(); &#125; 变量 b 调用了 group 方法将两个 group 放入了自己的字段中，用于后期引导使用 ServerBootstrap.java 123456789101112131415161718192021222324252627282930313233343536public class ServerBootstrap extends AbstractBootstrap&lt;ServerBootstrap, ServerChannel&gt; &#123; private static final InternalLogger logger = InternalLoggerFactory.getInstance(ServerBootstrap.class); // The order in which child ChannelOptions are applied is important they may depend on each other for validation // purposes. // Channel 的配置属性项 private final Map&lt;ChannelOption&lt;?&gt;, Object&gt; childOptions = new LinkedHashMap&lt;ChannelOption&lt;?&gt;, Object&gt;(); // AttributeMap 的key, AttributeMap 是 Channel 接口的顶级接口，每个Channel 必定是个 AttributeMap private final Map&lt;AttributeKey&lt;?&gt;, Object&gt; childAttrs = new ConcurrentHashMap&lt;AttributeKey&lt;?&gt;, Object&gt;(); // ServerBootstrap 配置类，主要提供获取 BossGroup 和 WorkerGroup配置能力 private final ServerBootstrapConfig config = new ServerBootstrapConfig(this); // WorkerGroup private volatile EventLoopGroup childGroup; // WorkerGroup 对应的handler, 用于处理 initChannel 事件。当Channel在注册WorkerGroup时触发 private volatile ChannelHandler childHandler; ... /** * 设置BossGroup 和 WorkerGroup * Set the &#123;@link EventLoopGroup&#125; for the parent (acceptor) and the child (client). These * &#123;@link EventLoopGroup&#125;&#x27;s are used to handle all the events and IO for &#123;@link ServerChannel&#125; and * &#123;@link Channel&#125;&#x27;s. */ public ServerBootstrap group(EventLoopGroup parentGroup, EventLoopGroup childGroup) &#123; //调用父类方法 BossGroup 的相关属性都放在父类 AbstractBootstrap里 super.group(parentGroup); if (this.childGroup != null) &#123; throw new IllegalStateException(&quot;childGroup set already&quot;); &#125; // 将WorkerGroup赋值到当前对象 WorkerGroup相关配置都在 ServerBootstrap this.childGroup = ObjectUtil.checkNotNull(childGroup, &quot;childGroup&quot;); return this; &#125; ... &#125; .channel(NioServerSocketChannel.class) 用于指定所使用的Channel类型，并会创建一个Channel工厂用于反射生成Channel AbstractBootstrap.java 1234567891011121314151617181920212223242526/** * 设置服务端Channel类型，返回ReflectiveChannelFactory 它是一个反射工厂， 通过反射创建Channel对象 * The &#123;@link Class&#125; which is used to create &#123;@link Channel&#125; instances from. * You either use this or &#123;@link #channelFactory(io.netty.channel.ChannelFactory)&#125; if your * &#123;@link Channel&#125; implementation has no no-args constructor. */public B channel(Class&lt;? extends C&gt; channelClass) &#123; return channelFactory(new ReflectiveChannelFactory&lt;C&gt;( ObjectUtil.checkNotNull(channelClass, &quot;channelClass&quot;) ));&#125;/** * 设置Channel工厂 * @deprecated Use &#123;@link #channelFactory(io.netty.channel.ChannelFactory)&#125; instead. */@Deprecatedpublic B channelFactory(ChannelFactory&lt;? extends C&gt; channelFactory) &#123; ObjectUtil.checkNotNull(channelFactory, &quot;channelFactory&quot;); if (this.channelFactory != null) &#123; throw new IllegalStateException(&quot;channelFactory set already&quot;); &#125; this.channelFactory = channelFactory; return self();&#125; ReflectiveChannelFactory 是如何通过反射创建Channel的 ReflectiveChannelFactory.java 123456789101112131415161718192021222324252627282930313233343536/** * 反射Channel工厂 * A &#123;@link ChannelFactory&#125; that instantiates a new &#123;@link Channel&#125; by invoking its default constructor reflectively. */public class ReflectiveChannelFactory&lt;T extends Channel&gt; implements ChannelFactory&lt;T&gt; &#123; // Channel 的构造函数 private final Constructor&lt;? extends T&gt; constructor; /** * 构造函数，根据传递进来的Channel类型获取其无参构造函数 */ public ReflectiveChannelFactory(Class&lt;? extends T&gt; clazz) &#123; ObjectUtil.checkNotNull(clazz, &quot;clazz&quot;); try &#123; this.constructor = clazz.getConstructor(); &#125; catch (NoSuchMethodException e) &#123; throw new IllegalArgumentException(&quot;Class &quot; + StringUtil.simpleClassName(clazz) + &quot; does not have a public non-arg constructor&quot;, e); &#125; &#125; /** * 通过无参构造函数调用Java反射Api生成一个新的Channel,在有客户端建立新的连接时使用 * @return */ @Override public T newChannel() &#123; try &#123; return constructor.newInstance(); &#125; catch (Throwable t) &#123; throw new ChannelException(&quot;Unable to create Channel from class &quot; + constructor.getDeclaringClass(), t); &#125; &#125; ...&#125; .option(ChannelOption.SO_BACKLOG, 100) 设置BossGroup配置属性项 AbstractBootstrap.java 1234567891011121314151617/** * 设置BossGroup 的配置属性项 * Allow to specify a &#123;@link ChannelOption&#125; which is used for the &#123;@link Channel&#125; instances once they got * created. Use a value of &#123;@code null&#125; to remove a previous set &#123;@link ChannelOption&#125;. */public &lt;T&gt; B option(ChannelOption&lt;T&gt; option, T value) &#123; ObjectUtil.checkNotNull(option, &quot;option&quot;); synchronized (options) &#123; // options 是个map if (value == null) &#123; options.remove(option); &#125; else &#123; options.put(option, value); &#125; &#125; return self();&#125; .handler(…) .childHandler(…) ServerBootstrap.java AbstractBootstrap.java 分别为AbstractBootstrap#handler 和 ServerBootstrap#childHandler 赋值 ServerBootstrap的bind是最终启动服务的方法 它首先会通过反射创建一个指定类型的ServerSocketChannel 然后根据ServerBootStrap设置的配置去初始化ServerSocketChannel 通过ServerSocketChannel获得DefaultChannelPipeline，在图中能看到Channel 和Pipeline是相互包含关系,在创建Channel的同时就会创建Pipeline 获取ServerBootStrap设置相关的child配置（WorkerGroup的配置） 创建一个异步任务，为Pipeline添加一个handlerServerBootstrapAcceptor ServerBootstrapAcceptor处理客户端连接的Accect事件，获得SocketChannel后就会获取child配置初始化到Channel,然后注册到WorkerGroup SocketChannel注册到WorkerGroup的EventLoop中，默认采用的是轮询算法进行注册，注册完成之后就监听读事件，等待客户端请求 NioEventLoop NioEventLoop 就是一个事件循环类，几乎所有事件处理都会经过这个类 和NioEventLoopGroup一样都是扩展于JUC包下的线程池接口，不同的是事件循环是一个单线程的线程池 根据类图可知每个子线程都有自己的Selector (NioEventLoop#selector) 和TaskQueue (SingleThreadEventExecutor#taskQueue) 源码剖析 Channel 每个NioChannel只会绑定一个EventLoop Netty 网络通信的组件，能够用于执行网络 I/O 操作 通过Channel 可获得当前网络连接的通道的状态 通过Channel 可获得网络连接的配置参数 （例如接收缓冲区大小 ） Channel 提供异步的网络 I/O 操作(如建立连接，读写，绑定端口)，异步调用意味着任何 I/O 调用都将立即返回，并且不保证在调用结束时所请求的 I/O 操作已完成 调用立即返回一个 ChannelFuture 实例，通过注册监听器到 ChannelFuture 上，可以 I/O 操作成功、失败或取消时回调通知调用方 支持关联 I/O 操作与对应的处理程序 不同协议、不同的阻塞类型的连接都有不同的 Channel 类型与之对应 常用的 Channel 类型 通道类型 说明 NioSocketChannel 异步的客户端 TCP Socket 连接 NioServerSocketChannel 异步的服务器端 TCP Socket 连接 NioDatagramChannel 异步的 UDP 连接 NioSctpChannel 异步的客户端 Sctp 连接 NioSctpServerChannel 异步的 Sctp 服务器端连接，这些通道涵盖了 UDP 和 TCP 网络 IO 以及文件 IO OioSocketChannel 同步的客户端 TCP Socket 连接 OioServerSocketChannel 同步的服务器端 TCP Socket 连接 OioDatagramChannel 同步的 UDP 连接 OioSctpChannel 同步的 Sctp 服务器端连接 OioSctpServerChannel 同步的客户端 TCP Socket 连接 ChannelOption 异步模型 异步的概念和同步是相对的。当一个异步过程调用发出后，调用者不能立刻得到结果。实际处理这个调用的组件在完成后，通过状态，通知和回调来通知调用者 Netty中的 I/O 操作是异步的，包括 Bind, Write, Connect等操作会简单地返回一个ChannelFuture 调用者并不能立即获得结果，而是通过 Future-Listener 机制，用户可以方便主动获取或通过通知机制获得 I/O 操作结果 Netty的异步模型是建立在 Future和Callback之上的（JUC的异步任务）。Callback就是回调。重点说 Future,它的核心思想是：假设一个方法 fun, 计算过程可能非常耗时，等待fun返回显然不太合适。那么可以在调用fun的时候，立马返回一个 Future, 后续可以通过Future去监控方法fun的处理过程（Future-Listener机制） Future-Listener 机制 Future 表示异步的执行结果，可以通过它提供的方法来检测执行是否完成，比如检索计算等等 在使用Netty进行编程时，拦截操作和转换出入站数据只需要提供 callback 或利用 future 即可，这使得链式操作简单，高效，并有利编写可重用的，通用的代码 Netty框架的目标就是让你的业务代码逻辑从网络编程中分离出来，解脱出来 ChannelFuture ChannelFuture是一个接口，我们可以添加监听器，当监听的事件发生时，就会通知到监听器 常用方法： Channel channel()，返回当前正在进行 IO 操作的通道 ChannelFuture sync()，等待异步操作执行完毕 ChannelFuture addListener(GenericFutureListener&lt;? extends Future&lt;? super Void&gt;&gt; var1) 注册监听器 顶层的Future是JUC的接口，第二个是Netty包下的接口 Future-Listener 机制当 Future 对象刚刚创建时，处于非完成状态，调用者可以通过返回的 ChannelFuture 来获取操作执行的状态，注册监听函数来执行完成后的操作 常见有如下操作 通过 isDone 方法来判断当前操作是否完成 通过 isSuccess 方法来判断已完成的当前操作是否成功； 通过 getCause 方法来获取已完成的当前操作失败的原因； 通过 isCancelled 方法来判断已完成的当前操作是否被取消； 通过 addListener 方法来注册监听器，当操作已完成(isDone 方法返回完成)，将会通知指定的监听器；如果 Future 对象已完成，则通知指定的监听器 给一个 ChannelFuture 注册监听器，来监控我们关心的事件 1234567891011121314//启动服务器(并绑定端口),bind是一个异步操作 ChannelFuture channelFuture = bootstrap.bind(PORT).sync(); //给channelFuture 注册监听器，监控我们关心的事件 channelFuture.addListener(new ChannelFutureListener() &#123; @Override public void operationComplete(ChannelFuture future) throws Exception &#123; if(future.isSuccess()) &#123; System.out.println(&quot;监听端口 6666 成功&quot;); &#125; else &#123; System.out.println(&quot;监听端口 6666 失败&quot;); &#125; &#125; &#125;); Selector 参考 NIO Selector Netty 基于 Selector 对象实现 I/O 多路复用，通过 Selector 一个线程可以监听多个连接的 Channel 事件 当向一个 Selector 中注册 Channel 后，Selector 内部的机制就可以自动不断地查询 (Select) 这些注册的 Channel 是否有已就绪的 I/O 事件（例如可读，可写，网络连接 完成等），这样程序就可以很简单地使用一个线程高效地管理多个 Channel ChannelHandler ChannelHandler是一个接口，处理 I/O 事件或拦截 I/O 操作，并将其转发到ChannelPipeline（业务处理链）中的下一个处理程序, Handler是Netty业务处理的重要体系 ChannelHandler本身并没有提供很多方法，因为这个接口有许多方法需要实现，方便使用期间，可以继承它的子类 ChannelInboundHandler 用于处理入站 I/O 事件 ChannelOutboundHandler用于处理出站 I/O 事件 适配器 ChannelInboundHandlerAdapter 用于处理入站 I/O 事件 ChannelOutboundHandlerAdapter 用于处理出站 I/O 事件 ChannelDuplexHandler 用于处理出站和入站事件 123456789101112131415public class HttpServerInitialize extends ChannelInitializer&lt;SocketChannel&gt; &#123; @Override protected void initChannel(SocketChannel socketChannel) throws Exception &#123; // 向管道加入处理器 // 得到管道 final ChannelPipeline pipeline = socketChannel.pipeline(); // 加入一个Netty提供的 HttpServerCodec (http协议编解码器) codec =&gt; decoder pipeline.addLast(&quot;MyHttpServerCodec&quot;, new HttpServerCodec()); // 增加一个针对http协议的handler pipeline.addLast(&quot;MyHttpServerHandler&quot;, new HttpServerHandler()); &#125;&#125; Handler常用的方法 12345678910111213141516171819202122232425262728293031323334public interface ChannelInboundHandler extends ChannelHandler &#123; /** * Channel注册到EventLoop的时候，调用 */ void channelRegistered(ChannelHandlerContext ctx) throws Exception; /** * Channel从EventLoop注销的时候，调用 */ void channelUnregistered(ChannelHandlerContext ctx) throws Exception; /** * Channel活跃的时候，调用 */ void channelActive(ChannelHandlerContext ctx) throws Exception; /** * Channel不活跃的时候，调用，此时生命周期马上结束 */ void channelInactive(ChannelHandlerContext ctx) throws Exception; /** * Channel读取到消息的时候调用 */ void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception; /** * 抛出异常时调用 */ @Override @SuppressWarnings(&quot;deprecation&quot;) void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455/** * 发生IO出站事件的时候，方法会得到通知调用 */public interface ChannelOutboundHandler extends ChannelHandler &#123; /** * Called once a bind operation is made. * 绑定操作被执行的时候调用 */ void bind(ChannelHandlerContext ctx, SocketAddress localAddress, ChannelPromise promise) throws Exception; /** * Called once a connect operation is made. * 连接操作执行的时候调用 */ void connect(ChannelHandlerContext ctx, SocketAddress remoteAddress, SocketAddress localAddress, ChannelPromise promise) throws Exception; /** * Called once a disconnect operation is made. * 断开连接的时候调用 */ void disconnect(ChannelHandlerContext ctx, ChannelPromise promise) throws Exception; /** * Called once a close operation is made. * 关闭的时候调用 */ void close(ChannelHandlerContext ctx, ChannelPromise promise) throws Exception; /** * Called once a deregister operation is made from the current registered &#123;@link EventLoop&#125;. * 注销的时候调用 */ void deregister(ChannelHandlerContext ctx, ChannelPromise promise) throws Exception; /** * Intercepts &#123;@link ChannelHandlerContext#read()&#125;. * 拦截ChannelHandlerContext#read()方法 */ void read(ChannelHandlerContext ctx) throws Exception; /** * Called once a write operation is made. The write operation will write the messages through the * &#123;@link ChannelPipeline&#125;. Those are then ready to be flushed to the actual &#123;@link Channel&#125; once * &#123;@link Channel#flush()&#125; is called * 写操作时候调用，写的消息会经过ChannelPipeline，调用Channel#flush()的时候，消息会被flush到Channel */ void write(ChannelHandlerContext ctx, Object msg, ChannelPromise promise) throws Exception; /** * Called once a flush operation is made. The flush operation will try to flush out all previous written messages that are pending. * flush会将前面前部pending的消息flush到Channel */ void flush(ChannelHandlerContext ctx) throws Exception;&#125; 适配器 1234567891011121314151617181920212223242526272829303132333435363738public abstract class ChannelHandlerAdapter implements ChannelHandler &#123; boolean added; public ChannelHandlerAdapter() &#123; &#125; protected void ensureNotSharable() &#123; if (this.isSharable()) &#123; throw new IllegalStateException(&quot;ChannelHandler &quot; + this.getClass().getName() + &quot; is not allowed to be shared&quot;); &#125; &#125; // 判断当前hanlder是否是可共享(在多个pipeline中） public boolean isSharable() &#123; Class&lt;?&gt; clazz = this.getClass(); Map&lt;Class&lt;?&gt;, Boolean&gt; cache = InternalThreadLocalMap.get().handlerSharableCache(); Boolean sharable = (Boolean)cache.get(clazz); if (sharable == null) &#123; sharable = clazz.isAnnotationPresent(Sharable.class); cache.put(clazz, sharable); &#125; return sharable; &#125; // 在ChannelHandler被添加到实际上下文中并准备好处理事件后调 public void handlerAdded(ChannelHandlerContext ctx) throws Exception &#123; &#125; // 在ChannelHandler从实际上下文中移除后调用，表明它不再处理事件 public void handlerRemoved(ChannelHandlerContext ctx) throws Exception &#123; &#125; // 在抛出Throwable类后调用 public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; ctx.fireExceptionCaught(cause); &#125;&#125; 12345678910111213141516171819202122232425262728293031public class ChannelInboundHandlerAdapter extends ChannelHandlerAdapter implements ChannelInboundHandler &#123; // 通道注册事件 public void channelRegistered(ChannelHandlerContext ctx) throws Exception &#123; ctx.fireChannelRegistered(); &#125; // 通道注销事件 public void channelUnregistered(ChannelHandlerContext ctx) throws Exception &#123; ctx.fireChannelUnregistered(); &#125; // 通道就绪事件 public void channelActive(ChannelHandlerContext ctx) throws Exception &#123; ctx.fireChannelActive(); &#125; // 通道读取数据事件 public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; ctx.fireChannelRead(msg); &#125; // 通道读取数据完毕事件 public void channelReadComplete(ChannelHandlerContext ctx) throws Exception &#123; ctx.fireChannelReadComplete(); &#125; // 通道发生异常事件 public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; ctx.fireExceptionCaught(cause); &#125;&#125; 12345678910111213141516171819202122232425262728public interface ChannelOutboundHandler extends ChannelHandler &#123; void bind(ChannelHandlerContext var1, SocketAddress var2, ChannelPromise var3) throws Exception; /** * 当NioServerSocketChannel创建、初始化、注册到EventLoopGroup完成后，接下来就进行绑定，与本地 * 端口进行绑定以便接收数据,绑定的工作通过代码分析发现最后调用的是 AbstractBootstrap#doBind0方法 * @throws Exception */ void connect(ChannelHandlerContext var1, SocketAddress var2, SocketAddress var3, ChannelPromise var4) throws Exception; // 当请求将 Channel 连接到远程节点时被调用 void disconnect(ChannelHandlerContext var1, ChannelPromise var2) throws Exception; // 当请求关闭 Channel 时被调用 void close(ChannelHandlerContext var1, ChannelPromise var2) throws Exception; // 当请求将 Channel 从远程节点断开时被调用 void deregister(ChannelHandlerContext var1, ChannelPromise var2) throws Exception; // 当请求从 Channel 读取更多的数据时被调用 void read(ChannelHandlerContext var1) throws Exception; // 当请求通过 Channel 将数据写到远程节点时被调用 void write(ChannelHandlerContext var1, Object var2, ChannelPromise var3) throws Exception; // 当请求通过 Channel 将入队数据冲刷到远程节点时被调 void flush(ChannelHandlerContext var1) throws Exception;&#125; 出站和入站机制 ChannelPipeline 12345678910...childHandler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; //创建一个通道初始化对象(匿名对象) //给pipeline 设置处理器,可以设置多个，是个双向链表 @Override protected void initChannel(SocketChannel socketChannel) throws Exception &#123; System.out.println(&quot;client socketChannel hashCode = &quot; + socketChannel.hashCode()); socketChannel.pipeline().addLast(new NettyServerHandler()); &#125; &#125;); ChannelPipeline 是一个 Handler 的集合（双向链表），它负责处理和拦截 inbound（入站） 或者 outbound（出栈） 的事件和操作，相当于一个贯穿 Netty 的链。(也可以这样理解： ChannelPipeline 是 保存 ChannelHandler 的 List，用于处理或拦截 Channel 的入站 和出站 事件 / 操作) ChannelPipeline 实现了一种高级形式的拦截过滤器模式，使用户可以完全控制事件的处理方式，以及 Channel 中各个的 ChannelHandler 如何相互交互 在 Netty 中每个 Channel 都有且仅有一个 ChannelPipeline 与之对应它们的组成关系如下 入站和出站 入站：从ChannelHandlerContext链表的head调用到tail 出站：从ChannelHandlerContext链表的tail调用到head 说明： 一个 Channel 包含了一个 ChannelPipeline，而 ChannelPipeline 中又维护了一个由 ChannelHandlerContext 组成的双向链表，并且每个 ChannelHandlerContext 中又关联着一个 ChannelHandler 入站事件和出站事件在一个双向链表中，入站事件会从链表 head 往后传递到最后一个入站的 handler， 出站事件会从链表 tail 往前传递到最前一个出站的 handler，两种类型的 handler 互不干扰 ChannelHandlerContext 数据结构：双向链表, AbstractChannelHandlerContext 包含 next prev 包含 Channel ChannelHandler：ChannelHandlerContext和ChannelHandler是一一对应关系 ChannelPipeline 入站：从ChannelHandlerContext链表的head调用到tail 出站：从ChannelHandlerContext链表的tail调用到head Buf Netty 重新实现了体系，实现了读写操作不再需要flip操作 实现了同时读写功能 capacity: buf容量 readerIndex：readerIndex -&gt; writerIndex 之间是可读性范围 writerIndex： writerIndex -&gt; capacity 之间是可写范围 Buf实现了自动扩容 12345678910111213141516171819202122232425262728293031323334@Slf4jpublic class BufTest &#123; public static void main(String[] args) &#123; // 非池化的Buf ByteBuf buf = Unpooled.buffer(10); for (int i = 0; i &lt; 100; i++) &#123; // 能够自动扩容 buf.writeByte(i); &#125; for (int i = 0; i &lt; 5; i++) &#123; log.info(&quot;&#123;&#125;&quot;, buf.readByte()); &#125; log.info(&quot;类型：&#123;&#125;&quot;, buf.getClass()); log.info(&quot;capacity = &#123;&#125;&quot;, buf.capacity()); log.info(&quot;readIndex = &#123;&#125;&quot;, buf.readerIndex()); log.info(&quot;writeIndex = &#123;&#125;&quot;, buf.writerIndex()); log.info(&quot;可读取的 = &#123;&#125;&quot;, buf.readableBytes()); // 读取一部分 buf.getCharSequence(6, 2, Charset.forName(&quot;utf-8&quot;)); &#125;&#125;02:51:58.787 [main] INFO com.wgf.netty.BufTest - 002:51:58.790 [main] INFO com.wgf.netty.BufTest - 102:51:58.791 [main] INFO com.wgf.netty.BufTest - 202:51:58.791 [main] INFO com.wgf.netty.BufTest - 302:51:58.791 [main] INFO com.wgf.netty.BufTest - 402:51:58.791 [main] INFO com.wgf.netty.BufTest - 类型：class io.netty.buffer.UnpooledByteBufAllocator$InstrumentedUnpooledUnsafeHeapByteBuf02:51:58.791 [main] INFO com.wgf.netty.BufTest - capacity = 12802:51:58.791 [main] INFO com.wgf.netty.BufTest - readIndex = 502:51:58.791 [main] INFO com.wgf.netty.BufTest - writeIndex = 10002:51:58.791 [main] INFO com.wgf.netty.BufTest - 可读取的 = 95 ByteBuf分类 Netty使用ByteBuf对象作为数据容器，进行I/O读写操作，Netty的内存管理也是围绕着ByteBuf对象高效地分配和释放 当讨论ByteBuf对象管理，主要从以下方面进行分类 Pooled 和 Unpooled 池化内存分配时基于预分配的一整块大内存，取其中的部分封装成ByteBuf提供使用，用完后回收到内存池中 非池化内存每次分配时直接调用系统 API 向操作系统申请ByteBuf需要的同样大小内存，用完后通过系统调用进行释放Pooled Netty4默认使用Pooled的方式，可通过参数-Dio.netty.allocator.type=unpooled或pooled进行设置 Heap 和 Direct Heap，指ByteBuf关联的内存JVM堆内分配，分配的内存受GC 管理 Direct，指ByteBuf关联的内存在JVM堆外分配，分配的内存不受GC管理，需要通过系统调用实现申请和释放，底层基于Java NIO的DirectByteBuffer对象 HTTP服务 使用Netty实现Http协议编解码，支持浏览器访问 熟悉Netty Http开发 服务端 12345678910111213141516171819202122public class HttpServer &#123; public static void main(String[] args) throws InterruptedException &#123; EventLoopGroup bossGroup = new NioEventLoopGroup(1); EventLoopGroup workGroup = new NioEventLoopGroup(); try &#123; ServerBootstrap serverBootstrap = new ServerBootstrap() .group(bossGroup, workGroup) .channel(NioServerSocketChannel.class) .childHandler(new HttpServerInitialize()); // 自定义 handler final ChannelFuture channelFuture = serverBootstrap.bind(8080).sync(); channelFuture.channel().closeFuture().sync(); &#125; finally &#123; bossGroup.shutdownGracefully(); workGroup.shutdownGracefully(); &#125; &#125;&#125; HttpServerInitialize 123456789101112131415public class HttpServerInitialize extends ChannelInitializer&lt;SocketChannel&gt; &#123; @Override protected void initChannel(SocketChannel socketChannel) throws Exception &#123; // 向管道加入处理器 // 得到管道 final ChannelPipeline pipeline = socketChannel.pipeline(); // 加入一个Netty提供的 HttpServerCodec (http协议编解码器) codec =&gt; decoder pipeline.addLast(&quot;MyHttpServerCodec&quot;, new HttpServerCodec()); // 增加一个针对http协议的handler pipeline.addLast(&quot;MyHttpServerHandler&quot;, new HttpServerHandler()); &#125;&#125; HttpServerHandler 12345678910111213141516171819202122232425262728/** * 1. SimpleChannelInboundHandler 是 ChannelInboundHandlerAdapter 的子类 * 2. HttpObject 客户端和服务端相互通讯的数据对象 */@Slf4jpublic class HttpServerHandler extends SimpleChannelInboundHandler&lt;HttpObject&gt; &#123; @Override protected void channelRead0(ChannelHandlerContext channelHandlerContext, HttpObject httpObject) throws Exception &#123; // 判断 msg 是否为 http 请求 if (httpObject instanceof HttpRequest) &#123; log.info(&quot;httpObject 类型：&#123;&#125;&quot;, httpObject.getClass()); log.info(&quot;客户端地址：&#123;&#125;&quot;, channelHandlerContext.channel().remoteAddress()); // 回复客户端消息， 封装 httpResponse ByteBuf byteBuf = Unpooled.copiedBuffer(&quot;我是服务器&quot;, CharsetUtil.UTF_8); FullHttpResponse response = new DefaultFullHttpResponse(HttpVersion.HTTP_1_1, HttpResponseStatus.OK, byteBuf); // 设置响应头 response.headers().set(HttpHeaderNames.CONTENT_TYPE, &quot;application/json&quot;) .set(HttpHeaderNames.CONTENT_LENGTH, byteBuf.readableBytes()); // 将构建好的response 返回 channelHandlerContext.channel().writeAndFlush(response); &#125; &#125;&#125; 123456浏览器访问 http://localhost:8080/服务端输出：09:10:11.402 [nioEventLoopGroup-3-1] INFO com.wgf.http.HttpServerHandler - httpObject 类型：class io.netty.handler.codec.http.DefaultHttpRequest09:10:11.402 [nioEventLoopGroup-3-1] INFO com.wgf.http.HttpServerHandler - 客户端地址：/0:0:0:0:0:0:0:1:4999909:10:11.419 [nioEventLoopGroup-3-1] INFO com.wgf.http.HttpServerHandler - httpObject 类型：class io.netty.handler.codec.http.DefaultHttpRequest09:10:11.419 [nioEventLoopGroup-3-1] INFO com.wgf.http.HttpServerHandler - 客户端地址：/0:0:0:0:0:0:0:1:49999 可以看到服务端连续输出两次有BUG, 打印请求信息，优化 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869@Slf4jpublic class HttpServerHandler extends SimpleChannelInboundHandler&lt;HttpObject&gt; &#123; @Override protected void channelRead0(ChannelHandlerContext channelHandlerContext, HttpObject httpObject) throws Exception &#123; // 判断 msg 是否为 http 请求 if (httpObject instanceof HttpRequest) &#123; log.info(&quot;httpObject 类型：&#123;&#125;&quot;, httpObject.getClass()); log.info(&quot;客户端地址：&#123;&#125;&quot;, channelHandlerContext.channel().remoteAddress()); // 打印请求链接 // / // /favicon.ico 请求网络图标 HttpRequest httpRequest = (HttpRequest) httpObject; URI uri = new URI(httpRequest.uri()); log.info(uri.getPath()); // 过滤http请求 if (&quot;/favicon.ico&quot;.equals(uri.getPath())) &#123; log.info(&quot;过滤图标请求&quot;); return; &#125; // 打印每次请求信息 // http 属于短链接，浏览器每次刷新都会创建一个新的 channel -&gt; pipeline -&gt; handler, // 所以 channel -&gt; pipeline -&gt; handler 三者是连接独享关系 log.info(&quot;channel class: &#123;&#125;, hashCode: &#123;&#125;&quot;, channelHandlerContext.channel().getClass(), channelHandlerContext.channel().hashCode()); log.info(&quot;pipeline class: &#123;&#125;, pipeline: &#123;&#125;&quot;, channelHandlerContext.pipeline().getClass(), channelHandlerContext.pipeline().hashCode()); log.info(&quot;handler class: &#123;&#125;, handler: &#123;&#125;&quot;, channelHandlerContext.handler().getClass(), channelHandlerContext.handler().hashCode()); // 回复客户端消息， 封装 httpResponse ByteBuf byteBuf = Unpooled.copiedBuffer(&quot;我是服务器&quot;, CharsetUtil.UTF_8); FullHttpResponse response = new DefaultFullHttpResponse(HttpVersion.HTTP_1_1, HttpResponseStatus.OK, byteBuf); // 设置响应头 response.headers().set(HttpHeaderNames.CONTENT_TYPE, &quot;application/json&quot;) .set(HttpHeaderNames.CONTENT_LENGTH, byteBuf.readableBytes()); // 将构建好的response 返回 channelHandlerContext.channel().writeAndFlush(response); &#125; &#125;&#125;刷新两次浏览器服务端输出：10:59:53.346 [nioEventLoopGroup-3-1] INFO com.wgf.http.HttpServerHandler - 客户端地址：/0:0:0:0:0:0:0:1:6523210:59:53.346 [nioEventLoopGroup-3-1] INFO com.wgf.http.HttpServerHandler - /10:59:53.347 [nioEventLoopGroup-3-1] INFO com.wgf.http.HttpServerHandler - channel class: class io.netty.channel.socket.nio.NioSocketChannel, hashCode: -81762080110:59:53.347 [nioEventLoopGroup-3-1] INFO com.wgf.http.HttpServerHandler - pipeline class: class io.netty.channel.DefaultChannelPipeline, pipeline: 93052906010:59:53.347 [nioEventLoopGroup-3-1] INFO com.wgf.http.HttpServerHandler - handler class: class com.wgf.http.HttpServerHandler, handler: 165236518310:59:53.366 [nioEventLoopGroup-3-1] INFO com.wgf.http.HttpServerHandler - httpObject 类型：class io.netty.handler.codec.http.DefaultHttpRequest10:59:53.366 [nioEventLoopGroup-3-1] INFO com.wgf.http.HttpServerHandler - 客户端地址：/0:0:0:0:0:0:0:1:6523210:59:53.366 [nioEventLoopGroup-3-1] INFO com.wgf.http.HttpServerHandler - /favicon.ico10:59:53.366 [nioEventLoopGroup-3-1] INFO com.wgf.http.HttpServerHandler - 过滤图标请求11:00:38.945 [nioEventLoopGroup-3-2] INFO com.wgf.http.HttpServerHandler - httpObject 类型：class io.netty.handler.codec.http.DefaultHttpRequest11:00:38.945 [nioEventLoopGroup-3-2] INFO com.wgf.http.HttpServerHandler - 客户端地址：/0:0:0:0:0:0:0:1:6527611:00:38.945 [nioEventLoopGroup-3-2] INFO com.wgf.http.HttpServerHandler - /11:00:38.945 [nioEventLoopGroup-3-2] INFO com.wgf.http.HttpServerHandler - channel class: class io.netty.channel.socket.nio.NioSocketChannel, hashCode: 163280876211:00:38.945 [nioEventLoopGroup-3-2] INFO com.wgf.http.HttpServerHandler - pipeline class: class io.netty.channel.DefaultChannelPipeline, pipeline: 95837278311:00:38.945 [nioEventLoopGroup-3-2] INFO com.wgf.http.HttpServerHandler - handler class: class com.wgf.http.HttpServerHandler, handler: 208227607211:00:38.962 [nioEventLoopGroup-3-2] INFO com.wgf.http.HttpServerHandler - httpObject 类型：class io.netty.handler.codec.http.DefaultHttpRequest11:00:38.962 [nioEventLoopGroup-3-2] INFO com.wgf.http.HttpServerHandler - 客户端地址：/0:0:0:0:0:0:0:1:6527611:00:38.962 [nioEventLoopGroup-3-2] INFO com.wgf.http.HttpServerHandler - /favicon.ico11:00:38.962 [nioEventLoopGroup-3-2] INFO com.wgf.http.HttpServerHandler - 过滤图标请求 结论： 两次请求是因为有个图标请求/favicon.ico http属于段连接，每次刷新都会建立一个新的连接 channel , pipeline , handler 三者都是连接独享，非共享的对象 多人聊天室 熟悉 ChannelHandler API使用 熟悉 ChannelGroup 使用 了解 Netty 心跳机制 IdleStateHandler(空闲状态处理器) 服务端源码 1234567891011121314151617181920212223242526272829303132333435public class ChatServer &#123; private int port; public ChatServer(int port) &#123; this.port = port; &#125; public void run() &#123; // 创建bossGroup,workGroup EventLoopGroup bossGroup = new NioEventLoopGroup(); EventLoopGroup workGroup = new NioEventLoopGroup(8); try &#123; // 创建启动器 ServerBootstrap serverBootstrap = new ServerBootstrap(); serverBootstrap.group(bossGroup, workGroup) .channel(NioServerSocketChannel.class) .option(ChannelOption.SO_BACKLOG, 128) .childOption(ChannelOption.SO_KEEPALIVE, true) .handler(new LoggingHandler(LogLevel.INFO)) // 添加日志打印 .childHandler(new ServerHandler()); ChannelFuture channelFuture = serverBootstrap.bind(port).sync(); channelFuture.channel().closeFuture().sync(); &#125; catch (Exception e) &#123; // 优雅停机 bossGroup.shutdownGracefully(); workGroup.shutdownGracefully(); &#125; &#125; public static void main(String[] args) &#123; new ChatServer(9999).run(); &#125;&#125; 12345678910111213141516171819202122232425public class ServerHandler extends ChannelInitializer&lt;SocketChannel&gt; &#123; @Override protected void initChannel(SocketChannel ch) throws Exception &#123; // 加入编解码器 ch.pipeline().addLast(&quot;decode&quot;, new StringDecoder()) .addLast(&quot;encode&quot;, new StringEncoder()) .addLast(new ServerChatHandler()); // 加入业务处理Handler /** * 说明： * 1. IdleStateHandler 是 Netty 提供的 空闲状态处理器 * 2. 四个参数： * readerIdleTime : 表示多久没有 读 事件后，就会发送一个心跳检测包，检测是否还是连接状态 * writerIdleTime : 表示多久没有 写 事件后，就会发送一个心跳检测包，检测是否还是连接状态 * allIdleTime : 表示多久时间既没读也没写 后，就会发送一个心跳检测包，检测是否还是连接状态 * TimeUnit : 时间单位 * 3. 当 Channel 一段时间内没有执行 读 / 写 / 读写 事件后，就会触发一个 IdleStateEvent 空闲状态事件 * 4. 当 IdleStateEvent 触发后，就会传递给 Pipeline 中的下一个 Handler 去处理，通过回调下一个 Handler 的 userEventTriggered 方法，在该方法中处理 IdleStateEvent */ ch.pipeline().addLast(new IdleStateHandler(20, 20, 40, TimeUnit.SECONDS)) .addLast(new ServerHeartbeatHandler()); // 添加一个Handler 处理空闲状态事件 &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465/** * 消息处理 * SimpleChannelInboundHandler 是 ChannelInboundHandlerAdapter的子类 * 配合编码解码器，可以范型消息类型 */@Slf4jpublic class ServerChatHandler extends SimpleChannelInboundHandler&lt;String&gt; &#123; /** * 定义一个 Channel 线程组，管理所有的 Channel, channel断开连接会自动删除, 参数 执行器 * GlobalEventExecutor =&gt; 全局事件执行器 * INSTANCE =&gt; 表示是单例的 */ private static final ChannelGroup channelGroup = new DefaultChannelGroup(GlobalEventExecutor.INSTANCE); SimpleDateFormat simpleDateFormat = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;); // 在ChannelHandler被添加到实际上下文中并准备好处理事件后调用 @Override public void handlerAdded(ChannelHandlerContext ctx) throws Exception &#123; // 提示聊天室所有人有新用户上线 Channel channel = ctx.channel(); Date date = new Date(); // 向整个ChannelGroup发送消息 channelGroup.writeAndFlush(String.format(&quot;%s [channel %s] 加入聊天&quot;, simpleDateFormat.format(new Date()), channel.remoteAddress())); channelGroup.add(channel); &#125; // 在ChannelHandler从实际上下文中移除后调用，表明它不再处理事件 @Override public void handlerRemoved(ChannelHandlerContext ctx) throws Exception &#123; Channel channel = ctx.channel(); channelGroup.writeAndFlush(String.format(&quot;%s [channel %s] 离开聊天&quot;, simpleDateFormat.format(new Date()), channel.remoteAddress())); // channelGroup.remove(channel); 不需要，handlerRemoved（）直接删除了channel &#125; // Channel不活跃的时候，调用，此时生命周期马上结束 @Override public void channelInactive(ChannelHandlerContext ctx) throws Exception &#123; log.info(&quot;&#123;&#125; [channel &#123;&#125;] 下线了&quot;, simpleDateFormat.format(new Date()), ctx.channel().remoteAddress()); &#125; // Channel活跃的时候，调用 @Override public void channelActive(ChannelHandlerContext ctx) throws Exception &#123; log.info(&quot;&#123;&#125; [channel &#123;&#125;] 上线了&quot;, simpleDateFormat.format(new Date()), ctx.channel().remoteAddress()); &#125; // 读取事件 @Override protected void channelRead0(ChannelHandlerContext ctx, String msg) throws Exception &#123; Channel channel = ctx.channel(); String date = simpleDateFormat.format(new Date()); // 读取客户端发送的数据，发送给其他人包括自己 channelGroup.forEach(ch -&gt; &#123; // 判断是否为自己 if (ch.equals(channel)) &#123; ch.writeAndFlush(String.format(&quot;%s [自己]: %s\\n&quot;, date, msg)); &#125; else &#123; ch.writeAndFlush(String.format(&quot;%s [%s]: %s\\n&quot;, date, channel.remoteAddress(), msg)); &#125; &#125;); &#125;&#125; 12345678910111213141516171819202122232425262728293031@Slf4jpublic class ServerHeartbeatHandler extends ChannelInboundHandlerAdapter &#123; // 用户事件触发，处理IdleStateHandler触发的用户事件 @Override public void userEventTriggered(ChannelHandlerContext ctx, Object evt) throws Exception &#123; // 判断是否空闲状态事件 if (!(evt instanceof IdleStateEvent)) &#123; return; &#125; IdleStateEvent idleStateEvent = (IdleStateEvent) evt; String evtType = null; switch (idleStateEvent.state()) &#123; case READER_IDLE: evtType = &quot;读空闲&quot;; break; case WRITER_IDLE: evtType = &quot;写空闲&quot;; break; case ALL_IDLE: evtType = &quot;读写空闲&quot;; break; &#125; Channel channel = ctx.channel(); log.info(&quot;[channel: &#123;&#125; &#123;&#125;]&quot;, channel.remoteAddress(), evtType); &#125;&#125; 客户端源码 123456789101112131415161718192021222324252627282930313233343536373839public class ChatClient &#123; private String host; private int port; public ChatClient(String host, int port) &#123; this.host = host; this.port = port; &#125; public void run() &#123; NioEventLoopGroup nioEventLoopGroup = new NioEventLoopGroup(); try &#123; Bootstrap bootstrap = new Bootstrap(); bootstrap.group(nioEventLoopGroup) .channel(NioSocketChannel.class) .handler(new ClientHandler()); ChannelFuture channelFuture = bootstrap.connect(host, port).sync(); // 得到当前建立的Channel Channel channel = channelFuture.channel(); // 扫描用户输入 Scanner scanner = new Scanner(System.in); while (scanner.hasNextLine()) &#123; String msg = scanner.nextLine(); channel.writeAndFlush(msg); &#125; channelFuture.channel().closeFuture().sync(); &#125; catch (Exception e) &#123; nioEventLoopGroup.shutdownGracefully(); &#125; &#125; public static void main(String[] args) &#123; new ChatClient(&quot;127.0.0.1&quot;, 9999).run(); &#125;&#125; 12345678public class ClientHandler extends ChannelInitializer &#123; @Override protected void initChannel(Channel ch) throws Exception &#123; ch.pipeline().addLast(&quot;decode&quot;, new StringDecoder()) .addLast(&quot;encode&quot;, new StringEncoder()) .addLast(new ClientChatHandler()); &#125;&#125; 12345678@Slf4jpublic class ClientChatHandler extends SimpleChannelInboundHandler&lt;String&gt; &#123; @Override protected void channelRead0(ChannelHandlerContext ctx, String msg) throws Exception &#123; // 直接输入服务端返回的消息 log.info(msg); &#125;&#125; ChannelGroup Netty提供了ChannelGroup来用于保存Channel组，ChannelGroup是一个线程安全的Channel集合，它提供一些对Channel的批量操作。当一个TCP连接关闭后，对应的Channel会自动从ChannelGroup移除，所以不需要手动去移除关闭的Channel 当有新的客户端连接到服务器，将对应的Channel加入到一个ChannelGroup中，当发布者发布消息时，服务器可以将消息通过ChannelGroup写入到所有客户端 心跳机制 什么是 IdleStateHandler ? ​ 当连接的空闲时间（读或者写）太长时，将会触发一个 IdleStateEvent 事件。然后，你可以通过你的 ChannelInboundHandler 中重写 userEventTrigged 方法来处理该事件 如何使用？ IdleStateHandler 既是出站处理器也是入站处理器，继承了 ChannelDuplexHandler 。通常在 initChannel 方法中将 IdleStateHandler 添加到 pipeline 中。然后在自己的 handler 中重写 userEventTriggered 方法，当发生空闲事件（读或者写），就会触发这个方法，并传入具体事件 这时，你可以通过 Context 对象尝试向目标 Socekt 写入数据，并设置一个 监听器，如果发送失败就关闭 Socket （Netty 准备了一个 ChannelFutureListener.CLOSE_ON_FAILURE 监听器用来实现关闭 Socket 逻辑） 说明： IdleStateHandler 是 Netty 提供的 空闲状态处理器 四个参数： readerIdleTime : 表示多久没有 读 事件后，就会发送一个心跳检测包，检测是否还是连接状态 writerIdleTime : 表示多久没有 写 事件后，就会发送一个心跳检测包，检测是否还是连接状态 allIdleTime : 表示多久时间既没读也没写 后，就会发送一个心跳检测包，检测是否还是连接状态 TimeUnit : 时间单位 当 Channel 一段时间内没有执行 读 / 写 / 读写 事件后，就会触发一个 IdleStateEvent 空闲状态事件 当 IdleStateEvent 触发后，就会传递给 Pipeline 中的下一个 Handler 去处理，通过回调下一个 Handler 的 userEventTriggered 方法，在该方法中处理 IdleStateEvent WebSocket 实现基于webSocket的长连接 的全双工的交互 改变Http协议多次请求的约束，实现长连接了， 服务器可以发送消息 给浏览器 客户端浏览器和服务器端会相互感 知，比如服务器关闭了，浏览器会感知，同样浏览器关闭了，服务器会感知 服务端源码 1234567891011121314151617181920212223public class WebServer &#123; public static void main(String[] args) throws InterruptedException &#123; //创建bossGroup,workGroup EventLoopGroup bossGroup = new NioEventLoopGroup(); EventLoopGroup workGroup = new NioEventLoopGroup(); try &#123; //创建启动器 ServerBootstrap serverBootstrap = new ServerBootstrap(); //循环事件组 serverBootstrap.group(bossGroup, workGroup)//线程组 .channel(NioServerSocketChannel.class)//通道类型 .handler(new LoggingHandler(LogLevel.INFO)) .childHandler(new ServerHandler()); System.out.println(&quot;server is ok&quot;); ChannelFuture channelFuture = serverBootstrap.bind(8080).sync(); channelFuture.channel().closeFuture().sync(); &#125; finally &#123; bossGroup.shutdownGracefully(); workGroup.shutdownGracefully(); &#125; &#125;&#125; 12345678910111213141516171819202122232425262728293031public class ServerHandler extends ChannelInitializer&lt;SocketChannel&gt; &#123; @Override protected void initChannel(SocketChannel ch) throws Exception &#123; ChannelPipeline pipeline = ch.pipeline(); //基于http协议使用http的编码和解码器 pipeline.addLast(new HttpServerCodec()); /** * 添加块处理器 * 在需要将数据从文件系统复制到用户内存中时，可以使用 ChunkedWriteHandler， * 它支持异步写大型数据流，而又不会导致大量的内存消耗 * 每次只生成固定大小的数据块，防止客户端因为网速接收慢而导致服务端无限将数据写入内存 */ pipeline.addLast(new ChunkedWriteHandler()); /* 说明： 1. 因为 HTTP 数据传输时是分段的，HttpObjectAggregator 可以将多个端聚合 2. 这就是为什么浏览器发送大量数据时，就会发出多次 HTTP 请求 */ pipeline.addLast(new HttpObjectAggregator(8192)); /* 说明： 1. 对于 WebSocket 是以 帧(frame) 的形式传递的 2. 后面的参数表示 ：请求的 URL 3. WebSocketServerProtocolHandler 将 HTTP 协议升级为 WebSocket 协议，即保持长连接 4. 切换协议通过一个状态码101 */ pipeline.addLast(new WebSocketServerProtocolHandler(&quot;/hello&quot;)); // 自定义的 Handler pipeline.addLast(new TextWebSocketFrameHandler()); &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839@Slf4jpublic class TextWebSocketFrameHandler extends SimpleChannelInboundHandler&lt;TextWebSocketFrame&gt; &#123; @Override protected void channelRead0(ChannelHandlerContext ctx, TextWebSocketFrame msg) throws Exception &#123; String message = msg.text(); log.info(&quot;服务器收到消息：&#123;&#125;&quot;, message); // 回复消息 ctx.channel().writeAndFlush(new TextWebSocketFrame(String.format(&quot;%s 服务器收到消息：%s&quot;, LocalDateTime.now(), message))); &#125; /** * 客户端连接后，触发方法 * @param ctx * @throws Exception */ @Override public void handlerAdded(ChannelHandlerContext ctx) throws Exception &#123; // longText 保证全局唯一 log.info(&quot;handlerAdded 被调用：&#123;&#125;&quot;, ctx.channel().id().asLongText()); // shortText 不保证全局唯一 log.info(&quot;handlerAdded 被调用：&#123;&#125;&quot;, ctx.channel().id().asShortText()); &#125; @Override public void handlerRemoved(ChannelHandlerContext ctx) throws Exception &#123; // longText 保证全局唯一 log.info(&quot;handlerRemoved 被调用：&#123;&#125;&quot;, ctx.channel().id().asLongText()); &#125; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; log.error(&quot;发生异常&quot;, cause); ctx.close(); &#125;&#125; 客户端 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;Title&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;form onsubmit=&quot;return false&quot;&gt; &lt;textarea id=&quot;message&quot; name=&quot;message&quot; style=&quot;height: 300px; width: 300px&quot;&gt;&lt;/textarea&gt; &lt;input type=&quot;button&quot; value=&quot;发送消息&quot; onclick=&quot;send(this.form.message.value)&quot;&gt; &lt;textarea id=&quot;responseText&quot; style=&quot;height: 300px; width: 300px&quot;&gt;&lt;/textarea&gt; &lt;input type=&quot;button&quot; value=&quot;清空内容&quot; onclick=&quot;document.getElementById(&#x27;responseText&#x27;).value=&#x27;&#x27;&quot;&gt;&lt;/form&gt;&lt;script&gt; var socket; // 判断当前浏览器是否支持 WebSocket if (window.WebSocket)&#123; socket = new WebSocket(&quot;ws://localhost:8080/hello&quot;); // 相当于 channelRead0 方法，ev 收到服务器端回送的消息 socket.onmessage = function (ev)&#123; var rt = document.getElementById(&quot;responseText&quot;); rt.value = rt.value + &quot;\\n&quot; + ev.data; &#125; // 相当于连接开启，感知到连接开启 socket.onopen = function ()&#123; var rt = document.getElementById(&quot;responseText&quot;); rt.value = rt.value + &quot;\\n&quot; + &quot;连接开启……&quot;; &#125; // 感知连接关闭 socket.onclose = function ()&#123; var rt = document.getElementById(&quot;responseText&quot;); rt.value = rt.value + &quot;\\n&quot; + &quot;连接关闭……&quot;; &#125; &#125;else &#123; alert(&quot;不支持 WebSocket&quot;); &#125; // 发送消息到服务器 function send(message)&#123; // 判断 WebSocket 是否创建好了 if (!window.socket)&#123; return ; &#125; // 判断 WebSocket 是否开启 if (socket.readyState == WebSocket.OPEN)&#123; // 通过 Socket 发送消息 socket.send(message); &#125;else &#123; alert(&quot;连接未开启&quot;); &#125; &#125;&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 编码和解码 编码和解码的基本介绍 编写网络应用程序时，因为数据在网络中传输的都是二进制字节码数据，在发送数据时就需要编码，接收数据时就需要解码 codec（编码器）的组成部分有两个：decoder（解码器）和encoder（编码器）。encoder负责把业务数据缓存字节码数据，decoder负责把字节码数据转换成业务数据 Netty本身的编码解码的机制和问题分析 Netty提供的编码器 12345678910111213141516171819202122232425262728293031323334353637ChannelHandlerAdapter (io.netty.channel) ChannelOutboundHandlerAdapter (io.netty.channel) MessageToMessageEncoder (io.netty.handler.codec) ByteArrayEncoder (io.netty.handler.codec.bytes) DatagramDnsQueryEncoder (io.netty.handler.codec.dns) WebSocket08FrameEncoder (io.netty.handler.codec.http.websocketx) WebSocket07FrameEncoder (io.netty.handler.codec.http.websocketx) WebSocket13FrameEncoder (io.netty.handler.codec.http.websocketx) DatagramDnsResponseEncoder (io.netty.handler.codec.dns) SctpOutboundByteStreamHandler (io.netty.handler.codec.sctp) SpdyHttpEncoder (io.netty.handler.codec.spdy) WebSocketExtensionEncoder (io.netty.handler.codec.http.websocketx.extensions) DeflateEncoder (io.netty.handler.codec.http.websocketx.extensions.compression) RedisEncoder (io.netty.handler.codec.redis) LineEncoder (io.netty.handler.codec.string) ProtobufEncoderNano (io.netty.handler.codec.protobuf) StringEncoder (io.netty.handler.codec.string) SmtpRequestEncoder (io.netty.handler.codec.smtp) WebSocket00FrameEncoder (io.netty.handler.codec.http.websocketx) ProtobufEncoder (io.netty.handler.codec.protobuf) MqttEncoder (io.netty.handler.codec.mqtt) HttpObjectEncoder (io.netty.handler.codec.http) HttpRequestEncoder (io.netty.handler.codec.http) Encoder in HttpClientCodec (io.netty.handler.codec.http) HttpResponseEncoder (io.netty.handler.codec.http) HttpServerResponseEncoder in HttpServerCodec (io.netty.handler.codec.http) RtspObjectEncoder (io.netty.handler.codec.rtsp) RtspEncoder (io.netty.handler.codec.rtsp) RtspRequestEncoder (io.netty.handler.codec.rtsp) RtspResponseEncoder (io.netty.handler.codec.rtsp) DatagramPacketEncoder (io.netty.handler.codec) AbstractMemcacheObjectEncoder (io.netty.handler.codec.memcache) AbstractBinaryMemcacheEncoder (io.netty.handler.codec.memcache.binary) StompSubframeEncoder (io.netty.handler.codec.stomp) LengthFieldPrepender (io.netty.handler.codec) Base64Encoder (io.netty.handler.codec.base64) ... Netty提供的解码器 123456789101112131415161718192021222324252627282930ChannelHandlerAdapter (io.netty.channel) ChannelInboundHandlerAdapter (io.netty.channel) MessageToMessageDecoder (io.netty.handler.codec) SctpInboundByteStreamHandler (io.netty.handler.codec.sctp) SctpMessageCompletionHandler (io.netty.handler.codec.sctp) Base64Decoder (io.netty.handler.codec.base64) DatagramDnsQueryDecoder (io.netty.handler.codec.dns) WebSocketExtensionDecoder (io.netty.handler.codec.http.websocketx.extensions) DeflateDecoder (io.netty.handler.codec.http.websocketx.extensions.compression) RedisArrayAggregator (io.netty.handler.codec.redis) SpdyHttpDecoder (io.netty.handler.codec.spdy) ProtobufDecoder (io.netty.handler.codec.protobuf) HttpContentDecoder (io.netty.handler.codec.http) HttpContentDecompressor (io.netty.handler.codec.http) DatagramPacketDecoder (io.netty.handler.codec) SctpMessageToMessageDecoder (io.netty.handler.codec.sctp) StringDecoder (io.netty.handler.codec.string) WebSocketProtocolHandler (io.netty.handler.codec.http.websocketx) WebSocketClientProtocolHandler (io.netty.handler.codec.http.websocketx) WebSocketServerProtocolHandler (io.netty.handler.codec.http.websocketx) DatagramDnsResponseDecoder (io.netty.handler.codec.dns) ByteArrayDecoder (io.netty.handler.codec.bytes) MessageAggregator (io.netty.handler.codec) WebSocketFrameAggregator (io.netty.handler.codec.http.websocketx) AbstractMemcacheObjectAggregator (io.netty.handler.codec.memcache) StompSubframeAggregator (io.netty.handler.codec.stomp) RedisBulkStringAggregator (io.netty.handler.codec.redis) HttpObjectAggregator (io.netty.handler.codec.http) ProtobufDecoderNano (io.netty.handler.codec.protobuf) ... Netty本身自带的ObjectDecoder和ObjectEncoder可以用实现POJO对象或业务对象的编码和解码，底层使用的仍然是Java序列化技术，而Java序列化技术本身效率就不高，存在如下问题: 无法跨语言 序列化后体积太大，是二进制编码的5倍多 序列化后传输效率太低 序列化性能太低 解决方案：Google ProtoBuf ProtoBuf 官方文档 数据类型 1.ProtoBuf 简介 ​ protobuf (protocol buffer) 是谷歌内部的混合语言数据标准。通过将结构化的数据进行序列化(串行化)，用于通讯协议、数据存储等领域的语言无关、平台无关、可扩展的序列化结构数据格式 2.为什么使用protobuf ​ 我们知道数据在网络传输中是以二进制进行的，一般我们使用字节byte来表示， 一个byte是8bits，如果要在网络上中传输对象，一般需要将对象序列化，序列化的目的就是将对象转换成byte数组在网络中传输，当接收方接收到byte数组之后，再对byte数组进行反序列化，最终转换成java中的对象 java对象序列化 常见的方式 使用JDK自带的对象序列化，但是JDK自带的序列化本身存在一些问题，并且这种序列化手段只适合在java程序之间进行传输，如果是非java程序，比如PHP或者GO，那么序列化就不通用了 你还可以自定义序列化协议，这种方式的灵活程度比较高，但是不够通用，并且实现起来也比较复杂，很可能出现意想不到的问题 将数据转换成为XML或者JSON进行传输。XML和JSON的好处在于他们都有可以区分对象的起始符号，通过判断这些符号的位置就可以读取到完整的对象。但是不管是XML还是JSON的缺点都是转换成的数据比较大。在反序列化的时候对资源的消耗也比较多 所以我们需要一种新的序列化的方法，这就是protobuf，它是一种灵活、高效、自动化的解决方案 ​ 通过编写一个.proto的数据结构定义文件，然后调用protobuf的编译器，就会生成对应的类，该类以高效的二进制格式实现protobuf数据的自动编码和解析。 生成的类为定义文件中的数据字段提供了getter和setter方法，并提供了读写的处理细节。 重要的是，protobuf可以向前兼容，也就是说老的二进制代码也可以使用最新的协议进行读取 message 介绍 message：protobuf中定义一个消息类型是通过关键字message字段指定的，这个关键字类似于C++/Java中的class关键字。使用protobuf编译器将proto编译成Java代码之后，每个message都会生成一个名字与之对应的Java类，该类公开继承自com.google.protobuf.Message message 消息定义 Msg.proto 文件 12345678syntax = &quot;proto3&quot;; // 协议版本option java_outer_classname = &quot;NettyMsg&quot;; // 生成的外部类名，同时也是文件名// proto 使用 message 管理数据， 会在 NettyMsg 下生成一个内部类message Msg &#123;int32 id = 1; // 类似java对象定义属性，1 表示的是属性的序号，不是属性的值string msg = 2;&#125; example 添加 Maven 坐标 12345&lt;dependency&gt; &lt;groupId&gt;com.google.protobuf&lt;/groupId&gt; &lt;artifactId&gt;protobuf-java&lt;/artifactId&gt; &lt;version&gt;3.21.1&lt;/version&gt;&lt;/dependency&gt; 下载Protobuf3 将Msg.proto文件拷贝到 Protobuf3 bin 目录下 执行命令 protoc.exe --java_out=. Msg.proto 生成 NettyMsg.java 服务端源码 12345678910111213141516171819202122232425262728293031323334353637383940public class NettyServer &#123; private final static int PORT = 6666; public static void main(String[] args) throws Exception &#123; EventLoopGroup bossGroup = new NioEventLoopGroup(1); EventLoopGroup workerGroup = new NioEventLoopGroup(); try &#123; ServerBootstrap bootstrap = new ServerBootstrap(); bootstrap.group(bossGroup, workerGroup) .channel(NioServerSocketChannel.class) .option(ChannelOption.SO_BACKLOG, 128) .childOption(ChannelOption.SO_KEEPALIVE, true) .childHandler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; //创建一个通道初始化对象(匿名对象) //给pipeline 设置处理器 @Override protected void initChannel(SocketChannel socketChannel) throws Exception &#123; final ChannelPipeline pipeline = socketChannel.pipeline(); // 添加ProtoBuf解码器, 指定对哪种对象进行节码 pipeline.addLast(new ProtobufDecoder(NettyMsg.Msg.getDefaultInstance())); pipeline.addLast(new NettyServerHandler()); &#125; &#125;); System.out.println(&quot;server is ready&quot;); ChannelFuture channelFuture = bootstrap.bind(PORT).sync(); channelFuture.channel().closeFuture().sync(); &#125; finally &#123; bossGroup.shutdownGracefully(); workerGroup.shutdownGracefully(); &#125; &#125;&#125; 123456789@Slf4jpublic class NettyServerHandler extends SimpleChannelInboundHandler&lt;NettyMsg.Msg&gt; &#123; @Override protected void channelRead0(ChannelHandlerContext channelHandlerContext, NettyMsg.Msg msg) throws Exception &#123; log.info(&quot;消息id：&#123;&#125;&quot;, msg.getId()); log.info(&quot;消息：&#123;&#125;&quot;, msg.getMsg()); &#125;&#125; 客户端源码 123456789101112131415161718192021222324252627282930313233public class NettyClient &#123; public static void main(String[] args) throws Exception &#123; // 客户端需要一个时间循环组 EventLoopGroup eventLoopGroup = new NioEventLoopGroup(); try &#123; //创建客户端启动对象 //注意客户端使用的不是 ServerBootstrap 而是 Bootstrap Bootstrap bootstrap = new Bootstrap(); bootstrap.group(eventLoopGroup) .channel(NioSocketChannel.class) // 设置客户端通道的实现类(反射) .handler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override protected void initChannel(SocketChannel ch) throws Exception &#123; final ChannelPipeline pipeline = ch.pipeline(); // 设置ProtoBuf编码器 pipeline.addLast(&quot;encode&quot;, new ProtobufEncoder()); // 给pipeline 设置处理器 pipeline.addLast(new NettyClientHandler()); &#125; &#125;); ChannelFuture channelFuture = bootstrap.connect(&quot;localhost&quot;, 6666).sync(); // 对关闭通道进行监听 channelFuture.channel().closeFuture().sync(); &#125; finally &#123; // 优雅停机 eventLoopGroup.shutdownGracefully(); &#125; &#125;&#125; 1234567891011@Slf4jpublic class NettyClientHandler extends ChannelInboundHandlerAdapter &#123; // 当通道准备就绪就会触发该方法 @Override public void channelActive(ChannelHandlerContext ctx) throws Exception &#123; // 发送一个Msg对象到服务端 final NettyMsg.Msg msg = NettyMsg.Msg.newBuilder().setId(1).setMsg(&quot;hi! 服务端~&quot;).build(); ctx.writeAndFlush(msg); &#125;&#125; ProtoBuf 传输多种类型 核心： 枚举DataType的定义 oneof的使用 通过一个message管理多个message MultiMsg.proto 1234567891011121314151617181920212223242526272829303132syntax = &quot;proto3&quot;; // 指定协议版本option optimize_for = SPEED; // 加快解析option java_package = &quot;com.wgf.protobuf&quot;; // 指定生成在哪个包下option java_outer_classname = &quot;MultiMsg&quot;; // 外部类名// ProtoBuf 可以使用 message 管理其他的messagemessage MsgType &#123; // 定义一个枚举 enum DataType &#123; commonMsgType = 0; // 在 ProtoBuf 要求Enum的编号从0开始 customerMsgType = 1; &#125; // 定义一个属性来标识传的是哪一个枚举类型 DataType data_type = 1; // 表示枚举类型最多只能出现其中的一个，节省空间 oneof dataBody &#123; CommonMsg commonMsg = 2; CustomerMsg customerMsg = 3; &#125;&#125;message CommonMsg &#123; string msg = 1;&#125;message CustomerMsg &#123; string msg = 1; string customerAddress = 2;&#125; 服务端源码 12345678910111213141516171819202122232425262728293031323334353637383940public class NettyServer &#123; private final static int PORT = 6666; public static void main(String[] args) throws Exception &#123; EventLoopGroup bossGroup = new NioEventLoopGroup(1); EventLoopGroup workerGroup = new NioEventLoopGroup(); try &#123; ServerBootstrap bootstrap = new ServerBootstrap(); bootstrap.group(bossGroup, workerGroup) .channel(NioServerSocketChannel.class) .option(ChannelOption.SO_BACKLOG, 128) .childOption(ChannelOption.SO_KEEPALIVE, true) .childHandler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; //创建一个通道初始化对象(匿名对象) //给pipeline 设置处理器 @Override protected void initChannel(SocketChannel socketChannel) throws Exception &#123; final ChannelPipeline pipeline = socketChannel.pipeline(); // 添加ProtoBuf解码器, 指定对哪种对象进行节码， 多种类型使用一个message管理多个message pipeline.addLast(new ProtobufDecoder(MultiMsg.MsgType.getDefaultInstance())); pipeline.addLast(new NettyServerHandler()); &#125; &#125;); System.out.println(&quot;server is ready&quot;); ChannelFuture channelFuture = bootstrap.bind(PORT).sync(); channelFuture.channel().closeFuture().sync(); &#125; finally &#123; bossGroup.shutdownGracefully(); workerGroup.shutdownGracefully(); &#125; &#125;&#125; 123456789101112131415161718@Slf4jpublic class NettyServerHandler extends SimpleChannelInboundHandler&lt;MultiMsg.MsgType&gt; &#123; @Override protected void channelRead0(ChannelHandlerContext channelHandlerContext, MultiMsg.MsgType msgType) throws Exception &#123; MultiMsg.MsgType.DataType dataType = msgType.getDataType(); // 判断接收的对象类型 if (MultiMsg.MsgType.DataType.commonMsgType == dataType) &#123; MultiMsg.CommonMsg commonMsg = msgType.getCommonMsg(); log.info(commonMsg.getMsg()); &#125; else if (MultiMsg.MsgType.DataType.customerMsgType == dataType) &#123; MultiMsg.CustomerMsg customerMsg = msgType.getCustomerMsg(); log.info(customerMsg.getCustomerAddress()); log.info(customerMsg.getMsg()); &#125; &#125;&#125; 客户端源码 123456789101112131415161718192021222324252627282930313233public class NettyClient &#123; public static void main(String[] args) throws Exception &#123; // 客户端需要一个时间循环组 EventLoopGroup eventLoopGroup = new NioEventLoopGroup(); try &#123; //创建客户端启动对象 //注意客户端使用的不是 ServerBootstrap 而是 Bootstrap Bootstrap bootstrap = new Bootstrap(); bootstrap.group(eventLoopGroup) .channel(NioSocketChannel.class) // 设置客户端通道的实现类(反射) .handler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override protected void initChannel(SocketChannel ch) throws Exception &#123; final ChannelPipeline pipeline = ch.pipeline(); // 设置ProtoBuf编码器 pipeline.addLast(&quot;encode&quot;, new ProtobufEncoder()); // 给pipeline 设置处理器 pipeline.addLast(new NettyClientHandler()); &#125; &#125;); ChannelFuture channelFuture = bootstrap.connect(&quot;localhost&quot;, 6666).sync(); // 对关闭通道进行监听 channelFuture.channel().closeFuture().sync(); &#125; finally &#123; // 优雅停机 eventLoopGroup.shutdownGracefully(); &#125; &#125;&#125; 12345678910111213141516171819202122232425@Slf4jpublic class NettyClientHandler extends ChannelInboundHandlerAdapter &#123; // 当通道准备就绪就会触发该方法 @Override public void channelActive(ChannelHandlerContext ctx) throws Exception &#123; final int i = new Random().nextInt(3); // 发送一个Msg对象到服务端 if (i == 0) &#123; final MultiMsg.MsgType common = MultiMsg.MsgType.newBuilder() .setDataType(MultiMsg.MsgType.DataType.commonMsgType) .setCommonMsg(MultiMsg.CommonMsg.newBuilder().setMsg(&quot;commonMsg ~~&quot;).build()) .build(); ctx.writeAndFlush(common); &#125; else &#123; final MultiMsg.MsgType custome = MultiMsg.MsgType.newBuilder() .setDataType(MultiMsg.MsgType.DataType.customerMsgType) .setCustomerMsg(MultiMsg.CustomerMsg.newBuilder().setMsg(&quot;customer ~~&quot;) .setCustomerAddress(ctx.channel().remoteAddress().toString()).build()) .build(); ctx.writeAndFlush(custome); &#125; &#125;&#125; maven 插件使用 自定义编解码器 从类图上可知，编码和节码其实就是建立在入站和出站的Handler中的 解码：入站时，数据是以二进制形式传递的，解码器就是在入站的Handler中将二进制数据转换为特定的数据格式 编码：出站时，将特定的数据格式转换成二进制字节流后传输到网络间 服务端 12345678910111213141516171819202122232425262728public class Server &#123;private final static int PORT = 6666;public static void main(String[] args) throws Exception &#123; EventLoopGroup bossGroup = new NioEventLoopGroup(1); EventLoopGroup workerGroup = new NioEventLoopGroup(); try &#123; ServerBootstrap bootstrap = new ServerBootstrap(); bootstrap.group(bossGroup, workerGroup) .channel(NioServerSocketChannel.class) .option(ChannelOption.SO_BACKLOG, 128) .childOption(ChannelOption.SO_KEEPALIVE, true) .childHandler(new ServerHandler()); System.out.println(&quot;server is ready&quot;); ChannelFuture channelFuture = bootstrap.bind(PORT).sync(); channelFuture.channel().closeFuture().sync(); &#125; finally &#123; bossGroup.shutdownGracefully(); workerGroup.shutdownGracefully(); &#125;&#125;&#125; 12345678910111213141516public class ServerHandler extends ChannelInitializer&lt;SocketChannel&gt; &#123;@Overrideprotected void initChannel(SocketChannel ch) throws Exception &#123; final ChannelPipeline pipeline = ch.pipeline(); // 加入自定义解码器 pipeline.addLast(new MyByteToLongDecoder()); // 加入自定义编码器 pipeline.addLast(new MyLongToByteEncoder()); // 业务逻辑处理 pipeline.addLast(new ServerBusinessHandler());&#125;&#125; 12345678910@Slf4jpublic class ServerBusinessHandler extends SimpleChannelInboundHandler&lt;Long&gt; &#123;@Overrideprotected void channelRead0(ChannelHandlerContext ctx, Long msg) throws Exception &#123; log.info(&quot;读取客户端数据：&#123;&#125;&quot;, msg); // 回复客户端 ctx.writeAndFlush(987654L);&#125;&#125; 编解码器 123456789@Slf4jpublic class MyLongToByteEncoder extends MessageToByteEncoder&lt;Long&gt; &#123;@Overrideprotected void encode(ChannelHandlerContext ctx, Long msg, ByteBuf out) throws Exception &#123; log.info(&quot;MyLongToByteEncoder encoder 被调用&quot;); log.info(&quot;msg = &#123;&#125;&quot;, msg); out.writeLong(msg);&#125;&#125; 123456789101112131415@Slf4jpublic class MyByteToLongDecoder extends ByteToMessageDecoder &#123;//上下文channelHandlerContext//入站的ByteBuf//List集合，将解码后的数据传给下一个handler@Overrideprotected void decode(ChannelHandlerContext ctx, ByteBuf byteBuf, List&lt;Object&gt; list) throws Exception &#123; log.info(&quot;MyByteToLongDecoder被调用&quot;); //因为long8字节,8个字节，才能读取一个long if (byteBuf.readableBytes() &gt;= 8) &#123; list.add(byteBuf.readLong()); &#125;&#125;&#125; 客户端 1234567891011121314151617181920212223public class Client &#123;public static void main(String[] args) throws Exception &#123; // 客户端需要一个时间循环组 EventLoopGroup eventLoopGroup = new NioEventLoopGroup(); try &#123; //创建客户端启动对象 //注意客户端使用的不是 ServerBootstrap 而是 Bootstrap Bootstrap bootstrap = new Bootstrap(); bootstrap.group(eventLoopGroup) .channel(NioSocketChannel.class) // 设置客户端通道的实现类(反射) .handler(new ClientHandler()); ChannelFuture channelFuture = bootstrap.connect(&quot;localhost&quot;, 6666).sync(); // 对关闭通道进行监听 channelFuture.channel().closeFuture().sync(); &#125; finally &#123; // 优雅停机 eventLoopGroup.shutdownGracefully(); &#125;&#125;&#125; 12345678910111213141516@Slf4jpublic class ClientHandler extends ChannelInitializer&lt;SocketChannel&gt; &#123;@Overrideprotected void initChannel(SocketChannel ch) throws Exception &#123; final ChannelPipeline pipeline = ch.pipeline(); // 添加自定义解码器 pipeline.addLast(new MyByteToLongDecoder()); // 添加自定义编码器 pipeline.addLast(new MyLongToByteEncoder()); // 业务逻辑处理 pipeline.addLast(new ClientBusinessHandler());&#125;&#125; 12345678910111213@Slf4jpublic class ClientBusinessHandler extends SimpleChannelInboundHandler&lt;Long&gt; &#123;@Overrideprotected void channelRead0(ChannelHandlerContext ctx, Long msg) throws Exception &#123; log.info(&quot;服务端回复: &#123;&#125;&quot;, msg);&#125;@Overridepublic void channelActive(ChannelHandlerContext ctx) throws Exception &#123; ctx.writeAndFlush(123456L);&#125;&#125; 问题1 如果客户端自定义的handler发送的是下面这个代码，数据是16个字节，那么服务端的入解码器的decode方法会被调用几次? 1234567@Overridepublic void channelActive(ChannelHandlerContext ctx) throws Exception &#123; // ctx.writeAndFlush(123456L); // 一次性发送16字节，服务端规定每次大于等于8字节才读取 ctx.writeAndFlush(Unpooled.copiedBuffer(&quot;1234567890123456&quot;, CharsetUtil.UTF_8));&#125; 由图可见，一个16字节的消息被自定义解码器MyByteToLongDecoder进行两次读取，原因在于服务器只知道每次要大于8个字节才去读取数据，但是一条完整的数据有多长，不得而知，这就是拆包现象 对于decode方法会根据接收到的数据，被调用多次，直到确定没有新的元素被添加到list或者是ByteBuf没有更多的可读字节为止 如果list不为空，就会将list内容传递给下一个ChannelInboundhandler处理，该处理器的方法也会被调用多次 这里&quot;1234567890123456&quot;是16个字节，所以服务端解码的时候decode会被调两次，每次解码出来的数据放到list里面，list的里数据传给自定义的handler进行处理 问题2 使用问题1的代码片段后，发现，客户端的出站编码handler-》MyLongToByteEncoder，没有被调用，怎么回事呢？ 对于客户端的自定义handler的前一个handler是MyLongToByteEncoder，MyLongToByteEncoder父类MessageToByteEncoder有一个write方法 1234567891011121314151617181920212223242526public void write(ChannelHandlerContext ctx, Object msg, ChannelPromise promise) throws Exception &#123; ByteBuf buf = null; try &#123; //判断数据是不是应该处理的类型，是的话调用encode方法，进行编码，不是，就不编码，直接write,然后交给前一个handler if (acceptOutboundMessage(msg)) &#123; @SuppressWarnings(&quot;unchecked&quot;) I cast = (I) msg; buf = allocateBuffer(ctx, cast, preferDirect); try &#123; //我们写子类的时候，重写了该方法 encode(ctx, cast, buf); &#125; finally &#123; ReferenceCountUtil.release(cast); &#125; if (buf.isReadable()) &#123; ctx.write(buf, promise); &#125; else &#123; buf.release(); ctx.write(Unpooled.EMPTY_BUFFER, promise); &#125; buf = null; &#125; else &#123; ctx.write(msg, promise); &#125; 因此，我们编写的Encoder是要注意传入的数据类型和处理的数据类型一致 结论 不论解码器handler 还是 编码器handler 即接收的消息类型必须与待处理的消息类型一致，否则该handler不会被执行 在解码器进行数据解码时，需要判断缓存区(ByteBuf)的数据是否足够 ，否则接收到的结果会期望结果可能不一致 其他常见的编解码器 ReplayingDecoder 1public abstract class ReplayingDecoder&lt;S&gt; extends ByteToMessageDecoder ReplayingDecoder扩展了ByteToMessageDecoder类，使用这个类，我们不必调用readableBytes()方法。参数S指定了用户状态管理的类型，其中Void代表不需要状态管理 应用实例：使用ReplayingDecoder 编写解码器，对前面的案例进行简化 12345678910@Slf4jpublic class MyByteToLongDecoder2 extends ReplayingDecoder&lt;Void&gt; &#123; @Override protected void decode(ChannelHandlerContext ctx, ByteBuf byteBuf, List&lt;Object&gt; list) throws Exception &#123; log.info(&quot;MyByteToLongDecoder2被调用&quot;); // 在ReplayingDecoder不需要判断数据是否足够读取，内部会进行处理判断 list.add(byteBuf.readLong()); &#125;&#125; 测试：使用MyByteToLongDecoder2 替代之前的MyByteToLongDecoder，结果和之前的一样 它有一些局限性: 并不是所有的 ByteBuf 操作都被支持，如果调用了一个不被支持的方法，将会抛出一个UnsupportedOperationException ReplayingDecoder 在某些情况下可能稍慢于ByteToMessageDecoder，例如网络缓慢并且消息格式复杂时，消息会被拆成了多个碎片，速度变慢 其他解码器 LineBasedFrameDecoder：这个类在Netty内部也有使用，它使用行尾控制字符（\\n或者\\r\\n）作为分隔符来解析数据 DelimiterBasedFrameDecoder：使用自定义的特殊字符作为消息的分隔符 HttpObjectDecoder：一个HTTP数据的解码器 LengthFieldBasedFrameDecoder：通过指定长度来标识整包消息，这样就可以自动的处理粘包和拆包消息 粘包和拆包 什么是粘包和拆包？ ​ TCP是个流的协议，所谓流就是没有界限的一串数据，大家可以想想河里的水，他们是连成一片的，其间并没有分界线。TCP底层并不了解上层业务数据的具体含义，它会根据TCP缓冲区的实际情况进行包的划分。所以在业务上认为，一个完整的包可能会被TCP拆分成多个包进行发送，也可能会把多个小的包封装成一个大的数据包发送，这就是所谓的TCP粘包/拆包问题 ​ 由于TCP无消息保护边界，需要在接收端处理消息边界问题，也就是我们所说的粘包，拆包问题 假设客户端分别发送了两个数据包D1和D2给服务端，由于服务端一次读取到的字节数不是确定的，故可能存在四种情况： 服务端分两次读取到两个独立的包，分别是D1和D2，没有粘包和拆包 服务端一次接受到两个数据包，D1和D1粘合在一起，被称为TCP粘包 服务端分两次接收到了两个数据包，第一次读取到了完成的D1包和D2包的部分内容，第二次读取到了D2包的剩余内容，这被称为TCP的拆包 服务端分两次接受到了两个数据包，第一次读取到了D1包的部分内容D1-1，第二次读取到了D1包的剩余内容D1-2和D2包的整包 粘包例子 1234567891011121314151617181920212223242526272829303132public class NettyServer &#123; private final static int PORT = 6666; public static void main(String[] args) throws Exception &#123; EventLoopGroup bossGroup = new NioEventLoopGroup(1); EventLoopGroup workerGroup = new NioEventLoopGroup(); try &#123; ServerBootstrap bootstrap = new ServerBootstrap(); bootstrap.group(bossGroup, workerGroup) .channel(NioServerSocketChannel.class) .option(ChannelOption.SO_BACKLOG, 128) .childOption(ChannelOption.SO_KEEPALIVE, true) .childHandler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; //创建一个通道初始化对象(匿名对象) //给pipeline 设置处理器 @Override protected void initChannel(SocketChannel socketChannel) throws Exception &#123; socketChannel.pipeline().addLast(new NettyServerHandler()); &#125; &#125;); ChannelFuture channelFuture = bootstrap.bind(PORT).sync(); channelFuture.channel().closeFuture().sync(); &#125; finally &#123; bossGroup.shutdownGracefully(); workerGroup.shutdownGracefully(); &#125; &#125;&#125; 1234567891011121314151617181920212223242526@Slf4jpublic class NettyServerHandler extends SimpleChannelInboundHandler&lt;ByteBuf&gt; &#123; private int count; // 异常处理 @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; cause.printStackTrace(); ctx.close(); &#125; @Override protected void channelRead0(ChannelHandlerContext ctx, ByteBuf msg) throws Exception &#123; byte[] buffer = new byte[msg.readableBytes()]; msg.readBytes(buffer); // 将buffer转成字符串 String message = new String(buffer, CharsetUtil.UTF_8); log.info(&quot;服务端接收到数据： &#123;&#125;&quot;, message); log.info(&quot;服务端接收到数据的次数：&#123;&#125;&quot;, ++count); // 回复客户端 ByteBuf byteBuf = Unpooled.copiedBuffer(UUID.randomUUID().toString() + &quot;\\r\\n&quot;, CharsetUtil.UTF_8); ctx.writeAndFlush(byteBuf); &#125;&#125; 1234567891011121314151617181920212223public class NettyClient &#123; public static void main(String[] args) throws Exception &#123; EventLoopGroup eventLoopGroup = new NioEventLoopGroup(); try &#123; Bootstrap bootstrap = new Bootstrap(); bootstrap.group(eventLoopGroup) .channel(NioSocketChannel.class) .handler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override protected void initChannel(SocketChannel ch) throws Exception &#123; ch.pipeline().addLast(new NettyClientHandler()); &#125; &#125;); ChannelFuture channelFuture = bootstrap.connect(&quot;localhost&quot;, 6666).sync(); channelFuture.channel().closeFuture().sync(); &#125; finally &#123; eventLoopGroup.shutdownGracefully(); &#125; &#125;&#125; 1234567891011121314151617181920212223242526272829@Slf4jpublic class NettyClientHandler extends SimpleChannelInboundHandler&lt;ByteBuf&gt; &#123; private int count; @Override public void channelActive(ChannelHandlerContext ctx) throws Exception &#123; for (int i = 0; i &lt; 10; i++) &#123; ByteBuf buf = Unpooled.copiedBuffer(&quot;hello server &quot; + i + &quot; &quot;, CharsetUtil.UTF_8); ctx.writeAndFlush(buf); &#125; &#125; @Override protected void channelRead0(ChannelHandlerContext ctx, ByteBuf msg) throws Exception &#123; byte[] bytes = new byte[msg.readableBytes()]; msg.readBytes(bytes); String message = new String(bytes, CharsetUtil.UTF_8); log.info(&quot;客户端接收到消息：&#123;&#125;&quot;, message); log.info(&quot;客户端接收消息次数：&#123;&#125;&quot;, ++count); &#125; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; cause.printStackTrace(); ctx.close(); &#125;&#125; 第一次启动客户端 服务端输出 客户端输出 第二次启动客户端 服务端输出 客户端输出 解决方案 原因：服务端或客户端并不知道每个请求（write操作）消息的长度是多少，服务器有可能多读或少读数据。所以每次读取数据的时候就有可能遇到粘包和拆包的情况 解决方案： 使用自定义协议 + 编解码器 来解决 关键就是要解决 服务器端每次读取数据长度的问题，这个问题解决就不会出现服务器多读或者少读数据的问题，从而避免TCP的粘包和拆包 具体实现： 自定义一个协议实体 msg字段：需要发送的消息 length: 消息长度，用于告诉接收端本次消息应该读取的长度是多少，避免多读和少读 自定义一个编解码器来将实体转成byte，自定义一个解码器将byte转成实体 自定义协议 1234567@Data@NoArgsConstructor@AllArgsConstructorpublic class MsgProtocol &#123; private int length; private byte[] msg;&#125; 自定义编码器 12345678910@Slf4jpublic class MsgEncoder extends MessageToByteEncoder&lt;MsgProtocol&gt; &#123; @Override protected void encode(ChannelHandlerContext ctx, MsgProtocol msg, ByteBuf out) throws Exception &#123; // 将对象转成字节流 log.info(&quot;自定义编码器被调用&quot;); out.writeInt(msg.getLength()); out.writeBytes(msg.getMsg()); &#125;&#125; 自定义解码器 123456789101112131415@Slf4jpublic class MsgDecoder extends ReplayingDecoder&lt;ByteBuf&gt; &#123; @Override protected void decode(ChannelHandlerContext ctx, ByteBuf msg, List&lt;Object&gt; out) throws Exception &#123; log.info(&quot;自定义解码器被调用&quot;); // 将二进制字节流转换成对象 int length = msg.readInt(); byte[] bytes = new byte[length]; msg.readBytes(bytes); // 封装成 MsgEncoder 对象加入 List, 传递给下一个Handler处理 MsgProtocol msgEncoder = new MsgProtocol(length, bytes); out.add(msgEncoder); &#125;&#125; 服务端源码 123456789101112131415161718192021222324252627282930313233343536public class NettyServer &#123; private final static int PORT = 6666; public static void main(String[] args) throws Exception &#123; EventLoopGroup bossGroup = new NioEventLoopGroup(1); EventLoopGroup workerGroup = new NioEventLoopGroup(); try &#123; ServerBootstrap bootstrap = new ServerBootstrap(); bootstrap.group(bossGroup, workerGroup) .channel(NioServerSocketChannel.class) .option(ChannelOption.SO_BACKLOG, 128) .childOption(ChannelOption.SO_KEEPALIVE, true) .childHandler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; //给pipeline 设置处理器 @Override protected void initChannel(SocketChannel ch) throws Exception &#123; // 加入自定义编码器 ch.pipeline().addLast(new MsgEncoder()); // 加入自定义解码器 ch.pipeline().addLast(new MsgDecoder()); ch.pipeline().addLast(new NettyServerHandler()); &#125; &#125;); ChannelFuture channelFuture = bootstrap.bind(PORT).sync(); channelFuture.channel().closeFuture().sync(); &#125; finally &#123; bossGroup.shutdownGracefully(); workerGroup.shutdownGracefully(); &#125; &#125;&#125; 1234567891011121314151617181920212223242526272829@Slf4jpublic class NettyServerHandler extends SimpleChannelInboundHandler&lt;MsgProtocol&gt; &#123; private int count; // 异常处理 @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; cause.printStackTrace(); ctx.close(); &#125; @Override protected void channelRead0(ChannelHandlerContext ctx, MsgProtocol msg) throws Exception &#123; // 接收对象并处理 int length = msg.getLength(); byte[] bytes = msg.getMsg(); log.info(&quot;服务端接收到消息长度：&#123;&#125;&quot;, length); log.info(&quot;服务端接收到的消息：&#123;&#125;&quot;, new String(bytes, CharsetUtil.UTF_8)); log.info(&quot;服务端接收到数据包的次数：&#123;&#125;&quot;, ++count); // 回复消息 String response = String.format(&quot;%s=%s&quot;, &quot;服务端回复消息&quot;, UUID.randomUUID().toString()); byte[] responseBytes = response.getBytes(CharsetUtil.UTF_8); int responseLength = responseBytes.length; MsgProtocol responseMsg = new MsgProtocol(responseLength, responseBytes); ctx.writeAndFlush(responseMsg); &#125;&#125; 客户端源码 123456789101112131415161718192021222324252627public class NettyClient &#123; public static void main(String[] args) throws Exception &#123; EventLoopGroup eventLoopGroup = new NioEventLoopGroup(); try &#123; Bootstrap bootstrap = new Bootstrap(); bootstrap.group(eventLoopGroup) .channel(NioSocketChannel.class) .handler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override protected void initChannel(SocketChannel ch) throws Exception &#123; // 加入自定义编码器 ch.pipeline().addLast(new MsgEncoder()); // 加入自定义解码器 ch.pipeline().addLast(new MsgDecoder()); ch.pipeline().addLast(new NettyClientHandler()); &#125; &#125;); ChannelFuture channelFuture = bootstrap.connect(&quot;localhost&quot;, 6666).sync(); channelFuture.channel().closeFuture().sync(); &#125; finally &#123; eventLoopGroup.shutdownGracefully(); &#125; &#125;&#125; 12345678910111213141516171819202122232425262728293031323334@Slf4jpublic class NettyClientHandler extends SimpleChannelInboundHandler&lt;MsgProtocol&gt; &#123; private int count; @Override public void channelActive(ChannelHandlerContext ctx) throws Exception &#123; // 发送10条数据 for (int j = 0; j &lt; 5; j++) &#123; String msg = String.format(&quot;%s=%s&quot;, j + 1, UUID.randomUUID().toString()); byte[] bytes = msg.getBytes(CharsetUtil.UTF_8); int length = bytes.length; MsgProtocol msgProtocol = new MsgProtocol(length, bytes); ctx.writeAndFlush(msgProtocol); &#125; &#125; @Override protected void channelRead0(ChannelHandlerContext ctx, MsgProtocol msg) throws Exception &#123; int length = msg.getLength(); byte[] bytes = msg.getMsg(); String message = new String(bytes, CharsetUtil.UTF_8); log.info(&quot;客户端收到服务端消息长度：&#123;&#125;&quot;, length); log.info(&quot;客户端收到服务端消息：&#123;&#125;&quot;, message); log.info(&quot;客户端接收消息次数：&#123;&#125;&quot;, ++count); &#125; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; cause.printStackTrace(); ctx.close(); &#125;&#125; 由输出的消息可知，当告知接收方数据包的大小后，数据就避免了多读和少读问题，从而避免了粘包和拆包的产生","categories":[],"tags":[]},{"title":"Elasticsearch","slug":"elasticsearch","date":"2022-05-01T09:53:47.000Z","updated":"2023-07-14T06:49:15.591Z","comments":true,"path":"2022/05/01/elasticsearch/","link":"","permalink":"https://wugengfeng.cn/2022/05/01/elasticsearch/","excerpt":"","text":"整体架构 知识体系 思维导图 环境准备 ElasticSearch 简称 ES ，是基于 Apache Lucene 构建的 开源搜索引擎，是当前最流行的 企业级搜索引擎。Lucene 本身就可以被认为迄今为止性能最好的一款开源搜索引擎工具包，但是Lucene的API相对复杂，需要深厚的搜索理论。很难集成到实际的应用中去。ES是采用java语言编写，提供了简单易用的RestFul API，开发者可以使用其简单的RestFul API，开发相关的搜索功能，从而避免Lucene的复杂性 The Elastic Stack，包括 Elasticsearch、Kibana、和Logstash（也称为ELK Stack）。能够安全可靠地获取任何来源、任何格式的数据，然后实时地对数据进行搜索、分析和可视化。Elaticsearch，简称为ES， ES是一个开源的高扩展的分布式全文搜索引擎，是整个Elastic Stack技术栈的核心。它可以近乎实时的存储、检索数据。本身扩展性很好，可以扩展到上百台服务器，处理PB级别的数据 ElasticSearch诞生 多年前，一个叫做 Shay Banon 的刚结婚不久的失业开发者，由于妻子要去伦敦学习厨师，他便跟着也去了。在他找工作的过程中，为了给妻子构建一个食谱的搜索引擎，他开始构建一个早期版本的Lucene。 直接基于Lucene工作会比较困难，所以 Shay 开始抽象 Lucene 代码以便Java程序员可以在应用中添加搜索功能。他发布了他的第一个开源项目，叫做 “Compass”。 后来Shay找到一份工作，这份工作处在高性能和内存数据网格的分布式环境中，因此 高性能的、实时的、分布式的 搜索引擎也是理所当然需要的。然后他决定重写 Compass 库使其成为一个独立的服务叫做 Elasticsearch。 第一个公开版本出现在 2010年2月，在那之后Elasticsearch已经成为Github上最受欢迎的项目之一，代码贡献者超过300人。一家主营 Elasticsearch 的公司就此成立，他们一边提供商业支持一边开发新功能，不过Elasticsearch将永远开源且对所有人可用。 Shay的妻子依旧等待着她的食谱搜索…… 目前国内大厂几乎无一不用Elasticsearch，阿里，腾讯，京东，美团 等等 … ES安装 传统Linux安装 123456789# 操作系统 ubuntu， elasticsearch 不允许使用root用户操作创建新用户 sudo adduser username# 下载ES到目录/data/elasticsearch https://www.elastic.co/cn/downloads/past-releases/elasticsearch-7-14-2解压es tar -zvxf elasticsearch-7.14.2-linux-x86_64.tar.gz # 进入bin 目录启动ES ./elasticsearch后台启动 ./elasticsearch -d ​ 解压ES后会发现ES目录下自带JDK环境，这是因为ES7版本后需要JDK11，所以如果不修改其他参数的情况下，ES默认使用自带的JDK 验证是否启动 ​ 输入 curl localhost:9200, 如果返回节点信息就说明启动成功 (es默认不开启外网访问) 开启远程链接 vi conf/elasticsearch.yml 12# 将 network 修改为4个0， 修改这个选项默认会以集群方式启动，启动会报错network.host: 0.0.0.0 vi /etc/sysctl.conf 1234vm.max_map_count=262144# 配置生效sysctl -p vi conf/elasticsearch.yml 12# 指定集群只有一个节点discovery.seed_hosts: [&quot;host1&quot;] 重启ES，这个时候就可以使用浏览器远程访问 Kibana安装 Kibana Navicat是一个针对Elasticsearch的开源分析及可视化平台，使用Kibana可以查询、查看并与存储在ES索引的数据进行交互操作，使用Kibana能执行高级的数据分析，并能以图表、表格和地图的形式查看数据。 123456789101112131415161718192021# 1. 下载Kibana- https://artifacts.elastic.co/downloads/kibana/kibana-7.2.1-linux-x86_64.tar.gz# 2. 安装下载的kibana- $ tar -zxvf kibana-7.14.0-linux-x86_64.tar.gz # 3. 编辑kibana配置文件- $ vim /Kibana 安装目录中 config 目录/kibana/kibana.yml# 4. 修改如下配置- server.host: &quot;0.0.0.0&quot; # 开启kibana远程访问- elasticsearch.hosts: [&quot;http://localhost:9200&quot;] #ES服务器地址# 文件夹授权- 5.chown -R elasticsearch:elasticsearch kibana-7.14.0-linux-x86_64/# 6. 启动kibana- ./bin/kibana# 7. 访问kibana的web界面 - http://10.15.0.5:5601/ #kibana默认端口为5601 初级阶段 数据类型 String 类型 类型 说明 text 会被分词处理，用于全文检索，很少用于聚合处理（需要设置fielddata属性)，不能用于排序 keyword 不可分词，用于精确搜索，过滤、排序、聚合等操作 Number 类型 类型 说明 byte 有符号的8位整数, 范围: [-128 ~ 127] short 有符号的16位整数, 范围: [-32768 ~ 32767] integer 有符号的32位整数, 范围: [−2^31 ~ 2^31-1] long 有符号的64位整数, 范围: [−2^63 ~ 2^63-1] float 32位单精度浮点数 double 64位双精度浮点数 half_float 16位半精度IEEE 754浮点类型 scaled_float 缩放类型的浮点数, 比如price字段只需精确到分, 57.34缩放因子为100, 存储结果为5734 应当尽可能选择范围小的数据类型, 字段的长度越短, 索引和搜索的效率越高;优先考虑使用带缩放因子的浮点类型 Date类型 Date类型在Elasticsearch中以数值形式(long类型)存储。文档在索引Date类型数据时候，会根据format选项来指定日期格式，Elasticsearch默认解析ISO 8601格式字符串。format选项有 格式化的日期字符串 说明 yyyy-MM-dd 2020-01-01 ISO8601 2020-01-01T05:04:03Z 毫秒数 1584930153000 秒数 1584930153 多个格式使用双竖线||分隔，每个格式都会被依次尝试, 直到找到匹配的，比如 yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis 在创建索引映射时指定日期格式 12345678910&#123; &quot;mappings&quot;: &#123; &quot;properties&quot;: &#123; &quot;create_time&quot;: &#123; &quot;type&quot;: &quot;date&quot;, &quot;format&quot;: &quot;yyyy-MM-dd&quot; &#125; &#125; &#125;&#125; 数组 创建索引 1234567891011121314151617181920PUT /order&#123; &quot;settings&quot;:&#123; &quot;number_of_shards&quot;:1, &quot;number_of_replicas&quot;:0 &#125;, &quot;mappings&quot;:&#123; &quot;properties&quot;:&#123; &quot;id&quot;:&#123; &quot;type&quot;:&quot;integer&quot; &#125;, &quot;order_num&quot;:&#123; &quot;type&quot;:&quot;keyword&quot; &#125;, &quot;products&quot;:&#123; &quot;type&quot;:&quot;keyword&quot; &#125; &#125; &#125;&#125; 初始化数据 1234567POST /order/_doc/_bulk&#123;&quot;index&quot;:&#123;&quot;_id&quot;: &quot;1&quot;&#125;&#125;&#123;&quot;id&quot;:1, &quot;order_number&quot;: &quot;zx054374368&quot;, &quot;products&quot;: [&quot;hrthrt&quot;, &quot;eqwreer&quot;]&#125;&#123;&quot;index&quot;:&#123;&quot;_id&quot;: &quot;2&quot;&#125;&#125;&#123;&quot;id&quot;:2, &quot;order_number&quot;: &quot;zx054374320&quot;, &quot;products&quot;: [&quot;dfgdf&quot;, &quot;erhfgf&quot;]&#125; &#123;&quot;index&quot;:&#123;&quot;_id&quot;: &quot;3&quot;&#125;&#125;&#123;&quot;id&quot;:3, &quot;order_number&quot;: &quot;zx05437430&quot;, &quot;products&quot;: [&quot;hrthrt&quot;, &quot;fdssdfsd&quot;]&#125; 查询 12345678910GET /order/_search&#123; &quot;query&quot;:&#123; &quot;term&quot;:&#123; &quot;products&quot;:&#123; &quot;value&quot;:&quot;hrthrt&quot; &#125; &#125; &#125;&#125; 返回 123456789101112131415161718192021222324252627282930313233&#123; &quot;took&quot; : 0, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 1, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : 1.2330425, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;order&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;3&quot;, &quot;_score&quot; : 1.2330425, &quot;_source&quot; : &#123; &quot;id&quot; : 3, &quot;order_number&quot; : &quot;zx05437430&quot;, &quot;products&quot; : [ &quot;hrthrt&quot;, &quot;fdssdfsd&quot; ] &#125; &#125; ] &#125;&#125; 对象 创建索引 1234567891011121314151617181920212223242526272829303132333435363738PUT /t_user&#123; &quot;settings&quot;:&#123; &quot;number_of_shards&quot;:1, &quot;number_of_replicas&quot;:0 &#125;, &quot;mappings&quot;:&#123; &quot;properties&quot;:&#123; &quot;name&quot;:&#123; &quot;type&quot;:&quot;keyword&quot; &#125;, &quot;age&quot;:&#123; &quot;type&quot;:&quot;short&quot; &#125;, &quot;address&quot;:&#123; &quot;properties&quot;:&#123; &quot;phone&quot;:&#123; &quot;type&quot;:&quot;keyword&quot; &#125;, &quot;address&quot;:&#123; &quot;type&quot;:&quot;keyword&quot; &#125; &#125; &#125; &#125; &#125;&#125;POST /t_user/_doc/1&#123; &quot;name&quot;:&quot;test&quot;, &quot;age&quot;:18, &quot;address&quot;:&#123; &quot;phone&quot;:&quot;123456&quot;, &quot;address&quot;:&quot;shenzhen&quot; &#125;&#125; 查询 12345678910GET /t_user/_search&#123; &quot;query&quot;: &#123; &quot;term&quot;: &#123; &quot;address.address&quot;: &#123; &quot;value&quot;: &quot;shenzhen&quot; &#125; &#125; &#125;&#125; 嵌套字段 嵌套类型是对象数据类型的特殊版本，它允许 对象数组 以一种可以彼此独立查询的方式进行索引 创建索引 12345678910111213141516171819202122232425262728293031323334353637PUT /t_user&#123; &quot;settings&quot;:&#123; &quot;number_of_shards&quot;:1, &quot;number_of_replicas&quot;:0 &#125;, &quot;mappings&quot;:&#123; &quot;properties&quot;:&#123; &quot;name&quot;:&#123; &quot;type&quot;:&quot;keyword&quot; &#125;, &quot;age&quot;:&#123; &quot;type&quot;:&quot;short&quot; &#125;, &quot;address_list&quot;:&#123; &quot;type&quot;: &quot;nested&quot; &#125; &#125; &#125;&#125;POST /t_user/_doc/1&#123; &quot;name&quot;:&quot;test&quot;, &quot;age&quot;:18, &quot;address_list&quot;:[ &#123; &quot;phone&quot;:&quot;123456&quot;, &quot;address&quot;:&quot;shenzhen&quot; &#125;, &#123; &quot;phone&quot;:&quot;654321&quot;, &quot;address&quot;:&quot;guangzhou&quot; &#125; ]&#125; 查询 123456789101112131415GET /t_user/_search&#123; &quot;query&quot;: &#123; &quot;nested&quot;: &#123; &quot;path&quot;: &quot;address_list&quot;, &quot;query&quot;: &#123; &quot;term&quot;: &#123; &quot;address_list.address&quot;: &#123; &quot;value&quot;: &quot;guangzhou&quot; &#125; &#125; &#125; &#125; &#125;&#125; 数据结构 Elasticsearch MySql Index 数据库 Type（废弃） 表 Document 行 Mapping DDL Field 列 ES的操作是服务端提供一个RESTful风格的HTTP接口，所有的操作都是基于服务端暴露的9200端口进行操作的，数据操作的语句以JSON为基础 索引 Index 一个索引就是一个拥有几分相似特征的文档的集合。比如说，你可以有一个商品数据的索引，一个订单数据的索引，还有一个用户数据的索引。一个索引由一个名字来标识 (必须全部是小写字母的)，并且当我们要对这个索引中的文档进行索引、搜索、更新和删除的时候，都要使用到这个名字。索引就像关系型数据库中库的概念，7.X后相当于表的概念 能搜索的数据必须索引，这样的好处是可以提高查询速度，比如：新华字典前面的目录就是索引的意思，目录可以提高查询速度 Elasticsearch索引的精髓：一切设计都是为了提高搜索的性能 一个index包含1个或多个shard 索引健康状况 颜色 说明 绿色 索引的所有分片都正常分配 黄色 至少有一个副本没有得到正确的分配 红色 至少有一个主分片没有得到正确的分配 索引状态: status 说明 open 重新打开被关闭的索引 close 关闭索引后, 不能再对索引进行读写. 都会报错。索引数据会持久化到磁盘, 关闭索引需要消费大量的磁盘空间，可能会对现有环境造成问题。关闭后不再占用内存资源 查询全局索引 GET /_cat/indices?v GET /_cat/indices?v&amp;index=test2 12345678910111213# 查询索引GET /_cat/indices?v# 查询结果health status index uuid pri rep docs.count docs.deleted store.size pri.store.sizegreen open .kibana-event-log-7.14.0-000001 kCIyNfTcTmKyWNgd_6yp7A 1 0 1 0 5.6kb 5.6kbgreen open .geoip_databases 39giqtCIRWWaYvUQ_OwGNg 1 0 40 0 37.7mb 37.7mbgreen open .kibana_7.14.0_001 bJdOG8tFRSG2QbWIsDSh2Q 1 0 32 13 2.1mb 2.1mbgreen open .apm-custom-link gpDH2bwsRtKZLCUafqhOXg 1 0 0 0 208b 208bgreen open .apm-agent-configuration DsCz8otyTe2oePHJreZN0w 1 0 0 0 208b 208bgreen open .kibana_task_manager_7.14.0_001 GMit5wspRXOk-Ut7fhw7GA 1 0 14 702 676.6kb 676.6kbyellow open products nDOyws-CTuOFY4KiySDHjg 1 1 0 0 208b 208bgreen open .tasks vK6PRYVkS26-NMPORegoMQ 1 0 6 0 22.8kb 22.8kb 创建索引 PUT /&lt;index&gt; PUT /products 12345&#123; &quot;acknowledged&quot; : true, &quot;shards_acknowledged&quot; : true, &quot;index&quot; : &quot;products&quot;&#125; 创建索引指定分片数量和副本数量以及映射 修改映射 1234567891011121314151617181920PUT /order&#123; &quot;settings&quot;: &#123; &quot;number_of_shards&quot;: 1, &quot;number_of_replicas&quot;: 0 &#125;, &quot;mappings&quot;: &#123; &quot;properties&quot;: &#123; &quot;id&quot;: &#123; &quot;type&quot;: &quot;integer&quot; &#125;, &quot;order_num&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125;, &quot;products&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125; &#125; &#125;&#125; 单节点部署Elasticsearch, 索引状态可能为yellow, 因为单点部署Elasticsearch，默认的 分片 和 副本 数目配置为1，而相同的分片不能在一个节点上，所以就存在副本分片指定不明确的问题，所以显示为yello，可以通过在Elasticsearch集群上添加一个节点来解决问题 关闭索引 一旦索引被关闭，那么这个索引只能显示 元数据 信息，不能够进行读写操作，关闭文档后进行检索会报400错误 POST /&lt;index&gt;/_close POST /products/_close 123456789&#123; &quot;acknowledged&quot;:true, &quot;shards_acknowledged&quot;:true, &quot;indices&quot;:&#123; &quot;products&quot;:&#123; &quot;closed&quot;:true &#125; &#125;&#125; 打开索引 打开被关闭的索引 POST /&lt;target&gt;/_open POST /products/_open 1234&#123;&quot;acknowledged&quot; : true,&quot;shards_acknowledged&quot; : true&#125; 获取索引信息 GET /&lt;target&gt; GET /order 12345678910111213141516171819202122232425262728293031323334353637383940414243444546&#123; &quot;order&quot; : &#123; &quot;aliases&quot; : &#123; &#125;, &quot;mappings&quot; : &#123; &quot;properties&quot; : &#123; &quot;id&quot; : &#123; &quot;type&quot; : &quot;integer&quot; &#125;, &quot;order_num&quot; : &#123; &quot;type&quot; : &quot;keyword&quot; &#125;, &quot;order_number&quot; : &#123; &quot;type&quot; : &quot;text&quot;, &quot;fields&quot; : &#123; &quot;keyword&quot; : &#123; &quot;type&quot; : &quot;keyword&quot;, &quot;ignore_above&quot; : 256 &#125; &#125; &#125;, &quot;products&quot; : &#123; &quot;type&quot; : &quot;keyword&quot; &#125; &#125; &#125;, &quot;settings&quot; : &#123; &quot;index&quot; : &#123; &quot;routing&quot; : &#123; &quot;allocation&quot; : &#123; &quot;include&quot; : &#123; &quot;_tier_preference&quot; : &quot;data_content&quot; &#125; &#125; &#125;, &quot;number_of_shards&quot; : &quot;1&quot;, &quot;provided_name&quot; : &quot;order&quot;, &quot;creation_date&quot; : &quot;1659524556073&quot;, &quot;number_of_replicas&quot; : &quot;0&quot;, &quot;uuid&quot; : &quot;1o5h02XgSVGZoJgf4Iyzrw&quot;, &quot;version&quot; : &#123; &quot;created&quot; : &quot;7140299&quot; &#125; &#125; &#125; &#125;&#125; 索引别名 创建别名 索引别名有个好处，在创建索引时使用别名，当要重建索引时： 先建立一个先的索引 导入数据 将索引别名指向新的索引 删除老的索引 POST &lt;target&gt;/_alias/&lt;alias&gt; POST order/_alias/product_order 12345678GET /order&#123; &quot;order&quot; : &#123; &quot;aliases&quot; : &#123; &quot;product_order&quot; : &#123; &#125; &#125;...&#125; 再次查看索引信息，能看到索引 order 存在别名 product_order 删除别名 DELETE &lt;target&gt;/_alias/&lt;alias&gt; DELETE /order/_alias/product_order 删除索引 DELETE /&lt;index&gt; DELETE /products 123&#123; &quot;acknowledged&quot; : true&#125; 检查是否存在 检查索引或别名是否存在 HEAD &lt;target&gt; HEAD products Http状态码 说明 200 所有目标都存在 404 一个或多个目标不存在 123HEAD /order200 - OK 收缩索引 将现有索引收缩为具有更少主分片的新索引，数据会复制到新索引 1234567891011121314151617181920212223# 索引准备PUT /test3&#123; &quot;settings&quot;: &#123; &quot;number_of_shards&quot;: 4, &quot;number_of_replicas&quot;: 0 &#125;&#125;# 先将索引设置为只读PUT /test3/_settings&#123; &quot;settings&quot;: &#123; &quot;index.blocks.write&quot;: true &#125;&#125;#收缩索引POST /test3/_shrink/test4&#123; &quot;settings&quot;: &#123; &quot;index.number_of_shards&quot;: 2, &quot;index.number_of_replicas&quot;: 0 &#125;&#125; 拆分索引 将现有索引拆分为具有更多主分片的新索引，数据会复制到新索引 1234567891011121314# 先将索引设置为只读PUT /test/_settings&#123; &quot;settings&quot;: &#123; &quot;index.blocks.write&quot;: false &#125;&#125;POST /test/_split/test2&#123; &quot;settings&quot;: &#123; &quot;index.number_of_shards&quot;: 4 &#125;&#125; 克隆索引 克隆现有索引，前提需要将索引设置为只读模式 PUT /&lt;index&gt;/_clone/&lt;target-index&gt; 123456789# 先将索引设置为只读PUT /products/_settings&#123; &quot;settings&quot;: &#123; &quot;index.blocks.write&quot;: true &#125;&#125;# 克隆PUT /products/_clone/product2 类型 Type 官方说是type是一个设计失误，在ES7后被移除。相当于表的概念 在一个索引中，你可以定义一种或多种类型，一个类型是你的索引的一个逻辑上的分类/分区，其语义完全由你来定。通常，会为具有一组共同字段的文档定义一个类型。不同的版本，类型发生了不同的变化 版本 Type 5.X 支持多种type 6.X 只能有一种type 7.X 默认不再支持自定义索引类型 (默认类型为:_doc) 文档 Document 文档是索引中存储的一条条数据。一条文档是一个 可被索引 的最小单元。ES中的文档采用了轻量级的JSON格式数据来表示。文档 在ES中相当于传统数据库中的行的概念 12345&#123; &quot;name&quot;:&quot;jack&quot;, &quot;age&quot;:18, &quot;gender&quot;:1&#125; 单文档操作 准备索引 1234567891011121314151617181920212223PUT /products&#123; &quot;mappings&quot;: &#123; &quot;properties&quot;: &#123; &quot;title&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125;, &quot;price&quot;: &#123; &quot;type&quot;: &quot;float&quot; &#125;, &quot;create_date&quot;: &#123; &quot;type&quot;: &quot;date&quot; &#125;, &quot;description&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125; &#125; &#125;, &quot;settings&quot;: &#123; &quot;number_of_shards&quot;: 1, &quot;number_of_replicas&quot;: 0 &#125;&#125; 添加文档 指定ID PUT /&lt;target&gt;/_doc/&lt;_id&gt; 123456789# 添加文档# 指定id,路径最后紧跟idPUT /products/_doc/1&#123; &quot;title&quot;: &quot;iphone13&quot;, &quot;price&quot;: 6399, &quot;create_date&quot;: &quot;2022-05-02&quot;, &quot;description&quot;: &quot;降价大促销&quot;&#125; 不指定ID，默认使用UUID POST /&lt;target&gt;/_doc 1234567891011121314151617181920212223POST /products/_doc&#123; &quot;title&quot;: &quot;iphone12&quot;, &quot;price&quot;: 5999, &quot;create_date&quot;: &quot;2022-05-02&quot;, &quot;description&quot;: &quot;八折销售&quot;&#125;返回：&#123; &quot;_index&quot;:&quot;products&quot;, &quot;_type&quot;:&quot;_doc&quot;, &quot;_id&quot;:&quot;_Z3bgIAB_jzT9lCXoXcS&quot;, &quot;_version&quot;:1, &quot;result&quot;:&quot;created&quot;, &quot;_shards&quot;:&#123; &quot;total&quot;:1, &quot;successful&quot;:1, &quot;failed&quot;:0 &#125;, &quot;_seq_no&quot;:3, &quot;_primary_term&quot;:1&#125; 查询文档 GET &lt;index&gt;/_doc/&lt;_id&gt; 12345678910111213141516171819# 查询文档 根据id查询GET /products/_doc/1返回：&#123; &quot;_index&quot;:&quot;products&quot;, &quot;_type&quot;:&quot;_doc&quot;, &quot;_id&quot;:&quot;1&quot;, &quot;_version&quot;:2, &quot;_seq_no&quot;:2, &quot;_primary_term&quot;:1, &quot;found&quot;:true, &quot;_source&quot;:&#123; &quot;title&quot;:&quot;iphone13&quot;, &quot;price&quot;:6399, &quot;create_date&quot;:&quot;2022-05-02&quot;, &quot;description&quot;:&quot;降价大促销&quot; &#125;&#125; 更新文档 全量更新 123456# 更新文档# 这种操作是先删除后插入，采用全量覆盖形式，会有字段丢失PUT /products/_doc/1&#123; &quot;title&quot;: &quot;iphone13 128g&quot;&#125; 再次查询 12345678910111213141516GET /products/_doc/1返回：&#123; &quot;_index&quot; : &quot;products&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_version&quot; : 2, &quot;_seq_no&quot; : 4, &quot;_primary_term&quot; : 1, &quot;found&quot; : true, &quot;_source&quot; : &#123; &quot;title&quot; : &quot;iphone13 128g&quot; &#125;&#125;# 其他字段不见了 部分更新 POST /&lt;index&gt;/_update/&lt;_id&gt; 1234567# 更新文档，指定字段POST /products/_update/1&#123; &quot;doc&quot;: &#123; &quot;title&quot;: &quot;iphone13 128g&quot; &#125;&#125; 再次查询 12345678910111213141516171819# 查询文档GET /products/_doc/1返回：&#123; &quot;_index&quot; : &quot;products&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_version&quot; : 4, &quot;_seq_no&quot; : 6, &quot;_primary_term&quot; : 1, &quot;found&quot; : true, &quot;_source&quot; : &#123; &quot;title&quot; : &quot;iphone13 128g&quot;, &quot;price&quot; : 6399, &quot;create_date&quot; : &quot;2022-05-02&quot;, &quot;description&quot; : &quot;降价大促销&quot; &#125;&#125; 只修改title字段，其他字段还在 删除文档 DELETE /&lt;index&gt;/_doc/&lt;_id&gt; 12345678910111213141516171819# 删除文档DELETE /products/_doc/1返回：&#123; &quot;_index&quot; : &quot;products&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_version&quot; : 7, &quot;result&quot; : &quot;deleted&quot;, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;failed&quot; : 0 &#125;, &quot;_seq_no&quot; : 9, &quot;_primary_term&quot; : 1&#125; 批量操作 批量获取 GET /_mget GET /&lt;index&gt;/_mget 12345678910111213141516171819202122232425262728293031323334353637383940GET /products/_mget&#123; &quot;ids&quot;: [1,2]&#125;//返回&#123; &quot;docs&quot; : [ &#123; &quot;_index&quot; : &quot;products&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_version&quot; : 1, &quot;_seq_no&quot; : 10, &quot;_primary_term&quot; : 1, &quot;found&quot; : true, &quot;_source&quot; : &#123; &quot;title&quot; : &quot;iphone13&quot;, &quot;price&quot; : 6399, &quot;create_date&quot; : &quot;2022-05-02&quot;, &quot;description&quot; : &quot;降价大促销&quot; &#125; &#125;, &#123; &quot;_index&quot; : &quot;products&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;2&quot;, &quot;_version&quot; : 1, &quot;_seq_no&quot; : 3, &quot;_primary_term&quot; : 1, &quot;found&quot; : true, &quot;_source&quot; : &#123; &quot;title&quot; : &quot;iphone12&quot;, &quot;price&quot; : 5999, &quot;create_date&quot; : &quot;2022-05-02&quot;, &quot;description&quot; : &quot;八折销售&quot; &#125; &#125; ]&#125; 批量API _bulk 批量操作是将多个命令一起执行，执行错误则跳过，不保证原子性。批量操作不能格式化命令 命令 说明 index 新增 update 修改 delete 删除 批量新增 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647POST /products/_doc/_bulk&#123;&quot;index&quot;: &#123;&quot;_id&quot;: &quot;2&quot;&#125;&#125; &#123;&quot;title&quot;: &quot;pihone13 256g&quot;,&quot;price&quot;: 7399,&quot;create_date&quot;:&quot;2022-05-02&quot;,&quot;description&quot;:&quot;新上市&quot;&#125;&#123;&quot;index&quot;: &#123;&quot;_id&quot;: &quot;3&quot;&#125;&#125; &#123;&quot;title&quot;: &quot;iphone13 512g&quot;,&quot;price&quot;: 8399,&quot;create_date&quot;:&quot;2022-05-02&quot;,&quot;description&quot;:&quot;预售&quot;&#125;返回： &#123; &quot;took&quot;:6, &quot;errors&quot;:false, &quot;items&quot;:[ &#123; &quot;index&quot;:&#123; &quot;_index&quot;:&quot;products&quot;, &quot;_type&quot;:&quot;_doc&quot;, &quot;_id&quot;:&quot;2&quot;, &quot;_version&quot;:1, &quot;result&quot;:&quot;created&quot;, &quot;_shards&quot;:&#123; &quot;total&quot;:1, &quot;successful&quot;:1, &quot;failed&quot;:0 &#125;, &quot;_seq_no&quot;:7, &quot;_primary_term&quot;:1, &quot;status&quot;:201 &#125; &#125;, &#123; &quot;index&quot;:&#123; &quot;_index&quot;:&quot;products&quot;, &quot;_type&quot;:&quot;_doc&quot;, &quot;_id&quot;:&quot;3&quot;, &quot;_version&quot;:1, &quot;result&quot;:&quot;created&quot;, &quot;_shards&quot;:&#123; &quot;total&quot;:1, &quot;successful&quot;:1, &quot;failed&quot;:0 &#125;, &quot;_seq_no&quot;:8, &quot;_primary_term&quot;:1, &quot;status&quot;:201 &#125; &#125; ]&#125; 批量操作 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566# 批量操作POST /products/_doc/_bulk&#123;&quot;index&quot;:&#123;&quot;_id&quot;: 4&#125;&#125; &#123;&quot;title&quot;: &quot;pihone13 1t&quot;,&quot;price&quot;: 8399,&quot;create_date&quot;:&quot;2022-05-02&quot;,&quot;description&quot;:&quot;售罄&quot;&#125;&#123;&quot;update&quot;: &#123;&quot;_id&quot;: 2&#125;&#125; &#123;&quot;doc&quot;: &#123;&quot;price&quot;: 7999&#125;&#125;&#123;&quot;delete&quot;: &#123;&quot;_id&quot;: 3&#125;&#125;返回：&#123;&quot;took&quot;:23, &quot;errors&quot;:false, &quot;items&quot;:[ &#123; &quot;index&quot;:&#123; &quot;_index&quot;:&quot;products&quot;, &quot;_type&quot;:&quot;_doc&quot;, &quot;_id&quot;:&quot;4&quot;, &quot;_version&quot;:1, &quot;result&quot;:&quot;created&quot;, &quot;_shards&quot;:&#123; &quot;total&quot;:1, &quot;successful&quot;:1, &quot;failed&quot;:0 &#125;, &quot;_seq_no&quot;:9, &quot;_primary_term&quot;:1, &quot;status&quot;:201 &#125; &#125;, &#123; &quot;update&quot;:&#123; &quot;_index&quot;:&quot;products&quot;, &quot;_type&quot;:&quot;_doc&quot;, &quot;_id&quot;:&quot;2&quot;, &quot;_version&quot;:2, &quot;result&quot;:&quot;updated&quot;, &quot;_shards&quot;:&#123; &quot;total&quot;:1, &quot;successful&quot;:1, &quot;failed&quot;:0 &#125;, &quot;_seq_no&quot;:10, &quot;_primary_term&quot;:1, &quot;status&quot;:200 &#125; &#125;, &#123; &quot;delete&quot;:&#123; &quot;_index&quot;:&quot;products&quot;, &quot;_type&quot;:&quot;_doc&quot;, &quot;_id&quot;:&quot;3&quot;, &quot;_version&quot;:2, &quot;result&quot;:&quot;deleted&quot;, &quot;_shards&quot;:&#123; &quot;total&quot;:1, &quot;successful&quot;:1, &quot;failed&quot;:0 &#125;, &quot;_seq_no&quot;:11, &quot;_primary_term&quot;:1, &quot;status&quot;:200 &#125; &#125; ]&#125; 根据条件删除 删除与指定查询匹配的文档 POST /&lt;target&gt;/_delete_by_query 12345678910POST /products/_delete_by_query&#123; &quot;query&quot;: &#123; &quot;term&quot;: &#123; &quot;title&quot;: &#123; &quot;value&quot;: &quot;iphone13&quot; &#125; &#125; &#125; &#125; 根据条件修改 更新与指定查询匹配的文档 POST /&lt;target&gt;/_update_by_query 1234567891011121314POST /products/_update_by_query&#123; &quot;query&quot;: &#123; &quot;term&quot;: &#123; &quot;title&quot;: &#123; &quot;value&quot;: &quot;iphone13&quot; &#125; &#125; &#125;, &quot;script&quot;: &#123; &quot;source&quot;: &quot;ctx._source[&#x27;title&#x27;]=&#x27;iphone13 256G&#x27;;&quot;, &quot;lang&quot;: &quot;painless&quot; &#125;&#125; 重建索引 将文档从源复制到目标，可以全量或者根据查询条件重建 12345678910111213141516POST /_reindex&#123; &quot;source&quot;: &#123; &quot;index&quot;: &quot;products&quot;, &quot;query&quot;: &#123; &quot;term&quot;: &#123; &quot;title&quot;: &#123; &quot;value&quot;: &quot;iphone13 256G&quot; &#125; &#125; &#125; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;product2&quot; &#125;&#125; 映射 Mapping Mapping 参数 映射是一种定义索引中字段及其属性的过程。它类似于关系数据库中的表结构定义 例如：string 的数据会被作为全文本来处理，这种数据类型适合需要搜索的场景。有些数据类型，你不需要对它进行搜索，相反需要对它做聚合运算，那么keyword、integer 数据类型就更合适 动态映射 正如上面说的，每个文档都有映射，但是在大多数使用场景中，我们并不需要显示的创建映射，因为ES中实现了 动态映射。我们在索引中写入一个下面的JSON文档，在动态映射的作用下，name 会映射成&gt; text类型，age会映射成 long 类型 1234&#123; &quot;name&quot;:&quot;jack&quot;, &quot;age&quot;:18,&#125; 但是记住，映射字段可以新增，字段不可以删除，字段类型不可以修改，如果非要删除或修改，需要重建索引 常见的Mapping字段属性 属性名称 类型 描述 type string 字段的数据类型。ES支持多种类型，例如text、keyword、date、long、float、boolean等。 analyzer string 用于处理文本类型数据的分词器。默认情况下ES使用标准分词器（standard analyzer）。 search_analyzer string 查询时使用的分词器，如果未设置则使用analyzer属性设置的分词器。 index boolean 指定字段是否被索引。默认为true，如果设置为false则字段值不会被索引。 doc_values boolean 指定字段是否用于聚合和排序。数值和日期类型的字段应该开启该属性。 store boolean 指定是否在_source中存储字段值，默认为false。 boost float 指定搜索时该字段的权重，默认为1.0。 null_value string 指定如果文档中缺少属性该属性应该具有的值。仅对缺少属性的字段有效。 ignore_above integer 对于文本类型的字段，指定字段值的最大允许长度。 format string 对日期类型字段，指定日期格式字符串。 coerce boolean 在强制类型转换时自动将字符串转换为数字或日期类型，以减少错误。 copy_to string 用于将字段值“复制”到其他字段，以便在查询时搜索多个字段。 fields object 可以指定多个子字段，每个子字段可以定义不同的属性，用于支持不同类型的搜索或聚合操作。 创建索引和映射 可以在创建索引并将字段添加到现有索引时创建字段映射 123456789101112131415161718192021222324252627282930# 创建索引和映射# number_of_shards 数据分片# number_of_replicas 副本数# mapper 自定义映射，类似mysql定义表结构PUT /products&#123; &quot;settings&quot;:&#123; &quot;number_of_shards&quot;:1, &quot;number_of_replicas&quot;:0 &#125;, &quot;mappings&quot;:&#123; &quot;properties&quot;:&#123; &quot;id&quot;:&#123; &quot;type&quot;:&quot;integer&quot; &#125;, &quot;title&quot;:&#123; &quot;type&quot;:&quot;keyword&quot; &#125;, &quot;price&quot;:&#123; &quot;type&quot;:&quot;double&quot; &#125;, &quot;create_date&quot;:&#123; &quot;type&quot;:&quot;date&quot; &#125;, &quot;description&quot;:&#123; &quot;type&quot;:&quot;text&quot; &#125; &#125; &#125;&#125; 查看索引映射信息 123456789101112131415161718192021222324252627# 查看索引的映射信息GET /products/_mapping# 返回&#123; &quot;products&quot;:&#123; &quot;mappings&quot;:&#123; &quot;properties&quot;:&#123; &quot;create_date&quot;:&#123; &quot;type&quot;:&quot;date&quot; &#125;, &quot;description&quot;:&#123; &quot;type&quot;:&quot;text&quot; &#125;, &quot;id&quot;:&#123; &quot;type&quot;:&quot;integer&quot; &#125;, &quot;price&quot;:&#123; &quot;type&quot;:&quot;double&quot; &#125;, &quot;title&quot;:&#123; &quot;type&quot;:&quot;keyword&quot; &#125; &#125; &#125; &#125;&#125; 查看特定字段映射 GET /&lt;index&gt;/_mapping/field/&lt;field&gt; 1234567891011121314151617GET /products/_mapping/field/title返回&#123; &quot;products&quot; : &#123; &quot;mappings&quot; : &#123; &quot;title&quot; : &#123; &quot;full_name&quot; : &quot;title&quot;, &quot;mapping&quot; : &#123; &quot;title&quot; : &#123; &quot;type&quot; : &quot;keyword&quot; &#125; &#125; &#125; &#125; &#125;&#125; 向现有映射添加字段 可以使用更新映射 API 将一个或多个新字段添加到现有索引 123456789PUT /products/_mapping&#123; &quot;properties&quot;: &#123; &quot;brand&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;index&quot;: true &#125; &#125;&#125; 字段 Field Json格式文档的Key，下面的JSON数据中，name 就是一个字段。ES和mongo一样，字段可以根据插入的数据动态添加与之对应的值有数据类型 1234&#123; &quot;name&quot;:&quot;jack&quot;, &quot;age&quot;: 12&#125; 上面的 name 是 string 类型的 keyword 或 text 上面的 age 是数值类型的 integer 除此之外，字段上还可以指定相对应的分词器，用于不同类型的搜索 倒排索引 正向索引 通过数据匹配关键词 比如在一张用户表中,通过兴趣爱好找到对应的用户 id name hobby 1 张三 篮球,唱歌 2 李四 跳舞,看书 3 王五 篮球,唱跳 1select * from user where hobby like &#x27;篮球%&#x27; mysql 底层通过遍历数据的 hobby 字段判断是否包含 篮球 关键词 (如果存在二级索引则遍历二级索引数据), 如果存在则挑选出当前数据; 所以正向索引是 数据 -&gt; 关键词 (通过数据找关键词) Elasticsearch 使用一种称为 倒排索引 的结构，它适用于快速的全文搜索。一个倒排索引由文档中所有 不重复词的列表 构成，对于其中每个词，有一个包含它的文档列表 倒排索引 通过关键词找到数据 Elasticsearch分别为每个字段都建立了一个倒排索引。因此查询时查询字段的 词条（Term），就能知道文档ID，就能快速找到文档 例如，假设我们有两个文档，每个文档的 content 包含如下内容： The quick brown fox jumped over the lazy dog Quick brown foxes leap over lazy dogs in summer 为了创建倒排索引，我们首先将每个文档的 content 字段拆分成单独的 词（我们称它为 词条 或 tokens ），创建一个包含所有不重复词条的排序列表，然后列出每个词条出现在哪个文档。结果如下所示： 现在，如果我们想搜索 quick brown ，我们只需要查找包含每个词条的文档： 两个文档都匹配，但是第一个文档比第二个匹配度更高。如果我们使用仅计算匹配词条数量的简单 相似性算法 ，那么，我们可以说，对于我们查询的相关性来讲，第一个文档比第二个文档更佳 索引文件结构 参考 123456789101112131415161718192021222324252627282930├── XX8rpADdS6GU3qvZxXvL0g │ ├── 0 │ │ ├── index │ │ │ ├── _d.fdm │ │ │ ├── _d.fdt │ │ │ ├── _d.fdx │ │ │ ├── _d.fnm │ │ │ ├── _d.kdd │ │ │ ├── _d.kdi │ │ │ ├── _d.kdm │ │ │ ├── _d_Lucene80_0.dvd │ │ │ ├── _d_Lucene80_0.dvm │ │ │ ├── _d_Lucene84_0.doc │ │ │ ├── _d_Lucene84_0.pos │ │ │ ├── _d_Lucene84_0.tim │ │ │ ├── _d_Lucene84_0.tip │ │ │ ├── _d_Lucene84_0.tmd│ │ │ ├── _d.nvd │ │ │ ├── _d.nvm │ │ │ ├── _d.si │ │ │ ├── segments_f│ │ │ └── write.lock│ │ ├── _state│ │ │ ├── retention-leases-92.st│ │ │ └── state-18.st│ │ └── translog // 事务日志│ │ ├── translog-28.tlog│ │ └── translog.ckp│ └── _state│ └── state-39.st Name Extension Brief Description Segment Info .si segment的元数据文件 Compound File .cfs, .cfe 一个segment包含了如下表的各个文件，为减少打开文件的数量，在segment小的时候，segment的所有文件内容都保存在cfs文件中，cfe文件保存了lucene各文件在cfs文件的位置信息 Fields .fnm 保存了fields的相关信息 Field Index .fdx 正排存储文件的元数据信息 Field Data .fdt 存储了正排存储数据，写入的原文存储在这 Term Dictionary .tim 倒排索引的元数据信息 Term Index .tip 倒排索引文件，存储了所有的倒排索引数据 Frequencies .doc 保存了每个term的doc id列表和term在doc中的词频 Positions .pos Stores position information about where a term occurs in the index 全文索引的字段，会有该文件，保存了term在doc中的位置 Payloads .pay Stores additional per-position metadata information such as character offsets and user payloads 全文索引的字段，使用了一些像payloads的高级特性会有该文件，保存了term在doc中的一些高级特性 Norms .nvd, .nvm 文件保存索引字段加权数据 Per-Document Values .dvd, .dvm lucene的docvalues文件，即数据的列式存储，用作聚合和排序 Term Vector Data .tvx, .tvd, .tvf Stores offset into the document data file 保存索引字段的矢量信息，用在对term进行高亮，计算文本相关性中使用 Live Documents .liv 记录了segment中删除的doc 索引 index Elasticsearch Index 的每个分片底层对应的就是Lucene索引，它由多个段文件组成，这些文件放在同一个目录下 段 segment 组成Lucene索引的文件，一个段里包含多个文档 一个Lucene的索引由多个段组成，段与段之间是独立的。添加新的文档时可以生成新的段，达到阈值（段的个数，段中包含的文件数等）时，不同的段可以合并，段是一个不可变文件 在文件夹下，具有相同前缀的文件属于同一个段 segments.gen 和 segments_N（N表示一个具体数字，eg：segments_5）是段的元数据文件，他们保存了段的属性信息 文档 document 文档时建索引的基本单位，一个段中可以包含多篇文档 新添加的文档时单独保存在一个新生成的段中，随着段的合并，不同的文档会合并到至相同的段中 字段 Field 一个文档有可由多个字段（Field）组成，比如一篇新闻，有 标题，作者，正文等多个属性，这些属性可以看作是文档的字段，不同的字段可以指定不同的索引方式，比如指定不同的分词方式，是否构建索引，是否存储等 词条 Term 词条 是索引的最小单位，是经过词法分词和语言处理后的字符串 Term 词条 在传统关系型数据库中，假如想要存储一篇几千字的文本，可以通过 text 直接存进去，和存储其它类型的数据没什么不同。存储虽然很方便，但是要对文本中的关键词进行搜索，查询速度非常慢，尤其是在大数据量的时候。还是上面的场景，在ES中存储这篇文章，它不会直接存进去，而是先把大文本切割成很多个小的词，这些词就是我们所说的 词条，它是ES搜索的 最小单位 ，每个查询都是按 词条 搜索的。ES使用了 倒排索引 来存储数据，什么是倒排索引？在关系型数据中，最好的方式是用主键id来查询，可以快速定位到文章内容，而 倒排索引 则相反，它建立的是 词条 和文章id的对应关系，索引它更适合文本搜索，下面是一个倒排索引，hello和world这两个词都命中了id=1的文章 content：hello world!，进行分词 词项 term 文章id hello 1 world 1 倒排索引底层原理决定了ES天生适合做全文本搜索 Term Index 单词索引 数据结构：trie 树 指向单词字典的索引，单词的分词依赖于 分词器 Term Dictionary 词典 Terms Dictionary（单词字典或者叫词典）存储所有的 词条（Term） 数据，同时它也是 Term 与 Postings（倒排列表） 的关系纽带，存储了每个 Term 和其对应的 Postings 文件位置指针 数据结构：B+树，HashMap Posting List 倒排列表 Elasticsearch分别为每个 field 都建立了一个倒排索引， 倒排列表记录了出现过某个单词的所有文档的ID及单词在该文档中出现的位置信息，每条记录称为一个倒排项(Posting)。根据倒排列表，即可获知哪些文档包含某个单词。（PS：实际的倒排列表中并不只是存了文档ID这么简单，还有一些其它的信息，比如：词频（Term出现的次数）、偏移量（offset）等） 倒排索引查询逻辑 查询字典，如果单词不再在字典中直接返回空 获取倒排列表中的指针，通过倒排列表获取对应的文档id 通过文档id获取数据 根据关键词出现的位置，频率进行相关性打分，排序 查询返回 什么是相关性 相关度评分背后的理论 我们曾经讲过，默认情况下，返回结果是按相关性倒序排列的。 但是什么是相关性？ 相关性如何计算？ 每个文档都有相关性评分，用一个正浮点数字段 _score 来表示 。 _score 的评分越高，相关性越高。 查询语句会为每个文档生成一个 _score 字段。评分的计算方式取决于查询类型 不同的查询语句用于不同的目的： fuzzy 查询会计算与关键词的拼写相似程度，terms 查询会计算 找到的内容与关键词组成部分匹配的百分比，但是通常我们说的 relevance 是我们用来计算全文本字段的值相对于全文本检索词相似程度的算法。 Elasticsearch 的相似度算法被定义为检索词频率/反向文档频率， TF/IDF ，包括以下内容： TF(词频) Term Frequency：搜索文本中的各个词条（term）在查询文本中出现了多少次，出现次数越多就越相关，得分会比较高 IDF(逆文档频率) Inverse Document Frequency: 搜索文本中的各个词条 (term)在整个索引的所有文档中出现了多少次，出现的次数越多，说明越不重要，也就越不相关，得分就比较低。 执行计划查询评分计算 12345678GET /goods/_search?explain=true&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;description&quot;: &quot;小米&quot; &#125; &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697&#123; &quot;took&quot; : 58, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 1, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : 1.4131414, &quot;hits&quot; : [ &#123; &quot;_shard&quot; : &quot;[goods][0]&quot;, &quot;_node&quot; : &quot;WTbxxdLZT8mbJAHsplGS8g&quot;, &quot;_index&quot; : &quot;goods&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_score&quot; : 1.4131414, &quot;_source&quot; : &#123; &quot;title&quot; : &quot;小米11&quot;, &quot;price&quot; : 3699.0, &quot;created_at&quot; : &quot;2021-02-28T18:00:00&quot;, &quot;description&quot; : &quot;小米11，骁龙888，1亿像素。&quot; &#125;, &quot;_explanation&quot; : &#123; &quot;value&quot; : 1.4131414, &quot;description&quot; : &quot;weight(description:小米 in 2) [PerFieldSimilarity], result of:&quot;, &quot;details&quot; : [ &#123; &quot;value&quot; : 1.4131414, &quot;description&quot; : &quot;score(freq=1.0), computed as boost * idf * tf from:&quot;, &quot;details&quot; : [ &#123; &quot;value&quot; : 2.2, &quot;description&quot; : &quot;boost&quot;, &quot;details&quot; : [ ] &#125;, &#123; &quot;value&quot; : 1.1631508, &quot;description&quot; : &quot;idf, computed as log(1 + (N - n + 0.5) / (n + 0.5)) from:&quot;, &quot;details&quot; : [ &#123; &quot;value&quot; : 2, &quot;description&quot; : &quot;n, number of documents containing term&quot;, &quot;details&quot; : [ ] &#125;, &#123; &quot;value&quot; : 7, &quot;description&quot; : &quot;N, total number of documents with field&quot;, &quot;details&quot; : [ ] &#125; ] &#125;, &#123; &quot;value&quot; : 0.55223876, &quot;description&quot; : &quot;tf, computed as freq / (freq + k1 * (1 - b + b * dl / avgdl)) from:&quot;, &quot;details&quot; : [ &#123; &quot;value&quot; : 1.0, &quot;description&quot; : &quot;freq, occurrences of term within document&quot;, &quot;details&quot; : [ ] &#125;, &#123; &quot;value&quot; : 1.2, &quot;description&quot; : &quot;k1, term saturation parameter&quot;, &quot;details&quot; : [ ] &#125;, &#123; &quot;value&quot; : 0.75, &quot;description&quot; : &quot;b, length normalization parameter&quot;, &quot;details&quot; : [ ] &#125;, &#123; &quot;value&quot; : 9.0, &quot;description&quot; : &quot;dl, length of field&quot;, &quot;details&quot; : [ ] &#125;, &#123; &quot;value&quot; : 15.857142, &quot;description&quot; : &quot;avgdl, average length of field&quot;, &quot;details&quot; : [ ] &#125; ] &#125; ] &#125; ] &#125; &#125; ] &#125;&#125; 最终打分：Boot(2.2) * TF * IDF 更改权重 控制相关度 修改查询权重，如果想让 小米 查询排在 华为 前面，可以修改查询值的权重 12345678910111213141516171819202122232425GET /goods/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;should&quot;: [ &#123; &quot;match&quot;: &#123; &quot;description&quot;: &#123; &quot;query&quot;: &quot;小米&quot;, &quot;boost&quot;: 5 &#125; &#125; &#125;, &#123; &quot;match&quot;: &#123; &quot;description&quot;: &#123; &quot;query&quot;: &quot;华为&quot;, &quot;boost&quot;: 1 &#125; &#125; &#125; ] &#125; &#125;&#125; 分析器 Analyzer ES中不会把一篇文章直接存入磁盘，在存储时它会先对文本进行分析，分析器的就是用来分析这些文本，中间包括 过滤、分词 等过程，经过分析处理后再存储到磁盘。分析器由3部分组成，分别是 字符过滤器、分词器 和 词项处理器 字符过滤器（character filters）：首先，字符串按顺序通过每个 字符过滤器 。他们的任务是在分词前整理字符串。一个字符过滤器可以用来去掉HTML标签，或者将 &amp; 转化成 and 分词器（tokenizers）：其次，字符串被 分词器 分为单个的词条。一个简单的分词器遇到空格和标点的时候，可能会将文本拆分成词条，比如文本 “Quick brown fox!” ，可以被分割成 3个词项，[Quick, brown, fox!] Token 过滤器（token filters）：最后，词条按顺序通过每个 token 过滤器 。这个过程可能会改变词条（例如，小写化 Quick ），删除词条（例如， 像 a， and， the 等无用词），或者增加词条（例如，像 jump 和 leap 这种同义词） ES提供了多种分析器，默认使用标准的分析器，能满足大部分的需求，实在不行也可以使用自定义的 分析器，除了分析器以外，分词器、字符过滤器等在ES中也提供了多种选择 内置分析器 测试文本：“Set the shape to semi-transparent by calling set_trans(5)” Standard Analyzer - 标准分词器，英文按单词切分，删除绝大部分标点符号，并小写处理 set, the, shape, to, semi, transparent, by, calling, set_trans, 5 Simple Analyzer - 简单分析器，在任何不是字母的地方分隔文本，将词条小写 set, the, shape, to, semi, transparent, by, calling, set, trans Stop Analyzer - 停用词分析器，小写处理，停用词过滤(the,a,is) Whitespace Analyzer - 空格分析器，按照空格切分，不转小写 Set, the, shape, to, semi-transparent, by, calling, set_trans(5) Keyword Analyzer - 不分词，直接将输入当作输出 内置分词器使用 标准分词器 特点: 按照单词分词，英文统一转为小写，过滤标点符号， 中文单字分词 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667POST /_analyze&#123; &quot;analyzer&quot;: &quot;standard&quot;, &quot;text&quot;: &quot;Are you OK? 我是中国人&quot;&#125;结果&#123; &quot;tokens&quot;:[ &#123; &quot;token&quot;:&quot;are&quot;, &quot;start_offset&quot;:0, &quot;end_offset&quot;:3, &quot;type&quot;:&quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot;:0 &#125;, &#123; &quot;token&quot;:&quot;you&quot;, &quot;start_offset&quot;:4, &quot;end_offset&quot;:7, &quot;type&quot;:&quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot;:1 &#125;, &#123; &quot;token&quot;:&quot;ok&quot;, &quot;start_offset&quot;:8, &quot;end_offset&quot;:10, &quot;type&quot;:&quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot;:2 &#125;, &#123; &quot;token&quot;:&quot;我&quot;, &quot;start_offset&quot;:12, &quot;end_offset&quot;:13, &quot;type&quot;:&quot;&lt;IDEOGRAPHIC&gt;&quot;, &quot;position&quot;:3 &#125;, &#123; &quot;token&quot;:&quot;是&quot;, &quot;start_offset&quot;:13, &quot;end_offset&quot;:14, &quot;type&quot;:&quot;&lt;IDEOGRAPHIC&gt;&quot;, &quot;position&quot;:4 &#125;, &#123; &quot;token&quot;:&quot;中&quot;, &quot;start_offset&quot;:14, &quot;end_offset&quot;:15, &quot;type&quot;:&quot;&lt;IDEOGRAPHIC&gt;&quot;, &quot;position&quot;:5 &#125;, &#123; &quot;token&quot;:&quot;国&quot;, &quot;start_offset&quot;:15, &quot;end_offset&quot;:16, &quot;type&quot;:&quot;&lt;IDEOGRAPHIC&gt;&quot;, &quot;position&quot;:6 &#125;, &#123; &quot;token&quot;:&quot;人&quot;, &quot;start_offset&quot;:16, &quot;end_offset&quot;:17, &quot;type&quot;:&quot;&lt;IDEOGRAPHIC&gt;&quot;, &quot;position&quot;:7 &#125; ]&#125; Simple 分词器 特点: 在任何不是字母的地方分隔文本（下划线），英文统一转为小写，去掉符号，中文按照空格进行分词 12345678910111213141516171819202122232425262728293031323334353637383940414243444546POST /_analyze&#123; &quot;analyzer&quot;: &quot;simple&quot;, &quot;text&quot;: &quot;Are you OK? 我是 中国人&quot;&#125;结果&#123; &quot;tokens&quot; : [ &#123; &quot;token&quot; : &quot;are&quot;, &quot;start_offset&quot; : 0, &quot;end_offset&quot; : 3, &quot;type&quot; : &quot;word&quot;, &quot;position&quot; : 0 &#125;, &#123; &quot;token&quot; : &quot;you&quot;, &quot;start_offset&quot; : 4, &quot;end_offset&quot; : 7, &quot;type&quot; : &quot;word&quot;, &quot;position&quot; : 1 &#125;, &#123; &quot;token&quot; : &quot;ok&quot;, &quot;start_offset&quot; : 8, &quot;end_offset&quot; : 10, &quot;type&quot; : &quot;word&quot;, &quot;position&quot; : 2 &#125;, &#123; &quot;token&quot; : &quot;我是&quot;, &quot;start_offset&quot; : 12, &quot;end_offset&quot; : 14, &quot;type&quot; : &quot;word&quot;, &quot;position&quot; : 3 &#125;, &#123; &quot;token&quot; : &quot;中国人&quot;, &quot;start_offset&quot; : 15, &quot;end_offset&quot; : 18, &quot;type&quot; : &quot;word&quot;, &quot;position&quot; : 4 &#125; ]&#125; Whitespace 分词器 特点: 中文、英文按照空格分词，英文不会转为小写 ，不去掉标点符号 12345678910111213141516171819202122232425262728293031323334353637383940414243444546POST /_analyze&#123; &quot;analyzer&quot;: &quot;whitespace&quot;, &quot;text&quot;: &quot;Are you OK? 我是 中国人&quot;&#125;结果&#123; &quot;tokens&quot; : [ &#123; &quot;token&quot; : &quot;Are&quot;, &quot;start_offset&quot; : 0, &quot;end_offset&quot; : 3, &quot;type&quot; : &quot;word&quot;, &quot;position&quot; : 0 &#125;, &#123; &quot;token&quot; : &quot;you&quot;, &quot;start_offset&quot; : 4, &quot;end_offset&quot; : 7, &quot;type&quot; : &quot;word&quot;, &quot;position&quot; : 1 &#125;, &#123; &quot;token&quot; : &quot;OK?&quot;, &quot;start_offset&quot; : 8, &quot;end_offset&quot; : 11, &quot;type&quot; : &quot;word&quot;, &quot;position&quot; : 2 &#125;, &#123; &quot;token&quot; : &quot;我是&quot;, &quot;start_offset&quot; : 12, &quot;end_offset&quot; : 14, &quot;type&quot; : &quot;word&quot;, &quot;position&quot; : 3 &#125;, &#123; &quot;token&quot; : &quot;中国人&quot;, &quot;start_offset&quot; : 15, &quot;end_offset&quot; : 18, &quot;type&quot; : &quot;word&quot;, &quot;position&quot; : 4 &#125; ]&#125; Keyword 分词器 特点: 不分词，直接将输入当作输出 123456789101112131415161718POST /_analyze&#123; &quot;analyzer&quot;: &quot;keyword&quot;, &quot;text&quot;: &quot;Are you OK? 我是 中国人&quot;&#125; 结果&#123; &quot;tokens&quot; : [ &#123; &quot;token&quot; : &quot;Are you OK? 我是 中国人&quot;, &quot;start_offset&quot; : 0, &quot;end_offset&quot; : 18, &quot;type&quot; : &quot;word&quot;, &quot;position&quot; : 0 &#125; ]&#125; 分词器的使用 在创建映射的时候指定分词器 12345678910111213&#123; &quot;settings&quot;:&#123; &#125;, &quot;mappings&quot;:&#123; &quot;properties&quot;:&#123; &quot;title&quot;:&#123; &quot;type&quot;:&quot;text&quot;, &quot;analyzer&quot;:&quot;standard&quot; //指定分词器 &#125; &#125; &#125;&#125; 中文分词器 ES的分词器是ES的插件，因此第三方分词器可以方便快捷集成 安装IK 下载地址 注意：IK分词器和ElasticSearch的版本关系是一一对应的，所以在下载IK分词器前需要确认ES版本 安装：先下载IK分词器，到ES安装目录下的plugins目录下创建analysis-ik目录并解压下载文件，重启ES即可 IK使用 IK有两种颗粒度的拆分： ik_smart 会做最粗粒度的拆分 12345678910111213141516171819202122232425POST /_analyze&#123; &quot;analyzer&quot;: &quot;ik_smart&quot;, &quot;text&quot;: &quot;中华人民共和国国歌&quot;&#125;结果:&#123; &quot;tokens&quot; : [ &#123; &quot;token&quot; : &quot;中华人民共和国&quot;, &quot;start_offset&quot; : 0, &quot;end_offset&quot; : 7, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 0 &#125;, &#123; &quot;token&quot; : &quot;国歌&quot;, &quot;start_offset&quot; : 7, &quot;end_offset&quot; : 9, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 1 &#125; ]&#125; ik_max_word 会将文本做最细粒度的拆分 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081POST /_analyze&#123; &quot;analyzer&quot;: &quot;ik_max_word&quot;, &quot;text&quot;: &quot;中华人民共和国国歌&quot;&#125;结果：&#123; &quot;tokens&quot; : [ &#123; &quot;token&quot; : &quot;中华人民共和国&quot;, &quot;start_offset&quot; : 0, &quot;end_offset&quot; : 7, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 0 &#125;, &#123; &quot;token&quot; : &quot;中华人民&quot;, &quot;start_offset&quot; : 0, &quot;end_offset&quot; : 4, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 1 &#125;, &#123; &quot;token&quot; : &quot;中华&quot;, &quot;start_offset&quot; : 0, &quot;end_offset&quot; : 2, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 2 &#125;, &#123; &quot;token&quot; : &quot;华人&quot;, &quot;start_offset&quot; : 1, &quot;end_offset&quot; : 3, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 3 &#125;, &#123; &quot;token&quot; : &quot;人民共和国&quot;, &quot;start_offset&quot; : 2, &quot;end_offset&quot; : 7, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 4 &#125;, &#123; &quot;token&quot; : &quot;人民&quot;, &quot;start_offset&quot; : 2, &quot;end_offset&quot; : 4, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 5 &#125;, &#123; &quot;token&quot; : &quot;共和国&quot;, &quot;start_offset&quot; : 4, &quot;end_offset&quot; : 7, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 6 &#125;, &#123; &quot;token&quot; : &quot;共和&quot;, &quot;start_offset&quot; : 4, &quot;end_offset&quot; : 6, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 7 &#125;, &#123; &quot;token&quot; : &quot;国&quot;, &quot;start_offset&quot; : 6, &quot;end_offset&quot; : 7, &quot;type&quot; : &quot;CN_CHAR&quot;, &quot;position&quot; : 8 &#125;, &#123; &quot;token&quot; : &quot;国歌&quot;, &quot;start_offset&quot; : 7, &quot;end_offset&quot; : 9, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 9 &#125; ]&#125; 扩展词 IK支持自定义 扩展词典 和 停用词典 扩展词典 就是有些词并不是关键词，但是也希望被ES用来作为检索的关键词，可以将这些词加入扩展词典 123456789101112131415161718191. 修改vim IKAnalyzer.cfg.xml &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;!DOCTYPE properties SYSTEM &quot;http://java.sun.com/dtd/properties.dtd&quot;&gt; &lt;properties&gt; &lt;comment&gt;IK Analyzer 扩展配置&lt;/comment&gt; &lt;!--用户可以在这里配置自己的扩展字典 --&gt; &lt;entry key=&quot;ext_dict&quot;&gt;ext_dict.dic&lt;/entry&gt; &lt;!--用户可以在这里配置自己的扩展停止词字典--&gt; &lt;entry key=&quot;ext_stopwords&quot;&gt;ext_stopword.dic&lt;/entry&gt; &lt;/properties&gt;2. 在ik分词器目录下config目录中创建ext_dict.dic文件 编码一定要为UTF-8才能生效 vim ext_dict.dic 加入扩展词即可3. 在ik分词器目录下config目录中创建ext_stopword.dic文件 vim ext_stopword.dic 加入停用词即可4.重启es生效 注意： 词典的编码必须为UTF-8，否则无法生效 测试 比如想分词一个句子 吴耿锋正在学习ES 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960POST /_analyze&#123; &quot;analyzer&quot;: &quot;ik_max_word&quot;, &quot;text&quot;: &quot;吴耿锋正在学习ES&quot;&#125;结果：&#123; &quot;tokens&quot; : [ &#123; &quot;token&quot; : &quot;吴&quot;, &quot;start_offset&quot; : 0, &quot;end_offset&quot; : 1, &quot;type&quot; : &quot;CN_CHAR&quot;, &quot;position&quot; : 0 &#125;, &#123; &quot;token&quot; : &quot;耿&quot;, &quot;start_offset&quot; : 1, &quot;end_offset&quot; : 2, &quot;type&quot; : &quot;CN_CHAR&quot;, &quot;position&quot; : 1 &#125;, &#123; &quot;token&quot; : &quot;锋&quot;, &quot;start_offset&quot; : 2, &quot;end_offset&quot; : 3, &quot;type&quot; : &quot;CN_CHAR&quot;, &quot;position&quot; : 2 &#125;, &#123; &quot;token&quot; : &quot;正在&quot;, &quot;start_offset&quot; : 3, &quot;end_offset&quot; : 5, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 3 &#125;, &#123; &quot;token&quot; : &quot;在学&quot;, &quot;start_offset&quot; : 4, &quot;end_offset&quot; : 6, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 4 &#125;, &#123; &quot;token&quot; : &quot;学习&quot;, &quot;start_offset&quot; : 5, &quot;end_offset&quot; : 7, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 5 &#125;, &#123; &quot;token&quot; : &quot;es&quot;, &quot;start_offset&quot; : 7, &quot;end_offset&quot; : 9, &quot;type&quot; : &quot;ENGLISH&quot;, &quot;position&quot; : 6 &#125; ]&#125; 没有达到效果，如果希望将 吴耿锋、正在学习 作为关键词，那么就要自定义扩展词典 在IK分词器的config 目录下创建自定义词典 vi ext_dict.dic， 并输入以下内容 12吴耿锋正在学习 修改IKAnalyzer.cfg.xml配置 1&lt;entry key=&quot;ext_dict&quot;&gt;ext_dict.dic&lt;/entry&gt; 重新启动ES 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253POST /_analyze&#123; &quot;analyzer&quot;: &quot;ik_max_word&quot;, &quot;text&quot;: &quot;吴耿锋正在学习ES&quot;&#125;结果：&#123; &quot;tokens&quot; : [ &#123; &quot;token&quot; : &quot;吴耿锋&quot;, &quot;start_offset&quot; : 0, &quot;end_offset&quot; : 3, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 0 &#125;, &#123; &quot;token&quot; : &quot;正在学习&quot;, &quot;start_offset&quot; : 3, &quot;end_offset&quot; : 7, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 1 &#125;, &#123; &quot;token&quot; : &quot;正在&quot;, &quot;start_offset&quot; : 3, &quot;end_offset&quot; : 5, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 2 &#125;, &#123; &quot;token&quot; : &quot;在学&quot;, &quot;start_offset&quot; : 4, &quot;end_offset&quot; : 6, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 3 &#125;, &#123; &quot;token&quot; : &quot;学习&quot;, &quot;start_offset&quot; : 5, &quot;end_offset&quot; : 7, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 4 &#125;, &#123; &quot;token&quot; : &quot;es&quot;, &quot;start_offset&quot; : 7, &quot;end_offset&quot; : 9, &quot;type&quot; : &quot;ENGLISH&quot;, &quot;position&quot; : 5 &#125; ]&#125; 停用词 停用词典 就是有些词是关键词，但是出于业务场景不想使用这些关键词被检索到，可以将这些词放入停用词典 比如上面的例子不想让 正在、在学 这些词被检索到，那就可以自定义一个停用词典 在IK分词器的 config 目录下创建自定义词典 vi ext_stopword.dic， 并输入以下内容 12正在在学 修改IKAnalyzer.cfg.xml配置 1&lt;entry key=&quot;ext_stopwords&quot;&gt;ext_stopword.dic&lt;/entry&gt; 重新启动ES 123456789101112131415161718192021222324252627282930313233343536373839POST /_analyze&#123;&quot;analyzer&quot;: &quot;ik_max_word&quot;,&quot;text&quot;: &quot;吴耿锋正在学习ES&quot;&#125;结果：&#123; &quot;tokens&quot;:[ &#123; &quot;token&quot;:&quot;吴耿锋&quot;, &quot;start_offset&quot;:0, &quot;end_offset&quot;:3, &quot;type&quot;:&quot;CN_WORD&quot;, &quot;position&quot;:0 &#125;, &#123; &quot;token&quot;:&quot;正在学习&quot;, &quot;start_offset&quot;:3, &quot;end_offset&quot;:7, &quot;type&quot;:&quot;CN_WORD&quot;, &quot;position&quot;:1 &#125;, &#123; &quot;token&quot;:&quot;学习&quot;, &quot;start_offset&quot;:5, &quot;end_offset&quot;:7, &quot;type&quot;:&quot;CN_WORD&quot;, &quot;position&quot;:2 &#125;, &#123; &quot;token&quot;:&quot;es&quot;, &quot;start_offset&quot;:7, &quot;end_offset&quot;:9, &quot;type&quot;:&quot;ENGLISH&quot;, &quot;position&quot;:3 &#125; ]&#125; 查询语法 数据搜索 数据搜索返回字段详解 字段名 含义 _index 索引名称 _type 文档类型 _id 文档唯一标识 _score 每个文档评分，反映结果的相关度 _source 检索结果文档的实际内容 _source_excludes 返回结果中未包含的字段 _source_includes 返回结果中包含的字段 _version 文档版本号 _seq_no 分片中文档的序列号 _primary_term 分片的主要术语 fields 返回文档中指定字段的内容，也称为投影操作 highlight 查询语句与相关匹配片段的高亮显示 inner_hits 返回匹配嵌套对象的子查询结果 matched_queries 返回与当前匹配相关的查询名称 sort 返回结果排名的排序字段及排序方式 _explanation 返回与评分相关的解释信息 aggregations 返回搜索聚合结果 took 返回查询响应时间 timed_out 返回此次查询是否超时（true 或 false） suggest 返回自动纠正查询的建议 found 返回查询是否找到任何文档编号（true 或 false） num_reduce_phases 返回执行缩减阶段的模式数 shards 返回搜索操作的状态，包括成功和失败的碎片数量 max_score 返回匹配文档中最高评分 hits.total 返回query语句匹配的文档总数 hits.max_score 返回最高评分 hits.hits 返回检索结果列表 折叠搜索结果 Collapse 查询在 Elasticsearch 中用于在返回的结果中合并具有相同值的文档。具体而言，它允许您根据某个字段的唯一性来折叠文档，并只返回每个唯一值的第一个匹配的文档（和排序有关），类似数据库的 distinct 12345678910111213141516171819202122232425PUT /user&#123; &quot;mappings&quot;: &#123; &quot;properties&quot;: &#123; &quot;nage&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125;, &quot;age&quot;: &#123; &quot;type&quot;: &quot;integer&quot; &#125; &#125; &#125;&#125;POST /user/_bulk&#123; &quot;index&quot; : &#123; &quot;_index&quot; : &quot;user&quot;, &quot;_id&quot;: &quot;1&quot; &#125; &#125;&#123; &quot;user&quot;: &quot;小张&quot;, &quot;age&quot;: 15&#125;&#123; &quot;index&quot; : &#123; &quot;_index&quot; : &quot;user&quot;, &quot;_id&quot;: &quot;2&quot; &#125; &#125;&#123; &quot;user&quot;: &quot;小王&quot;, &quot;age&quot;: 15&#125;&#123; &quot;index&quot; : &#123; &quot;_index&quot; : &quot;user&quot;, &quot;_id&quot;: &quot;3&quot; &#125; &#125;&#123; &quot;user&quot;: &quot;小李&quot;, &quot;age&quot;: 14&#125;&#123; &quot;index&quot; : &#123; &quot;_index&quot; : &quot;user&quot;, &quot;_id&quot;: &quot;4&quot; &#125; &#125;&#123; &quot;user&quot;: &quot;小黄&quot;, &quot;age&quot;: 12&#125;&#123; &quot;index&quot; : &#123; &quot;_index&quot; : &quot;user&quot;, &quot;_id&quot;: &quot;5&quot; &#125; &#125;&#123; &quot;user&quot;: &quot;小张&quot;, &quot;age&quot;: 14&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475GET user/_search&#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;, &quot;collapse&quot;: &#123; &quot;field&quot;: &quot;age&quot; &#125;&#125;&#123; &quot;took&quot; : 396, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 5, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : null, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;user&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_score&quot; : 1.0, &quot;_source&quot; : &#123; &quot;user&quot; : &quot;小张&quot;, &quot;age&quot; : 15 &#125;, &quot;fields&quot; : &#123; &quot;age&quot; : [ 15 ] &#125; &#125;, &#123; &quot;_index&quot; : &quot;user&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;3&quot;, &quot;_score&quot; : 1.0, &quot;_source&quot; : &#123; &quot;user&quot; : &quot;小李&quot;, &quot;age&quot; : 14 &#125;, &quot;fields&quot; : &#123; &quot;age&quot; : [ 14 ] &#125; &#125;, &#123; &quot;_index&quot; : &quot;user&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;4&quot;, &quot;_score&quot; : 1.0, &quot;_source&quot; : &#123; &quot;user&quot; : &quot;小黄&quot;, &quot;age&quot; : 12 &#125;, &quot;fields&quot; : &#123; &quot;age&quot; : [ 12 ] &#125; &#125; ] &#125;&#125; 过滤搜索结果 post_filter 是一个可以用于过滤搜索结果的查询子句，它类似于 filter 子句，但不会影响搜索结果的相关性评分。与 filter 子句不同的是，post_filter 子句在查询完成后执行，所以它可以用于在搜索结果中进行筛选和过滤，而不会影响计算得分。这在需要对结果进行过滤、聚合、统计等操作时非常有用 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455GET /goods/_search&#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;, &quot;post_filter&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;:[ &#123; &quot;term&quot;: &#123; &quot;title&quot;: &quot;小米11&quot; &#125; &#125;, &#123; &quot;match&quot;: &#123; &quot;description&quot;: &quot;小米&quot; &#125; &#125; ] &#125; &#125;&#125;&#123; &quot;took&quot; : 71, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 1, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : 1.0, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;goods&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_score&quot; : 1.0, &quot;_source&quot; : &#123; &quot;title&quot; : &quot;小米11&quot;, &quot;price&quot; : 3699.0, &quot;created_at&quot; : &quot;2021-02-28T18:00:00&quot;, &quot;description&quot; : &quot;小米11，骁龙888，1亿像素。&quot; &#125; &#125; ] &#125;&#125; 关键词高亮 Highlighting是指在搜索结果中将查询关键字进行标记，以便让用户更加直观地看到搜索结果与查询关键字的匹配情况 参数名 类型 描述 fields object 需要高亮的字段和其设置。每个字段可以有不同的设置，比如定制标签和碎片大小等。 require_field_match boolean 指定是否所有字段都需要匹配，即只有包含查询中所有字段的文档才会被高亮显示。默认为 false。 pre_tags array of strings 指定高亮显示的前缀标签，默认为 [&quot;&lt;em&gt;&quot;]。 post_tags array of strings 指定高亮显示的后缀标签，默认为 [&quot;&lt;/em&gt;&quot;]。 fragment_size integer 指定高亮显示的碎片大小，即高亮文本的长度。默认为 100。 number_of_fragments integer 指定每个字段高亮的最大碎片数。默认为 5。 boundary_scanner string 指定边界扫描器，用于避免截断单词。可选值包括 chars、sentence和word。默认为 word。 boundary_scanner_locale string 指定边界扫描器的语言环境。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253GET /goods/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;description&quot;: &quot;小米&quot; &#125; &#125;, &quot;highlight&quot;: &#123; &quot;pre_tags&quot;: [&quot;&lt;span color=&#x27;red&#x27;&gt;&quot;], &quot;post_tags&quot;: [&quot;&lt;/span&gt;&quot;], &quot;require_field_match&quot;: &quot;false&quot;, &quot;fields&quot;: &#123; &quot;description&quot;: &#123;&#125; &#125; &#125;&#125;&#123; &quot;took&quot; : 229, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 1, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : 1.4131414, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;goods&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_score&quot; : 1.4131414, &quot;_source&quot; : &#123; &quot;title&quot; : &quot;小米11&quot;, &quot;price&quot; : 3699.0, &quot;created_at&quot; : &quot;2021-02-28T18:00:00&quot;, &quot;description&quot; : &quot;小米11，骁龙888，1亿像素。&quot; &#125;, &quot;highlight&quot; : &#123; &quot;description&quot; : [ &quot;&lt;span color=&#x27;red&#x27;&gt;小米&lt;/span&gt;11，骁龙888，1亿像素。&quot; ] &#125; &#125; ] &#125;&#125; 长时间查询 Elasticsearch 通常可以快速搜索大量数据。但有些情况下，搜索需要同时执行多个分片、可能针对冻结的索引和跨多个远程集群，结果不会在毫秒级别返回。当你需要执行长时间运行的搜索时，同步等待结果返回并不理想。相反，异步搜索允许你提交一个异步搜索请求，该请求将异步执行，你可以在稍后的阶段监视请求的进度并检索结果。你也可以在搜索完成之前，随着部分结果的可用性，检索部分结果。 你可以使用 submit async search API 提交异步搜索请求。 get async search API 允许你监视异步搜索请求的进度并检索其结果。可以通过 delete async search API 删除正在进行的异步搜索。 分页搜索 默认情况下，搜索将返回前10个匹配的结果。要浏览更多的结果，您可以使用搜索API的 from 和 size 参数。from 参数定义要跳过的数据，其默认值为 0。size 参数是要返回的每页条数。这两个参数一起定义了一个结果页面。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152GET /goods/_search&#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;, &quot;from&quot;: 2, &quot;size&quot;: 2&#125;&#123; &quot;took&quot; : 11, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 6, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : 1.0, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;goods&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;3&quot;, &quot;_score&quot; : 1.0, &quot;_source&quot; : &#123; &quot;title&quot; : &quot;iPhone 12&quot;, &quot;price&quot; : 7799.0, &quot;created_at&quot; : &quot;2021-02-27T10:00:00&quot;, &quot;description&quot; : &quot;iPhone 12，包邮！促销期间送黑色苹果创意耳机一副！&quot; &#125; &#125;, &#123; &quot;_index&quot; : &quot;goods&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;4&quot;, &quot;_score&quot; : 1.0, &quot;_source&quot; : &#123; &quot;title&quot; : &quot;一加 9T&quot;, &quot;price&quot; : 3199.0, &quot;created_at&quot; : &quot;2021-02-27T15:00:00&quot;, &quot;description&quot; : &quot;一加 9T，8GB+128GB，双模5G，骁龙865。&quot; &#125; &#125; ] &#125;&#125; 避免使用 from 和 size 进行过深的翻页或一次请求过多的结果。搜索请求通常跨越多个分片。每个分片必须将其请求的命中和任何前面页面的命中加载到内存中。对于深层页面或大量结果集，这些操作可能会显著增加内存和CPU使用率，导致性能下降或节点故障。 Search After 一般的分页需求我们可以使用form和size的方式实现，但是这种分页方式在深度分页的场景下应该是要避免使用的。深度分页会随着请求的页次增加，所消耗的内存和时间的增长也是成比例的增加，为了避免深度分页产生的问题，elasticsearch从2.0版本开始，增加了一个限制 1index.max_result_window =10000 建议使用Scroll api进行高效深度滚动，但滚动上下文代价很高，建议不要将其用于实时用户请求。该 search_after 参数通过提供实时游标来解决此问题 123456789101112131415GET /goods/_search&#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;, &quot;search_after&quot;:[3], // 上次查询结果最后一条id &quot;size&quot;: 20, &quot;sort&quot;: [ &#123; &quot;_id&quot;: &#123; &quot;order&quot;: &quot;asc&quot; // id排序 &#125; &#125; ]&#125; search_after 需要使用一个 唯一值 的字段作为排序字段，否则不能使用search_after方法 当我们使用 search_after 参数的时候，from参数必须被设置成 0 或 -1 （当然你也可以不设置这个from参数） 原理：根据上次的请求排序结果过滤掉已查询的数据 search_after不是自由跳转到随机页面而是并行滚动多个查询的解决方案。它与滚动API非常相似，但与它不同，search_after参数是无状态的，它始终针对最新版本的搜索器进行解析。因此，排序顺序可能会在运行期间发生变化，具体取决于索引的更新和删除 point in time 解决使用 search_after 期间索引数据变更而导致的查询数据顺序不一致可以使用 point in time (PIT)（创建一个时间点），它是一个轻量级视图，用于查看启动时存在的数据状态。在某些情况下，最好使用同一时间点执行多个搜索请求。例如，如果在 search_after 请求之间发生刷新，那么这些请求的结果可能不一致，因为搜索之间发生的更改仅对最近的时间点可见 12345POST /goods/_pit?keep_alive=5m&#123; &quot;id&quot; : &quot;y-ezAwEFZ29vZHMWYXNBVFdCNGFRaG1pcmJKZEdKZDI2UQAWV1RieHhkTFpUOG1iSkFIc3BsR1M4ZwAAAAAAAAB9oRZyNjEyNkFuelJOYVctM1J0V3o4OVBRAAEWYXNBVFdCNGFRaG1pcmJKZEdKZDI2UQAA&quot;&#125; 12345678910111213141516171819GET /_search&#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;, &quot;search_after&quot;:[3], &quot;size&quot;: 20, &quot;sort&quot;: [ &#123; &quot;_id&quot;: &#123; &quot;order&quot;: &quot;asc&quot; &#125; &#125; ], &quot;pit&quot;: &#123; &quot;id&quot;: &quot;y-ezAwEFZ29vZHMWYXNBVFdCNGFRaG1pcmJKZEdKZDI2UQAWV1RieHhkTFpUOG1iSkFIc3BsR1M4ZwAAAAAAAAB-RRZyNjEyNkFuelJOYVctM1J0V3o4OVBRAAEWYXNBVFdCNGFRaG1pcmJKZEdKZDI2UQAA&quot;, &quot;keep_alive&quot;: &quot;1m&quot; &#125;&#125; 查询不能带索引名称 Scroll 官方文档强调：不再建议使用scroll API进行深度分页 我们可以把scroll理解为关系型数据库里的 cursor（游标），因此，scroll 并不适合用来做实时搜索，而更适合用于后台批处理任务，比如群发 这个分页的用法，不是为了实时查询数据，而是为了一次性查询大量的数据（甚至是全部的数据） 因为这个scroll相当于维护了一份当前索引段的 快照信息，这个快照信息是你执行这个 scroll 查询时的快照。在这个查询后的任何新索引进来的数据，都不会在这个快照中查询到 但是它相对于from和size，不是查询所有数据然后剔除不要的部分，而是记录一个读取的位置，保证下一次快速继续读取 scroll 可以分为初始化和遍历两步，初始化时将所有符合搜索条件的搜索结果缓存起来（注意，这里只是缓存的doc_id，而并不是真的缓存了所有的文档数据，取数据是在fetch阶段完成的），可以想象成快照 在遍历时，从这个快照里取数据，也就是说，在初始化后，对索引插入、删除、更新数据都不会影响遍历结果 12345678910111213141516171819202122232425262728# 第一次查询，指定scroll缓存1分钟, size为2GET /goods/_search?from=0&amp;size=2&amp;scroll=1m&#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123; &#125; &#125;&#125;返回：&#123; &quot;_scroll_id&quot; : &quot;FGluY2x1ZGVfY29udGV4dF91dWlkDXF1ZXJ5QW5kRmV0Y2gBFllTclRQQS1JUkNXcXZEQVFGSVkxbGcAAAAAAAxpqBZXVGJ4eGRMWlQ4bWJKQUhzcGxHUzhn&quot;, &quot;took&quot; : 0, &quot;timed_out&quot; : false, ...&#125; # 第二次查询直接使用scroll_id进行查询，返回2条数据，因为size在第一次查询已经指定GET /_search/scroll&#123; &quot;scroll&quot;: &quot;1m&quot;, &quot;scroll_id&quot;: &quot;FGluY2x1ZGVfY29udGV4dF91dWlkDXF1ZXJ5QW5kRmV0Y2gBFllTclRQQS1JUkNXcXZEQVFGSVkxbGcAAAAAAAxqQRZXVGJ4eGRMWlQ4bWJKQUhzcGxHUzhn&quot;&#125;// 后续通过上一次查询的scroll_id接着向后查询， 重复这一步骤，直到返回的数据为空，即遍历完成// 设置scroll的时候，需要使搜索结果缓存到下一次遍历完成，同时，也不能太长，毕竟空间有限 优缺点 缺点： scroll_id会占用大量的资源（特别是排序的请求） 同样的，scroll后接超时时间，频繁的发起scroll请求，会出现一些列问题 是生成的历史快照，对于数据的变更不会反映到快照上 不能向前遍历，只能向后遍历 优点： ​ 适用于非实时处理大量数据的情况，比如要进行数据迁移或者索引变更之类的 Sliced Scroll 如果你数据量很大，用Scroll遍历数据那确实是接受不了，现在Scroll接口可以并发来进行数据遍历了 每个Scroll请求，可以分成多个Slice请求，可以理解为切片，各Slice独立并行，比用Scroll遍历要快很多倍 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768# 检查询分为2片，这是第一片，指定上下文再保存一分钟POST /sys_user/_search?scroll=1m&#123; &quot;size&quot;: 20, &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125;&#125;, &quot;slice&quot;: &#123; &quot;id&quot;: 0, &quot;max&quot;: 2 &#125;, &quot;sort&quot;: [ &#123; &quot;create_time&quot;: &#123; &quot;order&quot;: &quot;asc&quot; &#125; &#125; ]&#125;# 第一片第一次返回结果&#123; &quot;_scroll_id&quot; : &quot;FGluY2x1ZGVfY29udGV4dF91dWlkDnF1ZXJ5VGhlbkZldGNoAxZOVjBKMHZBUFM5LURCcEFsRzBMV21nAAAAAAAAdQEWMVl5TU90U29SMHlWdUVCRzNHT1dFdxZzNTNseXNlbFRoMm1Ycmh5Uno2ekdRAAAAAAAAntEWZEN4MnEyYTJUYVdvMXNPdzctVEVBdxZGNndJdGdtUVI2Qy1jeGdHNmhpR253AAAAAAAAK1QWeEhfYkNEbzhTUzZoX25EdkhpZ1pQQQ==&quot;, &quot;took&quot; : 2, &quot;timed_out&quot; : false,...&#125; # 检查询分为2片，这是第二片，指定上下文再保存一分钟POST /sys_user/_search?scroll=1m&#123; &quot;size&quot;: 20, &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125;&#125;, &quot;slice&quot;: &#123; &quot;id&quot;: 1, &quot;max&quot;: 2 &#125;, &quot;sort&quot;: [ &#123; &quot;create_time&quot;: &#123; &quot;order&quot;: &quot;asc&quot; &#125; &#125; ]&#125;# 第二片第一次返回结果&#123; &quot;_scroll_id&quot; : &quot;FGluY2x1ZGVfY29udGV4dF91dWlkDnF1ZXJ5VGhlbkZldGNoAxZOVjBKMHZBUFM5LURCcEFsRzBMV21nAAAAAAAAdcgWMVl5TU90U29SMHlWdUVCRzNHT1dFdxZzNTNseXNlbFRoMm1Ycmh5Uno2ekdRAAAAAAAAn5AWZEN4MnEyYTJUYVdvMXNPdzctVEVBdxZGNndJdGdtUVI2Qy1jeGdHNmhpR253AAAAAAAAK40WeEhfYkNEbzhTUzZoX25EdkhpZ1pQQQ==&quot;, &quot;took&quot; : 203,...&#125; # 第一片第二次分页查询，使用第一次的scroll_idPOST /_search/scroll&#123; &quot;scroll&quot;:&quot;1m&quot;, &quot;scroll_id&quot; : &quot;FGluY2x1ZGVfY29udGV4dF91dWlkDnF1ZXJ5VGhlbkZldGNoAxZOVjBKMHZBUFM5LURCcEFsRzBMV21nAAAAAAAAdQEWMVl5TU90U29SMHlWdUVCRzNHT1dFdxZzNTNseXNlbFRoMm1Ycmh5Uno2ekdRAAAAAAAAntEWZEN4MnEyYTJUYVdvMXNPdzctVEVBdxZGNndJdGdtUVI2Qy1jeGdHNmhpR253AAAAAAAAK1QWeEhfYkNEbzhTUzZoX25EdkhpZ1pQQQ==&quot; &#125;# 第二片第二次分页查询，使用第一次的scroll_idPOST /_search/scroll&#123; &quot;scroll&quot;:&quot;1m&quot;, &quot;scroll_id&quot; : &quot;FGluY2x1ZGVfY29udGV4dF91dWlkDnF1ZXJ5VGhlbkZldGNoAxZOVjBKMHZBUFM5LURCcEFsRzBMV21nAAAAAAAAdcgWMVl5TU90U29SMHlWdUVCRzNHT1dFdxZzNTNseXNlbFRoMm1Ycmh5Uno2ekdRAAAAAAAAn5AWZEN4MnEyYTJUYVdvMXNPdzctVEVBdxZGNndJdGdtUVI2Qy1jeGdHNmhpR253AAAAAAAAK40WeEhfYkNEbzhTUzZoX25EdkhpZ1pQQQ==&quot; &#125; 上边的示例可以单独请求两块数据，最终两块数据合并的结果与直接scroll scan相同 其中max是分块数，id是第几块，可以使用在多线程环境下 官方文档中建议 max 的值不要超过 shard 的数量，否则可能会导致内存爆炸 指定返回字段 _source 关键字: 是一个数组，在数组中用来指定展示那些字段，和mysql查询指定返回哪些字段一样 1234567891011121314151617181920212223242526272829303132333435363738GET /goods/_search&#123; &quot;query&quot;: &#123; &quot;ids&quot;: &#123; &quot;values&quot;: [1] &#125; &#125;, &quot;_source&quot;: [&quot;title&quot;]&#125;&#123; &quot;took&quot; : 2, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 1, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : 1.0, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;goods&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_score&quot; : 1.0, &quot;_source&quot; : &#123; &quot;title&quot; : &quot;小米11&quot; &#125; &#125; ] &#125;&#125; 过滤字段 includes：来指定想要显示的字段 excludes：来指定不想要显示的字段 123456789101112GET /goods/_search&#123; &quot;query&quot;: &#123; &quot;ids&quot;: &#123; &quot;values&quot;: [1] &#125; &#125;, &quot;_source&quot;: &#123; &quot;includes&quot;: [&quot;title&quot;], &quot;excludes&quot;: [&quot;description&quot;] &#125;&#125; 排序 注意 ：排序干扰了ES的查询，所有自定义排序查询是没有打分的 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273GET /goods/_search&#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;, &quot;sort&quot;: [ &#123; &quot;created_at&quot;: &#123; &quot;order&quot;: &quot;asc&quot;, &quot;format&quot;: &quot;yyyy-MM-dd HH-mm-ss&quot; &#125; &#125;, &#123; &quot;title&quot;: &#123; &quot;order&quot;: &quot;desc&quot; &#125; &#125; ], &quot;size&quot;: 2&#125;&#123; &quot;took&quot; : 6, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 6, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : null, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;goods&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;6&quot;, &quot;_score&quot; : null, &quot;_source&quot; : &#123; &quot;title&quot; : &quot;戴尔灵越 5406&quot;, &quot;price&quot; : 8299.0, &quot;created_at&quot; : &quot;2021-02-26T22:00:00&quot;, &quot;description&quot; : &quot;戴尔灵越 5406，酷睿i7-1165G7，16G+1TB+MX330独显，14英寸触控笔记本。&quot; &#125;, &quot;sort&quot; : [ &quot;2021-02-26 22-00-00&quot;, &quot;戴尔灵越 5406&quot; ] &#125;, &#123; &quot;_index&quot; : &quot;goods&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;3&quot;, &quot;_score&quot; : null, &quot;_source&quot; : &#123; &quot;title&quot; : &quot;iPhone 12&quot;, &quot;price&quot; : 7799.0, &quot;created_at&quot; : &quot;2021-02-27T10:00:00&quot;, &quot;description&quot; : &quot;iPhone 12，包邮！促销期间送黑色苹果创意耳机一副！&quot; &#125;, &quot;sort&quot; : [ &quot;2021-02-27 10-00-00&quot;, &quot;iPhone 12&quot; ] &#125; ] &#125;&#125; DSL查询 ES中提供了一种强大的检索数据方式，这种检索方式称之为 Query DSL （领域特定语言），Query DSL 是利用 Rest API 传递 JSON 格式的请求体（Request Body）数据与ES进行交互，这种方式的丰富查询语法让ES检索变得更强大，更简洁 语法 整个ES查询 1GET /_search &#123;json格式请求体数据&#125; 指定索引查询 12GET /&lt;index&gt;/_doc/_search &#123;json格式请求体数据&#125;GET /&lt;index&gt;/_search &#123;json格式请求体数据&#125; 测试数据 search_analyzer 123456789101112131415161718192021222324252627282930313233343536373839404142# 创建映射PUT /tehero_index&#123; &quot;settings&quot;:&#123; &quot;index&quot;:&#123; &quot;number_of_shards&quot;:1, &quot;number_of_replicas&quot;:1 &#125; &#125;, &quot;mappings&quot;:&#123; &quot;properties&quot;:&#123; &quot;id&quot;:&#123; &quot;type&quot;:&quot;integer&quot; &#125;, &quot;content&quot;:&#123; &quot;type&quot;:&quot;text&quot;, &quot;analyzer&quot;: &quot;ik_smart&quot; &#125;, &quot;name&quot;:&#123; &quot;type&quot;:&quot;text&quot;, &quot;analyzer&quot;: &quot;ik_smart&quot; &#125;, &quot;tag&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125;, &quot;createAt&quot;:&#123; &quot;type&quot;:&quot;date&quot; &#125; &#125; &#125;&#125;# 导入测试数据POST _bulk&#123; &quot;index&quot; : &#123; &quot;_index&quot; : &quot;tehero_index&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot; &#125; &#125;&#123; &quot;id&quot; : 1,&quot;content&quot;:&quot;关注我,系统学编程&quot;,&quot;name&quot;: &quot;java 编程思想&quot;,&quot;tag&quot;: &quot;java&quot; &#125;&#123; &quot;index&quot; : &#123; &quot;_index&quot; : &quot;tehero_index&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;2&quot; &#125; &#125;&#123; &quot;id&quot; : 2,&quot;content&quot;:&quot;系统学编程,关注我&quot;,&quot;name&quot;: &quot;MySql 入门到精通&quot;,&quot;tag&quot;: &quot;sql&quot; &#125;&#123; &quot;index&quot; : &#123; &quot;_index&quot; : &quot;tehero_index&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;3&quot; &#125; &#125;&#123; &quot;id&quot; : 3,&quot;content&quot;:&quot;系统编程,关注我&quot;,&quot;name&quot;: &quot;java 虚拟机&quot;,&quot;tag&quot;: &quot;java&quot;&#125;&#123; &quot;index&quot; : &#123; &quot;_index&quot; : &quot;tehero_index&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;4&quot; &#125; &#125;&#123; &quot;id&quot; : 4,&quot;content&quot;:&quot;关注我,间隔系统学编程&quot;,&quot;name&quot;: &quot;设计模式&quot;,&quot;tag&quot;: &quot;design&quot; &#125; 查询和过滤上下文 相关性分数 默认情况下，Elasticsearch 按相关性分数对匹配的搜索结果进行排序，相关性分数衡量每个文档与查询的匹配程度。 相关性分数是一个正浮点数，在 搜索_scoreAPI的元数据字段中返回。越高 ，文档越相关。虽然每种查询类型可以不同地计算相关性分数，但分数计算还取决于查询子句是在查询上下文中运行还是在过滤器上下文中运行。_score 查询上下文 在查询上下文中，查询子句回答了“该文档与该查询子句的匹配程度如何？” 除了决定文档是否匹配外，查询子句还计算 _score元数据字段中的相关性分数。 query 只要将查询子句传递给参数（例如搜索queryAPI中的参数） ，查询上下文就会生效。 过滤上下文 在过滤器上下文中，查询子句回答“该文档是否与该查询子句匹配？” 答案是简单的是或否——不计算分数。过滤上下文主要用于过滤结构化数据，例如 这是否timestamp属于 2015 年至 2016 年的范围？ 字段status 设置为&quot;published&quot;？ Elasticsearch 会自动缓存经常使用的过滤器，以加快性能。 每当将查询子句传递给filter 参数（例如查询中的filter或must_not参数 bool、查询filter中的参数 constant_score或 filter聚合）时，过滤器上下文就会生效。 查询和过滤上下文示例 下面是在 API 的查询和过滤上下文中使用的查询子句的示例search。此查询将匹配满足以下所有条件的文档： 该title字段包含单词search。 该content字段包含单词elasticsearch。 该status字段包含确切的词published。 该publish_date字段包含从 2015 年 1 月 1 日开始的日期。 123456789101112131415GET /_search&#123; &quot;query&quot;: &#123; ① &quot;bool&quot;: &#123; ② &quot;must&quot;: [ &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;Search&quot; &#125;&#125;, ③ &#123; &quot;match&quot;: &#123; &quot;content&quot;: &quot;Elasticsearch&quot; &#125;&#125; ④ ], &quot;filter&quot;: [ ⑤ &#123; &quot;term&quot;: &#123; &quot;status&quot;: &quot;published&quot; &#125;&#125;, ⑥ &#123; &quot;range&quot;: &#123; &quot;publish_date&quot;: &#123; &quot;gte&quot;: &quot;2015-01-01&quot; &#125;&#125;&#125; ⑦ ] &#125; &#125;&#125; ① query 参数表示查询上下文。 ②③④ 在查询上下文中使用 bool 和两个匹配子句，这意味着它们用于评估每个文档匹配的程度。 ⑤ filter 参数指示过滤器上下文。 ⑥⑦ term 和 range 子句在过滤器上下文中使用。 他们将过滤掉不匹配的文档，但不会影响匹配文档的分数。 复合查询 复合查询将其他复合查询或叶子查询进行包装，组合它们的结果和分数，以此改变它们的行为，或从查询切换到 过滤(filter) 上下文模式 Boolean 布尔查询 布尔查询是最常用的组合查询，不仅将多个查询条件组合在一起，并且将查询的结果和结果的评分组合在一起 子句 描述 must (&amp;&amp;) 该字句类型的查询语句，文档必须满足，并对评分产生影响（相关度） filter 子句(查询)必须出现在匹配的文档中。然而，与must不同的是，查询的分数将被忽略过滤器子句在过滤器上下文中执行，子句被考虑用于缓存 should (||) 应该匹配；如果没有must和filter，多个should只需要至少一个匹配即可，该数据可以通过参数minimum_should_match控制，如果包含了must或filter，则should不参与实际过滤，但会参与评分 must_not (!) 查询条件取反，及匹配到的文档必须不符合must_not的条件 must 在运算符中相当于 &amp;&amp; 判断，要多个条件必须同时成立 123456789101112131415161718192021GET /tehero_index/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123; &quot;match&quot;: &#123; &quot;name&quot;: &quot;mysql&quot; &#125; &#125;, &#123; &quot;term&quot;: &#123; &quot;tag&quot;: &#123; &quot;value&quot;: &quot;sql&quot; &#125; &#125; &#125; ] &#125; &#125;&#125; 要求文档 name 字段分词词条满足 mysql，并且 tag 字段精准查询满足 sql 返回 12345678910111213141516171819202122232425262728293031&#123; &quot;took&quot; : 3, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 1, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : 2.2191694, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;tehero_index&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;2&quot;, &quot;_score&quot; : 2.2191694, &quot;_source&quot; : &#123; &quot;id&quot; : 2, &quot;content&quot; : &quot;系统学编程,关注我&quot;, &quot;name&quot; : &quot;MySql 入门到精通&quot;, &quot;tag&quot; : &quot;sql&quot; &#125; &#125; ] &#125;&#125; should 相当于 || 运算， 多个条件成立一个就可以 123456789101112131415161718192021GET /tehero_index/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;should&quot;: [ &#123; &quot;match&quot;: &#123; &quot;name&quot;: &quot;mysql&quot; &#125; &#125;, &#123; &quot;term&quot;: &#123; &quot;tag&quot;: &#123; &quot;value&quot;: &quot;design&quot; &#125; &#125; &#125; ] &#125; &#125;&#125; 要求文档 name 字段分词词条满足 mysql 或 tag 字段精准查询满足 design 返回 12345678910111213141516171819202122232425262728293031323334353637383940414243&#123; &quot;took&quot; : 0, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 2, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : 1.2039728, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;tehero_index&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;4&quot;, &quot;_score&quot; : 1.2039728, &quot;_source&quot; : &#123; &quot;id&quot; : 4, &quot;content&quot; : &quot;关注我,间隔系统学编程&quot;, &quot;name&quot; : &quot;设计模式&quot;, &quot;tag&quot; : &quot;design&quot; &#125; &#125;, &#123; &quot;_index&quot; : &quot;tehero_index&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;2&quot;, &quot;_score&quot; : 1.0151966, &quot;_source&quot; : &#123; &quot;id&quot; : 2, &quot;content&quot; : &quot;系统学编程,关注我&quot;, &quot;name&quot; : &quot;MySql 入门到精通&quot;, &quot;tag&quot; : &quot;sql&quot; &#125; &#125; ] &#125;&#125; must_not 表示条件取反 12345678910111213141516171819202122GET /tehero_index/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must_not&quot;: [ &#123; &quot;term&quot;: &#123; &quot;tag&quot;: &#123; &quot;value&quot;: &quot;design&quot; &#125; &#125; &#125;,&#123; &quot;term&quot;: &#123; &quot;tag&quot;: &#123; &quot;value&quot;: &quot;sql&quot; &#125; &#125; &#125; ] &#125; &#125;&#125; tag 精确查询不等于 design 和 sql 返回 12345678910111213141516171819202122232425262728293031323334353637383940414243&#123; &quot;took&quot; : 10, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 2, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : 0.0, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;tehero_index&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_score&quot; : 0.0, &quot;_source&quot; : &#123; &quot;id&quot; : 1, &quot;content&quot; : &quot;关注我,系统学编程&quot;, &quot;name&quot; : &quot;java 编程思想&quot;, &quot;tag&quot; : &quot;java&quot; &#125; &#125;, &#123; &quot;_index&quot; : &quot;tehero_index&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;3&quot;, &quot;_score&quot; : 0.0, &quot;_source&quot; : &#123; &quot;id&quot; : 3, &quot;content&quot; : &quot;系统编程,关注我&quot;, &quot;name&quot; : &quot;java 虚拟机&quot;, &quot;tag&quot; : &quot;java&quot; &#125; &#125; ] &#125;&#125; filter 过滤，条件必须出现在匹配的文档中，filter是对查询结果进行二次过滤，因此不会影响打分 123456789101112131415161718192021GET /tehero_index/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123; &quot;match&quot;: &#123; &quot;content&quot;: &quot;关注我&quot; &#125; &#125; ], &quot;filter&quot;: [ &#123; &quot;term&quot;: &#123; &quot;tag&quot;: &quot;sql&quot; &#125; &#125; ] &#125; &#125;&#125; 先用 关注我 进行分词全文搜索 然后在结果集中过滤需要的数据 返回 12345678910111213141516171819202122232425262728293031&#123; &quot;took&quot; : 1, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 1, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : 0.2159169, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;tehero_index&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;2&quot;, &quot;_score&quot; : 0.2159169, &quot;_source&quot; : &#123; &quot;id&quot; : 2, &quot;content&quot; : &quot;系统学编程,关注我&quot;, &quot;name&quot; : &quot;MySql 入门到精通&quot;, &quot;tag&quot; : &quot;sql&quot; &#125; &#125; ] &#125;&#125; boosting 提升查询 返回查询匹配的文档，同时控制匹配文档的相关性得分，通过 boosting 查询就可人为影响搜索结果的相关性评分 子句 描述 positive（积极的，加分） 只有匹配上 positive 的查询的内容，才会被放到返回的结果集中 negative（消极的，减分） 如果匹配上 positive 并且也匹配上了 negative，就可以降低这样的文档score negative_boost 指定系数，[0, 1.0]，用于降低与 negative 查询匹配的文档的相关性得分 123456789101112131415161718GET /tehero_index/_search&#123; &quot;query&quot;: &#123; &quot;boosting&quot;: &#123; &quot;positive&quot;: &#123; &quot;match&quot;: &#123; &quot;name&quot;: &quot;java&quot; &#125; &#125;, &quot;negative&quot;: &#123; &quot;match&quot;: &#123; &quot;name&quot;: &quot;编程&quot; &#125; &#125;, &quot;negative_boost&quot;: 0.5 &#125; &#125;&#125; boosting 查询 positive、negative、negative_boost 是必须的 上面的查询一定要匹配 java 词条，如果文本中出现 编程 则降低相关文档的得分，得分可以由 negative_boost 系数控制，系数越小，分值越低 12345678910111213141516171819202122232425262728293031323334353637383940414243&#123; &quot;took&quot; : 1, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 2, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : 0.7801935, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;tehero_index&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;3&quot;, &quot;_score&quot; : 0.7801935, &quot;_source&quot; : &#123; &quot;id&quot; : 3, &quot;content&quot; : &quot;系统编程,关注我&quot;, &quot;name&quot; : &quot;java 虚拟机&quot;, &quot;tag&quot; : &quot;java&quot; &#125; &#125;, &#123; &quot;_index&quot; : &quot;tehero_index&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_score&quot; : 0.33414665, &quot;_source&quot; : &#123; &quot;id&quot; : 1, &quot;content&quot; : &quot;关注我,系统学编程&quot;, &quot;name&quot; : &quot;java 编程思想&quot;, &quot;tag&quot; : &quot;java&quot; &#125; &#125; ] &#125;&#125; constant_score 恒分查询 内部包装了过滤查询，故而不会计算相似度得分，该查询返回的相似度分与字段上指定boost参数值相同，一般用于不需要使用相似度得分排序的查询场景 12345678910111213GET /tehero_index/_search&#123; &quot;query&quot;: &#123; &quot;constant_score&quot;: &#123; &quot;filter&quot;: &#123; &quot;term&quot;: &#123; &quot;name&quot;: &quot;java&quot; &#125; &#125;, &quot;boost&quot;: 1.2 &#125; &#125;&#125; 使用 java 词条进行分词等值过滤查询，指定统一返回相关性得分 1.2 12345678910111213141516171819202122232425262728293031323334353637383940414243&#123; &quot;took&quot; : 6, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 2, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : 1.2, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;tehero_index&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_score&quot; : 1.2, &quot;_source&quot; : &#123; &quot;id&quot; : 1, &quot;content&quot; : &quot;关注我,系统学编程&quot;, &quot;name&quot; : &quot;java 编程思想&quot;, &quot;tag&quot; : &quot;java&quot; &#125; &#125;, &#123; &quot;_index&quot; : &quot;tehero_index&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;3&quot;, &quot;_score&quot; : 1.2, &quot;_source&quot; : &#123; &quot;id&quot; : 3, &quot;content&quot; : &quot;系统编程,关注我&quot;, &quot;name&quot; : &quot;java 虚拟机&quot;, &quot;tag&quot; : &quot;java&quot; &#125; &#125; ] &#125;&#125; dis_max 最大分离查询 子句 描述 queries 必须,数组对象,包含一个或多个查询子句,返回的文档必须匹配一个或多个查询条件,匹配的条件越多则分数越高 tie_breaker 可选,浮点值,参数介于0与1.0之间,用于增加匹配条件文档额外的分,默认为0.0 返回的文档必须要满足多个查询子句中的一项条件 若一个文档能匹配多个查询子句时，则 dis_max 查询将为能匹配上查询子句条件的项增加额外分，这能打破在多个文档都匹配某一个或多个条件时分数相同的情况 1234567891011121314151617181920GET /tehero_index/_search&#123; &quot;query&quot;: &#123; &quot;dis_max&quot;: &#123; &quot;tie_breaker&quot;: 0.7, &quot;queries&quot;: [ &#123; &quot;match&quot;: &#123; &quot;content&quot;: &quot;编程&quot; &#125; &#125;, &#123; &quot;match&quot;: &#123; &quot;name&quot;: &quot;编程&quot; &#125; &#125; ] &#125; &#125;&#125; 当文档满足的子查询越多，得分越高，可进行多字段多维度全文检索 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667&#123; &quot;took&quot; : 5, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 4, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : 1.2363734, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;tehero_index&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_score&quot; : 1.2363734, &quot;_source&quot; : &#123; &quot;id&quot; : 1, &quot;content&quot; : &quot;关注我,系统学编程&quot;, &quot;name&quot; : &quot;java 编程思想&quot;, &quot;tag&quot; : &quot;java&quot; &#125; &#125;, &#123; &quot;_index&quot; : &quot;tehero_index&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;2&quot;, &quot;_score&quot; : 0.10795845, &quot;_source&quot; : &#123; &quot;id&quot; : 2, &quot;content&quot; : &quot;系统学编程,关注我&quot;, &quot;name&quot; : &quot;MySql 入门到精通&quot;, &quot;tag&quot; : &quot;sql&quot; &#125; &#125;, &#123; &quot;_index&quot; : &quot;tehero_index&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;3&quot;, &quot;_score&quot; : 0.10795845, &quot;_source&quot; : &#123; &quot;id&quot; : 3, &quot;content&quot; : &quot;系统编程,关注我&quot;, &quot;name&quot; : &quot;java 虚拟机&quot;, &quot;tag&quot; : &quot;java&quot; &#125; &#125;, &#123; &quot;_index&quot; : &quot;tehero_index&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;4&quot;, &quot;_score&quot; : 0.09826641, &quot;_source&quot; : &#123; &quot;id&quot; : 4, &quot;content&quot; : &quot;关注我,间隔系统学编程&quot;, &quot;name&quot; : &quot;设计模式&quot;, &quot;tag&quot; : &quot;design&quot; &#125; &#125; ] &#125;&#125; function_score 自定义打分 old_score，ES默认打分 加强_score，自定义打分 score_mode：每个 function 的分数合并方式（加强_score） boost_mode：old_score 和 加强_score 的合并方式 决定 functions 里面的加强 score 怎么合并，会先合并加强 score 成一个 总加强_score ，再使用 总加强_score 去和 old_score 做合并，换言之就是 先执行score_mode，再执行boost_mode 序号 参数 描述 1 multiply 计算分数相乘(默认模式) score_mode: 加强score1 * 加强score2boost_mode: new_score = old_score * 总加强_score 2 sum 计算分数求和 3 avg 计算分数求平均 4 first 文档在函数中首个匹配的过滤器计算的分数 5 max 计算分数最大作为分数 6 min 计算分数最小最为分数 参数 max_boost 限制加强函数的最大效果，就是限制加强 score 最大能多少，但是不会限制 old_score 参数 boost 用于提升查询，和 weight 差不多 function_score 查询是用来控制评分过程的终极武器，它允许为每个与主查询匹配的文档应用一个函数，以达到改变甚至完全替换原始查询评分 _score 的目的 要使用function_score，需要定义一个或多个函数，这些函数为查询返回的文档计算一个新的分数 function_score 查询提供了多种类型的评分函数： script_score 脚本分数 weight 权重 random_score 随机分数 field_value_factor 字段值因子 function decay 衰减函数 高斯，线性，exp 12345678910111213141516171819202122232425262728GET /tehero_index/_search&#123; &quot;query&quot;: &#123; &quot;function_score&quot;: &#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;content&quot;: &quot;编程&quot; &#125; &#125;, &quot;functions&quot;: [ &#123; &quot;filter&quot;: &#123; &quot;match&quot;: &#123;&quot;content&quot;: &quot;编程&quot;&#125; &#125;, &quot;weight&quot;: 10 &#125;, &#123; &quot;filter&quot;: &#123; &quot;match&quot;: &#123;&quot;name&quot;: &quot;编程&quot;&#125; &#125;, &quot;weight&quot;: 5 &#125; ], &quot;score_mode&quot;: &quot;sum&quot;, &quot;boost_mode&quot;: &quot;sum&quot; &#125; &#125;&#125; 定义了一个匹配查询，使用 content 字段分词匹配 编程 自定义两个权重函数，第一个过滤匹配 content 字段，权重为10。第二个过滤匹配 name 字段，权重为5，最终将加强score使用加法合并，最后使用加强score 再和 ES的old_score 做加法合并。常用于平台的搜索引擎 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667&#123; &quot;took&quot; : 6, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 4, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : 15.107959, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;tehero_index&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_score&quot; : 15.107959, &quot;_source&quot; : &#123; &quot;id&quot; : 1, &quot;content&quot; : &quot;关注我,系统学编程&quot;, &quot;name&quot; : &quot;java 编程思想&quot;, &quot;tag&quot; : &quot;java&quot; &#125; &#125;, &#123; &quot;_index&quot; : &quot;tehero_index&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;2&quot;, &quot;_score&quot; : 10.107959, &quot;_source&quot; : &#123; &quot;id&quot; : 2, &quot;content&quot; : &quot;系统学编程,关注我&quot;, &quot;name&quot; : &quot;MySql 入门到精通&quot;, &quot;tag&quot; : &quot;sql&quot; &#125; &#125;, &#123; &quot;_index&quot; : &quot;tehero_index&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;3&quot;, &quot;_score&quot; : 10.107959, &quot;_source&quot; : &#123; &quot;id&quot; : 3, &quot;content&quot; : &quot;系统编程,关注我&quot;, &quot;name&quot; : &quot;java 虚拟机&quot;, &quot;tag&quot; : &quot;java&quot; &#125; &#125;, &#123; &quot;_index&quot; : &quot;tehero_index&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;4&quot;, &quot;_score&quot; : 10.098267, &quot;_source&quot; : &#123; &quot;id&quot; : 4, &quot;content&quot; : &quot;关注我,间隔系统学编程&quot;, &quot;name&quot; : &quot;设计模式&quot;, &quot;tag&quot; : &quot;design&quot; &#125; &#125; ] &#125;&#125; 全文搜索 全文查询使您能够搜索经过分析的文本字段（搜索进行分词处理），例如电子邮件正文。使用在索引期间应用于字段的相同分析器处理查询字符串 Intervals 间隔查询 间隔查询是一种按照 搜索词顺序 搜索文档的方法，比如在对一个邮件正文内容进行全文搜索时，要求正文先出现 深圳 再出现 交通情况 顺序的文章 下面的查询是按照搜索词 关注、编程 顺序在 content 中查询满足搜索词顺序的文本 12345678910111213141516171819202122232425262728293031POST /tehero_index/_search&#123; &quot;query&quot;:&#123; &quot;intervals&quot;:&#123; &quot;content&quot;:&#123; &quot;all_of&quot;:&#123; &quot;ordered&quot;:true, &quot;intervals&quot;:[ &#123; &quot;match&quot;:&#123; &quot;query&quot;:&quot;关注&quot;, &quot;ordered&quot;:true &#125; &#125;, &#123; &quot;any_of&quot;:&#123; &quot;intervals&quot;:[ &#123; &quot;match&quot;:&#123; &quot;query&quot;:&quot;编程&quot; &#125; &#125; ] &#125; &#125; ] &#125; &#125; &#125; &#125;&#125; 响应 123456789101112131415161718192021222324252627282930313233343536373839&#123; &quot;took&quot; : 2, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 2, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : 0.16666669, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;tehero_index&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_score&quot; : 0.16666669, &quot;_source&quot; : &#123; &quot;id&quot; : 1, &quot;content&quot; : &quot;关注我,系统学编程&quot; &#125; &#125;, &#123; &quot;_index&quot; : &quot;tehero_index&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;4&quot;, &quot;_score&quot; : 0.12500006, &quot;_source&quot; : &#123; &quot;id&quot; : 4, &quot;content&quot; : &quot;关注我,间隔系统学编程&quot; &#125; &#125; ] &#125;&#125; match 匹配查询 返回与提供的文本、数字、日期或布尔值匹配的文档。在匹配之前分析提供的文本 先根据查询条件进行分词，这里分为 系统、编程 再根据分词后的条件与字典进行匹配 12345678GET /tehero_index/_search&#123; &quot;query&quot;:&#123; &quot;match&quot;:&#123; &quot;content&quot;: &quot;系统编程&quot; &#125; &#125;&#125; 响应 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859&#123; &quot;took&quot; : 2, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 4, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : 1.3416183, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;tehero_index&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;3&quot;, &quot;_score&quot; : 1.3416183, &quot;_source&quot; : &#123; &quot;id&quot; : 3, &quot;content&quot; : &quot;系统编程,关注我&quot; &#125; &#125;, &#123; &quot;_index&quot; : &quot;tehero_index&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_score&quot; : 0.10795845, &quot;_source&quot; : &#123; &quot;id&quot; : 1, &quot;content&quot; : &quot;关注我,系统学编程&quot; &#125; &#125;, &#123; &quot;_index&quot; : &quot;tehero_index&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;2&quot;, &quot;_score&quot; : 0.10795845, &quot;_source&quot; : &#123; &quot;id&quot; : 2, &quot;content&quot; : &quot;系统学编程,关注我&quot; &#125; &#125;, &#123; &quot;_index&quot; : &quot;tehero_index&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;4&quot;, &quot;_score&quot; : 0.09826641, &quot;_source&quot; : &#123; &quot;id&quot; : 4, &quot;content&quot; : &quot;关注我,间隔系统学编程&quot; &#125; &#125; ] &#125;&#125; match_all 全部匹配 最简单的查询，它匹配所有文档，给它们所有的 _score 1.0 123456789101112131415# DSL 查询GET /products/_doc/_search&#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;&#125;# 简化写法GET /products/_search&#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;&#125; 响应 12345678910111213141516171819202122232425262728293031323334353637383940414243&#123; &quot;took&quot; : 2, // 响应耗时 &quot;timed_out&quot; : false, // 是否超时 &quot;_shards&quot; : &#123; // 分片信息 &quot;total&quot; : 1, // 搜索的分片总数 &quot;successful&quot; : 1, // 搜索成功的分片数量 &quot;skipped&quot; : 0, // 没有搜索的分片，跳过的分片 &quot;failed&quot; : 0 // 搜索失败的分片数量 &#125;, &quot;hits&quot; : &#123; // 搜索结果集。项目中，我们需要的一切数据都是从hits中获取 &quot;total&quot; : &#123; // 返回多少条数据 &quot;value&quot; : 2, // 数量 &quot;relation&quot; : &quot;eq&quot; // eq时为准确计数，当为gte时，value是不准确计数 &#125;, &quot;max_score&quot; : 1.0, // 返回结果中，最大的匹配度分值 &quot;hits&quot; : [ // 默认查询前十条数据，根据分值降序排序 &#123; &quot;_index&quot; : &quot;products&quot;, // 索引名称 &quot;_type&quot; : &quot;_doc&quot;, // 类型名称 &quot;_id&quot; : &quot;_J3PgIAB_jzT9lCXUHcj&quot;, // 该条数据的id &quot;_score&quot; : 1.0, // 关键字与该条数据的匹配度分值 &quot;_source&quot; : &#123; // 索引库中类型，返回结果字段，不指定的话，默认全部显示出来 &quot;title&quot; : &quot;iphone12&quot;, &quot;price&quot; : 5999, &quot;create_date&quot; : &quot;2022-05-02&quot;, &quot;description&quot; : &quot;八折销售&quot; &#125; &#125;, &#123; &quot;_index&quot; : &quot;products&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;2&quot;, &quot;_score&quot; : 1.0, &quot;_source&quot; : &#123; &quot;title&quot; : &quot;pihone13 256g&quot;, &quot;price&quot; : 7999, &quot;create_date&quot; : &quot;2022-05-02&quot;, &quot;description&quot; : &quot;新上市&quot; &#125; &#125; ] &#125;&#125; Match None Query 不匹配查询，和 match_all 相反，不匹配任何文档 123456GET /_search&#123; &quot;query&quot;: &#123; &quot;match_none&quot;: &#123;&#125; &#125;&#125; multi_match 多字段匹配 注意: 字段类型分词，将查询条件分词之后进行查询该字段 ，如果该字段不分词就会将查询条件作为整体进行查询 text：会将 query 条件进行分词后查询 keyword：整体等值查询 这个查询是ES使用最多的查询 123456789GET /tehero_index/_search&#123; &quot;query&quot;: &#123; &quot;multi_match&quot;: &#123; &quot;query&quot;: &quot;虚拟机&quot;, &quot;fields&quot;: [&quot;content&quot;, &quot;name&quot;] &#125; &#125;&#125; 响应 123456789101112131415161718192021222324252627282930&#123; &quot;took&quot; : 7, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 1, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : 3.8613348, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;tehero_index&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;3&quot;, &quot;_score&quot; : 3.8613348, &quot;_source&quot; : &#123; &quot;id&quot; : 3, &quot;content&quot; : &quot;系统编程,关注我&quot;, &quot;name&quot; : &quot;java 虚拟机&quot; &#125; &#125; ] &#125;&#125; match_phrase 短语查询 match_phrase会分词 目标文档需要包含分词后的所有词条 目标文档还要保持这些词的相对顺序和查询分词的顺序一致 12345678GET /tehero_index/_search&#123; &quot;query&quot;: &#123; &quot;match_phrase&quot;: &#123; &quot;content&quot;: &quot;关注我系统学&quot; &#125; &#125;&#125; 分词为 关注、我、系统学 123456789101112131415161718192021222324252627282930&#123; &quot;took&quot; : 6, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 1, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : 0.5813866, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;tehero_index&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_score&quot; : 0.5813866, &quot;_source&quot; : &#123; &quot;id&quot; : 1, &quot;content&quot; : &quot;关注我,系统学编程&quot;, &quot;name&quot; : &quot;java 编程思想&quot; &#125; &#125; ] &#125;&#125; match_phrase_prefix 短语前缀匹配 与 match_phrase 类似，但是会对最后一个 Token 在倒排索引做通配符搜索 12345678GET /tehero_index/_search&#123; &quot;query&quot;: &#123; &quot;match_phrase_prefix&quot;: &#123; &quot;content&quot;: &quot;系统学编&quot; &#125; &#125;&#125; 分词为 系统学、编，然后 编 做左前缀匹配 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152&#123; &quot;took&quot; : 3, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 3, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : 0.47342813, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;tehero_index&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_score&quot; : 0.47342813, &quot;_source&quot; : &#123; &quot;id&quot; : 1, &quot;content&quot; : &quot;关注我,系统学编程&quot;, &quot;name&quot; : &quot;java 编程思想&quot; &#125; &#125;, &#123; &quot;_index&quot; : &quot;tehero_index&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;2&quot;, &quot;_score&quot; : 0.47342813, &quot;_source&quot; : &#123; &quot;id&quot; : 2, &quot;content&quot; : &quot;系统学编程,关注我&quot;, &quot;name&quot; : &quot;MySql 入门到精通&quot; &#125; &#125;, &#123; &quot;_index&quot; : &quot;tehero_index&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;4&quot;, &quot;_score&quot; : 0.43092585, &quot;_source&quot; : &#123; &quot;id&quot; : 4, &quot;content&quot; : &quot;关注我,间隔系统学编程&quot;, &quot;name&quot; : &quot;设计模式&quot; &#125; &#125; ] &#125;&#125; match_bool_prefix 布尔前缀匹配 match_bool_prefix 查询内部将输入文本通过指定分词器处理为多个词条，然后基于这些个词条进行bool query，除了最后一个term使用前缀查询其它都是term query 12345678GET /tehero_index/_search&#123; &quot;query&quot;: &#123; &quot;match_bool_prefix&quot; : &#123; &quot;name&quot; : &quot;思想 mysql&quot; &#125; &#125;&#125; 第一个文本分词为 思想 使用等值查询，因为类型是Text并且指定IK分词，因此使用词条进行等值查询能够查询到 java 编程思想 第二个使用前缀查询为 mysql 等价于 1234567891011121314151617181920212223GET /tehero_index/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;should&quot;: [ &#123; &quot;term&quot;: &#123; &quot;name&quot;: &#123; &quot;value&quot;: &quot;思想&quot; &#125; &#125; &#125;, &#123; &quot;prefix&quot;: &#123; &quot;name&quot;: &#123; &quot;value&quot;: &quot;mysql&quot; &#125; &#125; &#125; ] &#125; &#125;&#125; 响应 12345678910111213141516171819202122232425262728293031323334353637383940414243&#123; &quot;took&quot; : 7, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 2, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : 1.1608025, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;tehero_index&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_score&quot; : 1.1608025, &quot;_source&quot; : &#123; &quot;id&quot; : 1, &quot;content&quot; : &quot;关注我,系统学编程&quot;, &quot;name&quot; : &quot;java 编程思想&quot;, &quot;tag&quot; : &quot;java&quot; &#125; &#125;, &#123; &quot;_index&quot; : &quot;tehero_index&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;2&quot;, &quot;_score&quot; : 1.0, &quot;_source&quot; : &#123; &quot;id&quot; : 2, &quot;content&quot; : &quot;系统学编程,关注我&quot;, &quot;name&quot; : &quot;MySql 入门到精通&quot;, &quot;tag&quot; : &quot;sql&quot; &#125; &#125; ] &#125;&#125; combined_fields 组合字段匹配 这个感觉和 multi_match 多字段匹配 一样，其实它们有很大的差别 multi_match：根据查询文本分词，然后分别单独到多个字段中进行匹配 combined_fields：根据查询文本分词，然后将多个字段组合起来，就好像它们的内容已被索引到一个组合字段中一样（字符串视图），再进行词条匹配后再使用 BM25 评分 123456789GET /tehero_index/_search&#123; &quot;query&quot;: &#123; &quot;combined_fields&quot;: &#123; &quot;query&quot;: &quot;关注java&quot;, &quot;fields&quot;: [&quot;content&quot;, &quot;name&quot;] &#125; &#125;&#125; 分词为 关注、java 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667&#123; &quot;took&quot; : 45, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 4, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : 1.1070688, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;tehero_index&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;3&quot;, &quot;_score&quot; : 1.1070688, &quot;_source&quot; : &#123; &quot;id&quot; : 3, &quot;content&quot; : &quot;系统编程,关注我&quot;, &quot;name&quot; : &quot;java 虚拟机&quot;, &quot;tag&quot; : &quot;java&quot; &#125; &#125;, &#123; &quot;_index&quot; : &quot;tehero_index&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_score&quot; : 1.0323758, &quot;_source&quot; : &#123; &quot;id&quot; : 1, &quot;content&quot; : &quot;关注我,系统学编程&quot;, &quot;name&quot; : &quot;java 编程思想&quot;, &quot;tag&quot; : &quot;java&quot; &#125; &#125;, &#123; &quot;_index&quot; : &quot;tehero_index&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;2&quot;, &quot;_score&quot; : 0.12776, &quot;_source&quot; : &#123; &quot;id&quot; : 2, &quot;content&quot; : &quot;系统学编程,关注我&quot;, &quot;name&quot; : &quot;MySql 入门到精通&quot;, &quot;tag&quot; : &quot;sql&quot; &#125; &#125;, &#123; &quot;_index&quot; : &quot;tehero_index&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;4&quot;, &quot;_score&quot; : 0.11930529, &quot;_source&quot; : &#123; &quot;id&quot; : 4, &quot;content&quot; : &quot;关注我,间隔系统学编程&quot;, &quot;name&quot; : &quot;设计模式&quot;, &quot;tag&quot; : &quot;design&quot; &#125; &#125; ] &#125;&#125; query_string 字符串查询 需要配合 查询字符串语法 使用 123456789GET /tehero_index/_search&#123; &quot;query&quot;: &#123; &quot;query_string&quot;: &#123; &quot;default_field&quot;: &quot;name&quot;, &quot;query&quot;: &quot;(虚拟机) OR (mysql)&quot; &#125; &#125;&#125; 分词 虚拟机、mysql 进行全文查询，只要满足其中一个条件的文档均可，如果换成 AND 则无匹配 返回 12345678910111213141516171819202122232425262728293031323334353637383940414243&#123; &quot;took&quot; : 4, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 2, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : 1.3551693, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;tehero_index&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;3&quot;, &quot;_score&quot; : 1.3551693, &quot;_source&quot; : &#123; &quot;id&quot; : 3, &quot;content&quot; : &quot;系统编程,关注我&quot;, &quot;name&quot; : &quot;java 虚拟机&quot;, &quot;tag&quot; : &quot;java&quot; &#125; &#125;, &#123; &quot;_index&quot; : &quot;tehero_index&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;2&quot;, &quot;_score&quot; : 1.0151966, &quot;_source&quot; : &#123; &quot;id&quot; : 2, &quot;content&quot; : &quot;系统学编程,关注我&quot;, &quot;name&quot; : &quot;MySql 入门到精通&quot;, &quot;tag&quot; : &quot;sql&quot; &#125; &#125; ] &#125;&#125; simple_query_string 简单字符串查询 和 query_string 严格的语法检查相比，simple_query_string 查询不会返回无效语法的错误。相反，它会忽略查询字符串的任何无效部分 12345678910GET /tehero_index/_search&#123; &quot;query&quot;: &#123; &quot;simple_query_string&quot;: &#123; &quot;query&quot;: &quot;虚拟机\\&quot; mysql&quot;, &quot;fields&quot;: [&quot;name&quot;], &quot;default_operator&quot;: &quot;OR&quot; &#125; &#125;&#125; \\&quot; 在 query_string 中会报语法错误，而这里会略 返回 12345678910111213141516171819202122232425262728293031323334353637383940414243&#123; &quot;took&quot; : 2, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 2, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : 1.3551693, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;tehero_index&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;3&quot;, &quot;_score&quot; : 1.3551693, &quot;_source&quot; : &#123; &quot;id&quot; : 3, &quot;content&quot; : &quot;系统编程,关注我&quot;, &quot;name&quot; : &quot;java 虚拟机&quot;, &quot;tag&quot; : &quot;java&quot; &#125; &#125;, &#123; &quot;_index&quot; : &quot;tehero_index&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;2&quot;, &quot;_score&quot; : 1.0151966, &quot;_source&quot; : &#123; &quot;id&quot; : 2, &quot;content&quot; : &quot;系统学编程,关注我&quot;, &quot;name&quot; : &quot;MySql 入门到精通&quot;, &quot;tag&quot; : &quot;sql&quot; &#125; &#125; ] &#125;&#125; 地理位置查询 Elasticsearch支持两种类型的地理数据： geo_point 类型支持成对的纬度/经度 geo_shape 类型支持点、线、圆、多边形、多个多边形等 在这组查询中： geo_shape查询 查找要么相交，包含的，要么指定形状不相交的地理形状的文档 geo_bounding_box查询 查找落入指定的矩形地理点的文档 geo_distance查询 查找在一个中心点指定范围内的地理点文档 geo_polygon查询 查找指定多边形内地理点的文档 形状查询 与 geo_shape 一样，Elasticsearch 支持索引任意二维（非地理空间）几何图形的能力，从而可以绘制出虚拟世界、体育场馆、主题公园和 CAD 图。 Elasticsearch 支持两种类型的笛卡尔数据： point支持 x/y 对的字段，以及 shape支持点、线、圆、多边形、多边形等的字段。 该组中的查询是： shape查询 查找包含以下内容的文档：shapes与指定形状相交、包含在其中、在指定形状内或不与指定形状相交 连接查询 在像 ElasticSearch 这样的分布式系统中执行全 SQL 风格的连接查询代价昂贵，是不可行的。相应地，为了实现水平规模地扩展，ElasticSearch 提供了两种形式的 join nested query (嵌套查询) 文档中可能包含嵌套类型的字段，这些字段用来索引一些数组对象，每个对象都可以作为一条独立的文档被查询出来(用嵌套查询) has_child (有子查询) 和 has_parent (有父查询) 一类父子关系可以存在单个的索引的两个类型的文档之间。has_child 查询将返回其子文档能满足特定的查询的父文档，而 has_parent 则返回其父文档能满足特定查询的子文档 参考 term 查询中的terms-lookup mechanism ，它允许你在另一个文档的值中创建一个term 查询 nested 嵌套查询 包装另一个查询以搜索 嵌套字段 嵌套查询搜索 嵌套字段 对象，就好像它们被索引为单独的文档一样。如果对象与搜索匹配，则嵌套查询返回父文档 子句 描述 path（必须） 要搜索的嵌套对象当前层级的全路径 query（必须） 希望在路径层级的嵌套对象上运行的查询，查询字段也要是层级的，比如：user.name score_mode（可选） 得分模式，指示匹配子对象的分数如何影响根父文档的相关性分数（多个嵌套查询）avg（默认），使用所有匹配子对象的平均相关性分数max，使用所有匹配子对象的最高相关性分数min，使用所有匹配子对象的最低相关性分数none，不要使用匹配子对象的相关分数。该查询为父文档分配 0 分sum，将所有匹配子对象的相关性分数加在一起 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152PUT /t_user&#123; &quot;settings&quot;:&#123; &quot;number_of_shards&quot;:1, &quot;number_of_replicas&quot;:0 &#125;, &quot;mappings&quot;:&#123; &quot;properties&quot;:&#123; &quot;name&quot;:&#123; &quot;type&quot;:&quot;keyword&quot; &#125;, &quot;age&quot;:&#123; &quot;type&quot;:&quot;short&quot; &#125;, &quot;address_list&quot;:&#123; &quot;type&quot;: &quot;nested&quot; &#125; &#125; &#125;&#125;POST /t_user/_doc/1&#123; &quot;name&quot;:&quot;test&quot;, &quot;age&quot;:18, &quot;address_list&quot;:[ &#123; &quot;phone&quot;:&quot;123456&quot;, &quot;address&quot;:&quot;shenzhen&quot; &#125;, &#123; &quot;phone&quot;:&quot;654321&quot;, &quot;address&quot;:&quot;guangzhou&quot; &#125; ]&#125;GET /t_user/_search&#123; &quot;query&quot;: &#123; &quot;nested&quot;: &#123; &quot;path&quot;: &quot;address_list&quot;, &quot;query&quot;: &#123; &quot;term&quot;: &#123; &quot;address_list.address&quot;: &#123; &quot;value&quot;: &quot;guangzhou&quot; &#125; &#125; &#125; &#125; &#125;&#125; 使用 nested 进行嵌套对象查询，path 指定嵌套的路径，允许多层嵌套查询，嵌套查询的字段名称必须是 全嵌套字段名 返回 123456789101112131415161718192021222324252627282930313233343536373839&#123; &quot;took&quot; : 4, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 1, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : 0.6931471, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;t_user&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_score&quot; : 0.6931471, &quot;_source&quot; : &#123; &quot;name&quot; : &quot;test&quot;, &quot;age&quot; : 18, &quot;address_list&quot; : [ &#123; &quot;phone&quot; : &quot;123456&quot;, &quot;address&quot; : &quot;shenzhen&quot; &#125;, &#123; &quot;phone&quot; : &quot;654321&quot;, &quot;address&quot; : &quot;guangzhou&quot; &#125; ] &#125; &#125; ] &#125;&#125; has_child 子文档查询 返回其 Join 字段类型的子文档与提供的查询匹配的父文档。您可以使用 Join 字段类型映射在同一索引中的文档之间创建父子关系 不建议使用多层次关系来复制关系模型。每个级别的关系都会在查询时增加内存和计算方面的开销。为了获得更好的搜索性能，请改为对数据进行非规范化的 嵌套对象 或 对象 has_parent 父文档查询 返回其 Join 字段类型的子文档与提供的查询匹配的父文档。您可以使用 Join 字段类型映射在同一索引中的文档之间创建父子关系 因为它执行连接，所以 has_parent 查询比其他查询慢。随着匹配的父文档数量的增加，它的性能会下降。搜索中的每个 has_parent 查询都会显着增加查询时间，不推荐使用 Join 类型 parent_id 父文档ID搜索 适用于 Join 字段类型的子文档，不推荐适用 跨度查询 Span查询（跨度查询）是低级位置查询，可以提供对指定条款的顺序和接近度的专家控制。这些通常用于对法律文件或专利执行非常具体的查询 通过该语句用户可以精准控制搜索词的先后顺序，以及多个搜索词在文档中的前后距离 子句 描述 span_term 等同于term 查询，但与其他Span查询一起使用 span_multi_term 包含term, range, prefix, wildcard, regexp 或者 fuzzy 查询 span_first 接受另一个Span查询，其匹配必须出现在字段的前N个位置 span_near 接受多个Span查询，其匹配必须在彼此的指定距离内，并且可能以相同的顺序 span_or 组合多个Span查询 - 返回与任何指定查询匹配的文档 span_not 包装另一个Span查询，并排除与该查询匹配的任何文档 span_containing 接受Span查询的列表，但仅返回也匹配第二个Span查询的跨度 span_within 单个Span查询的结果将返回，只要其跨度位于由其他Span查询列表返回的范围内 field_masking_span 允许跨越不同字段span-near 或者 span-or 的查询 专业查询 该组包含不适合其他组的查询 子句 描述 distance_feature 距离特征查询，它根据一个 原点 和文档的date, date_nanos, geo_point字段之间动态计算的距离来计算分数 more_like_this 近似文本查询，可以查询和提供文本类似的文档，通常用于近似文本的推荐场景 percolate 反向查询，字段类型 percolate 存储了一个查询语句，通过数据反向去匹配查询语句常用的场景是 监控 rank_feature 特征排序查询，用于 rank_feature 或 rank_features 字段类型，通常被放到boolean query中的&lt;bshould子句中用来提升文档score，需要注意的是这种查询的性能要高于function score script 脚本查询，此查询允许使用一段脚本充当过滤器，使用脚本会导致搜索速度变慢 script_score 允许使用脚本修改子查询分数的查询 wrapper 包装器查询，接受一段Base64 编码字符串的JSON查询串，一般在java客户端调用ES（字符串拼接查询）时使用 pinned 固定查询，将所选文档提升到比匹配给定查询的文档更高的排名。此功能通常用于引导搜索者找到精选文档（比如搜索引擎的广告，将广告的文档id做为选定文档） 术语级查询 术语级查询就是根据结构化数据中的 精确值 查找文档 子句 描述 exists 存在查询，指定某个文档的某个字段，如果该值不是 null，则认为该文档符合查询条件 fuzzy 模糊查询，会查询出近似于查询条件的结果，近似程度通过 Levenshtein edit distance 编辑距离来判断，1 或 2 个距离内的词才会被认为是近似的 ids id查询，根据多个文档id返回文档 prefix 前缀查询，根据提供的字段中包含特定前缀的文档 range 范围查询，返回包含给定范围内的文档 regexp 正则表达式查询，指定一个正则表达式，查询出指定字段的值符合这个则表达式的文档 term 精准查询，主要对 keywords 做精准查询 terms 多值精准查询，类似于 in terms_set 术语集查询，用于数组类型做多值查询，并使用数字类型字段辅助查询返回过滤 wildcard 通配符查询，使用 ? 和 * 进行通配查询 添加数据 1234567891011121314151617181920212223242526272829PUT /tehero_index/_mapping&#123; &quot;properties&quot;: &#123; &quot;author&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;index&quot;: true &#125;, &quot;price&quot;: &#123; &quot;type&quot;: &quot;float&quot;, &quot;index&quot;: true &#125;, &quot;alias&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;index&quot;: true &#125;, &quot;match&quot;: &#123; &quot;type&quot;: &quot;long&quot;, &quot;index&quot;: true &#125; &#125;&#125;POST _bulk&#123; &quot;index&quot; : &#123; &quot;_index&quot; : &quot;tehero_index&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;5&quot; &#125; &#125;&#123; &quot;id&quot; : 5,&quot;content&quot;:&quot;es,存在查询&quot;,&quot;author&quot;:&quot;吴耿锋&quot;&#125;&#123; &quot;index&quot; : &#123; &quot;_index&quot; : &quot;tehero_index&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;6&quot; &#125; &#125;&#123; &quot;id&quot; : 6,&quot;tag&quot;: &quot;范围查询1&quot;,&quot;price&quot;:19.9,&quot;alias&quot;:[&quot;range&quot;, &quot;range1&quot;],&quot;match&quot;:1&#125;&#123; &quot;index&quot; : &#123; &quot;_index&quot; : &quot;tehero_index&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;7&quot; &#125; &#125;&#123; &quot;id&quot; : 7,&quot;tag&quot;: &quot;范围查询2&quot;,&quot;price&quot;:29.9,&quot;alias&quot;:[&quot;range&quot;, &quot;range2&quot;],&quot;match&quot;:2&#125; exists 存在查询 返回包含字段索引值的文档 以下原因文档不被检索： 源 JSON 中的字段为 null 或 [] 该字段在映射中设置了 “index” : false 字段值的长度超过映射中的 ignore_above 设置 字段值格式错误，在映射中定义了 ignore_malformed 12345678GET /tehero_index/_search&#123; &quot;query&quot;: &#123; &quot;exists&quot;: &#123; &quot;field&quot;: &quot;author&quot; &#125; &#125;&#125; 只检索 author 字段不为空的文档 1234567891011121314151617181920212223242526272829&#123; &quot;took&quot; : 12, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 1, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : 1.0, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;tehero_index&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;5&quot;, &quot;_score&quot; : 1.0, &quot;_source&quot; : &#123; &quot;content&quot; : &quot;es,存在查询&quot;, &quot;author&quot; : &quot;wgf&quot; &#125; &#125; ] &#125;&#125; fuzzy 模糊查询 模糊查询会查询出近似于查询条件的结果，近似程度通过 Levenshtein edit distance 编辑距离来判断，1 或 2 个距离内的词才会被认为是近似的 注意: fuzzy 模糊查询 最大模糊错误必须在0-2之间 搜索关键词长度为 2 不允许存在模糊 搜索关键词长度为3-5 允许一次模糊 搜索关键词长度大于5 允许最大2模糊 12345678910GET /tehero_index/_search&#123; &quot;query&quot;: &#123; &quot;fuzzy&quot;: &#123; &quot;tag&quot;: &#123; &quot;value&quot;: &quot;ja&quot; &#125; &#125; &#125;&#125; 搜索不到文档，因为搜索词长度为2，不允许模糊 但是可以用过 fuzziness 修改规则 12345678910GET /tehero_index/_search&#123; &quot;query&quot;: &#123; &quot;fuzzy&quot;: &#123; &quot;tag&quot;: &#123; &quot;value&quot;: &quot;jav&quot; &#125; &#125; &#125;&#125; jav 长度为3，因此允许一次模糊，能够搜索到 tag = java 的文档 12345678910111213141516171819202122232425262728293031323334353637383940414243&#123; &quot;took&quot; : 5, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 2, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : 0.46209806, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;tehero_index&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_score&quot; : 0.46209806, &quot;_source&quot; : &#123; &quot;id&quot; : 1, &quot;content&quot; : &quot;关注我,系统学编程&quot;, &quot;name&quot; : &quot;java 编程思想&quot;, &quot;tag&quot; : &quot;java&quot; &#125; &#125;, &#123; &quot;_index&quot; : &quot;tehero_index&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;3&quot;, &quot;_score&quot; : 0.46209806, &quot;_source&quot; : &#123; &quot;id&quot; : 3, &quot;content&quot; : &quot;系统编程,关注我&quot;, &quot;name&quot; : &quot;java 虚拟机&quot;, &quot;tag&quot; : &quot;java&quot; &#125; &#125; ] &#125;&#125; ids id查询 根据其 ID 返回文档。此查询使用存储在_id字段中的文档 ID 12345678GET /tehero_index/_search&#123; &quot;query&quot;: &#123; &quot;ids&quot;: &#123; &quot;values&quot;: [1,2] &#125; &#125;&#125; 类似数据库的 in(ids) 查询 12345678910111213141516171819202122232425262728293031323334353637383940414243&#123; &quot;took&quot; : 1, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 2, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : 1.0, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;tehero_index&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_score&quot; : 1.0, &quot;_source&quot; : &#123; &quot;id&quot; : 1, &quot;content&quot; : &quot;关注我,系统学编程&quot;, &quot;name&quot; : &quot;java 编程思想&quot;, &quot;tag&quot; : &quot;java&quot; &#125; &#125;, &#123; &quot;_index&quot; : &quot;tehero_index&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;2&quot;, &quot;_score&quot; : 1.0, &quot;_source&quot; : &#123; &quot;id&quot; : 2, &quot;content&quot; : &quot;系统学编程,关注我&quot;, &quot;name&quot; : &quot;MySql 入门到精通&quot;, &quot;tag&quot; : &quot;sql&quot; &#125; &#125; ] &#125;&#125; prefix 前缀查询 keyword 就完全按照字符串前缀查询 text 类型需要考虑分词，是根据分词的前缀进行查询，比如中文的前缀只能一个汉字（默认分词器） 123456789101112131415161718192021GET /tehero_index/_search&#123; &quot;query&quot;: &#123; &quot;prefix&quot;: &#123; &quot;content&quot;: &#123; &quot;value&quot;: &quot;关注我&quot; &#125; &#125; &#125;&#125;GET /tehero_index/_search&#123; &quot;query&quot;: &#123; &quot;prefix&quot;: &#123; &quot;author&quot;: &#123; &quot;value&quot;: &quot;吴耿&quot; &#125; &#125; &#125;&#125; 第一个查询没有结果，因为 关注我 不是一个分词，所以不能作为前缀查询 第二个查询有查询结果，因为不分词，因此使用 吴耿 作为前缀查询 1234567891011121314151617181920212223242526272829&#123; &quot;took&quot; : 7, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 1, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : 1.0, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;tehero_index&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;5&quot;, &quot;_score&quot; : 1.0, &quot;_source&quot; : &#123; &quot;content&quot; : &quot;es,存在查询&quot;, &quot;author&quot; : &quot;吴耿锋&quot; &#125; &#125; ] &#125;&#125; range 范围查询 指定一个范围，如果某个文档的某个字段的值刚好在这个范围之中，则认为该文档符合查询条件 1234567891011GET /tehero_index/_search&#123; &quot;query&quot;: &#123; &quot;range&quot;: &#123; &quot;price&quot;: &#123; &quot;gte&quot;: 19.9, &quot;lte&quot;: 29.9 &#125; &#125; &#125;&#125; 返回 1234567891011121314151617181920212223242526272829303132333435363738394041&#123; &quot;took&quot; : 821, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 2, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : 1.0, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;tehero_index&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;6&quot;, &quot;_score&quot; : 1.0, &quot;_source&quot; : &#123; &quot;id&quot; : 6, &quot;tag&quot; : &quot;范围查询1&quot;, &quot;price&quot; : 19.9 &#125; &#125;, &#123; &quot;_index&quot; : &quot;tehero_index&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;7&quot;, &quot;_score&quot; : 1.0, &quot;_source&quot; : &#123; &quot;id&quot; : 7, &quot;tag&quot; : &quot;范围查询2&quot;, &quot;price&quot; : 29.9 &#125; &#125; ] &#125;&#125; regexp 正则表达式查询 指定一个正则表达式，查询出指定字段的值符合这个则表达式的文档 12345678GET /tehero_index/_search&#123; &quot;query&quot;: &#123; &quot;regexp&quot;: &#123; &quot;tag&quot;: &quot;des.*&quot; &#125; &#125;&#125; tag 字段使用正则表达式匹配 12345678910111213141516171819202122232425262728293031&#123; &quot;took&quot; : 19, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 1, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : 1.0, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;tehero_index&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;4&quot;, &quot;_score&quot; : 1.0, &quot;_source&quot; : &#123; &quot;id&quot; : 4, &quot;content&quot; : &quot;关注我,间隔系统学编程&quot;, &quot;name&quot; : &quot;设计模式&quot;, &quot;tag&quot; : &quot;design&quot; &#125; &#125; ] &#125;&#125; term 精准查询 text: 默认使用分词精准查询，要避免对 text 类型进行精准查询 keyword: 不分词查询 12345678910GET /tehero_index/_search&#123; &quot;query&quot;: &#123; &quot;term&quot;: &#123; &quot;author&quot;: &#123; &quot;value&quot;: &quot;吴耿锋&quot; &#125; &#125; &#125;&#125; 对 price 字段进行等值查询 12345678910GET /tehero_index/_search&#123; &quot;query&quot;: &#123; &quot;term&quot;: &#123; &quot;price&quot;: &#123; &quot;value&quot;: 19.9 &#125; &#125; &#125;&#125; terms 多值精准查询 和 term 查询差不多，只不过允许多个值进行匹配查询，类似数据库中的 in 1234567891011GET /tehero_index/_search&#123; &quot;query&quot;: &#123; &quot;terms&quot;: &#123; &quot;price&quot;: [ 19.9, 29.9 ] &#125; &#125;&#125; 查询价格为 19.9 和 29.9 的文档 1234567891011121314151617181920212223242526272829303132333435363738394041&#123; &quot;took&quot; : 36, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 2, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : 1.0, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;tehero_index&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;6&quot;, &quot;_score&quot; : 1.0, &quot;_source&quot; : &#123; &quot;id&quot; : 6, &quot;tag&quot; : &quot;范围查询1&quot;, &quot;price&quot; : 19.9 &#125; &#125;, &#123; &quot;_index&quot; : &quot;tehero_index&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;7&quot;, &quot;_score&quot; : 1.0, &quot;_source&quot; : &#123; &quot;id&quot; : 7, &quot;tag&quot; : &quot;范围查询2&quot;, &quot;price&quot; : 29.9 &#125; &#125; ] &#125;&#125; terms_set 术语集查询 针对数组类型字段的术语查询，相当于是根据两个数据集交集查询，会返回交集元素的个数 minimum_should_match_field：指向文档的数字（numeric）字段名称，其值应用于控制交集元素个数，以便返回文档 minimum_should_match_script：一个自定义脚本，用于确定为了返回文档而必须匹配的最少术语数 1234567891011GET /tehero_index/_search&#123; &quot;query&quot;: &#123; &quot;terms_set&quot;: &#123; &quot;alias&quot;: &#123; &quot;terms&quot;: [&quot;range&quot;,&quot;range2&quot;], &quot;minimum_should_match_field&quot;: &quot;match&quot; &#125; &#125; &#125;&#125; 指定了 match 作为 terms 这个数据集 和 alias 这个数据集的交集最少元素个数 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051&#123; &quot;took&quot; : 841, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 2, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : 3.0131938, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;tehero_index&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;7&quot;, &quot;_score&quot; : 3.0131938, &quot;_source&quot; : &#123; &quot;id&quot; : 7, &quot;tag&quot; : &quot;范围查询2&quot;, &quot;price&quot; : 29.9, &quot;alias&quot; : [ &quot;range&quot;, &quot;range2&quot; ], &quot;match&quot; : 2 &#125; &#125;, &#123; &quot;_index&quot; : &quot;tehero_index&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;6&quot;, &quot;_score&quot; : 0.8405091, &quot;_source&quot; : &#123; &quot;id&quot; : 6, &quot;tag&quot; : &quot;范围查询1&quot;, &quot;price&quot; : 19.9, &quot;alias&quot; : [ &quot;range&quot;, &quot;range1&quot; ], &quot;match&quot; : 1 &#125; &#125; ] &#125;&#125; id 为 7 的文档 alias 集合和 [“range”,“range2”] 的交集数大于等于指定数字类型字段的 match（2），条件满足 id 为 6 的文档 alias 集合和 [“range”,“range2”] 的交集数大于等于指定数字类型字段的 match（1），条件满足 通过数据字段辅助查询 wildcard 通配符查询 text类型需要分词前缀，比如中文的通配只能一个汉字（默认分词器），避免用在 text 类型上 ?：用来匹配一个任意字符 *：用来匹配多个任意字符 123456789101112131415161718192021GET /tehero_index/_search&#123; &quot;query&quot;: &#123; &quot;wildcard&quot;: &#123; &quot;author&quot;: &#123; &quot;value&quot;: &quot;吴*锋&quot; &#125; &#125; &#125;&#125;GET /tehero_index/_search&#123; &quot;query&quot;: &#123; &quot;wildcard&quot;: &#123; &quot;author&quot;: &#123; &quot;value&quot;: &quot;吴*&quot; &#125; &#125; &#125;&#125; 返回 123456789101112131415161718192021222324252627282930&#123; &quot;took&quot; : 10, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 1, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : 1.0, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;tehero_index&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;5&quot;, &quot;_score&quot; : 1.0, &quot;_source&quot; : &#123; &quot;id&quot; : 5, &quot;content&quot; : &quot;es,存在查询&quot;, &quot;author&quot; : &quot;吴耿锋&quot; &#125; &#125; ] &#125;&#125; minimum_should_match 最小匹配度查询 用在 bool 查询的 should 子句后面，用于控制查询精度 用法参考 rewrite 查询重写机制 Elasticsearch 在内部使用 Apache Lucene 来支持索引和搜索。但是在ES底层的 Lucene 无法执行以下查询： fuzzy prefix query_string regexp wildcard 为了执行它们，Lucene 将这些查询更改为更简单的形式，例如 bool 查询或 bit 位查询 用法 正则表达式语法 Elasticsearch 在以下查询中支持正则表达式： regexp query_string Filter Query 过滤查询 Filter Query 和 Query 是两种不同的查询方式，虽然它们在查询语法和使用方式上很相似，但它们的目的和影响是不同的 Filter Query 主要用于 过滤数据，它会根据指定的条件来筛选文档，只返回满足条件的文档，不涉及 打分 和 排序，而且它可以缓存文档，因此它的执行速度相对较快。Filter 适合在大范围筛选数据，以达到更精确的查询结果 Query 则是用于查询数据的，它会根据指定的条件来匹配文档，并计算每个文档的 相关度得分（也称为匹配度），然后按照得分从高到低排序返回结果。 Query 则更适合用于全文搜索、多条件匹配等需要计算得分和排序的场景 在电商领域搜索商品的时候，关键词是分词查询（query），价格区间、品牌等是通过过滤（filter）筛选的 注意：filte query必须配合 布尔查询 一起使用 常用子句 描述 term 精确匹配某个字段的值 terms 匹配某个字段的多个值 range 匹配某个范围内的值 exists 匹配某个字段存在的文档 prefix 匹配某个字段前缀的文档 wildcard 匹配某个字段通配符表达式的文档 regexp 匹配某个字段正则表达式的文档 区别 区别 Filter Query 执行顺序 先执行，过滤文档 后执行，计算相关度得分 匹配方式 精确匹配 模糊匹配 是否影响文档得分 否 是 是否支持排序 否 是 是否支持缓存 是 否 查询速度 较快 较慢 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253GET /goods/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123; &quot;match&quot;: &#123; &quot;description&quot;: &quot;小米&quot; &#125; &#125;, &#123; &quot;term&quot;: &#123; &quot;title&quot;: &#123; &quot;value&quot;: &quot;小米11&quot; &#125; &#125; &#125; ] &#125; &#125;&#125;&#123; &quot;took&quot; : 6, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 1, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : 2.7994356, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;goods&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_score&quot; : 2.7994356, &quot;_source&quot; : &#123; &quot;title&quot; : &quot;小米11&quot;, &quot;price&quot; : 3699.0, &quot;created_at&quot; : &quot;2021-02-28T18:00:00&quot;, &quot;description&quot; : &quot;小米11，骁龙888，1亿像素。&quot; &#125; &#125; ] &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253GET /goods/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123; &quot;match&quot;: &#123; &quot;description&quot;: &quot;小米&quot; &#125; &#125; ], &quot;filter&quot;: [ &#123; &quot;term&quot;: &#123; &quot;title&quot;: &quot;小米11&quot; &#125; &#125; ] &#125; &#125;&#125;&#123; &quot;took&quot; : 7, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 1, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : 1.4131414, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;goods&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_score&quot; : 1.4131414, &quot;_source&quot; : &#123; &quot;title&quot; : &quot;小米11&quot;, &quot;price&quot; : 3699.0, &quot;created_at&quot; : &quot;2021-02-28T18:00:00&quot;, &quot;description&quot; : &quot;小米11，骁龙888，1亿像素。&quot; &#125; &#125; ] &#125;&#125; 这两个查询在查询结果上没有区别，都是查询 “goods” 索引中标题为 “小米11” 描述包含 “小米” 的文档。但是，两者在查询过程中使用了不同的查询方式，具体的区别如下： 第一个查询使用的是 Bool Query，将 “match” 和 “term” 查询条件组合起来，并计算相关度得分，因此查询结果按照相关度得分排序（第一个查询比第二个查询的相关得分高的原因）。如果需要精确匹配，可以将查询条件转换为 Bool Filter 第二个查询中，使用了 “must” 和 “filter” 条件，其中 “must” 条件和第一个查询相同，都是筛选出所有描述中包含 “小米” 的商品，但是 “title” 字段的条件使用了 “filter” 过滤器，这意味着对于 “title” 字段的筛选是不会计算文档得分的，此外，“filter” 过滤器还可以被缓存，以便在下一次查询时能够更快地返回结果 term 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556GET /goods/_search&#123; &quot;query&quot;:&#123; &quot;bool&quot;:&#123; &quot;must&quot;:[ &#123; &quot;match&quot;:&#123; &quot;description&quot;:&quot;骁龙&quot; &#125; &#125; ], &quot;filter&quot;:[ &#123; &quot;range&quot;:&#123; &quot;price&quot;:&#123; &quot;gte&quot;:3500 &#125; &#125; &#125; ] &#125; &#125;&#125;&#123; &quot;took&quot; : 21, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 1, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : 2.0087056, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;goods&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_score&quot; : 2.0087056, &quot;_source&quot; : &#123; &quot;title&quot; : &quot;小米11&quot;, &quot;price&quot; : 3699.0, &quot;created_at&quot; : &quot;2021-02-28T18:00:00&quot;, &quot;description&quot; : &quot;小米11，骁龙888，1亿像素。&quot; &#125; &#125; ] &#125;&#125; 分词查询包含骁龙的商品描述，并且过滤价格大于等于3500的产品 terms 类似于mysql的 in 查询 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667GET /goods/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123; &quot;match_all&quot;: &#123;&#125; &#125; ], &quot;filter&quot;: [ &#123; &quot;terms&quot;: &#123; &quot;title&quot;: [ &quot;一加 9T&quot;, &quot;iPhone 12&quot; ] &#125; &#125; ] &#125; &#125;&#125;&#123; &quot;took&quot; : 5, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 2, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : 1.0, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;goods&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;3&quot;, &quot;_score&quot; : 1.0, &quot;_source&quot; : &#123; &quot;title&quot; : &quot;iPhone 12&quot;, &quot;price&quot; : 7799.0, &quot;created_at&quot; : &quot;2021-02-27T10:00:00&quot;, &quot;description&quot; : &quot;iPhone 12，包邮！促销期间送黑色苹果创意耳机一副！&quot; &#125; &#125;, &#123; &quot;_index&quot; : &quot;goods&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;4&quot;, &quot;_score&quot; : 1.0, &quot;_source&quot; : &#123; &quot;title&quot; : &quot;一加 9T&quot;, &quot;price&quot; : 3199.0, &quot;created_at&quot; : &quot;2021-02-27T15:00:00&quot;, &quot;description&quot; : &quot;一加 9T，8GB+128GB，双模5G，骁龙865。&quot; &#125; &#125; ] &#125;&#125; range 类似mysql 的 between 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657GET /goods/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123; &quot;match&quot;: &#123; &quot;description&quot;: &quot;小米&quot; &#125; &#125; ], &quot;filter&quot;: [ &#123; &quot;range&quot;: &#123; &quot;price&quot;: &#123; &quot;gte&quot;: 3000, &quot;lte&quot;: 4000 &#125; &#125; &#125; ] &#125; &#125;&#125;&#123; &quot;took&quot; : 7, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 1, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : 1.4131414, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;goods&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_score&quot; : 1.4131414, &quot;_source&quot; : &#123; &quot;title&quot; : &quot;小米11&quot;, &quot;price&quot; : 3699.0, &quot;created_at&quot; : &quot;2021-02-28T18:00:00&quot;, &quot;description&quot; : &quot;小米11，骁龙888，1亿像素。&quot; &#125; &#125; ] &#125;&#125; exists 过滤存在指定字段,获取字段不为空的索引记录使用，类似mysql里的 is not null 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455GET /goods/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123; &quot;term&quot;: &#123; &quot;title&quot;: &#123; &quot;value&quot;: &quot;iPhone 12&quot; &#125; &#125; &#125; ], &quot;filter&quot;: [ &#123; &quot;exists&quot;: &#123; &quot;field&quot;: &quot;description&quot; &#125; &#125; ] &#125; &#125;&#125;&#123; &quot;took&quot; : 41, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 1, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : 1.89712, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;goods&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;3&quot;, &quot;_score&quot; : 1.89712, &quot;_source&quot; : &#123; &quot;title&quot; : &quot;iPhone 12&quot;, &quot;price&quot; : 7799.0, &quot;created_at&quot; : &quot;2021-02-27T10:00:00&quot;, &quot;description&quot; : &quot;iPhone 12，包邮！促销期间送黑色苹果创意耳机一副！&quot; &#125; &#125; ] &#125;&#125; ids 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677GET /goods/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123; &quot;match_all&quot;: &#123;&#125; &#125; ], &quot;filter&quot;: [ &#123; &quot;ids&quot;: &#123; &quot;values&quot;: [ &quot;1&quot;,&quot;2&quot; ] &#125; &#125; ] &#125; &#125;&#125;&#123; &quot;took&quot; : 6, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 3, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : 1.0, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;goods&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_score&quot; : 1.0, &quot;_source&quot; : &#123; &quot;title&quot; : &quot;小米11&quot;, &quot;price&quot; : 3699.0, &quot;created_at&quot; : &quot;2021-02-28T18:00:00&quot;, &quot;description&quot; : &quot;小米11，骁龙888，1亿像素。&quot; &#125; &#125;, &#123; &quot;_index&quot; : &quot;goods&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;2&quot;, &quot;_score&quot; : 1.0, &quot;_source&quot; : &#123; &quot;title&quot; : &quot;华为Mate 40 Pro&quot;, &quot;price&quot; : 6399.0, &quot;created_at&quot; : &quot;2021-02-28T12:00:00&quot;, &quot;description&quot; : &quot;华为Mate 40 Pro，陶瓷版，华为麒麟9000。&quot; &#125; &#125;, &#123; &quot;_index&quot; : &quot;goods&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;3&quot;, &quot;_score&quot; : 1.0, &quot;_source&quot; : &#123; &quot;title&quot; : &quot;iPhone 12&quot;, &quot;price&quot; : 7799.0, &quot;created_at&quot; : &quot;2021-02-27T10:00:00&quot;, &quot;description&quot; : &quot;iPhone 12，包邮！促销期间送黑色苹果创意耳机一副！&quot; &#125; &#125; ] &#125;&#125; 聚合查询 Elasticsearch 将聚合分为三类： 从字段值计算指标的 指标聚合，例如总和或平均值 根据字段值、范围或其他条件将文档分组到桶（也称为箱）中的 桶聚合 从其他聚合而不是文档或字段获取输入的 管道聚合 注意： Text类型不支持聚合 聚合查询中如果不希望返回hits数据，可以在请求url加上参数 ?from=0&amp;size=0 测试数据 1234567891011121314151617181920212223242526272829303132333435PUT /goods&#123; &quot;mappings&quot;: &#123; &quot;properties&quot;: &#123; &quot;title&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125;, &quot;price&quot;: &#123; &quot;type&quot;: &quot;double&quot; &#125;, &quot;created_at&quot;: &#123; &quot;type&quot;: &quot;date&quot; &#125;, &quot;description&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;ik_max_word&quot; &#125; &#125; &#125;&#125;POST /goods/_bulk&#123; &quot;index&quot; : &#123; &quot;_index&quot; : &quot;goods&quot;, &quot;_id&quot;: &quot;1&quot; &#125; &#125;&#123; &quot;title&quot;: &quot;小米11&quot;, &quot;price&quot;: 3699.00, &quot;created_at&quot;: &quot;2021-02-28T18:00:00&quot;, &quot;description&quot;: &quot;小米11，骁龙888，1亿像素。&quot; &#125;&#123; &quot;index&quot; : &#123; &quot;_index&quot; : &quot;goods&quot;, &quot;_id&quot;: &quot;2&quot; &#125; &#125;&#123; &quot;title&quot;: &quot;华为Mate 40 Pro&quot;, &quot;price&quot;: 6399.00, &quot;created_at&quot;: &quot;2021-02-28T12:00:00&quot;, &quot;description&quot;: &quot;华为Mate 40 Pro，陶瓷版，华为麒麟9000。&quot; &#125;&#123; &quot;index&quot; : &#123; &quot;_index&quot; : &quot;goods&quot;, &quot;_id&quot;: &quot;3&quot; &#125; &#125;&#123; &quot;title&quot;: &quot;iPhone 12&quot;, &quot;price&quot;: 7799.00, &quot;created_at&quot;: &quot;2021-02-27T10:00:00&quot;, &quot;description&quot;: &quot;iPhone 12，包邮！促销期间送黑色苹果创意耳机一副！&quot; &#125;&#123; &quot;index&quot; : &#123; &quot;_index&quot; : &quot;goods&quot;, &quot;_id&quot;: &quot;4&quot; &#125; &#125;&#123; &quot;title&quot;: &quot;一加 9T&quot;, &quot;price&quot;: 3199.00, &quot;created_at&quot;: &quot;2021-02-27T15:00:00&quot;, &quot;description&quot;: &quot;一加 9T，8GB+128GB，双模5G，骁龙865。&quot; &#125;&#123; &quot;index&quot; : &#123; &quot;_index&quot; : &quot;goods&quot;, &quot;_id&quot;: &quot;5&quot; &#125; &#125;&#123; &quot;title&quot;: &quot;联想天逸 5300A&quot;, &quot;price&quot;: 3799.00, &quot;created_at&quot;: &quot;2021-02-27T20:00:00&quot;, &quot;description&quot;: &quot;联想天逸 5300A，AMD Ryzen 5 4600G，16G+512G，23.8英寸微端一体机。&quot; &#125;&#123; &quot;index&quot; : &#123; &quot;_index&quot; : &quot;goods&quot;, &quot;_id&quot;: &quot;6&quot; &#125; &#125;&#123; &quot;title&quot;: &quot;戴尔灵越 5406&quot;, &quot;price&quot;: 8299.00, &quot;created_at&quot;: &quot;2021-02-26T22:00:00&quot;, &quot;description&quot;: &quot;戴尔灵越 5406，酷睿i7-1165G7，16G+1TB+MX330独显，14英寸触控笔记本。&quot; &#125; 指标聚合 指标聚合（Metric Aggregation）是 Elasticsearch 中的一种聚合方式，可基于 文档属性 计算各种类型的统计数据，例如总和、平均值、最小值、最大值、百分比等。 指标聚合通常用于文档集合中的数值分析，允许用户对数据进行各种分析，例如计算特定时间段内销售总额、计算平均价格或计算每个类别中的数据点个数。 avg 平均聚合 single-value 计量聚合操作就是从聚合文本中提取的数据进行求平均数的操作。这些值可以是从文本具体的数值属性中提取的值，也可以是使用脚本生成的 12345678910GET /goods/_search?from=0&amp;size=0&#123; &quot;aggs&quot;: &#123; &quot;avg_price&quot;: &#123; &quot;avg&quot;: &#123; &quot;field&quot;: &quot;price&quot; &#125; &#125; &#125;&#125; 指定聚合字段名为 avg_price，对价格求平均值 1234567891011121314151617181920212223&#123; &quot;took&quot; : 1, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 6, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : null, &quot;hits&quot; : [ ] &#125;, &quot;aggregations&quot; : &#123; &quot;avg_price&quot; : &#123; &quot;value&quot; : 5532.333333333333 &#125; &#125;&#125; max 最大值聚合 最大值聚合是一个单值度量聚合，用来记录和返回从聚合的文档中提取出的数字型值中的最大值。这些值可以从文档中的特定数字类型的字段提取，也可以通过脚本生成 12345678910111213141516171819202122232425262728293031323334GET /goods/_search?from=0&amp;size=0&#123; &quot;aggs&quot;: &#123; &quot;max_price&quot;: &#123; &quot;max&quot;: &#123; &quot;field&quot;: &quot;price&quot; &#125; &#125; &#125;&#125;&#123; &quot;took&quot; : 11, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 6, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : null, &quot;hits&quot; : [ ] &#125;, &quot;aggregations&quot; : &#123; &quot;max_price&quot; : &#123; &quot;value&quot; : 8299.0 &#125; &#125;&#125; min 最小值聚合 用来记录和返回从聚合的文档中提取出的数字型值中的最小值。这些值可以从文档中的特定数字类型的字段提取，也可以通过脚本生成 12345678910111213141516171819202122232425262728293031323334GET /goods/_search?from=0&amp;size=0&#123; &quot;aggs&quot;: &#123; &quot;min_price&quot;: &#123; &quot;min&quot;: &#123; &quot;field&quot;: &quot;price&quot; &#125; &#125; &#125;&#125;&#123; &quot;took&quot; : 1, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 6, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : null, &quot;hits&quot; : [ ] &#125;, &quot;aggregations&quot; : &#123; &quot;min_price&quot; : &#123; &quot;value&quot; : 3199.0 &#125; &#125;&#125; sum 求和聚合 用于计算索引中某个数字字段的总和 12345678910111213141516171819202122232425262728293031323334GET /goods/_search?from=0&amp;size=0&#123; &quot;aggs&quot;: &#123; &quot;sum_price&quot;: &#123; &quot;sum&quot;: &#123; &quot;field&quot;: &quot;price&quot; &#125; &#125; &#125;&#125;&#123; &quot;took&quot; : 7, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 6, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : null, &quot;hits&quot; : [ ] &#125;, &quot;aggregations&quot; : &#123; &quot;sum_price&quot; : &#123; &quot;value&quot; : 33194.0 &#125; &#125;&#125; value_count 值统计聚合 用于计算从聚合文档中提取的值的条数，相当于 count(字段)，重复的值也会加入统计 12345678910111213141516171819202122232425262728293031323334GET /goods/_search?from=0&amp;size=0&#123; &quot;aggs&quot;: &#123; &quot;date&quot;: &#123; &quot;value_count&quot;: &#123; &quot;field&quot;: &quot;title&quot; &#125; &#125; &#125;&#125;&#123; &quot;took&quot; : 981, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 6, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : null, &quot;hits&quot; : [ ] &#125;, &quot;aggregations&quot; : &#123; &quot;date&quot; : &#123; &quot;value&quot; : 6 &#125; &#125;&#125; cardinality 基数聚合 基数聚合。它属于multi-value，基于文档的某个值（可以是特定的字段，也可以通过脚本计算而来），计算文档非重复的个数（去重计数），相当于sql中的 count(distinct xx) 12345678910111213141516171819202122232425262728293031323334GET /goods/_search?from=0&amp;size=0&#123; &quot;aggs&quot;: &#123; &quot;cardinality_count&quot;: &#123; &quot;cardinality&quot;: &#123; &quot;field&quot;: &quot;title&quot; &#125; &#125; &#125;&#125;&#123; &quot;took&quot; : 13, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 6, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : null, &quot;hits&quot; : [ ] &#125;, &quot;aggregations&quot; : &#123; &quot;cardinality_count&quot; : &#123; &quot;value&quot; : 6 &#125; &#125;&#125; 根据不重复的 title 字段统计基数 boxplot 箱形图聚合 箱形图 箱线图聚合返回制作箱线图的基本信息： “min”: 数据中的最小值。 “max”: 数据中的最大值。 “q1”: 数据中的第一个四分位数，即数据中最小的 25% 数值的中位数。 “q2”: 数据中的第二个四分位数，即数据中 50% 的数值的中位数。 “q3”: 数据中的第三个四分位数，即数据中最大的 25% 数值的中位数。 “lower”: 盒须下端，由 q1 减去 1.5 倍的四分位距(q3 - q1) 得到。 “upper”: 盒须上端，由 q3 加上 1.5 倍的四分位距(q3-q1) 得到。 12345678910111213141516171819202122232425262728293031323334353637383940GET /goods/_search?from=0&amp;size=0&#123; &quot;aggs&quot;: &#123; &quot;price_boxplot&quot;: &#123; &quot;boxplot&quot;: &#123; &quot;field&quot;: &quot;price&quot; &#125; &#125; &#125;&#125;&#123; &quot;took&quot; : 13, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 6, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : null, &quot;hits&quot; : [ ] &#125;, &quot;aggregations&quot; : &#123; &quot;price_boxplot&quot; : &#123; &quot;min&quot; : 3199.0, &quot;max&quot; : 8299.0, &quot;q1&quot; : 3699.0, &quot;q2&quot; : 5099.0, &quot;q3&quot; : 7799.0, &quot;lower&quot; : 3199.0, &quot;upper&quot; : 8299.0 &#125; &#125;&#125; stats 统计聚合 统计聚合。它属于multi-value，基于文档的某个值（可以是特定的数值型字段，也可以通过脚本计算而来），计算出一些统计信息（min、max、sum、count、avg）5个值 1234567891011121314151617181920212223242526272829303132333435363738GET /goods/_search?from=0&amp;size=0&#123; &quot;aggs&quot;: &#123; &quot;stat_price&quot;: &#123; &quot;stats&quot;: &#123; &quot;field&quot;: &quot;price&quot; &#125; &#125; &#125;&#125;&#123; &quot;took&quot; : 31, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 6, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : null, &quot;hits&quot; : [ ] &#125;, &quot;aggregations&quot; : &#123; &quot;stat_price&quot; : &#123; &quot;count&quot; : 6, &quot;min&quot; : 3199.0, &quot;max&quot; : 8299.0, &quot;avg&quot; : 5532.333333333333, &quot;sum&quot; : 33194.0 &#125; &#125;&#125; extended_stats 扩展统计聚合 扩展统计聚合。它属于multi-value，比 stats 多4个统计结果： 平方和、方差、标准差、平均值加/减两个标准差的区间 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253GET /goods/_search?from=0&amp;size=0&#123; &quot;aggs&quot;: &#123; &quot;extended&quot;: &#123; &quot;extended_stats&quot;: &#123; &quot;field&quot;: &quot;price&quot; &#125; &#125; &#125;&#125;&#123; &quot;took&quot; : 1, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 6, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : null, &quot;hits&quot; : [ ] &#125;, &quot;aggregations&quot; : &#123; &quot;extended&quot; : &#123; &quot;count&quot; : 6, &quot;min&quot; : 3199.0, &quot;max&quot; : 8299.0, &quot;avg&quot; : 5532.333333333333, &quot;sum&quot; : 33194.0, &quot;sum_of_squares&quot; : 2.08993606E8, &quot;variance&quot; : 4225555.555555557, &quot;variance_population&quot; : 4225555.555555557, &quot;variance_sampling&quot; : 5070666.666666669, &quot;std_deviation&quot; : 2055.615614738212, &quot;std_deviation_population&quot; : 2055.615614738212, &quot;std_deviation_sampling&quot; : 2251.8140835039353, &quot;std_deviation_bounds&quot; : &#123; &quot;upper&quot; : 9643.564562809757, &quot;lower&quot; : 1421.1021038569088, &quot;upper_population&quot; : 9643.564562809757, &quot;lower_population&quot; : 1421.1021038569088, &quot;upper_sampling&quot; : 10035.961500341204, &quot;lower_sampling&quot; : 1028.7051663254624 &#125; &#125; &#125;&#125; string_stats 字符串统计聚合 用于计算从聚合文档中提取的字符串值的统计信息。这些值可以从特定 keyword 类型字段中检索 字段 描述 count 计算的非空字段数 min_length 字符串最短长度 max_length 字符串最大长度 avg_length 字符串平均长度 entropy 熵,它是衡量数据集广泛属性（例如多样性、相似性、随机性等）的非常有用的指标 1234567891011121314151617181920212223242526272829303132333435363738GET /goods/_search?from=0&amp;size=0&#123; &quot;aggs&quot;: &#123; &quot;title_aggs&quot;: &#123; &quot;string_stats&quot;: &#123; &quot;field&quot;: &quot;title&quot; &#125; &#125; &#125;&#125;&#123; &quot;took&quot; : 5, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 6, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : null, &quot;hits&quot; : [ ] &#125;, &quot;aggregations&quot; : &#123; &quot;title_aggs&quot; : &#123; &quot;count&quot; : 6, &quot;min_length&quot; : 4, &quot;max_length&quot; : 13, &quot;avg_length&quot; : 8.333333333333334, &quot;entropy&quot; : 4.878562939644916 &#125; &#125;&#125; geo_bounds 地理边界聚合 地理边界聚合。基于文档的某个字段（geo-point类型字段），计算出该字段所有地理坐标点的边界（左上角/右下角坐标点 geo_centroid 地理重心聚合 地理重心聚合。基于文档的某个字段（geo-point类型字段），计算所有坐标的加权重心 matrix_stats 矩阵统计聚合 matrix_stats聚合是一个数字聚合，它会对一组文档字段计算以下统计数据: count 计算中每个字段样本的数量 mean 每个字段的平均值。 variance 每个字段测量如何从mean 值展开样本。 skewness 每个字段测量量化mean周围的不对称分布。 kurtosis 每个字段测量量化分布的形状。 covariance 一个矩阵描述一个字段与另一个相关联字段的变化。 correlation 协方差矩阵缩放到-1到1的范围。描述字段分布之间的关系。 median_absolute_deviation 中位数绝对偏差聚合 中位数绝对偏差 中位数绝对偏差是用来描述一组数据的离散程度的统计量。它首先计算中位数，然后对于每个数据点，计算它和中位数的差的绝对值，最后取这些绝对值的中位数。中位数绝对偏差可以反映出数据分布的离散程度。如果一组数据的中位数绝对偏差较小，说明这组数据的离散程度相对较小，数据点比较集中；反之，如果中位数绝对偏差较大，说明这组数据的离散程度较大，数据点比较分散。中位数绝对偏差的优点在于，它可以用来度量数据的分布情况，而且计算方法简单，不受极端值的影响 percentile_ranks 百分比排位聚合 百分比排位 用于计算给定字段中数值的百分位数。具体而言，它可以计算给定字段中的值在分布中所处位置的百分比或排名。这些值可以从文档中的特定数字或直方图字段中提取 123456789101112131415161718192021222324252627282930313233343536373839GET /goods/_search?from=0&amp;size=0&#123; &quot;aggs&quot;: &#123; &quot;rank&quot;: &#123; &quot;percentile_ranks&quot;: &#123; &quot;field&quot;: &quot;price&quot;, &quot;values&quot;: [ 7799.00 ] &#125; &#125; &#125;&#125;&#123; &quot;took&quot; : 55, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 6, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : null, &quot;hits&quot; : [ ] &#125;, &quot;aggregations&quot; : &#123; &quot;rank&quot; : &#123; &quot;values&quot; : &#123; &quot;7799.0&quot; : 78.94736842105262 &#125; &#125; &#125;&#125; 根据 78.94736842105262 这个百分比，我们得知 7799 这个价格在商品里面是很高的价格 percentiles 百分位数聚合 用于计算从聚合文档中提取的数值的一个或多个百分位数。这些值可以从文档中的特定数字或 直方图 字段中提取 12345678910111213141516171819202122232425262728293031323334353637383940414243GET /goods/_search?from=0&amp;size=0&#123; &quot;aggs&quot;: &#123; &quot;load_price&quot;: &#123; &quot;percentiles&quot;: &#123; &quot;field&quot;: &quot;price&quot;, &quot;percents&quot;: [ 50, 75, 95 ] &#125; &#125; &#125;&#125;&#123; &quot;took&quot; : 6, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 6, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : null, &quot;hits&quot; : [ ] &#125;, &quot;aggregations&quot; : &#123; &quot;load_price&quot; : &#123; &quot;values&quot; : &#123; &quot;50.0&quot; : 5099.0, &quot;75.0&quot; : 7799.0, &quot;95.0&quot; : 8299.0 &#125; &#125; &#125;&#125; rate 速率聚合 用于计算一段时间内索引中某个字段的速率或者增长率，并将其作为返回聚合结果的一部分。Rate Aggregation 可以用于监控指标的变化趋势，例如对请求的响应时间进行监控并计算每分钟的平均响应时间，与之前的平均响应时间比较是否出现严重变化 scripted_metric 脚本式指标聚合 使用脚本执行以提供度量标准输出的度量标准聚合 但是使用脚本会让执行速度变慢 t_test t检验聚合 用于在给定数据集上执行两个样本 t 检验，并计算提供的字段之间的差异是否显着。在数据分析领域中，t 检验通常用于比较两个不同样本之间的均值差异 使用 t检验 聚合可以帮助我们确定两个不同的数据集（例如，不同的客户群体）之间是否存在重要的差异 top_hits 热门记录聚合 最高匹配权值聚合。获取到每组前n条数据，相当于sql 中Top（group by 后取出前n条）。它跟踪聚合中相关性最高的文档，该聚合一般用做 sub-aggregation，以此来聚合每个桶中的最高匹配的文档，较为常用的统计 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110GET /goods/_search?from=0&amp;size=0&#123; &quot;aggs&quot;: &#123; &quot;group_top&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;price&quot;, &quot;size&quot;: 2 &#125;, &quot;aggs&quot;: &#123; &quot;top_price_hits&quot;: &#123; &quot;top_hits&quot;: &#123; &quot;size&quot;: 1, &quot;sort&quot;: [ &#123; &quot;price&quot;: &#123;&quot;order&quot;: &quot;desc&quot;&#125; &#125; ] &#125; &#125; &#125; &#125; &#125;&#125;&#123; &quot;took&quot; : 14, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 6, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : null, &quot;hits&quot; : [ ] &#125;, &quot;aggregations&quot; : &#123; &quot;group_top&quot; : &#123; &quot;doc_count_error_upper_bound&quot; : 0, &quot;sum_other_doc_count&quot; : 4, &quot;buckets&quot; : [ &#123; &quot;key&quot; : 3199.0, &quot;doc_count&quot; : 1, &quot;top_price_hits&quot; : &#123; &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 1, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : null, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;goods&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;4&quot;, &quot;_score&quot; : null, &quot;_source&quot; : &#123; &quot;title&quot; : &quot;一加 9T&quot;, &quot;price&quot; : 3199.0, &quot;created_at&quot; : &quot;2021-02-27T15:00:00&quot;, &quot;description&quot; : &quot;一加 9T，8GB+128GB，双模5G，骁龙865。&quot; &#125;, &quot;sort&quot; : [ 3199.0 ] &#125; ] &#125; &#125; &#125;, &#123; &quot;key&quot; : 3699.0, &quot;doc_count&quot; : 1, &quot;top_price_hits&quot; : &#123; &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 1, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : null, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;goods&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_score&quot; : null, &quot;_source&quot; : &#123; &quot;title&quot; : &quot;小米11&quot;, &quot;price&quot; : 3699.0, &quot;created_at&quot; : &quot;2021-02-28T18:00:00&quot;, &quot;description&quot; : &quot;小米11，骁龙888，1亿像素。&quot; &#125;, &quot;sort&quot; : [ 3699.0 ] &#125; ] &#125; &#125; &#125; ] &#125; &#125;&#125; 使用 price 字段分桶（分组），指定只要2个桶 对桶里的价格进行排序，取一条，完成分组取前几条 top_metrics 热门度量值聚合 top_metrics 在本质上与 top_hits 非常相似，但因为它更受限制，所以它能够使用更少的内存来完成它的工作并且通常更快 允许用户在检索文档时，对每一个聚合桶内的文档根据某些指标进行排序，并返回符合条件的文档数量以及所需的相关指标值 相当于是根据什么字段分分桶（分组），什么字段排序，取排序的第一条 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061GET /goods/_search?from=0&amp;size=0&#123; &quot;aggs&quot;: &#123; &quot;tm&quot;: &#123; &quot;top_metrics&quot;: &#123; &quot;metrics&quot;: &#123;&quot;field&quot;: &quot;title&quot;&#125;, &quot;sort&quot;: &#123;&quot;created_at&quot;: &quot;desc&quot;&#125;, &quot;size&quot;: 3 &#125; &#125; &#125;&#125;&#123; &quot;took&quot; : 41, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 6, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : null, &quot;hits&quot; : [ ] &#125;, &quot;aggregations&quot; : &#123; &quot;tm&quot; : &#123; &quot;top&quot; : [ &#123; &quot;sort&quot; : [ &quot;2021-02-28T18:00:00.000Z&quot; ], &quot;metrics&quot; : &#123; &quot;title&quot; : &quot;小米11&quot; &#125; &#125;, &#123; &quot;sort&quot; : [ &quot;2021-02-28T12:00:00.000Z&quot; ], &quot;metrics&quot; : &#123; &quot;title&quot; : &quot;华为Mate 40 Pro&quot; &#125; &#125;, &#123; &quot;sort&quot; : [ &quot;2021-02-27T20:00:00.000Z&quot; ], &quot;metrics&quot; : &#123; &quot;title&quot; : &quot;联想天逸 5300A&quot; &#125; &#125; ] &#125; &#125;&#125; 根据 title 分组，created_at 排序，取第一条 weighted_avg 加权平均聚合 加权平均 文档需要两个字段，一个是 value 字段，用来做平均计算的，一个 weight 字段，用来设置当前 value 的权重 桶聚合 桶聚合（Bucket Aggregation）是 ElasticSearch 中的一种聚合方式，它将一组文档分成几个不同的桶(bucket)中，每个桶代表符合指定条件的文档集合。 桶聚合从文档中提取可聚合的值，将它们组成一组桶。例如，在一个包含商品信息的文档集合中，我们可以对商品的品牌、价格、颜色、尺寸等属性进行桶聚合。 桶里的文档可以使用指定的聚合函数计算，例如，计算每个桶内商品的平均价格、最低价格、最高价格、销售量等统计数据。有了桶聚合，您可以分析文档集合中的数据，并从中找到有用的模式和统计信息。 一下列出的是常用的桶聚合，更多聚合查询请参考官方文档 terms 词项聚合 将结果按照指定字段进行分组，类似于 SQL 中的 GROUP BY 子句。默认返回顺序是按照文档个数多少排序。它属于multi-bucket。当不返回所有 buckets 的情况（size控制），文档个数可能不准确 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950GET /goods/_search&#123; &quot;aggs&quot;: &#123; &quot;agg&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;price&quot;, &quot;size&quot;: 3 &#125; &#125; &#125;&#125;&#123; &quot;took&quot; : 7, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 6, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : null, &quot;hits&quot; : [ ] &#125;, &quot;aggregations&quot; : &#123; &quot;agg&quot; : &#123; &quot;doc_count_error_upper_bound&quot; : 0, &quot;sum_other_doc_count&quot; : 3, &quot;buckets&quot; : [ &#123; &quot;key&quot; : 3199.0, &quot;doc_count&quot; : 1 &#125;, &#123; &quot;key&quot; : 3699.0, &quot;doc_count&quot; : 1 &#125;, &#123; &quot;key&quot; : 3799.0, &quot;doc_count&quot; : 1 &#125; ] &#125; &#125;&#125; filter 过滤聚合 基于一定条件筛选出符合过滤条件的文档集合（装到单个桶），然后针对这个集合进行聚合操作。可以理解成是对数据做一个筛选操作，然后对筛选后的数据再做一些别的聚合操作，比如求平均值、求和等 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748GET /goods/_search?from=0&amp;size=0&#123; &quot;aggs&quot;: &#123; &quot;filter_agg&quot;: &#123; &quot;filter&quot;: &#123; &quot;range&quot;: &#123; &quot;price&quot;: &#123; &quot;gte&quot;: 5000 # 过滤数据放到一个聚合桶 &#125; &#125; &#125;, &quot;aggs&quot;: &#123; &quot;avg_agg&quot;: &#123; &quot;avg&quot;: &#123; &quot;field&quot;: &quot;price&quot; # 再对这个桶进行处理 &#125; &#125; &#125; &#125; &#125;&#125;&#123; &quot;took&quot; : 16, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 6, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : null, &quot;hits&quot; : [ ] &#125;, &quot;aggregations&quot; : &#123; &quot;filter_agg&quot; : &#123; &quot;doc_count&quot; : 3, &quot;avg_agg&quot; : &#123; &quot;value&quot; : 7499.0 &#125; &#125; &#125;&#125; filters 多过滤聚合 基于多个过滤条件，来对当前文档进行过滤的聚合，多桶聚合，其中每个过滤条件都是一个桶（多个bucket中可能重复），先过滤再聚合。它属于multi-bucket 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152GET /goods/_search?from=0&amp;size=0&#123; &quot;aggs&quot;: &#123; &quot;flts&quot;: &#123; &quot;filters&quot;: &#123; &quot;filters&quot;: &#123; &quot;phone&quot;: &#123; &quot;match&quot;: &#123; &quot;description&quot;: &quot;骁龙&quot; &#125; &#125;, &quot;pc&quot;: &#123; &quot;match&quot;: &#123; &quot;description&quot;: &quot;16g&quot; &#125; &#125; &#125; &#125; &#125; &#125;&#125;&#123; &quot;took&quot; : 25, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 6, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : null, &quot;hits&quot; : [ ] &#125;, &quot;aggregations&quot; : &#123; &quot;flts&quot; : &#123; &quot;buckets&quot; : &#123; &quot;pc&quot; : &#123; &quot;doc_count&quot; : 3 &#125;, &quot;phone&quot; : &#123; &quot;doc_count&quot; : 2 &#125; &#125; &#125; &#125;&#125; 匹配词条 骁龙、16g 放到桶 phone、pc 桶中，并对文档数量进行统计 range 范围聚合 以字段的范围值来桶分聚合，每个范围为一个桶。范围聚合包括 from 值，不包括 to 值（区间前闭后开）。它属于multi-bucket 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768GET /goods/_search?from=0&amp;size=0&#123; &quot;aggs&quot;: &#123; &quot;level&quot;: &#123; &quot;range&quot;: &#123; &quot;field&quot;: &quot;price&quot;, &quot;ranges&quot;: [ &#123; &quot;key&quot;: &quot;低档&quot;, &quot;from&quot;: 0, &quot;to&quot;: 4000 &#125;, &#123; &quot;key&quot;: &quot;中档&quot;, &quot;from&quot;: 4000, &quot;to&quot;: 7000 &#125;, &#123; &quot;key&quot;: &quot;高档&quot;, &quot;from&quot;: 7000 &#125; ] &#125; &#125; &#125;&#125;&#123; &quot;took&quot; : 10, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 6, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : null, &quot;hits&quot; : [ ] &#125;, &quot;aggregations&quot; : &#123; &quot;level&quot; : &#123; &quot;buckets&quot; : [ &#123; &quot;key&quot; : &quot;低档&quot;, &quot;from&quot; : 0.0, &quot;to&quot; : 4000.0, &quot;doc_count&quot; : 3 &#125;, &#123; &quot;key&quot; : &quot;中档&quot;, &quot;from&quot; : 4000.0, &quot;to&quot; : 7000.0, &quot;doc_count&quot; : 1 &#125;, &#123; &quot;key&quot; : &quot;高档&quot;, &quot;from&quot; : 7000.0, &quot;doc_count&quot; : 2 &#125; ] &#125; &#125;&#125; 以上查询使用三个价格区间定义了三个桶，统计不同等级产品的数量 date_range 时间范围聚合 基于日期类型的值，以 日期范围 来桶分聚合。期范围可以用各种 Date Math 表达式。同样的，包括 from 的值，不包括 to 的值（区间前闭后开）。它属于multi-bucket 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162GET /goods/_search?from=0&amp;size=0&#123; &quot;aggs&quot;: &#123; &quot;t_range&quot;: &#123; &quot;date_range&quot;: &#123; &quot;field&quot;: &quot;created_at&quot;, &quot;format&quot;: &quot;yyyy-MM-dd&quot;, &quot;ranges&quot;: [ &#123; &quot;from&quot;: &quot;2021-02-26&quot;, &quot;to&quot;: &quot;2021-02-27&quot; &#125;, &#123; &quot;from&quot;: &quot;2021-02-27&quot;, &quot;to&quot;: &quot;2021-02-28&quot; &#125; ] &#125; &#125; &#125;&#125;&#123; &quot;took&quot; : 12, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 6, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : null, &quot;hits&quot; : [ ] &#125;, &quot;aggregations&quot; : &#123; &quot;t_range&quot; : &#123; &quot;buckets&quot; : [ &#123; &quot;key&quot; : &quot;2021-02-26-2021-02-27&quot;, &quot;from&quot; : 1.6142976E12, &quot;from_as_string&quot; : &quot;2021-02-26&quot;, &quot;to&quot; : 1.614384E12, &quot;to_as_string&quot; : &quot;2021-02-27&quot;, &quot;doc_count&quot; : 1 &#125;, &#123; &quot;key&quot; : &quot;2021-02-27-2021-02-28&quot;, &quot;from&quot; : 1.614384E12, &quot;from_as_string&quot; : &quot;2021-02-27&quot;, &quot;to&quot; : 1.6144704E12, &quot;to_as_string&quot; : &quot;2021-02-28&quot;, &quot;doc_count&quot; : 3 &#125; ] &#125; &#125;&#125; histogram 柱状图聚合 按照指定的间隔（interval）将某个字段的值进行分段，并统计每个段中文档的数量。例如，可以将一个数字字段按照 100 为间隔进行分段，然后统计每个段中的文档数量。这样就可以得到一个分布直方图（histogram），展示该字段的值在整个文档集合中的分布情况 参数 详细描述 field 聚合的字段名。 interval 聚合的间隔。可以是数字或时间段（如果聚合的字段是日期类型）。例如：10，1h，1d。 offset 调整聚合间隔的偏移量。可以是数字或时间段。例如：2h，1d。默认为 0。 order 对聚合计算值的排序方式。可以按照聚合 key 值升序或降序排列，也可以按照聚合计算值升序或降序排列。例如：&quot;_count&quot;（按值的大小升序排列），&quot;_key:desc&quot;（按 key 值降序排列）。默认为 &quot;_key:asc&quot;。 min_doc_count 过滤掉文档数量低于该值的聚合结果。可以是数字。默认为 1。 missing 如果出现缺失的聚合 key 值该如何处理。可以将它们聚合到指定的桶中，或者忽略它们。例如：&quot;_other&quot;（将其聚合到一个桶中），&quot;skip&quot;（忽略该值）。默认情况下缺失值被视为一种特殊的“桶”。 extended_bounds 用于对聚合结果进行边界控制，只计算在指定范围内的聚合结果。它包括两个值，分别表示开始边界和结束边界。每个值可以是数字或时间段。例如：&#123;&quot;min&quot;: 10, &quot;max&quot;: 20&#125;，&#123;&quot;min&quot;: &quot;2021-01-01&quot;, &quot;max&quot;: &quot;now&quot;&#125;。默认情况下，不进行边界控制。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253GET /goods/_search?from=0&amp;size=0&#123; &quot;aggs&quot;: &#123; &quot;h_time&quot;: &#123; &quot;histogram&quot;: &#123; &quot;field&quot;: &quot;price&quot;, &quot;interval&quot;: 1000, &quot;min_doc_count&quot;: 1 &#125; &#125; &#125;&#125;&#123; &quot;took&quot; : 3, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 6, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : null, &quot;hits&quot; : [ ] &#125;, &quot;aggregations&quot; : &#123; &quot;h_time&quot; : &#123; &quot;buckets&quot; : [ &#123; &quot;key&quot; : 3000.0, &quot;doc_count&quot; : 3 &#125;, &#123; &quot;key&quot; : 6000.0, &quot;doc_count&quot; : 1 &#125;, &#123; &quot;key&quot; : 7000.0, &quot;doc_count&quot; : 1 &#125;, &#123; &quot;key&quot; : 8000.0, &quot;doc_count&quot; : 1 &#125; ] &#125; &#125;&#125; date_histogram 时间柱状图聚合 和 histogram 类似，不过字段类型为时间类型 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051GET /goods/_search?from=0&amp;size=0&#123; &quot;aggs&quot;: &#123; &quot;t_time&quot;: &#123; &quot;date_histogram&quot;: &#123; &quot;field&quot;: &quot;created_at&quot;, &quot;interval&quot;: &quot;day&quot; &#125; &#125; &#125;&#125;&#123; &quot;took&quot; : 2, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 6, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : null, &quot;hits&quot; : [ ] &#125;, &quot;aggregations&quot; : &#123; &quot;t_time&quot; : &#123; &quot;buckets&quot; : [ &#123; &quot;key_as_string&quot; : &quot;2021-02-26T00:00:00.000Z&quot;, &quot;key&quot; : 1614297600000, &quot;doc_count&quot; : 1 &#125;, &#123; &quot;key_as_string&quot; : &quot;2021-02-27T00:00:00.000Z&quot;, &quot;key&quot; : 1614384000000, &quot;doc_count&quot; : 3 &#125;, &#123; &quot;key_as_string&quot; : &quot;2021-02-28T00:00:00.000Z&quot;, &quot;key&quot; : 1614470400000, &quot;doc_count&quot; : 2 &#125; ] &#125; &#125;&#125; missing 缺失值聚合 用于查找一个字段中缺失或为 null 的文档数量 1234567891011121314151617181920212223242526272829303132333435GET /goods/_search?from=0&amp;size=0&#123; &quot;aggs&quot;: &#123; &quot;miss&quot;: &#123; &quot;missing&quot;: &#123; &quot;field&quot;: &quot;price&quot; &#125; &#125; &#125;&#125;返回：&#123; &quot;took&quot; : 3, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 6, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : null, &quot;hits&quot; : [ ] &#125;, &quot;aggregations&quot; : &#123; &quot;miss&quot; : &#123; &quot;doc_count&quot; : 0 &#125; &#125;&#125; ip_range IP范围聚合 基于一个 IPv4 字段，对文档进行 IPv4范围 的桶分聚合。和 Range Aggregation 类似，只是应用字段必须是 IPv4 数据类型。它属于multi-bucket 123456789101112131415GET /ip_addresses/_search&#123; &quot;size&quot;: 10, &quot;aggs&quot;: &#123; &quot;ip_ranges&quot;: &#123; &quot;ip_range&quot;: &#123; &quot;field&quot;: &quot;ip&quot;, &quot;ranges&quot;: [ &#123; &quot;to&quot;: &quot;10.0.0.5&quot; &#125;, &#123; &quot;from&quot;: &quot;10.0.0.5&quot; &#125; ] &#125; &#125; &#125;&#125; nested 嵌套聚合 种特殊的单桶聚合，可以聚合嵌套文档 管道聚合 它的主要作用是在已有的聚合结果上进行二次聚合来获得更加精确的结果，它允许用户在多个聚合器之间进行转换和传递数据，并且可以通过具有不同处理能力的聚合器来实现多个聚合功能的组合 常用的管道聚合有：avg_bucket、sum_bucket、min_bucket、max_bucket等 管道聚合不能包含子聚合，但是某些类型的管道聚合可以链式使用（比如计算导数的导数） 管道聚合大致分为两类 Parent：用于对父聚合进行后续处理。它将聚合结果进行转换和过滤，然后生成一种新的聚合结果 Sibling ：可以用来在多个兄弟节点之间进行聚合计算，其中兄弟节点指在同一级别上具有相同父聚合的节点 管道聚合通过 buckets_path 参数指定他们要进行聚合计算的权值对象，bucket_path语法 符号 描述 AGG_SEPARATOR 聚合符号，用于分隔聚合桶路径中的不同聚合桶 METRIC_SEPARATOR 度量符号，用于分隔聚合名称和度量名称 AGG_NAME 聚合名称，是引用聚合桶的标识符 METRIC 度量名称，在多值度量聚合的情况下，用于引用特定的度量值 MULTIBUCKET_KEY 多桶键，用于指定多个聚合桶的键 PATH 完整的聚合桶路径，使用上述符号组成的字符串 特殊情况 要进行 pipeline aggregation 聚合的对象名称或权值名称包含小数点，“buckets_path”: “my_percentile[99.9]” 处理对象中包含空桶（无文档的桶分），参数 gap_policy，可选值有 skip、insert_zeros avg_bucket 桶均值聚合 avg_bucket 聚合可以用于计算由子桶（sub-bucket）聚合级别生成的桶之间的平均值。 它针对每个桶计算出给定度量（field）的平均值，并返回所有桶的平均值 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475GET /goods/_search?from=0&amp;size=0&#123; &quot;aggs&quot;: &#123; &quot;h_day&quot;: &#123; &quot;date_histogram&quot;: &#123; &quot;field&quot;: &quot;created_at&quot;, &quot;interval&quot;: &quot;day&quot; &#125;, &quot;aggs&quot;: &#123; &quot;sales&quot;: &#123; &quot;sum&quot;: &#123; &quot;field&quot;: &quot;price&quot; &#125; &#125; &#125; &#125;, &quot;avg_pip&quot;: &#123; &quot;avg_bucket&quot;: &#123; &quot;buckets_path&quot;: &quot;h_day&gt;sales&quot; &#125; &#125; &#125;&#125;&#123; &quot;took&quot; : 16, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 6, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : null, &quot;hits&quot; : [ ] &#125;, &quot;aggregations&quot; : &#123; &quot;h_day&quot; : &#123; &quot;buckets&quot; : [ &#123; &quot;key_as_string&quot; : &quot;2021-02-26T00:00:00.000Z&quot;, &quot;key&quot; : 1614297600000, &quot;doc_count&quot; : 1, &quot;sales&quot; : &#123; &quot;value&quot; : 8299.0 &#125; &#125;, &#123; &quot;key_as_string&quot; : &quot;2021-02-27T00:00:00.000Z&quot;, &quot;key&quot; : 1614384000000, &quot;doc_count&quot; : 3, &quot;sales&quot; : &#123; &quot;value&quot; : 14797.0 &#125; &#125;, &#123; &quot;key_as_string&quot; : &quot;2021-02-28T00:00:00.000Z&quot;, &quot;key&quot; : 1614470400000, &quot;doc_count&quot; : 2, &quot;sales&quot; : &#123; &quot;value&quot; : 10098.0 &#125; &#125; ] &#125;, &quot;avg_pip&quot; : &#123; &quot;value&quot; : 11064.666666666666 &#125; &#125;&#125; 先按照天做一个直方图聚合，然后计算每天上架商品的价格总额 根据每个桶每天的价格总额，计算平均每天的商品总额（对每个桶的某个度量取平均值） max_bucket 桶最大值聚合 用于在构建多桶聚合时返回指定度量的最大值的桶。它可以在许多聚合类型中使用，例如 terms、histogram、date_histogram、range 等聚合 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778GET /goods/_search?from=0&amp;size=0&#123; &quot;aggs&quot;: &#123; &quot;h_day&quot;: &#123; &quot;date_histogram&quot;: &#123; &quot;field&quot;: &quot;created_at&quot;, &quot;interval&quot;: &quot;day&quot; &#125;, &quot;aggs&quot;: &#123; &quot;sales&quot;: &#123; &quot;sum&quot;: &#123; &quot;field&quot;: &quot;price&quot; &#125; &#125; &#125; &#125;, &quot;max_sales&quot;: &#123; &quot;max_bucket&quot;: &#123; &quot;buckets_path&quot;: &quot;h_day&gt;sales&quot; &#125; &#125; &#125;&#125;&#123; &quot;took&quot; : 61, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 6, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : null, &quot;hits&quot; : [ ] &#125;, &quot;aggregations&quot; : &#123; &quot;h_day&quot; : &#123; &quot;buckets&quot; : [ &#123; &quot;key_as_string&quot; : &quot;2021-02-26T00:00:00.000Z&quot;, &quot;key&quot; : 1614297600000, &quot;doc_count&quot; : 1, &quot;sales&quot; : &#123; &quot;value&quot; : 8299.0 &#125; &#125;, &#123; &quot;key_as_string&quot; : &quot;2021-02-27T00:00:00.000Z&quot;, &quot;key&quot; : 1614384000000, &quot;doc_count&quot; : 3, &quot;sales&quot; : &#123; &quot;value&quot; : 14797.0 &#125; &#125;, &#123; &quot;key_as_string&quot; : &quot;2021-02-28T00:00:00.000Z&quot;, &quot;key&quot; : 1614470400000, &quot;doc_count&quot; : 2, &quot;sales&quot; : &#123; &quot;value&quot; : 10098.0 &#125; &#125; ] &#125;, &quot;max_sales&quot; : &#123; &quot;value&quot; : 14797.0, &quot;keys&quot; : [ &quot;2021-02-27T00:00:00.000Z&quot; ] &#125; &#125;&#125; 先按照天做一个直方图聚合，然后计算每天上架商品的价格总额 根据每个桶每天的价格总额，计算最大的商品总额 min_bucket 桶最小值聚合 桶最小值聚合。基于兄弟聚合的某个权值，输出权值最小的一个桶。使用规则同上 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778GET /goods/_search?from=0&amp;size=0&#123; &quot;aggs&quot;: &#123; &quot;h_day&quot;: &#123; &quot;date_histogram&quot;: &#123; &quot;field&quot;: &quot;created_at&quot;, &quot;interval&quot;: &quot;day&quot; &#125;, &quot;aggs&quot;: &#123; &quot;sales&quot;: &#123; &quot;sum&quot;: &#123; &quot;field&quot;: &quot;price&quot; &#125; &#125; &#125; &#125;, &quot;min_sales&quot;: &#123; &quot;min_bucket&quot;: &#123; &quot;buckets_path&quot;: &quot;h_day&gt;sales&quot; &#125; &#125; &#125;&#125;&#123; &quot;took&quot; : 15, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 6, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : null, &quot;hits&quot; : [ ] &#125;, &quot;aggregations&quot; : &#123; &quot;h_day&quot; : &#123; &quot;buckets&quot; : [ &#123; &quot;key_as_string&quot; : &quot;2021-02-26T00:00:00.000Z&quot;, &quot;key&quot; : 1614297600000, &quot;doc_count&quot; : 1, &quot;sales&quot; : &#123; &quot;value&quot; : 8299.0 &#125; &#125;, &#123; &quot;key_as_string&quot; : &quot;2021-02-27T00:00:00.000Z&quot;, &quot;key&quot; : 1614384000000, &quot;doc_count&quot; : 3, &quot;sales&quot; : &#123; &quot;value&quot; : 14797.0 &#125; &#125;, &#123; &quot;key_as_string&quot; : &quot;2021-02-28T00:00:00.000Z&quot;, &quot;key&quot; : 1614470400000, &quot;doc_count&quot; : 2, &quot;sales&quot; : &#123; &quot;value&quot; : 10098.0 &#125; &#125; ] &#125;, &quot;min_sales&quot; : &#123; &quot;value&quot; : 8299.0, &quot;keys&quot; : [ &quot;2021-02-26T00:00:00.000Z&quot; ] &#125; &#125;&#125; sum bucket 桶求和聚合 桶求和聚合。基于兄弟聚合的权值，对所有桶的权值求和。使用规则同上 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475GET /goods/_search?from=0&amp;size=0&#123; &quot;aggs&quot;: &#123; &quot;h_day&quot;: &#123; &quot;date_histogram&quot;: &#123; &quot;field&quot;: &quot;created_at&quot;, &quot;interval&quot;: &quot;day&quot; &#125;, &quot;aggs&quot;: &#123; &quot;sales&quot;: &#123; &quot;sum&quot;: &#123; &quot;field&quot;: &quot;price&quot; &#125; &#125; &#125; &#125;, &quot;min_sales&quot;: &#123; &quot;sum_bucket&quot;: &#123; &quot;buckets_path&quot;: &quot;h_day&gt;sales&quot; &#125; &#125; &#125;&#125;&#123; &quot;took&quot; : 4, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 6, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : null, &quot;hits&quot; : [ ] &#125;, &quot;aggregations&quot; : &#123; &quot;h_day&quot; : &#123; &quot;buckets&quot; : [ &#123; &quot;key_as_string&quot; : &quot;2021-02-26T00:00:00.000Z&quot;, &quot;key&quot; : 1614297600000, &quot;doc_count&quot; : 1, &quot;sales&quot; : &#123; &quot;value&quot; : 8299.0 &#125; &#125;, &#123; &quot;key_as_string&quot; : &quot;2021-02-27T00:00:00.000Z&quot;, &quot;key&quot; : 1614384000000, &quot;doc_count&quot; : 3, &quot;sales&quot; : &#123; &quot;value&quot; : 14797.0 &#125; &#125;, &#123; &quot;key_as_string&quot; : &quot;2021-02-28T00:00:00.000Z&quot;, &quot;key&quot; : 1614470400000, &quot;doc_count&quot; : 2, &quot;sales&quot; : &#123; &quot;value&quot; : 10098.0 &#125; &#125; ] &#125;, &quot;min_sales&quot; : &#123; &quot;value&quot; : 33194.0 &#125; &#125;&#125; derivative 导数聚合 导数 求导聚合。基于父聚合（只能是histogram或date_histogram类型）的某个权值，对权值求导。用于求导的权值必须是数值类型。封闭直方图（histogram）聚合的 min_doc_count 必须是 0 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283GET /products/_search?from=0&amp;size=0&#123; &quot;aggs&quot;: &#123; &quot;price_range&quot;: &#123; # 桶聚合 &quot;histogram&quot;: &#123; &quot;field&quot;: &quot;price&quot;, &quot;interval&quot;: 2000, &quot;min_doc_count&quot;: 0 &#125;, &quot;aggs&quot;: &#123; # 指标聚合 &quot;sum_price&quot;: &#123; &quot;sum&quot;: &#123; &quot;field&quot;: &quot;price&quot; &#125; &#125;, &quot;price_deriv&quot;: &#123; &quot;derivative&quot;: &#123; &quot;buckets_path&quot;: &quot;sum_price&quot; # 获取父聚合数据再计算导出聚合 &#125; &#125; &#125; &#125; &#125;&#125;返回：&#123;... &quot;aggregations&quot; : &#123; &quot;price_range&quot; : &#123; &quot;buckets&quot; : [ &#123; &quot;key&quot; : 0.0, &quot;doc_count&quot; : 1, &quot;sum_price&quot; : &#123; &quot;value&quot; : 299.0 &#125; &#125;, &#123; &quot;key&quot; : 2000.0, &quot;doc_count&quot; : 2, &quot;sum_price&quot; : &#123; &quot;value&quot; : 4798.0 &#125;, &quot;price_deriv&quot; : &#123; &quot;value&quot; : 4499.0 &#125; &#125;, &#123; &quot;key&quot; : 4000.0, &quot;doc_count&quot; : 1, &quot;sum_price&quot; : &#123; &quot;value&quot; : 5999.0 &#125;, &quot;price_deriv&quot; : &#123; &quot;value&quot; : 1201.0 &#125; &#125;, &#123; &quot;key&quot; : 6000.0, &quot;doc_count&quot; : 3, &quot;sum_price&quot; : &#123; &quot;value&quot; : 20197.0 &#125;, &quot;price_deriv&quot; : &#123; &quot;value&quot; : 14198.0 &#125; &#125;, &#123; &quot;key&quot; : 8000.0, &quot;doc_count&quot; : 1, &quot;sum_price&quot; : &#123; &quot;value&quot; : 8399.0 &#125;, &quot;price_deriv&quot; : &#123; &quot;value&quot; : -11798.0 &#125; &#125; ] &#125; &#125;&#125; 1234567891011121314151617181920212223242526272829POST /sales/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;sales_per_month&quot;: &#123; &quot;date_histogram&quot;: &#123; &quot;field&quot;: &quot;date&quot;, &quot;calendar_interval&quot;: &quot;month&quot; &#125;, &quot;aggs&quot;: &#123; &quot;sales&quot;: &#123; &quot;sum&quot;: &#123; &quot;field&quot;: &quot;price&quot; &#125; &#125;, &quot;sales_deriv&quot;: &#123; # 父导合，父是sales &quot;derivative&quot;: &#123; &quot;buckets_path&quot;: &quot;sales&quot; &#125; &#125;, &quot;sales_2nd_deriv&quot;: &#123; &quot;derivative&quot;: &#123; &quot;buckets_path&quot;: &quot;sales_deriv&quot; # 兄弟聚合，兄弟是sales_deriv (同层级) &#125; &#125; &#125; &#125; &#125;&#125; moving average 窗口平均数聚合 Moving Average，窗口平均值聚合。基于已经排序过的数据，计算出处在当前窗口中数据的平均值。比如窗口大小为 5 （可以设置滑动窗口大小），对数据 1—10 的部分窗口平均值如下： (1 + 2 + 3 + 4 + 5) / 5 = 3 (2 + 3 + 4 + 5 + 6) / 5 = 4 (3 + 4 + 5 + 6 + 7) / 5 = 5 etc 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081GET /products/_search?from=0&amp;size=0&#123; &quot;aggs&quot;: &#123; &quot;price_range&quot;: &#123; &quot;histogram&quot;: &#123; &quot;field&quot;: &quot;price&quot;, &quot;interval&quot;: 2000 &#125;, &quot;aggs&quot;: &#123; &quot;sum_price&quot;: &#123; &quot;sum&quot;: &#123; &quot;field&quot;: &quot;price&quot; &#125; &#125;, &quot;the_movavg&quot;: &#123; &quot;moving_avg&quot;: &#123; &quot;buckets_path&quot;: &quot;sum_price&quot; &#125; &#125; &#125; &#125; &#125;&#125;返回：&#123;... &quot;aggregations&quot; : &#123; &quot;price_range&quot; : &#123; &quot;buckets&quot; : [ &#123; &quot;key&quot; : 0.0, &quot;doc_count&quot; : 1, &quot;sum_price&quot; : &#123; &quot;value&quot; : 299.0 &#125; &#125;, &#123; &quot;key&quot; : 2000.0, &quot;doc_count&quot; : 2, &quot;sum_price&quot; : &#123; &quot;value&quot; : 4798.0 &#125;, &quot;the_movavg&quot; : &#123; &quot;value&quot; : 299.0 &#125; &#125;, &#123; &quot;key&quot; : 4000.0, &quot;doc_count&quot; : 1, &quot;sum_price&quot; : &#123; &quot;value&quot; : 5999.0 &#125;, &quot;the_movavg&quot; : &#123; &quot;value&quot; : 2548.5 &#125; &#125;, &#123; &quot;key&quot; : 6000.0, &quot;doc_count&quot; : 3, &quot;sum_price&quot; : &#123; &quot;value&quot; : 20197.0 &#125;, &quot;the_movavg&quot; : &#123; &quot;value&quot; : 3698.6666666666665 &#125; &#125;, &#123; &quot;key&quot; : 8000.0, &quot;doc_count&quot; : 1, &quot;sum_price&quot; : &#123; &quot;value&quot; : 8399.0 &#125;, &quot;the_movavg&quot; : &#123; &quot;value&quot; : 7823.25 &#125; &#125; ] &#125; &#125;&#125; bucket_script 桶脚本聚合 桶脚本聚合。基于父聚合的【一个或多个权值】，对这些权值通过脚本进行运算。用于计算的父聚合必须是多桶聚合。用于计算的权值必须是数值类型。执行脚本必须要返回数值型结果 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107POST /sales/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;sales_per_month&quot;: &#123; &quot;date_histogram&quot;: &#123; &quot;field&quot;: &quot;date&quot;, &quot;calendar_interval&quot;: &quot;month&quot; &#125;, &quot;aggs&quot;: &#123; &quot;total_sales&quot;: &#123; &quot;sum&quot;: &#123; &quot;field&quot;: &quot;price&quot; &#125; &#125;, &quot;t-shirts&quot;: &#123; &quot;filter&quot;: &#123; &quot;term&quot;: &#123; &quot;type&quot;: &quot;t-shirt&quot; &#125; &#125;, &quot;aggs&quot;: &#123; &quot;sales&quot;: &#123; &quot;sum&quot;: &#123; &quot;field&quot;: &quot;price&quot; &#125; &#125; &#125; &#125;, &quot;t-shirt-percentage&quot;: &#123; &quot;bucket_script&quot;: &#123; &quot;buckets_path&quot;: &#123; &quot;tShirtSales&quot;: &quot;t-shirts&gt;sales&quot;, &quot;totalSales&quot;: &quot;total_sales&quot; &#125;, &quot;script&quot;: &quot;params.tShirtSales / params.totalSales * 100&quot; &#125; &#125; &#125; &#125; &#125;&#125;返回：&#123; &quot;took&quot;: 11, &quot;timed_out&quot;: false, &quot;_shards&quot;: ..., &quot;hits&quot;: ..., &quot;aggregations&quot;: &#123; &quot;sales_per_month&quot;: &#123; &quot;buckets&quot;: [ &#123; &quot;key_as_string&quot;: &quot;2015/01/01 00:00:00&quot;, &quot;key&quot;: 1420070400000, &quot;doc_count&quot;: 3, &quot;total_sales&quot;: &#123; &quot;value&quot;: 550.0 &#125;, &quot;t-shirts&quot;: &#123; &quot;doc_count&quot;: 1, &quot;sales&quot;: &#123; &quot;value&quot;: 200.0 &#125; &#125;, &quot;t-shirt-percentage&quot;: &#123; &quot;value&quot;: 36.36363636363637 &#125; &#125;, &#123; &quot;key_as_string&quot;: &quot;2015/02/01 00:00:00&quot;, &quot;key&quot;: 1422748800000, &quot;doc_count&quot;: 2, &quot;total_sales&quot;: &#123; &quot;value&quot;: 60.0 &#125;, &quot;t-shirts&quot;: &#123; &quot;doc_count&quot;: 1, &quot;sales&quot;: &#123; &quot;value&quot;: 10.0 &#125; &#125;, &quot;t-shirt-percentage&quot;: &#123; &quot;value&quot;: 16.666666666666664 &#125; &#125;, &#123; &quot;key_as_string&quot;: &quot;2015/03/01 00:00:00&quot;, &quot;key&quot;: 1425168000000, &quot;doc_count&quot;: 2, &quot;total_sales&quot;: &#123; &quot;value&quot;: 375.0 &#125;, &quot;t-shirts&quot;: &#123; &quot;doc_count&quot;: 1, &quot;sales&quot;: &#123; &quot;value&quot;: 175.0 &#125; &#125;, &quot;t-shirt-percentage&quot;: &#123; &quot;value&quot;: 46.666666666666664 &#125; &#125; ] &#125; &#125;&#125; bucket_selector 桶选择器聚合 桶选择器聚合。基于父聚合的【一个或多个权值】，通过脚本对权值进行计算，并决定父聚合的哪些桶需要保留，其余的将被丢弃。用于计算的父聚合必须是多桶聚合。用于计算的权值必须是数值类型。运算的脚本必须是返回 boolean 类型，如果脚本是脚本表达式形式给出，那么允许返回数值类型 类似 having 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657POST /sales/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;sales_per_month&quot;: &#123; &quot;date_histogram&quot;: &#123; &quot;field&quot;: &quot;date&quot;, &quot;calendar_interval&quot;: &quot;month&quot; &#125;, &quot;aggs&quot;: &#123; &quot;total_sales&quot;: &#123; &quot;sum&quot;: &#123; &quot;field&quot;: &quot;price&quot; &#125; &#125;, &quot;sales_bucket_filter&quot;: &#123; &quot;bucket_selector&quot;: &#123; &quot;buckets_path&quot;: &#123; &quot;totalSales&quot;: &quot;total_sales&quot; &#125;, &quot;script&quot;: &quot;params.totalSales &gt; 200&quot; &#125; &#125; &#125; &#125; &#125;&#125;返回：&#123; &quot;took&quot;: 11, &quot;timed_out&quot;: false, &quot;_shards&quot;: ..., &quot;hits&quot;: ..., &quot;aggregations&quot;: &#123; &quot;sales_per_month&quot;: &#123; &quot;buckets&quot;: [ &#123; &quot;key_as_string&quot;: &quot;2015/01/01 00:00:00&quot;, &quot;key&quot;: 1420070400000, &quot;doc_count&quot;: 3, &quot;total_sales&quot;: &#123; &quot;value&quot;: 550.0 &#125; &#125;, &#123; &quot;key_as_string&quot;: &quot;2015/03/01 00:00:00&quot;, &quot;key&quot;: 1425168000000, &quot;doc_count&quot;: 2, &quot;total_sales&quot;: &#123; &quot;value&quot;: 375.0 &#125; &#125; ] &#125; &#125;&#125; serial_diff 串行差分聚合 差分 串行差分聚合。基于父聚合（只能是histogram或date_histogram类型）的某个权值，对权值值进行差分运算，（取时间间隔，后一刻的值减去前一刻的值：f(X) = f(Xt) – f(Xt-n)）。用于计算的父聚合必须是多桶聚合 12345678910111213141516171819202122232425POST /_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;my_date_histo&quot;: &#123; &quot;date_histogram&quot;: &#123; &quot;field&quot;: &quot;timestamp&quot;, &quot;calendar_interval&quot;: &quot;day&quot; &#125;, &quot;aggs&quot;: &#123; &quot;the_sum&quot;: &#123; &quot;sum&quot;: &#123; &quot;field&quot;: &quot;lemmings&quot; &#125; &#125;, &quot;thirtieth_difference&quot;: &#123; &quot;serial_diff&quot;: &#123; &quot;buckets_path&quot;: &quot;the_sum&quot;, &quot;lag&quot; : 30 &#125; &#125; &#125; &#125; &#125;&#125; stats_bucket 桶统计信息聚合 桶统计信息聚合。基于兄弟聚合的某个权值，对【桶的信息】进行一些统计学运算（总计多少个桶、所有桶中该权值的最大值、最小等）。用于计算的权值必须是数值类型。用于计算的兄弟聚合必须是多桶聚合类型 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566POST /sales/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;sales_per_month&quot;: &#123; &quot;date_histogram&quot;: &#123; &quot;field&quot;: &quot;date&quot;, &quot;calendar_interval&quot;: &quot;month&quot; &#125;, &quot;aggs&quot;: &#123; &quot;sales&quot;: &#123; &quot;sum&quot;: &#123; &quot;field&quot;: &quot;price&quot; &#125; &#125; &#125; &#125;, &quot;stats_monthly_sales&quot;: &#123; &quot;stats_bucket&quot;: &#123; &quot;buckets_path&quot;: &quot;sales_per_month&gt;sales&quot; &#125; &#125; &#125;&#125;返回：&#123;... &quot;aggregations&quot;: &#123; &quot;sales_per_month&quot;: &#123; &quot;buckets&quot;: [ &#123; &quot;key_as_string&quot;: &quot;2015/01/01 00:00:00&quot;, &quot;key&quot;: 1420070400000, &quot;doc_count&quot;: 3, &quot;sales&quot;: &#123; &quot;value&quot;: 550.0 &#125; &#125;, &#123; &quot;key_as_string&quot;: &quot;2015/02/01 00:00:00&quot;, &quot;key&quot;: 1422748800000, &quot;doc_count&quot;: 2, &quot;sales&quot;: &#123; &quot;value&quot;: 60.0 &#125; &#125;, &#123; &quot;key_as_string&quot;: &quot;2015/03/01 00:00:00&quot;, &quot;key&quot;: 1425168000000, &quot;doc_count&quot;: 2, &quot;sales&quot;: &#123; &quot;value&quot;: 375.0 &#125; &#125; ] &#125;, &quot;stats_monthly_sales&quot;: &#123; &quot;count&quot;: 3, &quot;min&quot;: 60.0, &quot;max&quot;: 550.0, &quot;avg&quot;: 328.3333333333333, &quot;sum&quot;: 985.0 &#125; &#125;&#125; extended_stats_bucket 扩展桶统计聚合 扩展桶统计聚合。基于兄弟聚合的某个权值，对【桶信息】进行一系列统计学计算（比普通的统计聚合多了一些统计值）。用于计算的权值必须是数值类型。用于计算的兄弟聚合必须是多桶聚合类型 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081POST /sales/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;sales_per_month&quot;: &#123; &quot;date_histogram&quot;: &#123; &quot;field&quot;: &quot;date&quot;, &quot;calendar_interval&quot;: &quot;month&quot; &#125;, &quot;aggs&quot;: &#123; &quot;sales&quot;: &#123; &quot;sum&quot;: &#123; &quot;field&quot;: &quot;price&quot; &#125; &#125; &#125; &#125;, &quot;stats_monthly_sales&quot;: &#123; &quot;extended_stats_bucket&quot;: &#123; &quot;buckets_path&quot;: &quot;sales_per_month&gt;sales&quot; &#125; &#125; &#125;&#125;返回：&#123;... &quot;aggregations&quot;: &#123; &quot;sales_per_month&quot;: &#123; &quot;buckets&quot;: [ &#123; &quot;key_as_string&quot;: &quot;2015/01/01 00:00:00&quot;, &quot;key&quot;: 1420070400000, &quot;doc_count&quot;: 3, &quot;sales&quot;: &#123; &quot;value&quot;: 550.0 &#125; &#125;, &#123; &quot;key_as_string&quot;: &quot;2015/02/01 00:00:00&quot;, &quot;key&quot;: 1422748800000, &quot;doc_count&quot;: 2, &quot;sales&quot;: &#123; &quot;value&quot;: 60.0 &#125; &#125;, &#123; &quot;key_as_string&quot;: &quot;2015/03/01 00:00:00&quot;, &quot;key&quot;: 1425168000000, &quot;doc_count&quot;: 2, &quot;sales&quot;: &#123; &quot;value&quot;: 375.0 &#125; &#125; ] &#125;, &quot;stats_monthly_sales&quot;: &#123; &quot;count&quot;: 3, &quot;min&quot;: 60.0, &quot;max&quot;: 550.0, &quot;avg&quot;: 328.3333333333333, &quot;sum&quot;: 985.0, &quot;sum_of_squares&quot;: 446725.0, &quot;variance&quot;: 41105.55555555556, &quot;variance_population&quot;: 41105.55555555556, &quot;variance_sampling&quot;: 61658.33333333334, &quot;std_deviation&quot;: 202.74505063146563, &quot;std_deviation_population&quot;: 202.74505063146563, &quot;std_deviation_sampling&quot;: 248.3109609609156, &quot;std_deviation_bounds&quot;: &#123; &quot;upper&quot;: 733.8234345962646, &quot;lower&quot;: -77.15676792959795, &quot;upper_population&quot; : 733.8234345962646, &quot;lower_population&quot; : -77.15676792959795, &quot;upper_sampling&quot; : 824.9552552551645, &quot;lower_sampling&quot; : -168.28858858849787 &#125; &#125; &#125;&#125; percentiles_bucket 桶百分比聚合 Percentiles Bucket ，桶百分比聚合。基于兄弟聚合的某个权值，计算权值的百分比。用于计算的权值必须是数值类型。用于计算的兄弟聚合必须是多桶聚合类型。对百分比的计算是精确的（不像Percentiles Metric聚合是近似值），所以可能会消耗大量内存 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667POST /sales/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;sales_per_month&quot;: &#123; &quot;date_histogram&quot;: &#123; &quot;field&quot;: &quot;date&quot;, &quot;calendar_interval&quot;: &quot;month&quot; &#125;, &quot;aggs&quot;: &#123; &quot;sales&quot;: &#123; &quot;sum&quot;: &#123; &quot;field&quot;: &quot;price&quot; &#125; &#125; &#125; &#125;, &quot;percentiles_monthly_sales&quot;: &#123; &quot;percentiles_bucket&quot;: &#123; &quot;buckets_path&quot;: &quot;sales_per_month&gt;sales&quot;, &quot;percents&quot;: [ 25.0, 50.0, 75.0 ] &#125; &#125; &#125;&#125;返回：&#123; ... &quot;aggregations&quot;: &#123; &quot;sales_per_month&quot;: &#123; &quot;buckets&quot;: [ &#123; &quot;key_as_string&quot;: &quot;2015/01/01 00:00:00&quot;, &quot;key&quot;: 1420070400000, &quot;doc_count&quot;: 3, &quot;sales&quot;: &#123; &quot;value&quot;: 550.0 &#125; &#125;, &#123; &quot;key_as_string&quot;: &quot;2015/02/01 00:00:00&quot;, &quot;key&quot;: 1422748800000, &quot;doc_count&quot;: 2, &quot;sales&quot;: &#123; &quot;value&quot;: 60.0 &#125; &#125;, &#123; &quot;key_as_string&quot;: &quot;2015/03/01 00:00:00&quot;, &quot;key&quot;: 1425168000000, &quot;doc_count&quot;: 2, &quot;sales&quot;: &#123; &quot;value&quot;: 375.0 &#125; &#125; ] &#125;, &quot;percentiles_monthly_sales&quot;: &#123; &quot;values&quot; : &#123; &quot;25.0&quot;: 375.0, &quot;50.0&quot;: 375.0, &quot;75.0&quot;: 550.0 &#125; &#125; &#125;&#125; 总结，ES默认给 大多数 字段启用 doc values，所以在一些搜索场景大大的节省了内存使用量，但是需要注意的是只有不分词的 string 类型的字段才能使用这种特性。使聚合运行在 not_analyzed 字符串而不是 analyzed 字符串，这样可以有效的利用 doc values 客户端使用 spring boot 官方注解文档 官方文档 ElasticsearchOperations ElasticsearchOperations 客户端 ES7之后不推荐使用了，建议使用REST API 添加maven依赖 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-elasticsearch&lt;/artifactId&gt;&lt;/dependency&gt; 配置连接信息 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485@Configurationpublic class ElasticsearchClientConfig &#123; /** 协议 */ @Value(&quot;$&#123;elasticsearch.scheme:http&#125;&quot;) private String scheme; /** 集群地址，如果有多个用,隔开 */ @Value(&quot;$&#123;elasticsearch.address:localhost:9200&#125;&quot;) private String address; /** 获取连接的超时时间 */ @Value(&quot;$&#123;elasticsearch.connectionRequestTimeout:5000&#125;&quot;) private Integer connectionRequestTimeout; /** Socket 连接超时时间 */ @Value(&quot;$&#123;elasticsearch.socketTimeout:5000&#125;&quot;) private Integer socketTimeout; /** 连接超时时间 */ @Value(&quot;$&#123;elasticsearch.connectTimeout:5000&#125;&quot;) private Integer connectTimeout; /** 最大连接数 */ @Value(&quot;$&#123;elasticsearch.maxConnectNum:20&#125;&quot;) private Integer maxConnectNum; /** 最大路由连接数 */ @Value(&quot;$&#123;elasticsearch.maxConnectPerRoute:20&#125;&quot;) private Integer maxConnectPerRoute; @Bean public RestClientBuilder restClientBuilder() &#123; HttpHost[] hosts = Arrays.stream(address.split(&quot;,&quot;)) .map(this::makeHttpHost) .filter(Objects::nonNull) .toArray(HttpHost[]::new); RestClientBuilder restClientBuilder = RestClient.builder(hosts); // 设置一个监听器，每次节点出现故障时都会收到通知// restClientBuilder.setFailureListener(new RestClient.FailureListener() &#123;// @Override// public void onFailure(Node node) &#123;//// &#125;// &#125;); // 异步连接数配置 restClientBuilder.setHttpClientConfigCallback(httpClientBuilder -&gt; &#123; httpClientBuilder.setMaxConnTotal(maxConnectNum); httpClientBuilder.setMaxConnPerRoute(maxConnectPerRoute); return httpClientBuilder; &#125;); // 异步连接延时配置 restClientBuilder.setRequestConfigCallback(requestConfigBuilder -&gt; &#123; requestConfigBuilder.setConnectionRequestTimeout(connectionRequestTimeout); requestConfigBuilder.setSocketTimeout(socketTimeout); requestConfigBuilder.setConnectTimeout(connectTimeout); return requestConfigBuilder; &#125;); return restClientBuilder; &#125; @Bean(name = &quot;restHighLevelClient&quot;) public RestHighLevelClient restHighLevelClient(RestClientBuilder restClientBuilder) &#123; return new RestHighLevelClient(restClientBuilder); &#125; /** * 根据配置创建HttpHost * @param s * @return */ private HttpHost makeHttpHost(String s) &#123; int flag = s.lastIndexOf(&quot;:&quot;); if (flag &gt; 0) &#123; String host = s.substring(0, flag); String port = s.substring(flag + 1); return new HttpHost(host, Integer.parseInt(port), scheme); &#125; return new HttpHost(s); &#125;&#125; 创建实体对应索引 1234567891011121314151617@Data@ToString@Accessors(chain = true)@Document(indexName = &quot;goods&quot;)public class Goods &#123; @Id private Integer id; @Field(type = FieldType.Keyword) private String title; @Field(type = FieldType.Float) private Double price; @Field(type = FieldType.Text, analyzer = &quot;ik_max_word&quot;) private String description;&#125; 使用 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061@SpringBootTestpublic class OperationsTest &#123; @Autowired private ElasticsearchOperations elasticsearchOperations; /** * 保存和更新 */ @Test public void save() &#123; Goods goods = new Goods() .setId(1) .setTitle(&quot;小米手机&quot;) .setPrice(2199d) .setDescription(&quot;这是一个好手机&quot;); this.elasticsearchOperations.save(goods); &#125; /** * 删除文档 */ @Test public void delete() &#123; Goods goods = new Goods() .setId(1); this.elasticsearchOperations.delete(goods); &#125; /** * 删除所有文档 */ @Test public void deleteAll()&#123; this.elasticsearchOperations.delete(Query.findAll(), Goods.class); &#125; /** * 查询文档 */ @Test public void query() &#123; Goods goods = this.elasticsearchOperations.get(&quot;1&quot;, Goods.class); System.out.println(); &#125; @Test public void queryAll() &#123; SearchHits&lt;Goods&gt; search = this.elasticsearchOperations.search(Query.findAll(), Goods.class); search.forEach(x -&gt; &#123; System.out.println(x.getId()); System.out.println(x.getScore()); Goods goods = x.getContent(); System.out.println(goods); &#125;); &#125;&#125; RestHighLevelClient 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218public class RestHighLevelClientTest &#123; // 使用 spring-data-elasticsearch 可以直接注入使用 @Autowired private RestHighLevelClient restHighLevelClient; /** * 创建索引 * PUT /goods * &#123; * &quot;mappings&quot;: &#123; * &quot;properties&quot;: &#123; * &quot;title&quot;: &#123; * &quot;type&quot;: &quot;keyword&quot; * &#125;, * &quot;price&quot;: &#123; * &quot;type&quot;: &quot;double&quot; * &#125;, * &quot;created_at&quot;: &#123; * &quot;type&quot;: &quot;date&quot; * &#125;, * &quot;description&quot;: &#123; * &quot;type&quot;: &quot;text&quot;, * &quot;analyzer&quot;: &quot;ik_max_word&quot; * &#125; * &#125; * &#125; * &#125; */ @Test public void createIndex() throws IOException &#123; // 索引脚本，取mappings对应的部分 String json = &quot;&#123;\\n&quot; + &quot; \\&quot;properties\\&quot;: &#123;\\n&quot; + &quot; \\&quot;title\\&quot;: &#123;\\n&quot; + &quot; \\&quot;type\\&quot;: \\&quot;keyword\\&quot;\\n&quot; + &quot; &#125;,\\n&quot; + &quot; \\&quot;price\\&quot;: &#123;\\n&quot; + &quot; \\&quot;type\\&quot;: \\&quot;double\\&quot;\\n&quot; + &quot; &#125;,\\n&quot; + &quot; \\&quot;created_at\\&quot;: &#123;\\n&quot; + &quot; \\&quot;type\\&quot;: \\&quot;date\\&quot;\\n&quot; + &quot; &#125;,\\n&quot; + &quot; \\&quot;description\\&quot;: &#123;\\n&quot; + &quot; \\&quot;type\\&quot;: \\&quot;text\\&quot;,\\n&quot; + &quot; \\&quot;analyzer\\&quot;: \\&quot;ik_max_word\\&quot;\\n&quot; + &quot; &#125;\\n&quot; + &quot; &#125;\\n&quot; + &quot; &#125;&quot;; // 创建索引请求 CreateIndexRequest createIndexRequest = new CreateIndexRequest(&quot;goods&quot;); // 指定索引映射 createIndexRequest.mapping(json, XContentType.JSON); CreateIndexResponse createIndexResponse = this.restHighLevelClient.indices().create(createIndexRequest, RequestOptions.DEFAULT); System.out.println(createIndexResponse.isAcknowledged()); &#125; /** * 索引文档，添加数据 * PUT /goods/_doc/1 * &#123; * &quot;title&quot;:&quot;小米手机&quot;, * &quot;price&quot;:&quot;1999&quot;, * &quot;created_at&quot;:&quot;2020-05-06&quot;, * &quot;description&quot;:&quot;这是一个好手机&quot; * &#125; */ @Test public void index() throws IOException &#123; // 文档内容 String json = &quot;&#123;\\n&quot; + &quot; \\&quot;title\\&quot;:\\&quot;小米手机\\&quot;,\\n&quot; + &quot; \\&quot;price\\&quot;:\\&quot;1999\\&quot;,\\n&quot; + &quot; \\&quot;created_at\\&quot;:\\&quot;2020-05-06\\&quot;,\\n&quot; + &quot; \\&quot;description\\&quot;:\\&quot;这是一个好手机\\&quot;\\n&quot; + &quot;&#125;&quot;; // 指定索引 IndexRequest indexRequest = new IndexRequest(&quot;goods&quot;).id(&quot;1&quot;); // 设置内容 indexRequest.source(json, XContentType.JSON); IndexResponse response = restHighLevelClient.index(indexRequest, RequestOptions.DEFAULT); System.out.println(response.status()); &#125; /** * 更新文档 * POST /goods/_doc/1/_update * &#123; * &quot;doc&quot;: &#123; * &quot;price&quot;: &quot;小米手机&quot; * &#125; * &#125; */ @Test public void update() throws IOException &#123; UpdateRequest updateRequest = new UpdateRequest(&quot;goods&quot;, &quot;1&quot;); updateRequest.doc(&quot;&#123;\\&quot;price\\&quot;:\\&quot;1666\\&quot;&#125;&quot;, XContentType.JSON); UpdateResponse update = this.restHighLevelClient.update(updateRequest, RequestOptions.DEFAULT); System.out.println(update.status()); &#125; /** * 删除文档 * DELETE /goods/_doc/1 * @throws IOException */ @Test public void delete() throws IOException &#123; DeleteRequest deleteRequest = new DeleteRequest(&quot;goods&quot;, &quot;1&quot;); DeleteResponse delete = this.restHighLevelClient.delete(deleteRequest, RequestOptions.DEFAULT); System.out.println(delete.status()); &#125; /** * 根据id查询文档 * GET /goods/_doc/1 */ @Test public void get() throws IOException &#123; GetRequest getRequest = new GetRequest(&quot;goods&quot;, &quot;1&quot;); GetResponse getResponse = this.restHighLevelClient.get(getRequest, RequestOptions.DEFAULT); System.out.println(getResponse.getSourceAsString()); &#125; /** * 查询所有文档 * GET /goods/_search * &#123; * &quot;query&quot;: &#123; * &quot;match_all&quot;: &#123;&#125; * &#125; * &#125; * @throws IOException */ @Test public void searchAll() throws IOException &#123; SearchRequest searchRequest = new SearchRequest(&quot;goods&quot;); // 查询条件构造 SearchSourceBuilder searchSourceBuilder = new SearchSourceBuilder(); searchSourceBuilder.query(QueryBuilders.matchAllQuery()); searchRequest.source(searchSourceBuilder); SearchResponse response = this.restHighLevelClient.search(searchRequest, RequestOptions.DEFAULT); SearchHits hits = response.getHits(); // 获取hits 结果集 for (SearchHit hit : hits) &#123; System.out.println(hit.getSourceAsString()); &#125; &#125; /** * 综合查询 * GET /goods/_search * &#123; * &quot;query&quot;: &#123; * &quot;term&quot;: &#123; * &quot;description&quot;: &#123; * &quot;value&quot;: &quot;手机&quot; * &#125; * &#125; * &#125;, * &quot;from&quot;: 0, * &quot;size&quot;: 20, * &quot;sort&quot;: [ * &#123; * &quot;price&quot;: &#123; * &quot;order&quot;: &quot;desc&quot; * &#125; * &#125; * ], * &quot;_source&quot;: [&quot;title&quot;, &quot;description&quot;], * &quot;highlight&quot;: &#123; * &quot;pre_tags&quot;: [&quot;&lt;span color=&#x27;red&#x27;&gt;&quot;], * &quot;post_tags&quot;: [&quot;&lt;/span&gt;&quot;], * &quot;require_field_match&quot;: &quot;false&quot;, * &quot;fields&quot;: &#123; * &quot;description&quot;: &#123;&#125; * &#125; * &#125; * &#125; * @throws IOException */ @Test public void searchTest() throws IOException &#123; SearchRequest searchRequest = new SearchRequest(&quot;goods&quot;); // 查询条件构造 SearchSourceBuilder searchSourceBuilder = new SearchSourceBuilder(); searchSourceBuilder .query(QueryBuilders.termQuery(&quot;description&quot;, &quot;手机&quot;)) .from(0) .size(20) .sort(&quot;price&quot;, SortOrder.DESC) .fetchSource(new String[]&#123;&quot;title&quot;, &quot;description&quot;&#125;, new String[]&#123;&#125;) .highlighter(new HighlightBuilder().field(&quot;description&quot;).preTags(&quot;&lt;span color=&#x27;red&#x27;&gt;&quot;).postTags(&quot;&lt;/span&gt;&quot;).requireFieldMatch(false)); searchRequest.source(searchSourceBuilder); // 请求 SearchResponse response = this.restHighLevelClient.search(searchRequest, RequestOptions.DEFAULT); System.out.println(&quot;总条数: &quot;+response.getHits().getTotalHits().value); SearchHit[] hits = response.getHits().getHits(); for (SearchHit hit : hits) &#123; System.out.println(hit.getSourceAsString()); Map&lt;String, HighlightField&gt; highlightFields = hit.getHighlightFields(); highlightFields.forEach((k,v)-&gt; System.out.println(&quot;key: &quot;+k + &quot; value: &quot;+v.fragments()[0])); &#125; &#125;&#125; BBossElastic ORM框架 官方文档 使用 BBossElastic ORM框架最大的好处是能像Mybatis那样通过配置文件动态生成DSL语句 添加maven依赖 12345678910&lt;dependency&gt; &lt;groupId&gt;com.bbossgroups.plugins&lt;/groupId&gt; &lt;artifactId&gt;bboss-elasticsearch-rest-jdbc&lt;/artifactId&gt; &lt;version&gt;6.5.8&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.bbossgroups.plugins&lt;/groupId&gt; &lt;artifactId&gt;bboss-elasticsearch-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;6.5.8&lt;/version&gt;&lt;/dependency&gt; 添加配置 12345678910111213141516171819202122232425262728293031spring: elasticsearch: bboss: elasticUser: elasticPassword: elasticsearch: rest: hostNames: 121.37.23.172:9200 ##hostNames: 192.168.8.25:9200,192.168.8.26:9200,192.168.8.27:9200 ##集群地址配置 dateFormat: yyyy-MM-dd timeZone: Asia/Shanghai showTemplate: true discoverHost: false dslfile: refreshInterval: -1 http: timeoutConnection: 5000 timeoutSocket: 5000 connectionRequestTimeout: 5000 retryTime: 1 maxLineLength: -1a maxHeaderCount: 200 maxTotal: 400 defaultMaxPerRoute: 200 soReuseAddress: false soKeepAlive: false timeToLive: 3600000 keepAlive: 3600000 keystore: keyPassword: hostnameVerifier: 添加配置文件 在 resource/es目录下添加配置文件 123456789101112131415161718192021222324&lt;properties&gt; &lt;property name=&quot;searchDatas&quot;&gt; &lt;![CDATA[ &#123; &quot;query&quot;: &#123; &quot;term&quot;: &#123; &quot;description&quot;: &#123; &quot;value&quot;: #[description] &#125; &#125; &#125;#if($from),#end #if($from) &quot;from&quot;: #[from] #end #if($from),#end #if($size) &quot;size&quot;: #[size] #end &#125; ]]&gt; &lt;!-- #if($from),#end 用于判断是否要拼接逗号 --&gt; &lt;/property&gt;&lt;/properties&gt; 使用 12345678910111213141516171819202122232425262728293031323334@SpringBootTestpublic class BBossTest &#123; @Autowired private BBossESStarter bBossESStarter; /** * BBossSearch框架动态DSL * https://esdoc.bbossgroups.com/#/development?id=_53-dsl%e9%85%8d%e7%bd%ae%e8%a7%84%e8%8c%83 * */ @Test public void search() &#123; ClientInterface clientUtil = ElasticSearchHelper.getConfigRestClientUtil(&quot;es/goods.xml&quot;); Map&lt;String, Object&gt; param = new HashMap&lt;&gt;(); param.put(&quot;description&quot;, &quot;手机&quot;); // 动态参数,在xml文件里进行IF判断 // https://esdoc.bbossgroups.com/#/development?id=_5316-%e9%80%bb%e8%be%91%e5%88%a4%e6%96%ad%e8%af%ad%e6%b3%95 param.put(&quot;from&quot;, 0); param.put(&quot;size&quot;, 20); // 执行查询 // 第一个参数是请求API， 和Kibana是一样的 // 第二个是xml脚本配置对应的模板名称 // 第三个是请求参数 // 第四个是返回类型 ESDatas&lt;Goods&gt; goodsESDatas = clientUtil.searchList(&quot;/goods/_search&quot;, &quot;searchDatas&quot;, param, Goods.class); // 获取总条数 long totalSize = goodsESDatas.getTotalSize(); goodsESDatas.getDatas().forEach(System.out::println); &#125;&#125; Java API 高级进阶 集群原理 节点通过设置集群名称，在同一网络中发现具有相同集群名称的节点，组成集群。每个集群都有一个 集群名称 作为标识，默认的集群名称为 elasticsearch。如果在同一网络中只有一个节点，则这个节点成为一个单节点集群 集群就是同一网络下，具有相同集群名称的ES服务节点的集合 集群健康 集群健康 集群健康API 集群健康 = 索引健康 集群架构 集群节点、分片、副本 P0、P1、P2为主分片 R0、R1、R2为副本 Node-1为主节点 Node-2、Node-3为数据节点 节点 简单来讲，节点 就是一个ElasticSearch进程，当我们启动一个ElasticSearch程序，就启动了一个节点，很多个节点集合在一起就成了 集群，即使只有单个节点，也可以把它看成只有单个节点的 集群。一个节点可以是物理机器或虚拟机器。每个节点具有唯一的名称、唯一标识符和网络地址。节点通过网络通信进行交互和协作，以处理搜索请求、存储数据、传输分片和执行各种管理任务 节点可以同时执行多种任务，并且集群中的所有节点都可以承载数据、执行搜索和运行聚合查询。当您在一个节点上索引数据时，Elasticsearch 会自动将数据分配到适当数量的分片中，并将分片分配给集群内的节点。节点之间协调工作是通过使用 Elasticsearch 集群协议实现的，该协议基于 Gossip ES集群节点角色功能 角色 数量 功能 master节点 1 master节点控制整个集群的元数据。只有Master Node节点可以修改节点状态信息及元数据(metadata)的处理，比如索引的新增、删除、分片路由分配、所有索引和相关 Mapping 、Setting 配置等等 master备选节点 n 有资格成为Master节点但暂时并不是Master的节点被称为 eligible 节点，该节点只是与集群保持心跳，判断Master是否存活，如果Master故障则参加新一轮的Master选举，通过在配置文件中设置 node.master=true 来设置该节点成为候选主节点，在 Elasticsearch 集群中只有候选节点才有选举权和被选举权。其他节点是不参与选举工作的 数据节点 n 数据节点，负责数据的存储和相关具体操作，比如索引数据的创建、修改、删除、搜索、聚合 协调节点 n 协调节点接受客户端搜索请求后将请求转发到与查询条件相关的多个data节点的分片上，然后多个data节点的分片执行查询语句或者查询结果再返回给协调节点，协调节点把各个data节点的返回结果进行整合、排序等一系列操作后再将最终结果返回给用户请求。协调节点，是一种角色，而不是真实的Elasticsearch的节点，你没有办法通过配置项来配置哪个节点为协调节点。集群中的任何节点，都可以充当协调节点的角色 Ingest节点 n 预处理节点,可以在实际的 indexing（索引）发生之前使用 ingest node 来预处理 documents（文档）,类似于拦截器的存在 分片 Shard 写操作 Shards（分片）是 Elasticsearch 中将索引划分成多个逻辑分区的过程。默认情况下，在 Elasticsearch 中每个分片都是一个独立的 Lucene 索引。索引分片可以在集群中的不同节点上分配，以便数据可以更快地被搜索和处理。 将索引分割成分片可以满足以下需求： 海量数据存储：将一个索引划分多个分片分别存储在不同的服务器上 提高性能：每个分片只需要处理部分数据，可以并行地在多个节点上处理数据，从而提高搜索和查询的性能 负载均衡：分片的负载均衡功能可以确保索引数据在集群中的不同节点上均匀分布，从而使每个节点的负载更加均衡，防止某个节点被过度请求而导致性能问题 至于一个分片怎样分布，它的文档怎样聚合和搜索请求，是完全由Elasticsearch管理的，对于作为用户的你来说，这些都是透明的，无需过分关心，增减节点时，shard会自动在nodes中 负载均衡 分片主要解决海量数据存储和提高吞吐量 副本 Replica 故障转移 数据读取 在 Elasticsearch 中，为了提高系统的容错性和可用性，可以为每个分片（Shard）创建零个或多个副本，这些副本称为 Replica（副本）。每个副本都是其所属分片的完整副本，也就是说它包含与主分片（Primary Shard）完全相同的数据 具有多个副本的 Shard 可以提供以下好处： 故障转移：如果主分片发生故障，副本节点可以接管主分片的工作 提高吞吐量：由于 ElasticSearch 可以访问多个分片，因此在同时处理大量查询时，使用 Replica 可以提高搜索速度和性能 副本主要保证集群的高可用，提供故障转移能力，并且提高集群吞吐量 分配 Allocation 将分片分配给某个节点的过程，包括分配主分片或者副本。如果是副本，还包含从主分片复制数据的过程。这个过程是由master节点完成的 Allocation 的目标是要实现数据的高可用性和负载均衡，确保数据在集群中的备份，从而降低单点故障的风险，提高整个系统的稳定性。同时，它还可以确保在节点宕机时能够自动地重新分配分片，以避免数据的损失 搭建集群 注意 所有节点集群名称必须一致 cluster.name 每个节点必须有一个唯一名字 node.name 开启每个节点远程连接 network.host: 0.0.0.0 指定使用 IP地址进行集群节点通信 network.publish_host: 修改 web 端口 tcp 端口 http.port: transport.tcp.port 指定集群中所有节点通信列表 discovery.seed_hosts: node-1 node-2 node-3 相同 允许集群初始化 master 节点节点数: cluster.initial_master_nodes: [“node-1”, “node-2”,“node-3”] 集群最少几个节点可用 gateway.recover_after_nodes: 2 开启每个节点跨域访问http.cors.enabled: true http.cors.allow-origin: “*” 三个节点的配置文件 node-1 123456789101112131415161718192021# 指定集群名称 3个节点必须一致cluster.name: es-cluster# 指定节点名称 每个节点名字唯一node.name: node-1# 开放远程链接network.host: 0.0.0.0# 指定使用发布地址进行集群间通信network.publish_host: 127.0.0.1# 指定 web 端口http.port: 9201# 指定 tcp 端口transport.tcp.port: 9301# 指定所有节点的 tcp 通信discovery.seed_hosts: [&quot;127.0.0.1:9301&quot;, &quot;127.0.0.1:9302&quot;,&quot;127.0.0.1:9303&quot;]# 指定可以初始化集群的节点名称cluster.initial_master_nodes: [&quot;node-1&quot;, &quot;node-2&quot;,&quot;node-3&quot;]# 集群最少几个几点可用gateway.recover_after_nodes: 2# 解决跨域问题http.cors.enabled: truehttp.cors.allow-origin: &quot;*&quot; node-2 123456789101112131415161718192021# 指定集群名称 3个节点必须一致cluster.name: es-cluster# 指定节点名称 每个节点名字唯一node.name: node-2# 开放远程链接network.host: 0.0.0.0# 指定使用发布地址进行集群间通信network.publish_host: 127.0.0.1# 指定 web 端口http.port: 9202# 指定 tcp 端口transport.tcp.port: 9302# 指定所有节点的 tcp 通信discovery.seed_hosts: [&quot;127.0.0.1:9301&quot;, &quot;127.0.0.1:9302&quot;,&quot;127.0.0.1:9303&quot;]# 指定可以初始化集群的节点名称cluster.initial_master_nodes: [&quot;node-1&quot;, &quot;node-2&quot;,&quot;node-3&quot;]# 集群最少几个几点可用gateway.recover_after_nodes: 2# 解决跨域问题http.cors.enabled: truehttp.cors.allow-origin: &quot;*&quot; node-3 123456789101112131415161718192021# 指定集群名称 3个节点必须一致cluster.name: es-cluster# 指定节点名称 每个节点名字唯一node.name: node-3# 开放远程链接network.host: 0.0.0.0# 指定使用发布地址进行集群间通信network.publish_host: 127.0.0.1# 指定 web 端口http.port: 9203# 指定 tcp 端口transport.tcp.port: 9303# 指定所有节点的 tcp 通信discovery.seed_hosts: [&quot;127.0.0.1:9301&quot;, &quot;127.0.0.1:9302&quot;,&quot;127.0.0.1:9303&quot;]# 指定可以初始化集群的节点名称cluster.initial_master_nodes: [&quot;node-1&quot;, &quot;node-2&quot;,&quot;node-3&quot;]# 集群最少几个几点可用gateway.recover_after_nodes: 2# 解决跨域问题http.cors.enabled: truehttp.cors.allow-origin: &quot;*&quot; 查看集群状态 访问任意节点：http://192.168.0.29:9201/_cat/nodes?v Kibana整合 修改 Kibana 配置文件 12345# kibana配置文件 连接到ESserver.host: &quot;0&quot;server.shutdownTimeout: &quot;5s&quot;elasticsearch.hosts: [ &quot;http://127.0.0.1:9201&quot; ] #链接任意节点即可monitoring.ui.container.elasticsearch.enabled: true Head 插件 使用 chrome 扩展插件 故障转移 当集群中只有一个节点在运行时，意味着会有一个单点故障问题——没有冗余。 幸运的是，我们只需再启动一个节点即可防止数据丢失。当你在 同一台机器上 启动了第二个节点时，只要它和第一个节点有同样的cluster.name配置，它就会自动发现集群并加入到其中。但是在不同机器上启动节点的时候，为了加入到同一集群，你需要配置一个可连接到的 单播主机列表。之所以配置为使用单播发现，以 防止节点无意中加入集群。只有在同一台机器上运行的节点才会自动组成集群 单播主机列表 水平扩容 主分片 在创建索引的时候就被定死了，是不可以扩容的。唯一能做的是扩展副本数量提高吞吐量 怎样为我们的正在增长中的应用程序按需扩容呢？当启动了第三个节点，我们的集群将会拥有三个节点的集群：为了分散负载而对分片进行重新分配 节点扩容 集群由两个节点扩展到三个节点时的变化 Node 1 和 Node 2 上各有一个分片被迁移到了新的 Node 3 节点，现在每个节点上都拥有 2 个分片， 而不是之前的 3 个。 这表示每个节点的硬件资源（CPU, RAM, I/O）将被更少的分片所共享，每个分片 的性能将会得到提升 分片是一个功能完整的搜索引擎，它拥有使用一个节点上的所有资源的能力。 我们这个拥有 6 个分 片（3 个主分片和 3 个副本分片）的索引可以最大扩容到 6 个节点，每个节点上存在一个分片，并且每个 分片拥有所在节点的全部资源 但是如果我们想要扩容超过 6 个节点怎么办呢？ 副本扩容 主分片的数目在索引创建时就已经确定了下来。实际上，这个数目定义了这个索引能够存储的最大数据量。（实际大小取决于你的数据、硬件和使用场景） 但是，读操作、 搜索和返回数据可以同时被 主分片 或 副本 所处理，所以当你拥有越多的 副本 时，也将拥有越高的吞吐量。 在运行中的集群上是可以动态调整副本分片数目的，我们可以按需伸缩集群。让我们把 副本数从默认的 1 增加到 2 123456789PUT product/_settings&#123; &quot;number_of_replicas&quot; : 2&#125;返回：&#123; &quot;acknowledged&quot;: true&#125; product索引现在拥有 9 个分片：3 个主分片和 6 个副本分片。 这意味着我们可以将集群 扩容到 9 个节点，每个节点上一个分片。相比原来 3 个节点时，集群搜索性能可以提升 3 倍 当然，如果只是在相同节点数目的集群上增加更多的副本分片并不能提高性能，因为每个分片从节点上获得的资源会变少。 你需要增加更多的硬件资源来提升吞吐量。 但是更多的副本分片数提高了数据冗余量，按照上面的节点配置，我们可以在失去 2 个节点（同一个分片的两个副本）的情况下不丢失任何数据 重建索引 当某个索引的数据越来越庞大时，在建立索引时指定的分片数量不足以支撑后续的业务操作，这时就需要考虑重建索引了，建立一个新的索引重新指定分片数量，具体步骤如下： 重新根据索引映射建立一个数据类型一样的新索引，并重新指定分片数量（先不要指定副本） 使用重 建索引API，将索引数据重新导入新索引中 修改副本数量，在索引重建完成后修改副本数量有利于索引重建和副本数据拷贝效率 删除老索引 新索引 别名 配置老索引名称 应对故障 我们关闭第一个节点，这时集群的状态为：关闭了一个节点后的集群 我们关闭的节点是一个主节点。而集群必须拥有一个主节点来保证正常工作，所以发生的第一件事情就是选举一个新的主节点，在我们关闭 Node 2 的同时也失去了主分片 2 ，并且在缺失主分片的时候索引也不能正常工作。 如果此时来检查集群的状况，我们看到的状态将会为 red ，不是所有主分片都在正常工作 幸运的是，在其它节点上存在着这两个主分片的完整副本， 所以新的主节点立即将这 些分片在 Node 1 和 Node 3 上对应的副本分片提升为 主分片，此时集群的状态将会为 yellow。这个提升主分片的过程是瞬间发生的，如同按下一个开关一般 为什么集群状态是 yellow 原因: 至少有一个副本没有被正确分配 虽然我们拥有所有的三个主分片，但是同时设置了每个主分片需要对应 2 份副本分片，而此时只存在一份副本分片。 所以集群不能为 green 的状态，不过我们不必过于担心,如果我们同样关闭了 Node 3 ，我们的程序依然可以保持在不丢任何数据的情况下运行，因为 Node 1 为每一个分片都保留着一份副本。 如果我们重新启动 Node 2 ，集群可以将缺失的副本分片再次进行分配，那么集群的状 态也将恢复成之前的状态。 如果 Node 2 依然拥有着之前的分片，它将尝试去重用它们， 同时仅从主分片复制发生了修改的数据文件。和之前的集群相比，只是 Master 节点切换了 总结：主节点挂了马上选举其他节点作为新的主节点，如果宕机节点上有主分片，则新的主节点马上会将其他副本提升为主分片保证高可用 分布式文档存储 路由计算 自定义路由 当索引一个文档的时候，文档会被存储到一个主分片中。 Elasticsearch 如何知道一个文档应该存放到哪个分片中呢？当我们创建文档时，它如何决定这个文档应当被存储在分片 1 还是分片 2 中呢？首先这肯定不会是随机的，否则将来要获取文档的时候我们就不知道从何处寻找了。实际上，这个过程是根据下面这个公式决定的 shard = hash(routing) % number_of_primary_shards routing 是一个可变值，默认是文档的 _id ，也可以设置成一个自定义的值。 routing 通过 hash 函数生成一个数字，然后这个数字再除以 number_of_primary_shards （主分片的数量）后得到余数 。这个分布在 0 到 number_of_primary_shards-1 之间的余数，就是我们所寻求的文档所在分片的位置 这就解释了为什么我们要在创建索引的时候就确定好主分片的数量并且永远不会改变这个数量：因为如果数量变化了，那么所有之前路由的值都会无效，文档也再也找不到了 所有的文档 API（ get 、 index 、 delete 、 bulk 、 update 以及 mget ）都接受一 个叫做 routing 的路由参数 ，通过这个参数我们可以自定义文档到分片的映射。一个自定义的路由参数可以用来确保所有相关的文档——例如所有属于同一个用户的文档——都被 存储到同一个分片中 和mysql分表通过分片键hash取模一个原理 分片控制 我们假设有一个集群由三个节点组成。 它包含一个叫 product的索引，有两个主分片， 每个主分片有两个副本分片。相同分片的副本不会放在同一节点 通过 elasticsearch-head 插件查看集群情况，所以我们的集群是一个有三个节点和一个索 引的集群 我们可以发送请求到集群中的任一节点。 每个节点都有能力处理任意请求。 每个节点都知道集群中任一文档位置，所以可以直接将请求转发到需要的节点上（比如插入访问node2 实际主分片在node1 则node2-&gt;node1），将所有的请求发送到 Node 1，我们将其称为 协调节点(coordinating node, node2) 当发送请求的时候， 为了扩展负载，更好的做法是 轮询 集群中所有的节点，因此数据在查询时，每个分片对应的节点采用的是轮询访问机制 写流程 新建、修改和删除 请求都是写操作， 必须在主分片上面完成之后才能被复制到相关的副本分片 新建，更新和删除文档所需要的步骤顺序： 客户端向 Node 1（轮询） 发送新建、索引或者删除请求 节点使用文档的 _id 确定文档属于分片 0 (每个节点都有每个分片存储在哪个节点的信息，因此协调节点会将请求发送给对应的节点) 。请求会被转发到 Node 3，因为分片 0 的 主分片目前被分配在 Node 3上 Node 3 在主分片上面执行请求。如果成功了，它将请求并行转发到 Node 1 和 Node 2 的副本分片上。一旦所有的副本分片都报告成功, Node 3 将向协调节点报告成功，协调节点向客户端报告成功 在客户端收到成功响应时，文档变更已经在主分片和所有副本分片执行完成，变更是安全的。 有一些可选的请求参数允许您影响这个过程，可能以数据安全为代价提升性能。这些选项很少使用，因为 Elasticsearch 已经很快，但是为了完整起见，请参考一致性原理 副本数量越多，写入时间越长 详细写流程 写一致性保证 wait_for_active_shards 这个参数是指，索引请求返回前需要等待多少个分片写入成功，默认是1，只要主分片写入成功就返回，最大值是副本是加1（number_of_replicas+1），也就是等待主分片和副本都写入成功，请求才返回，代价当然是写入线程会阻塞，这很好理解 1234PUT /test_index/_doc/1?wait_for_active_shards=2&amp;timeout=10s&#123;&quot;name&quot;:&quot;xiao mi&quot;&#125; 写入超时 如果没有足够的副本分片会发生什么？ Elasticsearch会等待，希望更多的分片出现。默认情况下，它最多等待1分钟。 如果你需要，你可以使用 timeout 参数 使它更早终止： 100 100毫秒，30s 是30秒 读流程 我们可以从主分片或者从其它任意副本分片检索文档 (协调节点) 从主分片或者副本分片检索文档的步骤顺序: 客户端向Node 1发送获取请求 节点使用文档的 _id 来确定文档属于分片0。分片0的副本分片存在于所有的三个节点上。 在这种情况下，它将请求转发到Node 2 Node 2将文档返回给Node 1，然后将文档返回给客户端 在处理读取请求时，协调结点 在每次请求的时候都会通过轮询所有的副本分片来达到负载均衡。 在文档被检索时，已经被索引的文档可能已经存在于主分片上但是还没有复制到副本分片。 在这种情况下，副本分片可能会报告文档不存在，但是主分片可能成功返回文档。 一旦索引请求成功返回给用户，文档在主分片和副本分片都是可用的 更新流程 更新和删除文档的流程 部分更新 一个文档结合了先前说明的 读取 和 写入 流程 部分更新一个文档的步骤如下： 客户端向Node 1发送更新请求 它将请求转发到主分片所在的 Node 3 Node 3从主分片检索文档 (获取原数据)，修改_source字段中的JSON（部分字段覆盖），并且尝试重新索引主分片的文档（原文档标记删除，重新索引一份最新版本文档）。如果文档已经被另一个进程修改，它会重试步骤3，超过 retry_on_conflict（ 默认0 次）次后放弃 (写锁竞争) 如果Node 3成功地更新文档，它将新版本的文档并行转发到Node 1和Node 2上的副本分片 （意味着节点越多，可能写入的时间越长，但是数据查询和完整性更好，这个有参数可控制），重新建立索引。一旦所有副本分片都返回成功，Node 3向协调节点也返回成功，协调节点向客户端返回成功 当主分片把更改转发到副本分片时， 它不会转发更新请求。 相反，它转发完整文档的新版本。请记住，这些更改将会异步转发到副本分片，并且不能保证它们以发送它们相同的顺序到达。 如果Elasticsearch仅转发更改请求，则可能以错误的顺序应用更改，导致得到损坏的文档 执行分布式检索 一个 CRUD 操作只对单个文档进行处理，文档的唯一性由 _index 和 routing values （通常默认是该文档的 _id ）的组合来确定。 这表示我们确切的知道集群中哪个分片含有此文档 搜索需要一种更加复杂的执行模型因为我们不知道查询会命中哪些文档: 这些文档有可能在集群的任何分片上。 一个搜索请求必须询问我们关注的索引（index or indices）的所有分片的某个副本来确定它们是否含有任何匹配的文档 但是找到所有的匹配文档仅仅完成事情的一半。 在 search 接口返回一个 page 结果之前，多分片中的结果必须组合成单个排序列表。 为此，搜索被执行成一个两阶段过程，我们称之为 query then fetch 查询阶段 在初始 查询阶段 时， 查询会广播到索引中每一个分片（主分片或者副本分片）每个分片在本地执行搜索并构建一个匹配文档的 优先队列 优先队列：一个 优先队列 仅仅是一个存有 top-n 匹配文档的有序列表。优先队列的大小取决于分页参数 from 和 size 。例如，如下搜索请求将需要足够大的优先队列来放入100条文档 12345GET /_search&#123; &quot;from&quot;: 90, &quot;size&quot;: 10&#125; 查询阶段包含以下三个步骤: 客户端发送一个 search 请求到 Node 3 ， Node 3 会创建一个大小为 from + size 的空优先队列 Node 3 将查询请求转发到索引的每个主分片或副本分片中。每个分片在本地执行查询并添加结果到大小为 from + size 的本地有序优先队列中 每个分片返回各自优先队列中所有文档的 ID 和排序值给协调节点，也就是 Node 3 ，它合并这些值到自己的优先队列中来产生一个全局排序后的结果列表 当一个搜索请求被发送到某个节点时，这个节点就变成了协调节点。 这个节点的任务是广播查询请求到所有相关分片并将它们的响应整合成全局排序后的结果集合，这个结果集合会返回给客户端 第一步是广播请求到索引中每一个节点的分片拷贝。就像 document GET requests 所描述的， 查询请求可以被某个主分片或某个副本分片处理， 这就是为什么更多的副本（当结合更多的硬件）能够增加搜索吞吐率。 协调节点将在之后的请求中轮询所有的分片拷贝来分摊负载 每个分片在本地执行查询请求并且创建一个长度为 from + size 的优先队列，也就是说，每个分片创建的结果集足够大，均可以满足全局的搜索请求。 分片返回一个轻量级的结果列表到协调节点，它仅包含文档 ID 集合以及任何排序需要用到的值，例如 _score 协调节点将这些分片级的结果合并到自己的有序优先队列里，它代表了全局排序结果集合。至此查询过程结束 一个索引可以由一个或几个主分片组成， 所以一个针对单个索引的搜索请求需要能够把来自多个分片的结果组合起来。 针对 multiple 或者 all 索引的搜索工作方式也是完全一致的—仅仅是包含了更多的分片而已 取回阶段 查询阶段 标识哪些文档满足搜索请求，但是我们仍然需要取回这些文档。这是取回阶段的任务 分布式阶段由以下步骤构成： 协调节点辨别出哪些文档需要被取回并向相关的分片提交多个 GET 请求 每个分片根据 _id 进行文档加载，如果有需要的话，接着返回文档给协调节点 一旦所有的文档都被取回了，协调节点返回结果给客户端 协调节点首先决定哪些文档 确实 需要被取回。例如，如果我们的查询指定了 &#123; &quot;from&quot;: 90, &quot;size&quot;: 10 &#125; ，最初的90个结果会被丢弃，只有从第91个开始的10个结果需要被取回。这些文档可能来自和最初搜索请求有关的一个、多个甚至全部分片 协调节点给持有相关文档的每个分片创建一个 multi-get request ，并发送请求给同样处理查询阶段的分片副本 分片对文档进行加载， 一旦协调节点接收到所有的结果文档，它就组装这些结果为单个响应返回给客户端 深分页解决 搜索选项 7.x参数 有几个查询参数可以影响搜索过程 preference 偏好 preference 是一种查询参数，让你可以指定一个偏好节点或分片来执行你的搜索或聚合操作 值 说明 _only_local 仅返回已在本地节点上缓存的搜索请求结果，而不会发出网络请求。 _local 首选本地分片的副本，但可以使用其他节点上的分片进行搜索。 _only_nodes:&lt;node_ids&gt; 仅在指定的节点上搜索分片及其副本，返回最快的搜索响应。有效的 &lt;node_ids&gt; 列表由节点的 ID 组成，以逗号分隔。 _prefer_nodes:&lt;node_ids&gt; 排除指定的节点，尽可能使用其他节点。有效的 &lt;node_ids&gt; 列表由节点的 ID 组成，以逗号分隔。 _shards:&lt;shard_ids&gt; 指定要搜索的分片。有效的 &lt;shard_ids&gt; 列表由分片 ID 组成，以逗号分隔。默认情况下，将搜索所有可用的分片。 自定义字符串 返回所有分片上偏好字符串相同的文档，通常用于跨多个索引或分片搜索。 12345678&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;Java&quot; &#125; &#125;, &quot;preference&quot;: &quot;_shards:1&quot;&#125; 只在第一个分片执行 超时时间 通常分片处理完它所有的数据后再把结果返回给协同节点，协同节点把收到的所有结果合并为最终结果 这意味着花费的时间是最慢分片的处理时间加结果合并的时间。如果有一个节点有问题，就会导致所有的响应缓慢 参数 timeout 告诉 分片允许处理数据的最大时间。如果没有足够的时间处理所有数据，这个分片的结果可以是部分的，甚至是空数据 搜索的返回结果会用属性 timed_out 标明分片是否返回的是部分结果： 12345678&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;Elasticsearch&quot; &#125; &#125;, &quot;timeout&quot;: &quot;1m&quot;&#125; 123... &quot;timed_out&quot;: true, ... 路由 自定义路由字段 比如可以使用业务字段作为一个路由字段，当使用路由字段（分片键）查询时，就可以避免其他分片的扫描 分片内部原理 ES 按段搜索的发展历程 分片是Elasticsearch最小的工作单元。但是究竟什么是一个分片，它是如何工作的？传统的数据库每个字段存储单个值，但这对全文检索并不够。文本字段中的每个单词需要被搜索，对数据库意味着需要单个字段有索引多值的能力。最好的支持是一个字段多个值需求的数据结构是 倒排索引（适合快速的全文搜索） 分片：类似分表，将一个大的索引分为多个分片，一个分片底层对应的是一个 Lucene 索引，一个 Lucene 索引管理多个段文件，每个段文件就是一个完整的倒排索引 使文本可被搜索 分片是由段组成的 早期的全文检索会为整个文档集合建立一个很大的 倒排索引 并将其写入到磁盘。 一旦新的索引就绪，旧的就会被其替换，这样最近的变化便可以被检索到。 不变性 倒排索引文件被写入磁盘后是 不可改变 的， 不变性有重要的价值： 不需要锁。如果你从来不更新索引，你就不需要担心多进程同时修改数据的问题 一旦索引被读入内核的文件系统缓存，便会留在那里，由于其不变性。只要文件系统缓存中还有足够的空间，那么大部分读请求会直接请求内存，而不会命中磁盘。这提供了很大的性能提升 其它缓存(像filter缓存)，在索引的生命周期内始终有效。它们不需要在每次数据改变时被重建，因为数据不会变化 写入单个大的倒排索引允许数据被压缩，减少磁盘I/O和 需要被缓存到内存的索引的使用量 当然，一个不变的索引也有不好的地方。主要事实是它是不可变的! 你不能修改它。如果你需要让一个新的文档可被搜索，你需要重建整个索引。这要么对一个索引所能包含的数据量造成了很大的限制，要么对索引可被更新的频率造成了很大的限制 动态更新索引 段不可变，如何更新 如何在保留不变性的前提下实现倒排索引的更新? 答案是: 用更多的索引 通过增加新的补充索引来反映新近的修改，而不是直接重写整个倒排索引。每一个倒排索引都会被轮流查询到，从最早的开始查询完后再对结果进行合并 Elasticsearch基于 Lucene 这个java库引入了 按段搜索 的概念。 每一段本身都是一个 倒排索引， 但索引在Lucene中除表示所有段的集合外， 还增加了提交点 的概念（一个列出了所有已知段的文件） 按段搜索 按段搜索会以如下流程执行： 新文档被收集到内存索引缓存 不时地, 缓存被提交 一个新的段（segment 一个追加的倒排索引）被写入磁盘 一个新的包含新段名字的 提交点 被写入磁盘 磁盘进行同步， 所有在文件系统缓存中等待的写入都刷新到磁盘，以确保它们被写入物理文件（写入到一个基于磁盘的段） 新的段被开启，让它包含的文档可见以被搜索 内存缓存被清空，等待接收新的文档 当一个查询被触发，所有已知的段按顺序被查询。词项统计会对所有段的结果进行聚合，以保证每个词和每个文档的关联都被准确计算。 这种方式可以用相对较低的成本将新文档添加到索引 删除和更新 段是不可改变的，所以既不能从段把文档从旧的段中移除，也不能修改旧的段来进行反映文档的更新。 取而代之的是，每个提交点会包含一个 .del文件，文件中会列出这些被删除文档的段信息 当一个文档被 “删除” 时，它实际上只是在 .del文件 中被 标记 删除。一个被标记删除的文档仍然可以被查询匹配到， 但它会在最终结果被返回前从结果集中移除 文档 更新 也是类似的操作方式：当一个文档被更新时，旧版本文档 被标记删除，文档的新版本被索引到一个新的段中。 可能两个版本的文档都会被一个查询匹配到，但被删除的那个旧版本文档在结果集返回前就已经被移除 近实时搜索 优化段的刷盘,降低搜索延迟 随着按段（per-segment）搜索的发展，一个新的文档从索引到可被搜索的延迟显著降低了。新文档在几分钟之内即可被检索，但这样还是不够快。 磁盘在这里成为了瓶颈。提交（Commiting）一个新的段到磁盘需要一次 fsync 来确保段被物理性地写入磁盘，这样在断电的时候就不会丢失数据。 但是 fsync 操作代价很大，如果每次索引一个文档都去执行一次的话会造成很大的性能问题 我们需要的是一个更轻量的方式来使一个文档可被搜索，这意味着 fsync 要从整个主流程中被移除 在Elasticsearch和磁盘之间是文件系统缓存，像之前描述的一样， 在内存索引缓冲区中的文档会被写入到一个新的段中。 但是这里新段会被先写入到文件系统缓存（内存缓存），这一步代价会比较低。稍后再被刷新到磁盘，这一步代价比较高。不过只要文件已经在缓存中，就可以像其它文件一样被打开和读取了 fsync：我们把数据写到磁盘时，通常是先将数据写到操作系统的虚拟文件系统里(filesystem cache)，也既内存中，然后需要调用fsync函数才能把虚拟文件系统里的数据刷到磁盘中，如果不这样做在系统断电的时候就会导致数据丢失 近乎实时的保证 Lucene 允许新段被写入和打开（内存缓存），使其包含的文档在未进行一次完整提交时便对搜索可见。这种方式比进行一次提交代价要小得多，并且在不影响性能的前提下可以被频繁地执行 将内存中的段文件提交到系统缓冲区中，这时即使不进行 fsync 刷盘，使用 虚拟文件系统 便可打开系统缓冲区的段文件，使段可被搜索 在Elasticsearch中，写入和打开一个新段的轻量的过程叫做 refresh（内存数据写入系统缓冲区），默认情况下每个分片会 每秒 自动刷新一次。这就是为什么我们说Elasticsearch是近实时搜索，文档的变化并不是立即对搜索可见，但会在一秒之内变为可见 refresh API 这些行为可能会对新用户造成困惑，他们索引了一个文档然后尝试搜索它，但却没有搜到。这个问题的解决办法是用 refresh API 执行一次手动刷新: 12POST /_refresh POST /blogs/_refresh 尽管刷新是比提交轻量很多的操作，它还是会有性能开销。当写测试的时候， 手动刷新很有用，但是不要在生产环境下每次索引一个文档都去手动刷新。 相反，你的应用需要意识到Elasticsearch的近实时的性质，并接受它的不足。 并不是所有的情况都需要每秒刷新。可能你正在使用Elasticsearch索引大量的日志文件，你可能想优化索引速度而不是近实时搜索， 可以通过设置refresh_interval， 降低每个索引的刷新频率 12345&#123; &quot;setting&quot;: &#123; &quot;refresh_interval&quot;: &quot;30s&quot; &#125;&#125; refresh_interval 可以在既存索引上进行动态更新。 在生产环境中，当你正在建立一个大的新索引时，可以先关闭自动刷新，待开始使用该索引时，再把它们调回来 1234567891011# 关闭自动刷新PUT /index/_settings&#123; &quot;refresh_interval&quot;: -1&#125;# 每秒刷新PUT /index/_settings&#123; &quot;refresh_interval&quot;: 1&#125; 持久化变更 如果没有用 fsync 把数据从文件系统缓存刷（flush）到硬盘，我们不能保证数据在断电甚至是程序正常退出之后依然存在。为了保证 Elasticsearch 的可靠性，需要确保数据变化被持久化到磁盘。 在 动态更新索引，我们说一次完整的提交会将段刷到磁盘，并写入一个包含所有段列表的提交点。Elasticsearch 在启动或重新打开一个索引的过程中使用这个提交点来判断哪些段隶属于当前分片 即使通过每秒刷新（refresh）实现了近实时搜索，我们仍然需要经常进行完整提交来确保能从失败中恢复。但在两次提交之间发生变化的文档怎么办？我们也不希望丢失掉这些数据 Elasticsearch 增加了一个 translog ，或者叫事务日志，在每一次对 Elasticsearch 进行 操作时均进行了日志记录 整个流程如下： 一个文档被索引之后，就会被添加到内存缓冲区，并且追加到了 translog 刷新（refresh）使分片每秒被刷新（refresh）一次 这些在内存缓冲区的文档被写入到一个新的段中，且没有进行 fsync 操作 这个段被打开，使其可被搜索（OS cache） 内存缓冲区被清空 这个进程继续工作，更多的文档被添加到内存缓冲区和追加到事务日志 默认5秒，或者log文件大于512M，log被刷新到磁盘。一个新的 translog 被创建，并且一个全量提交被执行 所有在内存文档都被写入一个新的段，1秒后刷到系统缓冲区 内存区域被清空 一个提交点被写入硬盘（30m或512M） 虚拟文件系统缓存通过 fsync 被刷新（flush） 老的 translog 被删除 flush API 这个执行一个提交并且截断 translog 的行为在 Elasticsearch 被称作一次 flush 。 分片每30分钟被自动刷新（flush），或者在 translog 太大的时候也会刷新。请查看 translog 文档 来设置，它可以用来 控制这些阈值 flush API 可以被用来执行一个手工的刷新（flush）: 123POST /blogs/_flush POST /_flush?wait_for_ongoing 刷新（flush） blogs 索引 刷新（flush）所有的索引并且等待所有刷新在返回前完成 这就是说，在重启节点或关闭索引之前执行 flush 有益于你的索引。当 Elasticsearch 尝试恢复或重新打开一个索引， 它需要重放 translog 中所有的操作，所以如果日志越短，恢复越快 translog 记录些操作，防止数据丢失 translog 是一个日志文件，采用末尾追加方式的顺序写入，因此写入效率非常高，可以频繁进行写日志 保证在filesystem cache中的数据不会因为elasticsearch重启或是发生意外故障的时候丢失(redolog 功能) 当系统重启时会从translog中恢复之前记录的操作 translog还用于提供实时CRUD，当尝试按ID检索，更新或删除文档时。会先到translog之中进行查找，因为translog之中保存的是最新的数据 translog的清除时间在进行flush操作之后（将数据从filesystem cache刷入disk之中） translog 是 Elasticsearch 的一个特殊的日志，在索引文档时记录文档的每一个变化。它被设计的目的是在节点故障或重启后保证数据完整性 translog 主要用于在写入索引操作的过程中记录任何的变化，例如新增、更新或删除文档的操作，以加强对每次写操作的可靠性。在正常情况下，translog 会尽可能快地写入硬盘，以确保数据被安全地持久化。在节点发生故障时，Elasticsearch 依赖 translog 来进行恢复操作，以将其数据从最后一个已知状态回滚到最新状态 translog 刷新设置： 12345PUT /my_index/_settings&#123; &quot;index.translog.durability&quot;: &quot;async&quot;, &quot;index.translog.sync_interval&quot;: &quot;5s&quot;&#125; refresh 内存数据刷入到OS Cache 在ES中，当写入一个新文档时，首先被写入到内存缓存中，默认每1秒将 内存缓存 中的文档生成一个新的段并清空原有 内存缓存刷新到OS cache，新写入的段变为可读状态，但是还没有被完全提交。该新的段首先被写入文件系统缓存(内存缓存)，保证段文件可以正常被正常打开和读取，后续再进行刷盘操作。由此可以看到，ES并不是写入文档后马上就可以搜索到，而是一个近实时的搜索（默认1s后） index.refresh_interval这个参数，就是控制文件refresh到缓存cache中的时间间隔，当 index.refresh_interval 所定义的时间达到后，translog文件触发segment的refresh，默认这个参数是1s，所以ES的数据从写入到被查询到有1s的延迟 总结：将内存缓存的文档解析后生成一个新的段写入文件系统缓存让文档能被搜索的过程，此时数据还没有完全刷盘 flush segment文件刷盘 translog刷盘 ES默认每隔 30 分钟（或大小超过512M）会将 OS Cache 中的数据刷入磁盘同时清空 translog 日志文件，因此此时内存中的segments已经写入到磁盘中，就不需要translog来保障数据安全了，这个过程叫做 flush 刷盘时机 ES的各个shard会每隔 30分钟 缓冲区大小超过总内存的10% 进行一次 flush 操作 当 translog 每隔 5秒 或者文件大小达到 512M 进行一次 flush 操作 总结：flush 是把内存中的数据（包括 translog 和 segments）都刷到磁盘，通过 fsync (fsync 是一个 Unix 系统调用函数，作用是将缓冲区数据进行刷盘) 段合并 由于自动刷新流程每秒会创建一个新的段（由动态配置参数：refresh_interval 决定），这样会导致短时间内的段数量暴增，而段数目太多会带来较大的麻烦 消耗资源：每一个段都会消耗文件句柄、内存和cpu运行周期 搜索变慢：每个搜索请求都必须轮流检查每个段；所以段越多，搜索也就越慢 Elasticsearch 通过在后台进行段合并来解决这个问题，小的段被合并到大的段，然后这些大的段再被合并到更大的段 段合并做了什么 段合并的时候会将那些旧的已删除文档从文件系统中清除 被删除的文档（或被更新文档的旧版本）不会被拷贝到新的大段中 启动段合并不需要你做任何事。进行索引和搜索时会自动进行 当索引的时候，刷新（refresh）操作会创建新的段并将段打开以供搜索使用 合并进程选择一小部分大小相似的段，并且在后台将它们合并到更大的段中。这并不会中断索引和搜索 为什么要进行段合并 索引段的个数越多，搜索性能越低并且消耗更多的内存 索引段是不可变的，你并不能物理上从中删除信息 (逻辑删除 .del文件) 当段合并时，这些被标记为删除的文档并没有被拷贝至新的索引段中，这样，减少了最终的索引段中的 document 数目 段合并的好处是什么 减少索引段的数量并提高检索速度 减少索引的容量（文档数） 原因：段合并会移除(被标记删除的文档和低版本的文档) 段合并可能带来的问题 磁盘IO操作的代价 速度慢的系统中，段合并会显著影响性能 optimize API optimize API大可看做是 强制合并 API。它会将一个分片强制合并到 max_num_segments 参数指定大小的段数目。 这样做的意图是减少段的数量（通常减少到一个），来提升搜索性能（不要在一个活跃的索引执行这个API） 在特定情况下，使用 optimize API 颇有益处。例如在日志这种用例下，每天、每周、每月的日志被存储在一个索引中。 老的索引实质上是只读的；它们也并不太可能会发生变化 在这种情况下，使用optimize优化老的索引，将每一个分片合并为一个单独的段就很有用了；这样既可以节省资源，也可以使搜索更加快速 1POST /logstash-2014-10/_optimize?max_num_segments=1 文档处理 冲突处理 当我们使用index API更新文档 ，可以一次性读取原始文档，做我们的修改，然后重新索引整个文档。最近的索引请求将获胜：无论最后哪一个文档被索引，都将被唯一存储在Elasticsearch中。如果其他人同时更改这个文档，他们的更改将丢失 很多时候这是没有问题的。也许我们的主数据存储是一个关系型数据库，我们只是将数据复制到Elasticsearch中并使其可被搜索。 也许两个人同时更改相同的文档的几率很小。或者对于我们的业务来说偶尔丢失更改并不是很严重的问题 但有时丢失了一个变更就是非常严重的 。试想我们使用Elasticsearch存储我们网上商城商品库存的数量， 每次我们卖一个商品的时候，我们在Elasticsearch中将库存数量减少。有一天，管理层决定做一次促销。突然地，我们一秒要卖好几个商品。 假设有两个web程序并行运行，每一个都同时处理所有商品的销售 web_1对stock_count所做的更改已经丢失，因为web_2不知道它的stock_count的拷贝已经过期。 结果我们会认为有超过商品的实际数量的库存，因为卖给顾客的库存商品并不存在，我们将让他们非常失望 变更越频繁，读数据和更新数据的间隙越长，也就越可能丢失变更 在数据库领域中，有两种方法通常被用来确保并发更新时变更不会丢失 悲观并发控制（排他锁） 这种方法被关系型数据库广泛使用，它假定有变更冲突可能发生，因此阻塞访问资源以防止冲突。 一个典型的例子是读取一行数据之前先将其锁住，确保只有放置锁的线程能够对这行数据进行修改 乐观并发控制（版本控制） Elasticsearch是分布式的。当文档创建、更新或删除时， 新版本的文档必须复制到集群中的其他节点。Elasticsearch 也是异步和并发的，这意味着这些复制请求被并行发送，并且到达目的地时也许顺序是乱的 。Elasticsearch需要一种方法确保文档的旧版本不会覆盖新的版本 乐观所解决并发冲突 当我们之前讨论index，GET和delete请求时，我们指出每个文档都有一个_version（版本）号，当文档被修改时版本号递增。Elasticsearch使用这个 version 号来确保变更以正确顺序得到执行。如果旧版本的文档在新版本之后到达，它可以被简单的忽略 我们可以利用version号来确保 应用中相互冲突的变更不会导致数据丢失。我们通过指定想要修改文档的version号来达到这个目的。 如果该版本不是当前版本号，我们的请求将会失败 老的版本es使用 version，但是新版本不支持了，会报下面的错误，提示我们用 if_seq_no 和 if_primary_term 123456789101112131415161718GET /products/_doc/1返回：&#123; &quot;_index&quot; : &quot;products&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_version&quot; : 4, &quot;_seq_no&quot; : 18, #（sequence number） &quot;_primary_term&quot; : 10, #（primary term） &quot;found&quot; : true, &quot;_source&quot; : &#123; &quot;title&quot; : &quot;iphone13 128g&quot;, &quot;price&quot; : 6499, &quot;create_date&quot; : &quot;2022-05-02&quot;, &quot;description&quot; : &quot;降价大促销&quot; &#125;&#125; 更新失败场景 12345678910111213141516171819202122232425262728# 如果 if_seq_no和if_primary_term都比当前版本低则更新失败POST /products/_doc/1/_update?if_seq_no=17&amp;if_primary_term=9&#123; &quot;doc&quot;: &#123; &quot;price&quot;: 6599 &#125;&#125;返回：&#123; &quot;error&quot; : &#123; &quot;root_cause&quot; : [ &#123; &quot;type&quot; : &quot;version_conflict_engine_exception&quot;, &quot;reason&quot; : &quot;[1]: version conflict, required seqNo [17], primary term [9]. current document has seqNo [18] and primary term [10]&quot;, &quot;index_uuid&quot; : &quot;XX8rpADdS6GU3qvZxXvL0g&quot;, &quot;shard&quot; : &quot;0&quot;, &quot;index&quot; : &quot;products&quot; &#125; ], &quot;type&quot; : &quot;version_conflict_engine_exception&quot;, &quot;reason&quot; : &quot;[1]: version conflict, required seqNo [17], primary term [9]. current document has seqNo [18] and primary term [10]&quot;, &quot;index_uuid&quot; : &quot;XX8rpADdS6GU3qvZxXvL0g&quot;, &quot;shard&quot; : &quot;0&quot;, &quot;index&quot; : &quot;products&quot; &#125;, &quot;status&quot; : 409&#125; 更新成功场景 1234567891011121314151617181920212223# if_seq_no 和 if_primary_term 必须和当前文档的数据版本相等，才能更新POST /products/_doc/1/_update?if_seq_no=18&amp;if_primary_term=10&#123; &quot;doc&quot;: &#123; &quot;price&quot;: 6599 &#125;&#125;返回：&#123; &quot;_index&quot; : &quot;products&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_version&quot; : 5, &quot;result&quot; : &quot;updated&quot;, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;failed&quot; : 0 &#125;, &quot;_seq_no&quot; : 19, &quot;_primary_term&quot; : 10&#125; 指定重试次数 使用参数 retry_on_conflict 指定乐观锁重试次数 1234567POST /website/pageviews/1/_update?retry_on_conflict=5 &#123; &quot;script&quot; : &quot;ctx._source.views+=1&quot;, &quot;upsert&quot;: &#123; &quot;views&quot;: 0 &#125;&#125; 外部系统版本控制 一个常见的设置是使用其它数据库作为主要的数据存储，使用Elasticsearch做数据检索， 这意味着主数据库的所有更改发生时都需要被复制到Elasticsearch，如果多个进程负责这一数据同步，你可能遇到类似于之前描述的并发问题 如果你的主数据库已经有了版本号—或一个能作为版本号的字段值比如timestamp —那么你就可以在Elasticsearch中通过增加version_type=external到查询字符串的方式重用这些相同的版本号， 版本号必须是大于零的整数， 且小于9.2E+18 —一个Java中long类型的正值 外部版本号的处理方式和我们之前讨论的内部版本号的处理方式有些不同，Elasticsearch不是检查当前_version和请求中指定的版本号是否相同， 而是检查当前_version是否小于指定的版本号。 如果请求成功，外部的版本号作为文档的新_version进行存储 外部版本号不仅在索引和删除请求是可以指定，而且在 创建 新文档时也可以指定 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273GET /products/_doc/10返回：&#123; &quot;_index&quot; : &quot;products&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;10&quot;, &quot;_version&quot; : 2, # 当前版本号为2 &quot;_seq_no&quot; : 20, &quot;_primary_term&quot; : 10, &quot;found&quot; : true, &quot;_source&quot; : &#123; &quot;title&quot; : &quot;pihone13 512g&quot;, &quot;price&quot; : &quot;10999&quot;, &quot;create_date&quot; : &quot;2022-05-12&quot;, &quot;description&quot; : &quot;pro升级版本&quot; &#125;&#125;# 更新失败,因为版本号要高于当前文档版本POST /products/_doc/10/?version=2&amp;version_type=external&#123; &quot;doc&quot;: &#123; &quot;price&quot;: 10999 &#125;&#125;返回：&#123; &quot;error&quot; : &#123; &quot;root_cause&quot; : [ &#123; &quot;type&quot; : &quot;version_conflict_engine_exception&quot;, &quot;reason&quot; : &quot;[10]: version conflict, current version [2] is higher or equal to the one provided [2]&quot;, &quot;index_uuid&quot; : &quot;XX8rpADdS6GU3qvZxXvL0g&quot;, &quot;shard&quot; : &quot;0&quot;, &quot;index&quot; : &quot;products&quot; &#125; ], &quot;type&quot; : &quot;version_conflict_engine_exception&quot;, &quot;reason&quot; : &quot;[10]: version conflict, current version [2] is higher or equal to the one provided [2]&quot;, &quot;index_uuid&quot; : &quot;XX8rpADdS6GU3qvZxXvL0g&quot;, &quot;shard&quot; : &quot;0&quot;, &quot;index&quot; : &quot;products&quot; &#125;, &quot;status&quot; : 409&#125;# 更新成功POST /products/_doc/10/?version=3&amp;version_type=external&#123; &quot;doc&quot;: &#123; &quot;price&quot;: 10999 &#125;&#125;返回：&#123; &quot;_index&quot; : &quot;products&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;10&quot;, &quot;_version&quot; : 3, &quot;result&quot; : &quot;updated&quot;, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;failed&quot; : 0 &#125;, &quot;_seq_no&quot; : 21, &quot;_primary_term&quot; : 10&#125; 优化 硬件选择 Elasticsearch的基础是 Lucene，所有的索引和文档数据是存储在本地的磁盘中，具体的路径可在ES的配置文件…/config/elasticsearch.yml 中配置，如下 123456# Path to directory where to store the data (separate multiple locations by comma):#path.data: /path/to/data## Path to log files:##path.logs: /path/to/logs 磁盘 在现代服务器上通常都是 瓶颈。Elasticsearch 重度使用磁盘，你的磁盘能处理的吞吐量越大，你的节点就越稳定。这里有一些优化磁盘I/O的技巧 使用 SSD。就像其他地方提过的，他们比机械磁盘优秀多了 使用RAID 0 磁盘阵列。条带化 RAID会提高磁盘I/O，代价显然就是当一块硬盘故障时整个就故障了。不要使用镜像或者奇偶校验RAID因为副本已经提供了这个功能 另外，使用多块硬盘，并允许Elasticsearch通过多个path.data目录配置把数据条带化分配到它们上面 12设置索引数据的存储路径，默认是es根目录下的data文件夹，可以设置多个存储路径，用逗号隔开，例：path.data: /xxx/xxx/data,/xxx/xxx/data 不要使用远程挂载的存储，比如NFS或者SMB/CIFS。这个引入的延迟对性能来说完全是背道而驰的 分片策略 合理设置分片数 数据容量 节点内存 节点数量 分片和副本的设计为 ES 提供了支持分布式和故障转移的特性，但并不意味着分片和 副本是可以无限分配的。而且索引的分片完成分配后由于索引的路由机制，我们是不能重新修改分片数的 可能有人会说，我不知道这个索引将来会变得多大，并且过后我也不能更改索引的大小， 所以为了保险起见，还是给它设为 1000 个分片吧。但是需要知道的是，一个分片并不是没有代价的。需要了解： 一个分片的底层即为一个 Lucene 索引，会消耗一定 文件句柄、内存、以及 CPU 运转 每一个搜索请求都需要命中索引中的每一个分片，如果每一个分片都处于不同的节点还好， 但如果多个分片都需要在同一个节点上竞争使用相同的资源就有些糟糕了 用于计算相关度的词项统计信息是基于 分片 的。如果有许多分片，每一个都只有很少的数据会导致很低的相关度，如果没有带路由键查询，则协调节点会将请求转发到N个节点上（主分片数量个节点），消耗CPU 一个业务索引具体需要分配多少分片可能需要架构师和技术人员对业务的增长有个预 先的判断，横向扩展应当分阶段进行。为下一阶段准备好足够的资源。 只有当你进入到下 一个阶段，你才有时间思考需要作出哪些改变来达到这个阶段。一般来说，我们遵循一些原则： 控制每个分片占用的硬盘容量不超过 ES 的最大 JVM 的堆空间设置（一般设置不超过 32G，参考下文 的 JVM 设置原则），因此，如果索引的总容量在 500G 左右，那分片大小在 16 个左右即可；当然， 最好同时考虑原则 2 不要超过物理内存的 50%： Lucene 的设计目的是把底层 OS 里的数据缓存到内存中。Lucene 的段是分别存储到单个文件中的，这些文件都是不会变化的，所以很利于缓存，同时操作系统也会把这些段文件缓存起来，以便更快的访问。如果我们设置的堆内存过大， Lucene 可用的内存将会减少，就会严重影响降低 Lucene 的全文本查询性能 堆内存的大小最好不要超过 32GB：32GB之前会启用压缩对象指针，超过这个阈值，压缩对象指针会失效变为普通对象指针，指针膨胀，导致30G内存和40G内存无差别还影响CPU和GC性能 确保 Xmx 和 Xms 的大小是相同的，其目的是为了能够在 Java 垃圾回收机制清理完堆区后不需要重新分隔计算堆区的大小而浪费资源，可以减轻伸缩堆大小带来的压力 12-Xms 31g-Xmx 31g 考虑一下 node 数量，一般一个节点有时候就是一台物理机，如果分片数过多，大大超过了节点数， 很可能会导致一个节点上存在多个分片，一旦该节点故障，即使保持了 1 个以上的副本，同样有可能会导致数据丢失，集群无法恢复。所以， 一般都设置分片数不超过节点数的 3 倍（在开始阶段, 一个好的方案是根据你的节点数量按照1.5~3倍的原则来创建分片） 主分片，副本和节点最大数之间数量，我们分配的时候可以参考关系： 节点数&lt;=主分片数 *（副本数+1) 推迟分片分配 对于节点瞬时中断的问题，默认情况，集群会等待一分钟来查看节点是否会重新加入，如果这个节点在此期间重新加入，重新加入的节点会保持其现有的分片数据，不会触发新的分片分配。这样就可以减少 ES 在自动再平衡可用分片时所带来的极大开销 通过修改参数 delayed_timeout（默认为1分钟） ，可以延长再平衡的时间，可以全局设置也可以在索引 级别进行修改: 123456PUT /_all/_settings &#123; &quot;settings&quot;: &#123; &quot;index.unassigned.node_left.delayed_timeout&quot;: &quot;5m&quot; &#125;&#125; 再平衡 再平衡 再平衡是在集群的不同节点之间移动分片的过程，触发机制是原节点离线 cluster.routing.allocation.allow_rebalance always：表明再平衡可以在需要时随时开始 indices_primaries_active:所有主分片都初始化后，再平衡才会开始 indices_all_active：默认设置，所有分片和副本都初始化后，再平衡才会开始 再平衡测试：3个节点 5个分片1个副本 适合在网络环境不太稳定的场景优化 路由选择 当我们查询文档的时候，Elasticsearch 如何知道一个文档应该存放到哪个分片中呢？它其实是通过下面这个公式来计算出来: shard = hash(routing) % number_of_primary_shards routing 默认值是文档的 id，也可以采用自定义值，比如用户 id 不带 routing 查询 类似不带分片键的分表查询 ​ 在查询的时候因为不知道要查询的数据具体在哪个分片上，所以整个过程分为 2 个步骤 分发： 请求到达协调节点后，协调节点将查询请求分发到每个分片上 聚合：协调节点搜集到每个分片上查询结果，再将查询的结果进行排序，之后给用户返回结果 带 routing 查询 带分片键的分表查询 查询的时候，可以直接根据 routing 信息定位到某个分片查询，不需要查询所有的分片，经过协调节点排序 带routing的查询，在查询的时候通过路由算法就知道数据存在哪个分片，就可以直接查询出数据来， 效率提升很多 写入速度优化 ES的默认配置，是综合了数据可靠性、写入速度、搜索实时性等因素。实际使用时，我们需要根据公司要求，进行偏向性的优化 针对于搜索性能要求不高，但是对写入要求较高的场景，我们需要尽可能的选择恰当写优化策略。综合来说，可以考虑以下几个方面来提升写索引的性能 加大Translog Flush，目的是降低Iops、Writeblock 增加Index Refresh间隔，目的是减少Segment Merge的次数 调整Bulk线程池和队列 (设置更高的线程) threadpool.index.type: xxx # 写索引线程池类型 threadpool.index.size: 50 # 线程池大小（建议2~3倍cpu数） threadpool.index.queue_size # 队列大小 优化节点间的任务分布 优化Lucene层的索引建立，目的是降低CPU及IO 批量数据提交 ES 提供了 Bulk API 支持批量操作，当我们有大量的写任务时，可以使用 Bulk 来进 行批量写入 通用的策略如下：Bulk 默认设置批量提交的数据量不能超过 100M。数据条数一般是根据文档的大小和服务器性能而定的，但是单次批处理的数据大小应从 5MB～15MB 逐渐增加，当性能没有提升时，把这个数据量作为最大值 优化存储设备 ES 是一种密集使用磁盘的应用(升级为SSD)，在段合并的时候会频繁操作磁盘，所以对磁盘要求较 高，当磁盘速度提升之后，集群的整体性能会大幅度提高 合理使用合并 Lucene 以段的形式存储数据。当有新的数据写入索引时，Lucene 就会自动创建一个新的段 随着数据量的变化，段的数量会越来越多，消耗的多文件句柄数及CPU就越多，查询 效率就会下降 由于 Lucene 段合并的计算量庞大，会消耗大量的 I/O，所以 ES 默认采用较保守的策略，让后台定期进行段合并 减少 Refresh 的次数 Lucene 在新增数据时，采用了延迟写入的策略，默认情况下索引的 refresh_interval 为1秒 Lucene 将待写入的数据先写到内存中，超过 1 秒（默认）时就会触发一次 Refresh， 然后 Refresh 会把内存中的的数据刷新到操作系统的文件缓存系统中 ​ 如果我们对搜索的实效性要求不高，可以将 Refresh 周期延长，例如 30 秒 ​ 这样还可以有效地减少段刷新次数，但这同时意味着需要消耗更多的 Heap 内存 加大 Flush 设置 Flush 的主要目的是把文件缓存系统中的段持久化到硬盘，当 Translog 的数据量达到 512MB 或者 5秒钟，会触发一次 Flush index.translog.flush_threshold_size 参数的默认值是 512MB，我们进行修改。 增加参数值意味着文件缓存系统中可能需要存储更多的数据，所以我们需要为操作系统的文件缓存系统留下足够的空间 index.translog.sync_interval 参数默认值是5s，我们进行修改，比如修改到30s 减少副本的数量 ES 为了保证集群的可用性，提供了 Replicas（副本）支持，然而每个副本也会执行分析、索引及可能的合并过程，所以 Replicas 的数量会严重影响写索引的效率 当写索引时，需要把写入的数据都同步到副本节点，副本节点越多，写索引的效率就越慢 如 果 我 们 需 要 大 批 量 进 行 写 入 操 作 ， 可以先禁止 Replica 复 制 ， 设 置 index.number_of_replicas: 0 关闭副本。在写入完成后，Replica 修改回正常的状态 123456PUT sys_user/_settings&#123; &quot;index&quot; : &#123; &quot;number_of_replicas&quot; : 0 &#125;&#125; 副本多了影响写索引的消息，太多副本要同步数据，分析term了,大批量写入的时候先关闭副本同步功能 内存设置 ES 默认安装后设置的内存是 1GB，对于任何一个现实业务来说，这个设置都太小了。 如果是通过解压安装的 ES，则在 ES 安装文件中包含一个 jvm.option 文件，添加如下命令来设置 ES 的堆大小，Xms 表示堆的初始大小，Xmx 表示可分配的最大内存，都是 1GB 确保 Xmx 和 Xms 的大小是相同的，其目的是为了能够在 Java 垃圾回收机制清理完堆区后不需要重新分隔计算堆区的大小而浪费资源，可以减轻伸缩堆大小带来的压力 假设你有一个 64G 内存的机器，按照正常思维思考，你可能会认为把 64G 内存都给 ES 比较好，但现实是这样吗， 越大越好？虽然内存对 ES 来说是非常重要的，但是答案 是否定的！ 因为 ES 堆内存的分配需要满足以下两个原则 重要配置 参数名 参数值 说明 cluster.name elasticsearch 配置 ES 的集群名称，默认值是 ES，建议改成与所存数据相关的名称，ES 会自动发现在同一网段下的 集群名称相同的节点 node.name node-1 集群中的节点名，在同一个集群中不能重复。节点 的名称一旦设置，就不能再改变了。当然，也可以 设 置 成 服 务 器 的 主 机 名 称 ， 例 如 node.name😒{HOSTNAME} node.master true 指定该节点是否有资格被选举成为 Master 节点，默认是 True，如果被设置为 True，则只是有资格成为 Master 节点，具体能否成为 Master 节点，需要通过选举产生 node.data true 指定该节点是否存储索引数据，默认为 True。数据 的增、删、改、查都是在 Data 节点完成的 index.number_of_shards 1 设置索引分片个数，默认是 1 片。也可以在创建 索引时设置该值，具体设置为多大都值要根据数据 量的大小来定。如果数据量不大，则设置成 1 时效 率最高 index.number_of_replicas 1 设置默认的索引副本个数，默认为 1 个。副本数越多，集群的可用性越好，但是写索引时需要同步的数据越多 transport.tcp.compress true 设置在节点间传输数据时是否压缩，默认为 False， 不压缩 discovery.zen.minimum_master_nodes 1 设置在选举 Master 节点时需要参与的最少的候选主节点数，默认为 1。如果使用默认值，则当网络 不稳定时有可能会出现脑裂。 合理的数值为 (master_eligible_nodes/2)+1 ，其中 master_eligible_nodes 表示集群中的候选主节点数 discovery.zen.ping.timeout 3s 设置在集群中自动发现其他节点时 Ping 连接的超 时时间，默认为 3 秒。 在较差的网络环境下需要设置得大一点，防止因误 判该节点的存活状态而导致分片的转移 ES落地场景 数据同步 datax 同步数据库数据到ES Logstash 同步数据到ES canal实时同步数据到ES 其他问题 为什么要使用Elasticsearch 系统中的数据，随着业务的发展，时间的推移，将会非常多，而业务中往往采用模糊查询进行数据的搜索，而 模糊查询 会导致查询引擎放弃索引，导致系统查询数据时都是全表扫描，在百万级别的数据库中，查询效率是非常低下的，而我们使用ES做一个全文索引，将经常查询的系统功能的某些字段，比如说电商系统的商品表中商品名，描述、价格还有id这些字段我们放入ES索引库里，可以提高查询速度 模糊搜索，全文索引 使用ES作为索引，建立索引字段和id的文档，加速mysql大表复杂查询 Elasticsearch 的 master 选举流程 Elasticsearch的选主是ZenDiscovery模块负责的，主要包含Ping（节点之间经过这个RPC来发现彼此）和Unicast（单播模块包含-一个主机列表以控制哪些节点须要ping通）这两部分 对全部能够成为master的节点（node master: true）根据nodeId字典排序，每次选举每一个节点都把本身所知道节点排一次序，而后选出第一个（第0位）节点，暂且认为它是master节点 若是对某个节点的投票数达到必定的值（能够成为master节点数n/2+1）而且该节点本身也选举本身，那这个节点就是master。不然从新选举一直到知足上述条件 master节点的职责主要包括集群、节点和索引的管理，不负责文档级别的管理；data节点能够关闭http功能 term和match的区别 term查询是一种 精确匹配查询，它会在文本中精确匹配指定的术语（term） match查询则是一种 分析查询，它会对查询术语进行分析，并尝试将其与文档中的任何文本进行匹配。match查询的默认行为是使用分析器对查询术语进行分词 Elasticsearch 集群脑裂问题 脑裂问题在很多分布式集群方案中都会出现，包括Redis 网络问题 集群间的网络延迟致使一些节点访问不到master, 认为master 挂掉了从而选举出新的master,并对master上的分片和副本标红，分配新的主分片 discovery.zen.ping.timeout 节点状态的响应时间，默认为3s，能够适当调大，若是master在该响应时间的范围内没有作出响应应答，判断该节点已经挂掉了。调大参数（如6s，discovery.zen.ping_timeout:6），可适当减小误判 节点负载 主节点的角色既为master又为data，访问量较大时可能会致使ES中止响应形成大面积延迟，此时其余节点得不到主节点的响应认为主节点挂掉了，会从新选取主节点 进行 角色分离，即master节点与data节点分离，限制角色 主节点配置为：node master: true，node data: false 从节点配置为：node master: false，node data: true 内存回收 data 节点上的ES进程占用的内存较大，引起JVM的大规模内存回收，形成ES进程失去响应(主节点GC的 STW现象过长) discovery.zen.minimum. master nodes:n，该参数是用于控制选举行为发生的最小集群主节点数量。当备选主节点的个数大于等于该参数的值，且备选主节点中有该参数个节点认为主节点挂了，进行选举。官方建议为(n / 2) +1, n为主节点个数（即有资格成为主节点的节点个数） 索引文档的流程 参考持久化变更 协调节点默认使用文档 ID 参与计算（也支持经过 routing），以便为路由提供合适的分片：shard = hash(document_id) % (num_of_primary_shards) 当分片所在的节点接收到来自协调节点的请求后，会将请求写入到 Memory Buffer，而后定时（默认是每隔 1 秒）写入到 Filesystem Cache，这个从 Memory Buffer 到 Filesystem Cache 的过程就叫作 refresh 固然在某些状况下，存在 Momery Buffer 和 Filesystem Cache 的数据可能会丢失， ES 是经过 translog的机制来保证数据的可靠性的。其实现机制是接收到请求后，同时也会写入到 translog 中，当 Filesystemcache 中的数据写入到磁盘中时，才会清除掉，这个过程叫作 flush 在 flush 过程当中，内存中的缓冲将被清除，内容被写入一个新段，段的 fsync 将建立一个新的提交点，并将内容刷新到磁盘，旧的 translog 将被删除并开始一个新的 translog flush 触发的时机是定时触发（默认 30 分钟）或者 translog 变得太大（默认为 512M）时 更新和删除文档的流程 删除和更新也都是写操做，可是 Elasticsearch 中的文档是不可变的，所以不能被删除或者改动以展现其变动 磁盘上的每一个段都有一个相应的.del 文件。当删除请求发送后，文档并无真的被删除，而是在.del文件中被标记为删除。该文档依然能匹配查询，可是会在结果中被过滤掉。当段合并时，在.del 文件中被标记为删除的文档将不会被写入新段 在新的文档被建立时， Elasticsearch 会为该文档指定一个版本号，当执行更新时，旧版本的文档在 .del 文件中被标记为删除，新版本的文档被索引到一个新段。旧版本的文档依然能匹配查询，可是会在结果中被过滤掉 在后续的段合并中，旧版本的文档将不会被合并到新的段文件中 ES的搜索流程 参考 执行分布式检索 在ES中，查询默认返回最顶端的10条匹配hits（from默认为0, size参数默认值为10） 在ES中，搜索一般包括两个阶段，query 和 fetch 阶段，可以简单的理解，query 阶段确定要取哪些doc，fetch 阶段取出具体的 doc Query阶段 123456GET /producs/_search&#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125;&#125;, &quot;from&quot;: 100, &quot;size&quot;: 10&#125; Client 发送一次搜索请求，node1 接收到请求，然后，node1 创建一个大小为from + size的优先级队列用来存结果，我们管 node1 叫 coordinating node （协调节点） coordinating node将请求广播到涉及到的 shards，每个 shard 在内部执行搜索请求，然后，将结果存到内部的大小同样为from + size 的优先级队列里，可以把优先级队列理解为一个包含top N结果的列表 每个 shard 把暂存在自身优先级队列里的数据返回给 coordinating node，coordinating node 拿到各个 shards 返回的结果后对结果进行一次合并，产生一个全局的优先级队列，存到自身的优先级队列里 ​ 在上面的例子中，coordinating node 拿到(from + size) * 6条数据，然后合并并排序后选择前面的from + size条数据存到优先级队列，以便 fetch 阶段使用 ​ 另外，各个分片返回给 coordinating node 的数据用于选出前from + size条数据，所以，只需要返回唯一标记 doc 的_id以及用于排序的_score即可，这样也可以保证返回的数据量足够小 ​ coordinating node 计算好自己的优先级队列后，query 阶段结束，进入 fetch 阶段 Fetch阶段 query 阶段知道了要取哪些数据，但是并没有取具体的数据，这就是 fetch 阶段要做的 coordinating node 发送 GET 请求到相关shards shard 根据 doc 的_id取到数据详情，然后返回给 coordinating node coordinating node 返回数据给 Client ​ coordinating node 的优先级队列里有from + size 个_doc _id，但是，在 fetch 阶段，并不需要取回所有数据，在上面的例子中，前100条数据是不需要取的，只需要取优先级队列里的第101到110条数据即可 需要取的数据可能在不同分片，也可能在同一分片，coordinating node 使用 multi-get 来避免多次去同一分片取数据，从而提高性能 这种方式请求深度分页是有问题的： 我们可以假设在一个有 5 个主分片的索引中搜索。当我们请求结果的第一页（结果从 1 到 10 ），每一个分片产生前 10 的结果，并且返回给 协调节点 ，协调节点对 50 个结果排序得到全部结果的前 10 个 现在假设我们请求第 1000 页—结果从 10001 到 10010 。所有都以相同的方式工作除了每个分片不得不产生前10010个结果以外。然后协调节点对全部 50050 个结果排序最后丢弃掉这些结果中的 50040 个结果 对结果排序的成本随分页的深度成指数上升 size的大小不能超过index.max_result_window这个参数的设置，默认为10000, 如果搜索size大于10000，需要设置index.max_result_window参数 深度分页优化 Search After Scroll Sliced Scroll 在部署时，对 Linux 的设置有哪些优化方法 64 GB 内存的机器是很是理想的， 可是 32 GB 和 16 GB 机器也是很常见的。少于 8 GB 会拔苗助长 若是你要在更快的 CPUs 和更多的核心之间选择，选择更多的核心更好。多个内核提供的额外并发远赛过稍微快一点点的时钟频率 若是你负担得起 SSD，它将远远超出任何旋转介质。 基于 SSD 的节点，查询和索引性能都有提高。若是你负担得起， SSD 是一个好的选择 即便数据中心们近在咫尺，也要避免集群跨越多个数据中心。绝对要避免集群跨越大的地理距离 (不要跨网络大区尽量局域网组群) 请确保运行你应用程序的 JVM 和服务器的 JVM 是彻底同样的。 在 Elasticsearch 的几个地方，使用 Java 的本地序列化 经过设置 gateway.recover_after_nodes、 gateway.expected_nodes、 gateway.recover_after_time 能够在集群重启的时候避免过多的分片交换，这可能会让数据恢复从数个小时缩短为几秒钟 Elasticsearch 默认被配置为使用单播发现，以防止节点无心中加入集群。只有在同一台机器上运行的节点才会自动组成集群。最好使用单播代替组播 不要随意修改垃圾回收器（G1）和各个线程池的大小 把你的内存的（少于）一半给 Lucene（但不要超过 32 GB！），经过 ES_HEAP_SIZE 环境变量设置 内存交换到磁盘对服务器性能来讲是致命的。若是内存交换到磁盘上，一个 100 微秒的操做可能变成 10 毫秒。 再想一想那么多 10 微秒的操做时延累加起来。 不难看出 swapping 对于性能是多么可怕 Lucene 使用了大量的文件。同时， Elasticsearch 在节点和 HTTP 客户端之间进行通讯也使用了大量的套接字。 全部这一切都须要足够的文件描述符。你应该增长你的文件描述符，设置一个很大的值，如 64,000 索引阶段性能提升方法 使用批量请求并调整其大小：每次批量数据 5–15 MB 大是个不错的起始点 推荐使用SSD 段和合并：Elasticsearch 默认值是 20 MB/s，对机械磁盘应该是个不错的设置。如果你用的是 SSD， 可以考虑提高到 100–200 MB/s。如果你在做批量导入，完全不在意搜索，你可以彻底关掉合并限流。 另外还可以增加 index.translog.flush_threshold_size 设置，从默认的 512 MB 到更大一些的值，比如 1 GB，这可以在一次清空触发的时候在事务日志里积累出更大的段 如果你的搜索结果不需要近实时的准确度，考虑把每个索引的 index.refresh_interval 改到 30s 如果你在做大批量导入，考虑通过设置 index.number_of_replicas: 0 关闭副本 GC 方面，在使用 ES 时要注意什么 倒排索引词典的索引须要常驻内存(term )，没法 GC，须要监控 data node 上 segment memory 增加趋势 各种缓存 field cache, filter cache, indexing cache, bulk queue 等等，要设置合理的大小，而且要应该根据最坏的状况来看 heap 是否够用，也就是各种缓存所有占满的时候，还有 heap 空间能够分配给其余任务吗？避免采用 clear cache 等“自欺欺人”的方式来释放内存 避免 返回大量结果集 的 搜索 与 聚合。确实须要大量拉取数据的场景，能够采用 scroll &amp; Search After 来实现 cluster stats 驻留内存并没有法水平扩展，超大规模集群能够考虑分拆成多个集群经过 tribe node 链接 想知道 heap 够不够，必须结合实际应用场景，并对集群的 heap 使用状况作持续的监控 ES 对于大数据量（上亿量级）的聚合如何实现 Elasticsearch 提供的首个近似聚合是 cardinality 度量。它提供一个字段的基数，即该字段的 distinct或者 unique 值的数目。它是基于 HLL 算法的。HLL 会先对我们的输入作哈希运算，然后根据哈希运算的结果中的 bits 做概率估算从而得到基数。其特点是：可配置的精度，用来控制内存的使用（更精确 ＝ 更多内存）；小的数据集精度是非常高的；我们可以通过配置参数，来设置去重需要的固定内存使用量。无论数千还是数十亿的唯一值，内存使用量只与你配置的精确度相关 es，去重，cartinality metric，对每个bucket中的指定的field进行去重，取去重后的count，类似于count(distcint) 5%的错误率 123456789101112GET /sys_user/_search&#123; &quot;from&quot;: 0, &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;count&quot;: &#123; &quot;cardinality&quot;: &#123; &quot;field&quot;: &quot;nickname&quot; &#125; &#125; &#125;&#125; 在并发情况下，ES 如果保证读写一致 文档处理 写一致性保证 通过版本号使用乐观并发控制 ，以确保新版本不会被旧版本覆盖，由应用层来处理具体的冲突 另外对于写操作，可以使用 wait_for_active_shards 参数指定多少个分片写入成功才返回，以此保证主分片和副本写入一致性 对于读操作，可以设置 replication 为 sync(默认)，这使得操作在主分片和副本分片都完成后才会返回；如果设置 replication 为 async 时，也可以通过设置搜索请求参数_preference 为 primary 来查询主分片，确保文档是最新版本 查询指定分片 123456GET /goods/_search?preference=_shards:0&#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;&#125; 如何监控 Elasticsearch集群状态 elasticsearch-head 插件 通过 Kibana 监控 Elasticsearch。你可以实时查看你的集群健康状态和性能，也可以分析过去的集群、索引和节点指标 是否了解字典树 字典树又称单词查找树，Trie 树，是一种树形结构，是一种哈希树的变种。典型应用是用于统计，排序和保存大量的字符串（但不仅限于字符串），所以经常被搜索引擎系统用于文本词频统计。它的优点是：利用字符串的公共前缀来减少查询时间，最大限度地减少无谓的字符串比较，查询效率比哈希树高。 Trie 的核心思想是空间换时间，利用字符串的公共前缀来下降查询时间的开销以达到提升效率的目的。它有 3 个基本性质 根节点不包含字符，除根节点外每个节点都只包含一个字符 从根节点到某一节点，路径上经过的字符连接起来，为该节点对应的字符串 从根节点到某一节点，路径上经过的字符连接起来，为该节点对应的字符串 对于中文的字典树，每个节点的子节点用一个哈希表存储，这样就不用浪费太大的空间，而且查询速度上 可以保留哈希的复杂度 O(1) ES 中的集群、节点、索引、文档、类型是什么 集群是一个或多个节点（服务器）的集合，它们共同保存您的整个数据，并提供跨所有节点的联合索 引和搜索功能。群集由唯一名称标识，默认情况下为“elasticsearch”。此名称很重要，因为如果节点设 置为按名称加入群集，则该节点只能是群集的一部分 节点是属于集群一部分的单个服务器。它存储数据并参与群集索引和搜索功能 索引就像关系数据库中的“数据库”。它有一个定义多种类型的映射。索引是逻辑名称空间，映射到一 个或多个主分片，并且可以有零个或多个副本分片。 MySQL =&gt;数据库 Elasticsearch =&gt;索引 文档类似于关系数据库中的一行。不同之处在于索引中的每个文档可以具有不同的结构（字段），但 是对于通用字段应该具有相同的数据类型。 MySQL =&gt; Databases =&gt; Tables =&gt; Columns / Rows Elasticsearch =&gt; Indices =&gt; Types =&gt;具有属性的文档 类型是索引的逻辑类别/分区，其语义完全取决于用户 Elasticsearch 中的倒排索引是什么 倒排索引是搜索引擎的核心。搜索引擎的主要目标是在查找发生搜索条件的文档时提供快速搜索。ES中的倒排索引其实就是 lucene 的倒排索引，区别于传统的正向索引，倒排索引会再存储数据时将关键词和数据进行关联，保存到倒排表中，然后查询时，将查询内容进行分词后在倒排表中进行查询，最后匹配数据即可 ES索引数据多了怎么办，如何调优，部署 在设计的时候可以基于模板+时间滚动方式创建索引，每天递增数据，避免单个索引很大的情况出现。 在存储的时候，冷热数据分开存储,比如最近3天的数据作为热数据，其他的作为冷数据，冷数据的话，由于不会再写入新数据了，可以考虑定期force_merge（强制合并）和shrink（压缩）的方式进行处理，节约空间和检索效率 由于es支持动态扩展，所有可以多加几台机器来缓解集群压力 在 ES中，是怎么根据一个词找到对应的倒排索引的 参考","categories":[],"tags":[]},{"title":"设计模式","slug":"设计模式","date":"2022-04-22T16:41:55.000Z","updated":"2023-07-14T06:49:18.196Z","comments":true,"path":"2022/04/23/设计模式/","link":"","permalink":"https://wugengfeng.cn/2022/04/23/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"UML 类图 参考文章：https://cloud.tencent.com/developer/article/1012684 箭头方向 子类指向父类 定义子类时需要通过extends关键字指定父类 子类一定是知道父类定义的,但父类并不知道子类的定义 只有知道对方信息时才能指向对方 所以箭头方向是从子类指向父类 继承（泛化） 三角箭头-实线 实现 三角箭头-虚线 依赖 箭头-虚线 一个类用到了另一个类。这种使用关系是具有偶然性的、临时性的、非常弱的 12345678public class Student &#123; public void study(Book book)&#123; System.out.println(&quot;看书&quot;); &#125;&#125;public class Book &#123;&#125; 关联 箭头-实线 关联关系，即强调了“使用”，例如人和车的关系 idea插件 聚合 空心菱形 更强调整体与部分,整体与部分不是同生共死的 数字的含义 0…1: 0或1个实例. 0…: 0或多个实例. 1…1: 1个实例. 1只能有一个实例. 1…: 至少有一个实例. IDEA插件是实心菱形加虚线箭头 1234567891011121314public class School &#123; private List&lt;Student&gt; students ; public List&lt;Student&gt; getStudents() &#123; return students; &#125; public void setStudents(List&lt;Student&gt; students) &#123; this.students = students; &#125;&#125;public class Student &#123;&#125; 聚合：代表空器皿里可以放很多相同东西，聚在一起（箭头方向所指的类） 整体和局部的关系，两者有着独立的生命周期，是has a的关系 弱关系 组合 实心菱形 更强调整体与部分,整体与部分是同生共死的 IDEA插件是实心菱形加虚线箭头加create 12345678910public class Computer &#123; private Cpu cpu = new Cpu(); private Gpu gpu = new Gpu();&#125;public class Cpu &#123;&#125;public class Gpu &#123;&#125; 整体与局部的关系，和聚合的关系相比，关系更加强烈两者有相同的生命周期，contains-a的关系 强关系 访问修饰符 访问修饰符 符号 public + protected # default ~ private - 抽象属性 下划线标注 抽象方法 斜体 idea插件表示 1234567891011121314151617181920212223242526272829public abstract class User &#123; public int id; protected String name; int age; private String address; public static String sex; public int getId() &#123; return id; &#125; protected String getName() &#123; return name; &#125; int getAge() &#123; return age; &#125; private String getAddress() &#123; return address; &#125; public static String getSex() &#123; return sex; &#125; public abstract String abstractFun(int a);&#125; 软件设计七大原则 参考文章：https://cloud.tencent.com/developer/article/1650116 参考文章：https://juejin.cn/post/7027666637964705806 参考文章：https://juejin.cn/post/6844903795017646094#heading-20 设计模式 参考文章：https://pdai.tech/md/dev-spec/pattern/3_simple_factory.html 参考文章：https://refactoringguru.cn 创建型 设计模式 定义 适用场景 简单工厂 根据不同的参数创建不同的产品 工厂类负责创建的对象较少 工厂方法 定义一个创建对象的接口，允许子类决定实例化对象的类型 创建对象需要大量重复的代码（产品等级） 抽象工厂 抽象工厂模式提供一个创建一系列相关或相互依赖对象的接口 强调一系列相关的产品对象(属于同一产品族)一起使用创建对象需要大量重复的代码 建造者 将一个复杂对象的构建与它的表示分离，使得同样的构建过程可以创建不同的表示 一个对象有非常复杂的内部结构 原型 指原型实例指定创建对象的种类，并且通过拷贝这些原型创建新的对象 类初始化消耗较多资源；循环体中生产大量对象时 单例 保证一个类只有一个实例 程序中的某个类对于所有客户端只有一个可用的实例，可以使用单例模式 结构型 设计模式 定义 适用场景 外观（门面） 提供了一个统一的接口，用来访问子系统中的一群接口 构建多层系统结构，利用外观对象作为每层的入口，简化层间调用 装饰者 不改变原有对象的基础之上，将功能附加到对象上 扩展一个类的功能或给一个类添加附加职责 适配器 将一个类的接口转换成客户期望的另一个接口 已经存在的类，它的方法和需求不匹配时 享元 通过共享多个对象所共有的相同状态， 在有限的内存容量中载入更多对象 系统有大量相似对象、需要缓冲池的场景 组合 使客户端对单个对象和组合对象保持一致的方式处理 需要实现树状对象结构， 可以使用组合模式 桥接 可将一个大类或一系列紧密相关的类拆分为抽象和实现两个独立的层次结构， 从而能在开发时形成多维度的扩展体系 希望在几个独立维度上扩展一个类， 可使用该模式 代理 为其他对象提供一种代理，以控制对这个对象的访问 保护目标对象；增强目标对象 行为型 设计模式 定义 适用场景 模板方法 它在超类中定义了一个算法的框架， 允许子类在不修改结构的情况下重写算法的特定步骤 希望客户端扩展某个特定算法步骤， 而不是整个算法或其结构时， 可使用模板方法模式 迭代器 提供一种方法，顺序访问一个集合对象中的各个元素，而又不暴露该对象的内部表示 希望代码能够遍历不同的甚至是无法预知的数据结构， 可以使用迭代器模式 策略 它能让你定义一系列算法， 并将每种算法分别放入独立的类中， 以使算法的对象能够相互替换 当你想使用对象中各种不同的算法变体， 并希望能在运行时切换算法时， 可使用策略模式 解释器 为了解释一种语言，而为语言创建的解释器 （低频）某个特定类型问题发生频率足够高 观察者 定义一种订阅机制， 可在对象事件发生时通知多个 “观察” 该对象的其他对象 关联行为场景，建立一套触发机制 备忘录 保存一个对象的某个状态，以便在适当的时候恢复对象 保存及恢复数据相关业务场景（binlog） 命令 将请求转换为一个包含与请求相关的所有信息的独立对象，该转换让你能根据不同的请求将方法参数化、 延迟请求执行或将其放入队列中， 且能实现可撤销操作 想要将操作放入队列中、 操作的执行或者远程执行操作， 可使用命令模式 中介者 减少对象之间混乱无序的依赖关系。 该模式会限制对象之间的直接交互， 迫使它们通过一个中介者对象进行合作 当一些对象和其他对象紧密耦合以致难以对其进行修改时， 可使用中介者模式 责任链 允许你将请求沿着处理者链进行发送。 收到请求后， 每个处理者均可对请求进行处理， 或将其传递给链上的下个处理者 当程序需要使用不同方式处理不同种类请求， 而且请求类型和顺序预先未知时， 可以使用责任链模式 访问者 它能将算法与其所作用的对象隔离开来 数据结构与数据操作分离 状态 允许一个对象在其内部状态改变时,改变它的行为 如果对象需要根据自身当前状态进行不同行为， 同时状态的数量非常多且与状态相关的代码会频繁变更的话， 可使用状态模式 创建型 简单工厂 根据不同的参数创建不同的产品 适用场景 工厂类负责创建的对象比较少 客户端（应用层）只知道传入工厂类的参数对于如何创建对象（逻辑）不关心 优缺点 优点：只需要传入一个正确的参数，就可以获取你所需要的对象而无须知道其创建细节 缺点：工厂类的职责相对过重，增加新的产品需要修改工厂类的判断逻辑，违背开闭原则 角色说明 抽象产品：定义了产品的规范，描述了产品的主要特性和功能 具体产品：实现或继承了抽象产品的类 具体工厂：提供了创建产品的方法，调用者通过该方法来创建产品 模拟场景 咖啡店出售美式咖啡和拿铁咖啡，模拟下单场景 123456/** * 抽象产品 **/public interface Coffee &#123; void drink();&#125; 123456789/** * 具体产品 **/public class AmericanCoffee implements Coffee&#123; @Override public void drink() &#123; System.out.println(&quot;喝美式咖啡&quot;); &#125;&#125; 1234567891011121314151617/** * 具体工厂 **/public class CoffeeShop &#123; public static Coffee create(String type) &#123; Coffee coffee = null; switch (type) &#123; case &quot;american&quot;: coffee = new AmericanCoffee(); break; case &quot;latte&quot;: coffee = new LatteCoffee(); break; &#125; return coffee; &#125;&#125; 123456789public class Example &#123; public static void main(String[] args) &#123; Coffee american = CoffeeShop.create(&quot;american&quot;); american.drink(); Coffee latte = CoffeeShop.create(&quot;latte&quot;); latte.drink(); &#125;&#125; 源码解析 Spring框架的BeanFactory就使用简单工厂模式提供Bean的获取 DefaultListableBeanFactory#getBean(Class)源码，用户不必关心Bean的创建过程 123456789101112131415@Overridepublic &lt;T&gt; T getBean(Class&lt;T&gt; requiredType) throws BeansException &#123; return getBean(requiredType, (Object[]) null);&#125;@SuppressWarnings(&quot;unchecked&quot;)@Overridepublic &lt;T&gt; T getBean(Class&lt;T&gt; requiredType, @Nullable Object... args) throws BeansException &#123; Assert.notNull(requiredType, &quot;Required type must not be null&quot;); Object resolved = resolveBean(ResolvableType.forRawClass(requiredType), args, false); if (resolved == null) &#123; throw new NoSuchBeanDefinitionException(requiredType); &#125; return (T) resolved;&#125; 工厂方法 针对产品等级结构 不同的工厂生产不同的产品 定义 定义一个创建对象的接口，允许子类决定实例化对象的类型 适用场景 创建对象需要大量重复的代码 客户端(应用层)不依赖于产品类实例如何被创建、实现等细节 一个类通过其子类来指定创建哪个对象 优缺点 优点 用户只需要关心所需产品对应的工厂，无须关心创建细节 加入新产品符合开闭原则，提高可扩展性 缺点 类的个数容易过多，增加复杂度 增加了系统的抽象性和理解难度 角色说明 抽象产品：将会对接口进行声明。 对于所有由创建者及其子类构建的对象， 这些接口都是通用的 具体产品：是产品接口的不同实现 工厂方法：定义返回产品对象的工厂方法 具体创建者：将会重写基础工厂方法， 使其返回不同类型的产品 模拟场景 夏天，冰箱供销商需要买进一系列冰箱，冰箱有美的，格力的品牌，不同的品牌又有各自的工厂 123456/** * 冰箱（抽象产品） **/public abstract class Fridge &#123; abstract String getBrand();&#125; 123456789/** * 格力冰箱（具体产品） **/public class GreeFridge extends Fridge&#123; @Override String getBrand() &#123; return &quot;格力冰箱&quot;; &#125;&#125; 123456789/** * 美的冰箱（具体产品） **/public class MideaFirdge extends Fridge&#123; @Override String getBrand() &#123; return &quot;美的冰箱&quot;; &#125;&#125; 123456/** * 工厂方法 **/public abstract class Factory &#123; public abstract Fridge create();&#125; 123456789/** * 格力工厂（具体创建者） **/public class GreeFactory extends Factory &#123; @Override public Fridge create() &#123; return new GreeFridge(); &#125;&#125; 123456789/** * 美的工厂（具体创建者） **/public class MideaFactory extends Factory&#123; @Override public Fridge create() &#123; return new MideaFirdge(); &#125;&#125; 123456789public class Example &#123; public static void main(String[] args) &#123; Factory greeFactory = new GreeFactory(); Factory mideaFactory = new MideaFactory(); Fridge greeFridge = greeFactory.create(); Fridge mideaFridge = mideaFactory.create(); &#125;&#125; 源码解析 Spring框架的FactoryBean体系就用到了方法工厂模式，每个FactoryBean用户都可以自定义Bena的创建过程 抽象工厂 针对产品族 不同工厂生产不同品族的产品 定义 抽象工厂模式提供一个创建一系列相关或相互依赖对象的接口 无须指定它们具体的类 适用场景 客户端(应用层)不依赖于产品类实例如何被创建、实现等细节 强调一系列相关的产品对象(属于同一产品族)一起使用创建对象需要大量重复的代码 提供一个产品类的库，所有的产品以同样的接口出现，从而使客户端不依赖于具体实现 优缺点 优点 具体产品在应用层代码隔离，无须关心创建细节 将一个系列的产品族统一到一起创建 缺点 规定了所有可能被创建的产品集合，产品族中扩展新的产品困难，需要修改抽象工厂的接口 增加了系统的抽象性和理解难度 产品等级结构和产品族 上图表示产品族和产品等级结构的关系 产品族：具有同一个地区、同一个厂商、同一个开发包、同一个组织模块等，但是具备不同特点或功能的产品集合，称之为是一个产品族 产品等级结构：具有相同特点或功能，但是来自不同的地区、不同的厂商、不同的开发包、不同的组织模块等的产品集合，称之为是一个产品等级结构 当程序中的对象可以被划分为产品族和产品等级结构之后，那么抽象工厂方法模式才可以被适用 角色说明 抽象产品： 为构成系列产品的一组不同但相关的产品声明接口 具体产品：抽象产品的多种不同类型实现 抽象工厂：接口声明了一组创建各种抽象产品的方法 具体工厂：现抽象工厂的构建方法。 每个具体工厂都对应特定产品变体， 且仅创建此种产品变体 模拟场景 教育网有java、python课程，教程又分为视频和文章两部分 123456/** * 文章（抽象产品） **/public interface Article &#123; void produce();&#125; 123456789/** * java 文章（具体产品） **/public class JavaArticle implements Article &#123; @Override public void produce() &#123; System.out.println(&quot;编写java文章&quot;); &#125;&#125; 123456789/** * python 文章（具体产品） **/public class PythonArticle implements Article&#123; @Override public void produce() &#123; System.out.println(&quot;编写Python文章&quot;); &#125;&#125; 123456/** * 视频（抽象产品） **/public interface Video &#123; void produce();&#125; 123456789/** * java 视频（具体产品） **/public class JavaVideo implements Video&#123; @Override public void produce() &#123; System.out.println(&quot;录制Java视频&quot;); &#125;&#125; 123456789/** * python 视频（具体产品） **/public class PythonVideo implements Video&#123; @Override public void produce() &#123; System.out.println(&quot;录制Python视频&quot;); &#125;&#125; 1234567/** * 课程抽象工厂（抽象工厂，声明一组创建抽象产品的方法） **/public interface CourseFactory &#123; Video createVideo(); Article createArticle();&#125; 1234567891011121314/** * java工厂，创建java课程产品族 **/public class JavaCourseFactory implements CourseFactory&#123; @Override public Video createVideo() &#123; return new JavaVideo(); &#125; @Override public Article createArticle() &#123; return new JavaArticle(); &#125;&#125; 1234567891011121314/** * Python工厂，创建Python课程产品族 **/public class PythonCourseFactory implements CourseFactory&#123; @Override public Video createVideo() &#123; return new PythonVideo(); &#125; @Override public Article createArticle() &#123; return new PythonArticle(); &#125;&#125; 1234567public class Example &#123; public static void main(String[] args) &#123; CourseFactory courseFactory = new JavaCourseFactory(); Video javaVideo = courseFactory.createVideo(); Article javaArticle = courseFactory.createArticle(); &#125;&#125; 源码解析 Mybatis Spring 建造者 建造者模式更注重创建产品的步骤，工厂模式注重创建成品 定义 将一个复杂对象的构建与它的表示分离，使得同样的构建过程可以创建不同的表示 用户只需指定需要建造的类型就可以得到它们，建造过程及细节不需要知道 适用场景 如果一个对象有非常复杂的内部结构(很多属性) 想把复杂对象的创建和使用分离 优缺点 优点 封装性好，创建和使用分离 扩展性好、建造类之间独立、一定程度上解耦 缺点 产生多余的Builder对象 产品内部发生变化，建造者都要修改，成本较大 角色说明 产品：是最终生成的对象。 由不同生成器构造的产品无需属于同一类层次结构或接口 生成器：接口声明,在所有类型生成器中通用的产品构造步骤 具体生成器：提供构造过程的不同实现。 具体生成器也可以构造不遵循通用接口的产品 主管（Director）：定义调用构造步骤的顺序， 这样你就可以创建和复用特定的产品配置 使用 12345678910/** * 产品 **/@Data@ToStringpublic class Product &#123; private Integer id; private BigDecimal price; private Integer stock;&#125; 12345678910/** * 生成器 **/public abstract class Builder &#123; abstract void buildProductId(Integer id); abstract void buildProductPrice(BigDecimal price); abstract void buildProductStock(Integer stock); abstract Product makeProduct();&#125; 1234567891011121314151617181920212223242526/** * 具体生成器 **/public class ProductBuilder extends Builder&#123; private Product product = new Product(); @Override void buildProductId(Integer id) &#123; product.setId(id); &#125; @Override void buildProductPrice(BigDecimal price) &#123; product.setPrice(price); &#125; @Override void buildProductStock(Integer stock) &#123; product.setStock(stock); &#125; @Override Product makeProduct() &#123; return product; &#125;&#125; 1234567891011121314151617/** * 主管类，定义调用构造步骤的顺序 **/public class Director &#123; private Builder builder; public Director(Builder builder) &#123; this.builder = builder; &#125; public Product makeProduct(Integer id, BigDecimal price, Integer stock) &#123; this.builder.buildProductId(id); this.builder.buildProductPrice(price); this.builder.buildProductStock(stock); return builder.makeProduct(); &#125;&#125; 12345678public class Example &#123; public static void main(String[] args) &#123; Builder builder = new ProductBuilder(); Director director = new Director(builder); Product product = director.makeProduct(1, new BigDecimal(10), 10); System.out.println(product); &#125;&#125; 内部静态类模式（常用） 12345678910111213141516171819202122232425262728293031323334353637383940414243444546@Data@ToStringpublic class Product &#123; public Product(Integer id, BigDecimal price, Integer stock) &#123; this.id = id; this.price = price; this.stock = stock; &#125; private Integer id; private BigDecimal price; private Integer stock; public static ProductBuilder builder() &#123; return new ProductBuilder(); &#125; // 构建类 public static class ProductBuilder &#123; private Integer id; private BigDecimal price; private Integer stock; // 链式调用 public Product.ProductBuilder id(final Integer id) &#123; this.id = id; return this; &#125; public Product.ProductBuilder price(final BigDecimal price) &#123; this.price = price; return this; &#125; public Product.ProductBuilder stock(final Integer stock) &#123; this.stock = stock; return this; &#125; // 通过构造函数创建对象 public Product build() &#123; return new Product(this.id, this.price, this.stock); &#125; &#125;&#125; 12345678910public class Example &#123; public static void main(String[] args) &#123; Product product = Product.builder() .id(1) .price(new BigDecimal(10)) .stock(10) .build(); System.out.println(product); &#125;&#125; 源码解析 JDK StringBuilder StringBuffer 底层都是byte[]，使用构建者模式简化字符串拼接 Mybatis SqlSessionFactoryBuilder：使用mybatis.xml配置文件通过SqlSessionFactoryBuilder 建造 SqlSessionFactory XMLConfigBuilder XMLMapperBuilder XMLStatementBuilder CacheBuilder Spring BeanDefinitionBuilder用于构造BeanDefinition 原型 细胞分裂 依赖对象的克隆复制新的对象 定义 指原型实例指定创建对象的种类，并且通过拷贝这些原型创建新的对象 不需要知道任何创建的细节，不调用构造函数 适用场景 类初始化消耗较多资源 new产生的一个对象需要非常繁琐的过程(数据准备、访问权限等) 构造函数比较复杂 循环体中生产大量对象时 优缺点 优点 原型模式性能比直接new一个对象性能高 简化创建过程 缺点 必须配备克隆方法 对克隆复杂对象或对克隆出的对象进行复杂改造时，容易引入风险 深拷贝、浅拷贝要运用得当 深克隆、浅克隆 参考 深拷贝和浅拷贝 深拷贝还可以通过序列化的方式来实现 角色说明 原型：接口将对克隆方法进行声明，JDK提供的Cloneable接口是个不错的选择 具体原型：实现克隆方法。 除了将原始对象的数据复制到克隆体中之外， 该方法有时还需处理克隆过程中的极端情况， 例如克隆关联对象和梳理递归依赖等等 客户端：可以复制实现了原型接口的任何对象 模拟场景 批量发送营销邮件，邮件除了邮件地址其他内容都一样 123456789101112131415/** * 具体原型 **/@Data@ToStringpublic class Mail implements Cloneable&#123; private String emailAddress; private String content; // 实现克隆方法 @Override protected Object clone() throws CloneNotSupportedException &#123; return super.clone(); &#125;&#125; 123456789101112131415161718public class Example &#123; public static void main(String[] args) throws CloneNotSupportedException &#123; // 创建原型 Mail prototype = new Mail(); prototype.setContent(&quot;双十一大促销！&quot;); for (int i = 0; i &lt; 10; i++) &#123; // 根据原型克隆 Mail mail = (Mail) prototype.clone(); mail.setEmailAddress(i + &quot;@test.com&quot;); send(mail); &#125; &#125; public static void send(Mail mail) &#123; System.out.println(mail); &#125;&#125; 单例 系统中只存在一个实例 定义 能够保证一个类只有一个实例， 并提供一个访问该实例的全局入口 优缺点 优点 单例模式可以保证内存里只有一个实例，减少了内存的开销 为共享资源提供一个全局统一的访问点 缺点 单例模式一般没有实现接口，扩展困难 单例模式如果业务过重，则很容易违背单一责职原则 适用场景 如果程序中的某个类对于所有客户端只有一个可用的实例，可以使用单例模式 如果你需要更加严格地控制全局变量，可以使用单例模式 数据库连接池的设计一般采用单例模式 容器单例 12345678910111213141516171819202122232425262728293031323334353637public class ContainerSingleton &#123; // 防止实例化 private ContainerSingleton() &#123; &#125; // 单例容器 private static Map&lt;String, Object&gt; singletonMap = new ConcurrentHashMap&lt;&gt;(); /** * 将实例放入容器 * * @param key * @param instance */ public static void setInstance(String key, Object instance) &#123; if (Objects.isNull(key)) &#123; throw new IllegalArgumentException(&quot;key is null&quot;); &#125; if (Objects.isNull(instance)) &#123; throw new IllegalArgumentException(&quot;instance is null&quot;); &#125; singletonMap.computeIfAbsent(key, k -&gt; instance); &#125; public static Object getInstance(String key) &#123; if (Objects.isNull(key)) &#123; throw new IllegalArgumentException(&quot;key is null&quot;); &#125; return singletonMap.get(key); &#125;&#125; 123456public class Example &#123; public static void main(String[] args) &#123; ContainerSingleton.setInstance(&quot;one&quot;, 1); ContainerSingleton.setInstance(&quot;two&quot;, 2); &#125;&#125; 容器单例模式不负责对象的创建过程，而是将已有对象存入容器重复使用，让容器保证实例的单例性，常用于缓存 懒汉式 非线程安全 懒加载思想 12345678910111213141516public class Singleton &#123; private static Singleton singleton; // 避免被实例化 private Singleton() &#123; &#125; public static Singleton getSingleton() &#123; // 比较懒，用到才判断 if (singleton == null) &#123; // 并发情况下有线程安全问题 singleton = new Singleton(); &#125; return singleton; &#125;&#125; 懒汉式最大的缺点就是面对多线程获取单例的时候，无法保证线程安全，可能会创建多个实例 饿汉式 线程安全 123456789101112public class Singleton &#123; // 避免被实例化 private Singleton() &#123; &#125; // 非常饥饿，一上来就实例化 private static final Singleton instance = new Singleton(); public static Singleton getInstance() &#123; return instance; &#125;&#125; 饿汉式是线程安全的，依赖于JVM类加载时机创建对象。缺点在于无法实现懒加载，造成资源浪费 双重检查 volative修饰保证线程可见性 synchronized双重检查 123456789101112131415161718192021public class Singleton &#123; // 避免被实例化 private Singleton() &#123; &#125; // 保证多线程可见性 private static volatile Singleton instance; public Singleton getInstance() &#123; if (instance == null) &#123; synchronized (Singleton.class) &#123; // 存在并发线程安全 if (instance == null) &#123; instance = new Singleton(); &#125; &#125; &#125; return instance; &#125;&#125; 双重检查解决并发获取实例时线程安全问题 枚举(推荐) 破坏单例模式 内部枚举方式可以防止使用反射和序列化破坏单例模式 依赖JVM对枚举实例创建的线程安全性创建单例 1234567public enum Singleton &#123; INSTANCE; public void doSomething() &#123; System.out.println(&quot;doSomething&quot;); &#125;&#125; 12345public class Example &#123; public static void main(String[] args) &#123; Singleton.INSTANCE.doSomething(); &#125;&#125; 结构型 外观（门面） 微服务就是使用外观模式，向外提供简单的Api调用 定义 又叫门面模式，提供了一个统一的接口，用来访问子系统中的一群接口 外观模式定义了一个高层接口，让子系统更容易使用 适用场景 子系统越来越复杂，增加外观模式提供简单调用接口 构建多层系统结构，利用外观对象作为每层的入口，简化层间调用 优缺点 优点 简化了调用过程，无需了解深入子系统，防止带来风险 减少系统依赖、松散耦合 更好的划分访问层次 符合迪米特法则，即最少知道原则 缺点 增加子系统、扩展子系统行为容易引入风险 不符合开闭原则 与其他设计模式对比 外观模式和中介者 外观模式关注的是外界和子系统的交互 中介者模式关注的是子系统内部的交互 外观模式和单例模式 通常把外观模式的外观对象做成单例模式来使用 外观模式和抽象工厂模式 外观类可以通过抽象工厂获取子系统实例，子系统可以将内部对外观类屏蔽 模拟场景 分布式系统中的下单场景，系统分库存服务，订单服务，物流服务 12345678910111213141516171819/** * 订单外观类，调用分布式分服务 **/public class OrderFacade &#123; // 正常情况是注入，都是远程RPC服务 private StockService stockService = new StockService(); private OrderService orderService = new OrderService(); private LogisticsService logisticsService = new LogisticsService(); // 下单, 对外屏蔽子系统 public void create(OrderParam orderParam) &#123; // 库存扣减 stockService.deduction(orderParam); // 生成订单 orderService.create(orderParam); // 生成物流单 logisticsService.create(orderParam); &#125;&#125; 12345678/** * 库存服务 **/public class StockService &#123; public void deduction(OrderParam param) &#123; System.out.println(&quot;库存扣减&quot;); &#125;&#125; 12345678/** * 订单服务 **/public class OrderService &#123; public void create(OrderParam param) &#123; System.out.println(&quot;生成订单&quot;); &#125;&#125; 12345678/** * 物流服务 **/public class LogisticsService &#123; public void create(OrderParam param) &#123; System.out.println(&quot;生成物流单号&quot;); &#125;&#125; 123456789101112public class Client &#123; public static void main(String[] args) &#123; OrderParam param = new OrderParam(); param.setNum(1); param.setSku(&quot;test&quot;); // 外观类 OrderFacade facade = new OrderFacade(); facade.create(param); &#125;&#125; 源码解析 Mybatis Mybatis通过外观设计模式封装了SqlSession会话层，对外屏蔽了Executor调用的复杂性 Spring JDBC Spring JDBC的JdbcUtils通过对原生的 jdbc 进行了封装降低原生jdbc使用的复杂性 装饰者 不改变原有对象扩展对象功能 定义 在不改变原有对象的基础之上，将功能附加到对象上 提供了比继承更有弹性的替代方案(扩展原有对象功能) 适用场景 扩展一个类的功能或给一个类添加附加职责 动态的给一个对象添加功能，这些功能可以再动态的撤销 优缺点 优点 继承的有力补充，比继承灵活，不改变原有对象的情况下给一个对象扩展功能 通过使用不同装饰类以及这些装饰类的排列组合，可以实现不同效果 符合开闭原则 缺点 会出现更多的代码，更多的类，增加程序复杂性 动态装饰时，多层装饰时会更复杂 角色说明 抽象构件：它是具体构件和抽象装饰类的共同父类，声明了要在具体构件中实现的业务方法，它的引入可以使客户端以一致的方式处理未被装饰的对象以及装饰之后的对象。抽象构件一般定义为接口 具体构件：它是抽象构件类的子类，用于定义具体的构件对象，实现了在抽象构件中声明的方法，装饰对象可以给它增加额外的职责（方法） 抽象装饰类：它也是抽象构件类的子类，用于给具体构件增加职责，但是具体职责在其子类中实现。它维护一个指向抽象构件对象的引用，通过该引用可以调用装饰之前构件对象的方法，并通过其子类扩展该方法，以达到装饰的目的 具体装饰类：它是抽象装饰类的子类，负责向构件添加新的职责。每一个具体装饰类都定义了一些新的行为，它可以调用在抽象装饰类中定义的方法，并可以增加新的方法用以扩充对象的行为 与其他设计模式对比 装饰者模式和代理模式 相同点 两种从设计模式分类来看都属于结构型，因为两者均使用了组合关系。其次两者都能实现对对象方法进行增强处理的效果 不同点 装饰者模式：注重的是对对象提供增强功能 代理模式：对所代理对象的使用施加控制，并不提供对象本身的增强功能 装饰者模式和适配器模式 相同点 装饰器与适配器都有一个别名叫做 包装模式(Wrapper)，它们看似都是起到包装一个类或对象的作用 不同点 适配器模式：是要将一个接口转变成另一个接口，它的目的是通过改变接口来达到重复使用的目的 装饰器模式：不是要改变被装饰对象的接口，而是恰恰要保持原有的接口，但是增强原有对象的功能 模拟场景 奶茶店出售的奶茶可以是基本的珍珠奶茶，用户也可以选择自定义添加底料，比如加奶盖，焦糖 1234567/** * 定义奶茶（抽象构件） **/public abstract class AbstractMilkTea &#123; protected abstract String getDesc(); protected abstract int getPrice();&#125; 1234567891011121314/** * 奶茶（具体构件） **/public class MilkTea extends AbstractMilkTea &#123; @Override protected String getDesc() &#123; return &quot;珍珠奶茶&quot;; &#125; @Override protected int getPrice() &#123; return 8; &#125;&#125; 1234567891011121314/** * 抽象装饰类继承抽象构件 **/public abstract class AbstractDecorator extends AbstractMilkTea &#123; // 抽象装饰类持有构件 protected AbstractMilkTea milkTea; public AbstractDecorator(AbstractMilkTea milkTea) &#123; this.milkTea = milkTea; &#125; // 装饰类扩展的方法，是否赠送礼品 protected abstract boolean sendGifts();&#125; 123456789101112131415161718192021222324/** * 加焦糖 **/public class CaramelDecorator extends AbstractDecorator &#123; public CaramelDecorator(AbstractMilkTea milkTea) &#123; super(milkTea); &#125; @Override protected boolean sendGifts() &#123; return false; &#125; // 对原有对象进行增强 @Override protected String getDesc() &#123; return milkTea.getDesc() + &quot;加焦糖&quot;; &#125; @Override protected int getPrice() &#123; return milkTea.getPrice() + 2; &#125;&#125; 1234567891011121314151617181920212223/** * 加奶盖 **/public class MilkCapDecorator extends AbstractDecorator &#123; public MilkCapDecorator(AbstractMilkTea milkTea) &#123; super(milkTea); &#125; @Override protected boolean sendGifts() &#123; return true; &#125; @Override protected String getDesc() &#123; return milkTea.getDesc() + &quot;加奶盖&quot;; &#125; @Override protected int getPrice() &#123; return milkTea.getPrice() + 5; &#125;&#125; 1234567891011121314151617public class Example &#123; public static void main(String[] args) &#123; // 珍珠奶茶 AbstractMilkTea abstractMilkTea = new MilkTea(); // 加焦糖 abstractMilkTea = new CaramelDecorator(abstractMilkTea); System.out.println(abstractMilkTea.getDesc()); System.out.println(abstractMilkTea.getPrice()); // 加奶盖 abstractMilkTea = new MilkCapDecorator(abstractMilkTea); System.out.println(abstractMilkTea.getDesc()); System.out.println(abstractMilkTea.getPrice()); &#125;&#125; 源码解析 jdk的缓冲流体系 *Buffered* 的缓冲流都是对输入输出流进行包装达增强，达到快速读写目的 Spring Spring 在Cache体系中使用了装饰者模式对缓存进行事务感知。简单的、事务感知的缓存装饰器，它在提交之前暂时保存缓存值，以避免在回滚时用无效值污染缓存 适配器 电源适配器，通过电压转换后各种设备可以正常使用 定义 将一个类的接口转换成客户期望的另一个接口 使原本接口不兼容的类可以一起工作 适用场景 已经存在的类，它的方法和需求不匹配时（方法结果相同或相似，对不匹配的类进行复用） 不是软件设计阶段考虑的设计模式，是随着软件维护，由于不同产品、不同厂家造成功能类似而接口不相同情况下的解决方案 优缺点 优点 能提高类的透明性和复用，现有的类复用但不需要改变 目标类和适配器类解耦，提高程序扩展性 缺点 适配器编写过程需要全面考虑，可能会增加系统的复杂性 增加系统代码可读的难度 与其他设计模式对比 适配器和外观模式 相同点：都是对现有的类进行封装 外观模式：定义了新的统一接口封装了子系统的一群接口 适配器模式：主要是复用一个原有的接口，使已有的两个接口适配工作 模拟场景 （对象适配） 中国的电压是220V的，美国的电压是110V的，使用一个电源适配器让美国的家电能在中国使用 1234567/** * 中国电压标准 */public interface ChinaVoltageStandard &#123; // 定义输出220V int output220V();&#125; 12345678910/** * 中国电压 */public class ChinaVoltage implements ChinaVoltageStandard &#123; @Override public int output220V() &#123; System.out.println(&quot;中国电压输出220V&quot;); return 220; &#125;&#125; 12345678/** * 美国电压标准 */public interface UsVoltageStandard &#123; // 定义输出110V int output110V();&#125; 1234567891011121314/** * 美国电压适配器遵循美国电压标准 * 将220V的电转为110V的电 */public class UsVoltageAdapter implements UsVoltageStandard &#123; private ChinaVoltage chinaVoltage = new ChinaVoltage(); @Override public int output110V() &#123; System.out.println(&quot;电压适配器转换电压&quot;); int voltage = chinaVoltage.output220V() / 2; return voltage; &#125;&#125; 123456public class Example &#123; public static void main(String[] args) &#123; UsVoltageStandard usVoltage = new UsVoltageAdapter(); System.out.println(&quot;输出电压：&quot; + usVoltage.output110V()); &#125;&#125; 类适配 适配器继承被适配类实和现适配接口 场景：服务重构，老订单服务适配新订单服务接口 1234public interface OrderService &#123; // 下单支付 boolean pay(String orderNo, Integer total);&#125; 1234567public class OrderServiceImpl implements OrderService &#123; @Override public boolean pay(String orderNo, Integer total) &#123; System.out.println(&quot;OrderService 下单扣款金额&quot; + total); return true; &#125;&#125; 123public interface OrderServiceV2 &#123; boolean pay(String orderNo, Integer total, Integer discount);&#125; 12345678public class OrderServiceV2Impl implements OrderServiceV2&#123; @Override public boolean pay(String orderNo, Integer total, Integer discount) &#123; total = total - discount; System.out.println(&quot;OrderServiceV2 下单扣款金额&quot; + total); return true; &#125;&#125; 12345678public class OrderServiceV2Adapter extends OrderServiceImpl implements OrderServiceV2 &#123; @Override public boolean pay(String orderNo, Integer total, Integer discount) &#123; // 适配逻辑 total = total - discount; return super.pay(orderNo, total); &#125;&#125; 123456public class Example &#123; public static void main(String[] args) &#123; OrderServiceV2 orderService = new OrderServiceV2Adapter(); orderService.pay(&quot;t123&quot;, 10, 2); &#125;&#125; 源码解析 Spring AOP将Advisor适配为MethodInterceptor Spring MVC的HandlerAdapter，spring定义了一个适配器接口，使得每一种Controller有一种对应的适配器实现类。Controller种类很多，有需要视图渲染的，参数解析、封装的，有直接返回Json、Xml数据的。 享元 共享 常用于缓存框架 减少对象的创建，减少内存的占用 定义 提供了减少对象数量从而改善应用所需的对象结构的方式 通过共享多个对象所共有的相同状态， 在有限的内存容量中载入更多对象 适用场景 字符串常量池 数据库连接池 常常应用于系统底层的开发，以便解决系统的性能问题 系统有大量相似对象、需要缓冲池的场景 优缺点 优点 减少对象的创建，降低内存中对象的数量，降低系统的内存，提高效率 减少内存之外的其他资源占用（比如new对象占用的创建时间，文件句柄，窗口句柄） 缺点 关注内/外部状态、关注线程安全问题 使系统、程序的逻辑复杂化 扩展 内部状态：内部状态是存储在享元对象内部并且不会随环境变化而变化的状态，因此内部状态可以共享 外部状态：外部状态是随环境改变而改变的、不可以共享的状态。享元对象的外部状态必须由客户端保存，并在享元对象被创建之后，在需要使用的时候再传入到享元对象内部。外部状态之间是相互独立的 与其他设计模式对比 享元模式和代理模式 代理模式需要代理一个类，如果生成这个代理类需要花费的时间和资源非常多，则可以使用享元模式提高程序的处理速度 享元模式和单例模式 根据不同的业务场景使用，比如缓存级别的对象复用，则使用享元模式。如果是工具级别的对象并且其创建过程很耗费资源则使用单例模式 模拟场景 享元工厂 在一个数据库会话中实现一个数据库查询缓存插件，将查询结构存入缓存，不需要每次都去数据库中查询 12345678/** * 缓存接口 */public interface Cache &#123; // 获取缓存数据 Object getData();&#125; 12345678910111213141516171819202122232425262728/** * 缓存实现 */public class SimpleCache implements Cache &#123; public SimpleCache(Object data) &#123; this.data = data; &#125; //缓存时间，内部状态，不会随着外部环境改变，可用于控制缓存失效 private Date cacheTime = new Date(); // 外部状态 private Object data; @Override public Object getData() &#123; return data; &#125; @Override public String toString() &#123; return &quot;SimpleCache&#123;&quot; + &quot;cacheTime=&quot; + cacheTime + &quot;, data=&quot; + data + &#x27;&#125;&#x27;; &#125;&#125; 123456789101112/** * 缓存工厂 */public class CacheFactory &#123; private final static Map&lt;String, Cache&gt; CACHE_CONTAINER = new ConcurrentHashMap&lt;&gt;(); public static Cache getCache(String sql) &#123; // 模拟缓存没有则从数据库中查询 Cache cache = CACHE_CONTAINER.computeIfAbsent(sql, (key) -&gt; new SimpleCache((int)(Math.random() * 100))); return cache; &#125;&#125; 12345678910public class Example &#123; public static void main(String[] args) &#123; for (int i = 0; i &lt; 10; i++) &#123; int no = (int)(Math.random() * 2); String sql = &quot;select id from user where no=&quot; + no; Cache cache = CacheFactory.getCache(sql); System.out.println(sql + &quot; -&gt; &quot; + cache); &#125; &#125;&#125; 源码解析 JDK Java源码中的Integer存在缓存池，如果取值在[-128,127]，则取缓存池中的对象 Mybatis Mybatis的Configuration类缓存了MappedStatement、ResultMap、ParameterMap等信息 组合 目录 菜单 将多种类型的对象按照一致的方式处理 定义 将对象组合成树形结构以表示部分-整体的层次结构 组合模式使客户端对单个对象和组合对象保持一致的方式处理 适用场景 希望客户端可以忽略组合对象与单个对象的差异时 如果你需要实现树状对象结构， 可以使用组合模式 优缺点 优点 清楚地定义分层次的复杂对象，表示对象的全部或部分层次 让客户端忽略了层次的差异，方便对整个层次结构进行控制 简化客户端代码 缺点 限制类型时会较为复杂（多个不同的对象不一定都满足基类的方法） 使设计变得更加抽象 角色说明 组件接口：描述了树中简单项目和复杂项目所共有的操作 叶节点：是树的基本结构， 它不包含子项目 容器：又名 组合 （Composite）是包含叶节点或其他容器等子项目的单位。 容器不知道其子项目所属的具体类， 它只通过通用的组件接口与其子项目交互 与其他设计模式对比 组合模式和访问者模式 可以使用访问者模式来访问组合模式的递归结构 模拟场景 模拟系统菜单，菜单分为菜单目录和菜单项，菜单目录有名称并且能够添加菜单项，菜单项不能继续添加菜单项但有菜单名称和跳转Url，构建一个目录树，打印出所有的菜单目录和菜单项 12345678910111213141516171819202122232425262728/** * 菜单类（组件） */public abstract class Menu &#123; public String getName() &#123; throw new UnsupportedOperationException(&quot;不支持获取名称&quot;); &#125; public String getUrl() &#123; throw new UnsupportedOperationException(&quot;不支持获取url&quot;); &#125; public int getLevel() &#123; throw new UnsupportedOperationException(&quot;不支持获取level&quot;); &#125; public void add(Menu menu) &#123; throw new UnsupportedOperationException(&quot;不支持添加操作&quot;); &#125; public void remove(Menu menu) &#123; throw new UnsupportedOperationException(&quot;不支持删除操作&quot;); &#125; public void print() &#123; throw new UnsupportedOperationException(&quot;不支持删除操作&quot;); &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647/** * 菜单目录（容器） */public class MenuCatalog extends Menu &#123; // 目录下的菜单项 private List&lt;Menu&gt; items = new ArrayList&lt;&gt;(); private String name; private int level; public MenuCatalog(String name, int level) &#123; this.name = name; this.level = level; &#125; @Override public String getName() &#123; return this.name; &#125; @Override public int getLevel() &#123; return level; &#125; @Override public void add(Menu menu) &#123; this.items.add(menu); &#125; @Override public void remove(Menu menu) &#123; this.items.remove(menu); &#125; @Override public void print() &#123; System.out.println(this.name); for (Menu item : items) &#123; // 类型限制，组合模式的坑点，打印缩进 for (int i = 0; i &lt; item.getLevel(); i++) &#123; System.out.print(&quot; &quot;); &#125; item.print(); &#125; &#125;&#125; 12345678910111213141516171819202122232425262728293031323334/** * 菜单项（叶节点） */public class MenuItem extends Menu &#123; private String name; private String url; private int level; public MenuItem(String name, String url, int level) &#123; this.name = name; this.url = url; this.level = level; &#125; @Override public String getUrl() &#123; return url; &#125; @Override public int getLevel() &#123; return level; &#125; @Override public String getName() &#123; return name; &#125; @Override public void print() &#123; System.out.println(&quot;name：&quot; + name + &quot; url：&quot; + url); &#125;&#125; 123456789101112131415161718192021222324252627282930public class Example &#123; public static void main(String[] args) &#123; Menu root = new MenuCatalog(&quot;主目录&quot;, 1); Menu search = new MenuCatalog(&quot;搜索类&quot;, 2); Menu video = new MenuCatalog(&quot;视频类&quot;, 2); root.add(search); root.add(video); Menu google = new MenuItem(&quot;谷歌&quot;, &quot;https://google.com&quot;, 3); Menu baidu = new MenuItem(&quot;百度&quot;, &quot;https://www.baidu.com&quot;, 3); search.add(google); search.add(baidu); Menu bilibili = new MenuItem(&quot;B站&quot;, &quot;https://www.bilibili.com&quot;, 3); Menu youku = new MenuItem(&quot;B站&quot;, &quot;https://youku.com&quot;, 3); video.add(bilibili); video.add(youku); root.print(); &#125;&#125;// 输出主目录 搜索类 name：谷歌 url：https://google.com name：百度 url：https://www.baidu.com 视频类 name：B站 url：https://www.bilibili.com name：B站 url：https://youku.com 源码解析 JDK JDK源码的ArrayList#addAll方法的入参类型不是List，而是它的父类Collection Spring Spring缓存模块的CompositeCacheManager复合缓存管理器可以管理不同中间件的缓存。如CaffeineCacheManager,RedisCacheManager等 桥接 抽象和实现分离 避免子类爆炸 通过组合连接两个继承体系 定义 将抽象部分与它的具体实现部分分离，使它们都可以独立地变化 通过组合的方式建立两个类之间联系，而不是继承 适用场景 如果希望在几个独立维度上扩展一个类， 可使用该模式 如果需要在运行时切换不同实现方法， 可使用桥接模式（使用聚合实现） 不希望使用继承，或因为多层继承导致系统类的个数剧增 优缺点 优点 分离抽象部分及其具体实现部分 提高了系统的可扩展性 客户端代码仅与高层抽象部分进行互动， 不会接触到平台的详细信息（桥接是两个继承体系抽象层的聚合） 缺点 增加了系统的理解与设计难度 需要正确地识别出系统中两个独立变化的维度 与其他设计模式对比 桥接模式和组合模式 组合模式：强调的是部分和整体的组合 桥接模式：强调平行级别上不同类的组合 桥接模式和适配器模式 共同点：都是让两个不同的对象配合工作 不同点 适配器模式：改变已有的接口，让它们之间可以相互配合，让功能上相似但接口不同的类适配起来 桥接模式：分离抽象和具体的实现，目的是分离。在分离的基础上使这些层次结合起来 模拟场景 中国有很多家银行，每家银行又有储蓄卡和信用卡。模拟聚合支付平台查询用户的金额 123456789101112/** * 银行（抽象层聚合） */public abstract class Bank &#123; protected Card card; public Bank(Card card) &#123; this.card = card; &#125; public abstract float query();&#125; 123456789101112131415/** * 中国农业银行 */public class ABCBank extends Bank&#123; public ABCBank(Card card) &#123; super(card); &#125; @Override public float query() &#123; System.out.println(&quot;查询中国农业银行余额&quot;); return card.getTotal(); &#125;&#125; 123456789101112131415/** * 中国工商银行 */public class ICBCBank extends Bank&#123; public ICBCBank(Card card) &#123; super(card); &#125; @Override public float query() &#123; System.out.println(&quot;查询中国工商银行余额&quot;); return card.getTotal(); &#125;&#125; 1234567/** * 银行卡 */public interface Card &#123; // 获取余额 float getTotal();&#125; 12345678910111213141516/** * 储蓄卡 */public class BankCard implements Card&#123; private float debit; public BankCard(float debit) &#123; this.debit = debit; &#125; @Override public float getTotal() &#123; System.out.println(&quot;读取储蓄卡，余额：&quot; + this.debit); return this.debit; &#125;&#125; 12345678910111213141516/** * 信用卡 */public class CreditCard implements Card&#123; private float debit; public CreditCard(float debit) &#123; this.debit = debit; &#125; @Override public float getTotal() &#123; System.out.println(&quot;读取信用卡，余额：&quot; + this.debit); return this.debit; &#125;&#125; 12345678910111213141516171819202122public class Example &#123; public static void main(String[] args) &#123; // 中国农业银行储蓄卡 ABCBank abcBank = new ABCBank(new BankCard(100.f)); abcBank.query(); // 中国农业银行信用卡 abcBank = new ABCBank(new CreditCard(500.f)); abcBank.query(); // 中国工商银行信用卡 ICBCBank icbcBank = new ICBCBank(new CreditCard(1000.f)); icbcBank.query(); &#125;&#125;// 输出查询中国农业银行余额读取储蓄卡，余额：100.0查询中国农业银行余额读取信用卡，余额：500.0查询中国工商银行余额读取信用卡，余额：1000.0 源码解析 JDK JDK数据库连接（JDBC）的Driver接口，不同的数据库厂商都实现了Driver接口。通过各自的Driver都可以获取各自的数据连接Connection。数据库Driver是一个独立扩展体系，而数据库的Connection是另一个独立的扩展体系 代理 控制目标对象的访问 代理模式导致执行效率降低 定义 为其他对象提供一种代理，以控制对这个对象的访问 代理对象在客户端和目标对象之间起到中介的作用 适用场景 保护目标对象 增强目标对象 优缺点 优点 代理模式能将代理对象与真实被调用的目标对象分离 一定程度上降低了系统的耦合度，扩展性好 保护目标对象 增强目标对象 缺点 代理模式会造成系统设计中类的数目增加 在客户端和目标对象增加一个代理对象，会造成请求处理速度变慢 增加系统的复杂度 扩展 静态代理： 代理模式 动态代理：InvocationHandler（无法代理类，动态代理接口） CGLib代理：字节码增强（无法增强final） Spring：当有实现接口时使用JDK动态代理，否则使用CGLib代理 性能对比：JDK &gt; CGLib 角色说明 服务接口：声明了服务接口。 代理必须遵循该接口才能伪装成服务对象 服务：实现服务接口，实现业务逻辑 代理：代理对象和服务是组合关系，代理对象控制服务对象并且实现服务接口以实现对服务对象的代理 与其他设计模式对比 代理模式和装饰者模式 相同点 两种从设计模式分类来看都属于结构型，因为两者均使用了组合关系。其次两者都能实现对对象方法进行增强处理的效果 不同点 装饰者模式：注重的是对对象提供增强功能 代理模式：对所代理对象的使用施加控制，并不提供对象本身的增强功能 代理模式和适配器模式 适配器模式：主要改变所考虑对象的接口（类a转变为类b） 代理模式：不能改变所考虑类的接口 源码解析 Spring spring的代理模式在aop中有体现，如果目标类有接口实现则使用JDK动态代理JdkDynamicAopProxy否则使用CglibAopProxy Mybatis mybatis的Mapper接口动态代理实现 静态代理 模拟场景：给Dao层添加代理，如果本地缓存有数据则控制获取本地缓存数据，否则获取数据库数据 1234public interface OrderDao &#123; // 查询订单编号 String selectOrderOn(Integer id);&#125; 1234567public class OrderDaoImpl implements OrderDao&#123; @Override public String selectOrderOn(Integer id) &#123; System.out.println(&quot;数据库查询&quot;); return UUID.randomUUID().toString(); &#125;&#125; 12345678910111213141516171819202122232425262728293031323334public class OrderDaoImplProxy implements OrderDao &#123; private Map&lt;Integer, String&gt; cache = new HashMap&lt;&gt;(); // 装饰者一般是传入一个目标对象进行增强，而代理模式则是在代理中直接创建对象 private OrderDaoImpl orderDaoImpl = new OrderDaoImpl(); private String beforeMethod(Integer id) &#123; String orderNo = cache.get(id); // 施加控制，如果缓存有数据直接返回，否则查询数据库 if (orderNo != null) &#123; System.out.println(&quot;缓存数据返回&quot;); return orderNo; &#125; return null; &#125; @Override public String selectOrderOn(Integer id) &#123; String orderNo = beforeMethod(id); // 缓存没有数据库加载 if (orderNo == null) &#123; orderNo = orderDaoImpl.selectOrderOn(id); cache.put(id, orderNo); &#125; afterMethod(); return orderNo; &#125; private void afterMethod() &#123; System.out.println(&quot;执行静态代理方法&quot;); &#125;&#125; 123456789101112public class Example &#123; public static void main(String[] args) &#123; OrderDao orderDao = new OrderDaoImplProxy(); orderDao.selectOrderOn(1); orderDao.selectOrderOn(1); &#125;&#125;// 输出数据库查询执行静态代理方法缓存数据返回执行静态代理方法 动态代理 代理类需要实现InvocationHandler接口并且实现invoke方法 通过Proxy.newProxyInstance创建代理实例 模拟场景：模拟Mybatis对Mapper接口的动态代理 12345678/** * 代理接口 **/public interface UserMapper &#123; Map&lt;Object, Object&gt; selectById(Integer id); Integer deleteById(Integer id);&#125; 123456789101112131415161718192021222324252627/** * 代理类 * InvocationHandler 实现类 **/public class MapperProxy implements InvocationHandler &#123; private MapperImpl mapperImpl = new MapperImpl(); @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; // 调用Object方法 if (Object.class.equals(method.getDeclaringClass())) &#123; return method.invoke(this, args); &#125; Object result = null; if (method.getName().startsWith(&quot;select&quot;)) &#123; result = this.mapperImpl.select(args[0]); &#125; else if (method.getName().startsWith(&quot;update&quot;) || method.getName().startsWith(&quot;insert&quot;) || method.getName().startsWith(&quot;delete&quot;)) &#123; result = this.mapperImpl.update(args[0]); &#125; return result; &#125;&#125; 12345678910111213141516171819/** * Mapper 通用实现类 **/public class MapperImpl &#123; public Object select(Object param) &#123; System.out.println(&quot;执行查询语句，参数：&quot; + param.toString()); // 模拟返回数据 Map&lt;String, Object&gt; resultMap = new HashMap&lt;&gt;(); resultMap.put(&quot;name&quot;, UUID.randomUUID().toString()); return resultMap; &#125; public Integer update(Object param) &#123; // 底层都是调用JDBC的 Statement.executeUpdate(sql); System.out.println(&quot;执行 新增|修改|删除 语句，参数：&quot; + param.toString()); // 模拟返回受影响行数 return 1; &#125;&#125; 123456789101112131415161718192021222324public class Example &#123; public static void main(String[] args) &#123; MapperProxy proxy = new MapperProxy(); /** * 第一个是参数是代理对象的类加载器 * 第二个参数是代理接口数组 * 第三个参数是 InvocationHandler的具体实现类，接口的所有代理实现都在 invoke 中实现 */ UserMapper userMapper = (UserMapper) Proxy.newProxyInstance(MapperImpl.class.getClassLoader(), new Class[]&#123;UserMapper.class&#125;, proxy); Map&lt;Object, Object&gt; resultMap = userMapper.selectById(1); System.out.println(&quot;selectById 返回值：&quot; + resultMap); Integer result = userMapper.deleteById(10); System.out.println(&quot;deleteById 返回值：&quot; + result); &#125;&#125;// 输出执行查询语句，参数：1selectById 返回值：&#123;name=ab1de88a-cb57-461a-acea-d111f481a7c0&#125;执行 新增|修改|删除 语句，参数：10deleteById 返回值：1 CGLib动态字节码代理 CGlib的核心是 MethodInterceptor接口和 Enhancer字节码增强类 Maven依赖 12345&lt;dependency&gt; &lt;groupId&gt;cglib&lt;/groupId&gt; &lt;artifactId&gt;cglib&lt;/artifactId&gt; &lt;version&gt;3.3.0&lt;/version&gt;&lt;/dependency&gt; 12345public class Movie &#123; public void play() &#123; System.out.println(&quot;播放电影&quot;); &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445/** * cglib核心 MethodInterceptor 接口对方法拦截 * Enhancer 字节码增强类 * 原理，通过cglib字节码增强生成目标类的子类达到增强目的 */public class CglibProxy implements MethodInterceptor &#123; /** * 字节码增强对象 */ private Enhancer enhancer = new Enhancer(); /** * 获取cglib增强对象 * * @param clazz * @param &lt;T&gt; * @return */ public &lt;T&gt; T getProxy(Class&lt;T&gt; clazz) &#123; // 将指定类设置为增强类的父类 enhancer.setSuperclass(clazz); enhancer.setCallback(this); return (T) enhancer.create(); &#125; /** * 拦截被增强类的方法，在 invokeSuper * @param o * @param method * @param objects * @param methodProxy * @return * @throws Throwable */ @Override public Object intercept(Object o, Method method, Object[] objects, MethodProxy methodProxy) throws Throwable &#123; System.out.println(&quot;增强：播放公益广告&quot;); // 调用父类方法，相当于重写父类方法并且在这里调用了父类的方法（super.method） Object result = methodProxy.invokeSuper(o, objects); System.out.println(&quot;增强：播放彩蛋&quot;); return result; &#125;&#125; 1234567891011121314public class Example &#123; public static void main(String[] args) &#123; CglibProxy proxy = new CglibProxy(); // 获取字节码增强类 Movie movie = proxy.getProxy(Movie.class); movie.play(); System.out.println(movie.getClass()); &#125;&#125;// 输出增强：播放公益广告播放电影增强：播放彩蛋 行为型 模板方法 定义一个算法骨架，具体子类实现 定义 定义了一个算法的骨架，并允许子类为一个或多个步骤提供实现 模板方法使得子类可以在不改变算法结构的情况下，重写算法的特定步骤 适用场景 一次性实现一个算法的不变的部分，并将可变的行为留给子类来实现 各子类中公共的行为被提取出来并集中到一个公共父类中从而避免代码重复 优缺点 优点 提高复用性 提高扩展性 缺点 类数目增加 增加了系统实现的复杂度 继承关系自身缺点，如果父类添加新的抽象方法，所有子类都要改一遍 扩展 钩子方法：提供了缺省的行为，子类可以在必要时进行扩展 角色说明 抽象类：声明作为算法步骤的方法， 以及依次调用它们的实际模板方法（算法骨架） 具体类：可以重写所有步骤， 但不能重写模板方法自身 与其他设计模式对比 模板方法模式和工厂方法模式 工厂方法：是模板方法的一种特殊实现 模板方法模式和策略模式 策略模式：目的是使不同的算法可以相互替换并且不影响应用层客户端的使用，可以改变算法的流程并且可以相互替换 模板方法模式：针对的是定义一个算法的流程，而将一些实现步骤交给子类去实现，不改变算法的流程 模拟场景 泡茶和泡咖啡的流程大致是一样的，烧水-放调料（茶叶/咖啡粉）-装杯，咖啡有可选是否加糖 12345678910111213141516171819202122232425262728293031323334/** * 饮料（抽象类） **/public abstract class Drinks &#123; // 定义算法骨架,使用final修饰，不允许子类修改算法流程 public final void make() &#123; this.boilWater(); this.seasoning(); this.pourOut(); if (this.needSugar()) &#123; this.addSugar(); &#125; &#125; // 烧水 某个步骤如果是固化的则可以使用 final 修饰不让子类修改 protected final void boilWater() &#123; System.out.println(&quot;开始烧水&quot;); &#125; // 调料 protected abstract void seasoning(); // 装杯 protected abstract void pourOut(); // 钩子方法 是否需要加糖 protected abstract boolean needSugar(); // 加糖 protected void addSugar() &#123; System.out.println(&quot;加糖&quot;); &#125;&#125; 12345678910111213141516171819/** * 茶（具体类） **/public class Tea extends Drinks &#123; @Override protected void seasoning() &#123; System.out.println(&quot;放茶叶&quot;); &#125; @Override protected void pourOut() &#123; System.out.println(&quot;装入茶杯&quot;); &#125; @Override protected boolean needSugar() &#123; return false; &#125;&#125; 12345678910111213141516171819202122232425/** * 咖啡（具体类） **/public class Coffee extends Drinks &#123; private boolean addSugar; public Coffee(boolean addSugar) &#123; this.addSugar = addSugar; &#125; @Override protected void seasoning() &#123; System.out.println(&quot;放入咖啡粉&quot;); &#125; @Override protected void pourOut() &#123; System.out.println(&quot;装入咖啡杯&quot;); &#125; @Override protected boolean needSugar() &#123; return this.addSugar; &#125;&#125; 12345678910111213141516171819public class Example &#123; public static void main(String[] args) &#123; Drinks drinks = new Tea(); drinks.make(); System.out.println(); drinks = new Coffee(true); drinks.make(); &#125;&#125;// 输出开始烧水放茶叶装入茶杯开始烧水放入咖啡粉装入咖啡杯加糖 源码解析 JDK jdk类AbstractList#addAll方法就有很多实现 Mybatis Mybatis的Executor体系就是通过模板方法模式扩展出适应多个场景使用的执行器 BaseExecutor#query 方法 Spring SpringAbstractApplicationContext#refresh方法实现了 IOC 容器启动的主要逻辑，不管是 XML 还是注解配置的方式，对于核心容器启动流程都是一致的 Spring Template JDBCTemplate、RedisTemplate、MQTemplate都是使用模板方法实现的框架，它们都实现了获取连接、执行命令、释放连接这一基本流程 迭代器 迭代集合，很少自定义实现 定义 提供一种方法，顺序访问一个集合对象中的各个元素，而又不暴露该对象的内部表示 适用场景 访问一个集合对象的内容而无需暴露它的内部表示 为遍历不同的集合结构提供一个统一的接口 优缺点 优点 分离了集合对象的遍历行为 缺点 类的数目增加 迭代器模式和访问者模式 相同点：都是迭代的访问集合对象中的各个元素 不同点 访问者模式：扩展和开放的部分在作用于对象的操作上 访问者模式：扩展和开放的部分是在集合的种类型 模拟场景 一个班级有多个学生，班级可以接受插班生，也可以开除顽皮的学生。上课点名班上所有同学 12345678910111213/** * 学生 **/@ToStringpublic class Student &#123; public Student(String no, String name) &#123; this.no = no; this.name = name; &#125; private String no; private String name;&#125; 12345678/** * 学生迭代器 **/public interface StudentIterator &#123; Student nextStudent(); boolean isLastStudent();&#125; 12345678910111213141516171819202122232425262728/** * 学生迭代器实现 **/public class StudentIteratorImpl implements StudentIterator &#123; private List&lt;Student&gt; studentList; private int position; public StudentIteratorImpl(List&lt;Student&gt; classList) &#123; this.studentList = classList; &#125; @Override public Student nextStudent() &#123; if (position &lt; studentList.size()) &#123; return studentList.get(position++); &#125; return null; &#125; @Override public boolean isLastStudent() &#123; if (position == studentList.size()) &#123; return true; &#125; return false; &#125;&#125; 12345678910/** * 班级 **/public interface Class &#123; void addStudent(Student student); boolean removeStudent(Student student); // 获取迭代器 StudentIterator getIterator();&#125; 12345678910111213141516171819202122232425/** * 班级实现 **/public class ClassImpl implements Class &#123; private List&lt;Student&gt; list; public ClassImpl() &#123; this.list = new ArrayList&lt;Student&gt;(); &#125; @Override public void addStudent(Student student) &#123; this.list.add(student); &#125; @Override public boolean removeStudent(Student student) &#123; return this.list.remove(student); &#125; @Override public StudentIterator getIterator() &#123; return new StudentIteratorImpl(list); &#125;&#125; 12345678910111213141516171819202122232425262728public class Example &#123; public static void main(String[] args) &#123; Class clazz = new ClassImpl(); Student student1 = new Student(&quot;001&quot;, &quot;张三&quot;); Student student2 = new Student(&quot;002&quot;, &quot;李四&quot;); Student student3 = new Student(&quot;003&quot;, &quot;王五&quot;); Student student4 = new Student(&quot;004&quot;, &quot;陆六&quot;); Student student5 = new Student(&quot;005&quot;, &quot;陈七&quot;); clazz.addStudent(student1); clazz.addStudent(student2); clazz.addStudent(student3); clazz.addStudent(student4); clazz.addStudent(student5); clazz.removeStudent(student4); clazz.removeStudent(student5); StudentIterator iterator = clazz.getIterator(); while (!iterator.isLastStudent()) &#123; System.out.println(iterator.nextStudent()); &#125; &#125;&#125;输出：Student&#123;no=&#x27;001&#x27;, name=&#x27;张三&#x27;&#125;Student&#123;no=&#x27;002&#x27;, name=&#x27;李四&#x27;&#125;Student&#123;no=&#x27;003&#x27;, name=&#x27;王五&#x27;&#125; 源码解析 Mybatis Mybatis的默认游标DefaultCursor就自定义实现了迭代器CursorIterator 策略 田忌赛马，算法替换 定义 它能让你定义一系列算法， 并将每种算法分别放入独立的类中， 以使算法的对象能够相互替换 如果代码中有很多if...else..则可以使用策略模式优雅替换 适用场景 系统有很多类，而他们的区别仅仅在于他们的行为不同 一个系统需要动态地在几种算法中选择一种 优缺点 优点 避免使用多重条件转移语句 提高算法的保密性和安全性 缺点 客户端必须知道所有的策略类，并自行决定使用哪一个策略类 产生很多策略类 角色说明 上下文：维护指向具体策略的引用， 且仅通过策略接口与该对象进行交流（算法相互替换的关键） 策略接口：具体策略的通用接口， 它声明了一个上下文用于执行策略的方法 具体策略：实现了上下文所用算法的各种不同变体 与其他设计模式对比 策略模式和工厂模式 工厂模式：创建型模式，工厂模式接收指令，创建符合要求的具体对象 策略模式：行为型模式，策略模式接收创建好的对象，从而实现不同的行为 策略模式和状态模式 策略模式：客户端在使用时必须知道使用哪种策略 状态模式：客户端不需要关心具体的状态，状态会自动转换 模拟场景 GPS地图导航系统的导航出行方案有步行、骑行、公交、地铁、驾车方案，使用策略模式优化大量 if…else 判断 1234567/** * 出行策略 **/public interface Strategy &#123; // 出行 void setOff();&#125; 123456789/** * 步行出行 **/public class WalkStrategy implements Strategy &#123; @Override public void setOff() &#123; System.out.println(&quot;步行路线导航&quot;); &#125;&#125; 123456789/** * 骑行出行 **/public class RideStrategy implements Strategy &#123; @Override public void setOff() &#123; System.out.println(&quot;骑行路线导航&quot;); &#125;&#125; 123456789/** * 公交出行 **/public class BusStrategy implements Strategy &#123; @Override public void setOff() &#123; System.out.println(&quot;公交路线导航&quot;); &#125;&#125; 123456789/** * 地铁出行 **/public class SubwayStrategy implements Strategy &#123; @Override public void setOff() &#123; System.out.println(&quot;地铁路线导航&quot;); &#125;&#125; 123456789/** * 驾车出行 **/public class DriveStrategy implements Strategy &#123; @Override public void setOff() &#123; System.out.println(&quot;驾车路线导航&quot;); &#125;&#125; 123456789/** * 空策略，用于兼容 **/public class EmptyStrategy implements Strategy &#123; @Override public void setOff() &#123; System.out.println(&quot;空策略，兼容&quot;); &#125;&#125; 1234567891011121314/** * 上下文 */public class StrategyContext &#123; private Strategy strategy; public StrategyContext(Strategy strategy) &#123; this.strategy = strategy; &#125; public void executeStrategy() &#123; strategy.setOff(); &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637/** * 策略工厂 * 缓存工厂 **/public class StrategyFactory &#123; private StrategyFactory() &#123; &#125; private static final Map&lt;String, Strategy&gt; STRATEGY_CONTAINER = new HashMap&lt;&gt;(); // 初始化 static &#123; STRATEGY_CONTAINER.put(null, new EmptyStrategy()); STRATEGY_CONTAINER.put(StrategyKey.WALK, new WalkStrategy()); STRATEGY_CONTAINER.put(StrategyKey.RIDE, new RideStrategy()); STRATEGY_CONTAINER.put(StrategyKey.BUS, new BusStrategy()); STRATEGY_CONTAINER.put(StrategyKey.SUBWAY, new SubwayStrategy()); STRATEGY_CONTAINER.put(StrategyKey.DRIVE, new DriveStrategy()); &#125; public static Strategy getStrategy(String strategyKey) &#123; Strategy strategy = STRATEGY_CONTAINER.get(strategyKey); if (Objects.isNull(strategy)) &#123; strategy = STRATEGY_CONTAINER.get(null); &#125; return strategy; &#125; interface StrategyKey &#123; String WALK = &quot;WALK&quot;; String RIDE = &quot;RIDE&quot;; String BUS = &quot;BUS&quot;; String SUBWAY = &quot;SUBWAY&quot;; String DRIVE = &quot;DRIVE&quot;; &#125;&#125; 123456789101112131415public class Example &#123; public static void main(String[] args) &#123; String key = StrategyFactory.StrategyKey.BUS; StrategyContext context = new StrategyContext(StrategyFactory.getStrategy(key)); context.executeStrategy(); // 算法替换 key = StrategyFactory.StrategyKey.SUBWAY; context = new StrategyContext(StrategyFactory.getStrategy(key)); context.executeStrategy(); // 输出 // 公交路线导航 // 地铁路线导航 &#125;&#125; 源码详解 JDK JDK的排序接口Comparator就是使用策略模式进行各种类型的数据排序，Collections#sort方法就是要传递一个自定义的Comparator&lt;? super T&gt;对集合进行排序 Spring InstantiationStrategy 接口定义了 Spring Bean 实例化的策略，有简单通过构造函数或者方法工厂创建的，也有通过使用Cglib字节码增强创建的 解释器 非常低频使用 定义 给定一个语言，定义它的文法的一种表示，并定义一个解释器这个解释器使用该表示来解释语言中的句子 为了解释一种语言，而为语言创建的解释器 适用场景 某个特定类型问题发生频率足够高 优缺点 优点 语法由很多类表示，容易改变及扩展此语言 缺点 当语法规则数目太多时，增加了系统复杂度 解释器模式和适配器模式 适配器：不需要事先知道要适配的规则 解释器：需要事先写好规则，根据规则去进行解释 模拟场景 实现一个简单的加减法数学公式计算器，比如计算3 + 10这个字符串的和 12345678910111213141516171819/** * 表达式 **/public interface Expression &#123; // 解释 int interpret(String str); static List&lt;Integer&gt; parse(String str) &#123; List&lt;String&gt; collect = Arrays.stream(str.split(&quot; &quot;)) .filter(line -&gt; &#123; if (&quot;&quot;.equals(line) || null == line) &#123; return false; &#125; return true; &#125;).collect(Collectors.toList()); return Arrays.asList(Integer.valueOf(collect.get(0)), Integer.valueOf(collect.get(2))); &#125;&#125; 1234567891011/** * 加法表达式 **/public class AddExpression implements Expression &#123; @Override public int interpret(String str) &#123; List&lt;Integer&gt; list = Expression.parse(str); return list.get(0) + list.get(1); &#125;&#125; 12345678910/** * 减法表达式 **/public class SubExpression implements Expression &#123; @Override public int interpret(String str) &#123; List&lt;Integer&gt; list = Expression.parse(str); return list.get(0) - list.get(1); &#125;&#125; 1234567891011121314/** * 工具类 **/public class OperatorUtil &#123; public static int operator(String str) &#123; Expression expression = null; if (str.contains(&quot;+&quot;)) &#123; expression = new AddExpression(); &#125; else if (str.contains(&quot;-&quot;)) &#123; expression = new SubExpression(); &#125; return expression.interpret(str); &#125;&#125; 123456789101112public class Example &#123; public static void main(String[] args) &#123; String str = &quot;2 + 3&quot;; System.out.println(str + &quot; 解释器解释结果：&quot; + OperatorUtil.operator(str)); str = &quot;3 - 7&quot;; System.out.println(str + &quot; 解释器解释结果：&quot; + OperatorUtil.operator(str)); &#125;&#125;// 输出2 + 3 解释器解释结果：53 - 7 解释器解释结果：-4 源码解析 JDK JDK工具类Pattern用于解释正则表达式语法 Spring Spring的EL表达式也是使用解释器进行表达式解释 123456789public class Example &#123; public static void main(String[] args) &#123; ExpressionParser parser = new SpelExpressionParser(); Expression expression = parser.parseExpression(&quot;10 + 10 * 10&quot;); Integer result = expression.getValue(Integer.class); System.out.println(result); // 输出 110 &#125;&#125; 观察者 一对多 订阅机制 定义 定义了对象之间的一对多依赖，让多个观察者对象同时监听某个主题对象，当主题对象发生变化时，它的所有依赖者(观察者)都会收到通知并更新 适用场景 关联行为场景，建立一套触发机制 优缺点 优点 观察者和被观察者之间建立一个抽象的耦合 观察者模式支持广播通信 缺点 观察者之间有过多的细节依赖、提高时间消耗及程序复杂度 使用要得当，要避免循环调用 角色说明 发布者：会向其他对象发送值得关注的事件。 事件会在发布者自身状态改变或执行特定行为后发生。 发布者中包含一个允许新订阅者加入和当前订阅者离开列表的订阅构架 订阅者：接口声明了通知接口。 在绝大多数情况下， 该接口仅包含一个 update 更新方法 具体订阅者：可以执行一些操作来回应发布者的通知。 所有具体订阅者类都实现了同样的接口， 因此发布者不需要与具体类相耦合 模拟场景 微信群聊天，聊天室是消息发布者，微信用户是订阅者（登录），微信客户端是具体订阅者（展示消息） 123456789101112131415161718192021222324252627282930/** * 聊天室 (发布者) **/public class Chatroom &#123; // 当前消息 private String currentMsg; // 聊天室成员 private List&lt;WxUser&gt; userList = new CopyOnWriteArrayList&lt;&gt;(); // 添加成员 public void addUser(Observer observer) &#123; this.userList.add(observer); &#125; // 发送群消息 public void sendMsg(String msg) &#123; this.currentMsg = msg; // 消息通知 this.notifyAllObservers(); &#125; // 群消息通知 public void notifyAllObservers() &#123; for (WxUser wxUser : userList) &#123; wxUser.receive(this.currentMsg); &#125; &#125;&#125; 123456789101112131415/** * 微信用户（订阅者） **/public abstract class WxUser &#123; private Chatroom chatroom; public WxUser(Chatroom chatroom) &#123; this.chatroom = chatroom; // 把自己加入观察者队列 chatroom.addUser(this); &#125; // 接收信息 public abstract void receive(String msg);&#125; 12345678910111213/** * 微信客户端（具体订阅者） **/public class WxClient extends WxUser &#123; public WxClient(Chatroom chatroom) &#123; super(chatroom); &#125; @Override public void receive(String msg) &#123; System.out.println(this.toString() + &quot;: 接收到群消息：&quot; + msg); &#125;&#125; 12345678910111213141516171819public class Example &#123; public static void main(String[] args) &#123; Chatroom chatroom = new Chatroom(); new WxUser(chatroom); new WxUser(chatroom); new WxUser(chatroom); new WxUser(chatroom); new WxUser(chatroom); chatroom.sendMsg(&quot;Hello!&quot;); &#125;&#125;// 输出WxUser@4f933fd1: 接收到群消息：Hello!WxUser@548a9f61: 接收到群消息：Hello!WxUser@1753acfe: 接收到群消息：Hello!WxUser@7c16905e: 接收到群消息：Hello!WxUser@2a2d45ba: 接收到群消息：Hello! 源码解析 Spring Spring的事件体系就是使用观察者模式，可以通过自定义事件 事件发布者(主题) 事件监听器(观察者) 参考：https://segmentfault.com/a/1190000037666661 Zookeeper zk客户端对服务端的某个节点进行监听，也是使用观察者思想 备忘录 快照数据，可进行撤回操作 DB的binlog日志 IDEA的撤回功能 定义 保存一个对象的某个状态，以便在适当的时候恢复对象 适用场景 保存及恢复数据相关业务场景 后悔的时候，即想恢复到之前的状态 优缺点 优点 为用户提供一种可恢复机制 存档信息的封装 缺点 资源占用，如果类的成员变量过多，势必会占用比较大的资源，而且每一次保存都会消耗一定的内存 与其他设计模式对比 备忘录模式和状态模式 备忘录模式：使用实例表示状态，我们的存档是对象的一个实例 状态模式：用类表示状态 模拟场景 论坛发表留言时，可以暂存草稿。下次重新进入当前主题后又可以重新加载暂存草稿继续发言 1234567891011121314151617181920/** * 留言消息 */@Data@ToStringpublic class Message &#123; public Message(String title, String content) &#123; this.title = title; this.content = content; &#125; private String title; private String content; // 进行快照 public MessageSnapshot snapshot() &#123; return new MessageSnapshot(this.title, this.content); &#125;&#125; 12345678910111213141516171819/** * 留言消息快照,快照数据不提供set方法，不允许修改 */@Getterpublic class MessageSnapshot &#123; public MessageSnapshot(String title, String content) &#123; this.title = title; this.content = content; &#125; private String title; private String content; // 快照数据进行转换 public Message change() &#123; return new Message(this.title, this.content); &#125;&#125; 1234567891011121314/** * 消息快照管理 */public class MessageSnapshotManager &#123; private final Stack&lt;MessageSnapshot&gt; snapshotContainer = new Stack&lt;&gt;(); public void save(MessageSnapshot messageSnapshot) &#123; this.snapshotContainer.push(messageSnapshot); &#125; public MessageSnapshot getSnapshot() &#123; return this.snapshotContainer.pop(); &#125;&#125; 12345678910111213141516171819202122public class Example &#123; public static void main(String[] args) &#123; Message message = new Message(&quot;title A&quot;, &quot;回复内容 A&quot;); // 快照，保存草稿 MessageSnapshot snapshot = message.snapshot(); MessageSnapshotManager manager = new MessageSnapshotManager(); manager.save(snapshot); System.out.println(&quot;记录草稿，离开页面&quot;); message = null; System.out.println(&quot;重新进入页面获取草稿&quot;); snapshot = manager.getSnapshot(); message = snapshot.change(); System.out.println(message); &#125;&#125;// 输出记录草稿，离开页面重新进入页面获取草稿Message(title=title A, content=回复内容 A) 源码解析 JDK JDK的序列化Serializable，可以将一个对象进行序列化后保存在磁盘的文件中，待下次使用时进行反序列化 Mysql Mysql每次提交事务前都会记录 binlog 以便于事务回滚 命令 Linux命令执行 操作参数化 定义 将请求转换为一个包含与请求相关的所有信息的独立对象，该转换让你能根据不同的请求将方法参数化、 延迟请求执行或将其放入队列中， 且能实现可撤销操作 命令模式解决了应用程序中对象的职责以及它们之间的通信方式 适用场景 如果你需要通过操作来参数化对象， 可使用命令模式（将操作封装成独立对象，对象作为参数传递） 如果你想要将操作放入队列中、 操作的执行或者远程执行操作， 可使用命令模式 如果你想要实现操作撤回功能， 可使用命令模式 优缺点 优点 降低耦合 容易扩展新命令或者一组命令 缺点 命令的无限扩展会增加类的数量，提高系统实现复杂度 角色说明 命令接口：通常仅声明一个执行命令的方法 具体命令：会实现各种类型的请求。 具体命令自身并不完成工作， 而是会将调用委派给一个业务逻辑对象 接收者类：包含部分业务逻辑。 几乎任何对象都可以作为接收者。 绝大部分命令只处理如何将请求传递到接收者的细节， 接收者自己会完成实际的工作 发送者类：负责对请求进行初始化， 其中必须包含一个成员变量来存储对于命令对象的引用。 发送者触发命令， 而不向接收者直接发送请求 与其他设计模式对比 命令模式和备忘录模式 命令模式和备忘录模式经常结合使用，可以使用备忘录模式保存命令的历史记录，这样我们就可以直接调取上一个命令或历史执行过的命令（Linux的控制台） 模拟场景 模拟小度的语音交互，回家时使用语音命令让小度打开电灯，睡觉时让小度关闭电灯 123456/** * 命令接口 */public interface Command &#123; void execute();&#125; 12345678910111213141516171819/** * 开灯命令（具体命令） */public class LightOnCommand implements Command &#123; private Light light; // 语音 private String voice; public LightOnCommand(Light light, String voice) &#123; this.light = light; this.voice = voice; &#125; @Override public void execute() &#123; System.out.println(&quot;接收语音命令：&quot; + voice); light.on(); &#125;&#125; 12345678910111213141516171819/** * 关灯命令（具体命令） */public class LightOffCommand implements Command &#123; private Light light; // 语音 private String voice; public LightOffCommand(Light light, String voice) &#123; this.light = light; this.voice = voice; &#125; @Override public void execute() &#123; System.out.println(&quot;接收语音命令：&quot; + voice); light.off(); &#125;&#125; 1234567891011121314/** * 电灯（接收者类） * 命令模式好处是随着电灯操作的增加，只需要扩展相应的Command * 客户端使用不用发生改变 */public class Light &#123; public void on() &#123; System.out.println(&quot;接收开灯命令，正在打开电灯&quot;); &#125; public void off() &#123; System.out.println(&quot;接收段等命令，正在关闭电灯&quot;); &#125;&#125; 12345678910111213141516171819/* * 小度智能语音（发送者） */public class XiaoDu &#123; private List&lt;Command&gt; commandList = new ArrayList&lt;&gt;(); // 接收命令 public void receiveCommand(Command command) &#123; this.commandList.add(command); &#125; public void executeCommands() &#123; for (Command command : commandList) &#123; command.execute(); &#125; // 命令执行完成后清除 commandList.clear(); &#125;&#125; 1234567891011121314151617181920public class Example &#123; public static void main(String[] args) &#123; Light light = new Light(); XiaoDu xiaoDu = new XiaoDu(); // 回家 xiaoDu.receiveCommand(new LightOnCommand(light, &quot;小度小度，开灯&quot;)); // 可以一次性执行多条命令 xiaoDu.executeCommands(); // 准备睡觉 xiaoDu.receiveCommand(new LightOffCommand(light, &quot;小度小度，关灯&quot;)); xiaoDu.executeCommands(); &#125;&#125;//输出接收语音命令：小度小度，开灯接收开灯命令，正在打开电灯接收语音命令：小度小度，关灯接收段等命令，正在关闭电灯 源码解析 JDK JDK的Runnable接口其实就是一个抽象的命令，实现它的过程其实就是定义了一系列命令后，让线程(新建或线程池的线程)去执行你定义的命令(实现的run方法) 中介者 整理无序的依赖关系 一对多转一对一 定义 能让你减少对象之间混乱无序的依赖关系。 该模式会限制对象之间的直接交互， 迫使它们通过一个中介者对象进行合作 适用场景 当一些对象和其他对象紧密耦合以致难以对其进行修改时， 可使用中介者模式 当组件因过于依赖其他组件而无法在不同应用中复用时， 可使用中介者模式 如果为了能在不同情景下复用一些基本行为， 导致你需要被迫创建大量组件子类时， 可使用中介者模式 优缺点 优点 将一对多转化成了一对一、降低程序复杂度 类之间解耦 缺点 中介者过多，导致系统复杂 角色说明 组件：是各种包含业务逻辑的类。 每个组件都有一个指向中介者的引用， 该引用被声明为中介者接口类型 中介者接口：声明了与组件交流的方法， 但通常仅包括一个通知方法。 组件可将任意上下文 （包括自己的对象） 作为该方法的参数， 只有这样接收组件和发送者类之间才不会耦合 具体中介者：封装了多种组件间的关系。 具体中介者通常会保存所有组件的引用并对其进行管理， 甚至有时会对其生命周期进行管理 与其他设计模式对比 中介者模式和观察者模式 某些场景这两种模式会结合适用，使用观察者模式实现中介者模式角色之间的通讯 模拟场景 手机有闹钟，音乐播放，视频播放功能，手机就像这些功能的中介者整合了这些功能。比如我们可以设置一个闹钟提醒我们及时观看时下正在热播的连续剧 123456/*** 功能（抽象组件）*/public interface Function &#123; void onEvent(Mobile mobile);&#125; 1234567891011121314/*** 闹钟功能（具体组件）*/public class AlarmFunction implements Function &#123;@Overridepublic void onEvent(Mobile mobile) &#123; // 使用中介者 mobile.doEvent(&quot;alarm&quot;);&#125;public void alarm() &#123; System.out.println(&quot;闹钟触发时间到&quot;); &#125;&#125; 1234567891011121314/*** 放音乐功能（具体组件）*/public class MusicFunction implements Function &#123;@Overridepublic void onEvent(Mobile mobile) &#123; // 使用中介者 mobile.doEvent(&quot;music&quot;);&#125;public void playMusic() &#123; System.out.println(&quot;播放音乐&quot;); &#125;&#125; 1234567891011121314/*** 看视频功能（具体组件）*/public class VideoFunction implements Function &#123;@Overridepublic void onEvent(Mobile mobile) &#123; // 使用中介者 mobile.doEvent(&quot;video&quot;);&#125;public void playVideo() &#123; System.out.println(&quot;播放视频&quot;);&#125; 123456789101112131415161718192021222324252627282930313233343536/*** 中介（具体中介者）*/public class Mobile &#123;private AlarmFunction alarmFunction;private MusicFunction musicDemand;private VideoFunction videoDemand;public Mobile(AlarmFunction alarmFunction, MusicFunction musicDemand, VideoFunction videoDemand) &#123; this.alarmFunction = alarmFunction; this.musicDemand = musicDemand; this.videoDemand = videoDemand;&#125;public void doEvent(String type) &#123; switch (type) &#123; case &quot;alarm&quot;: doAlarmEvent(); break; case &quot;video&quot;: doVideoEvent(); break; &#125;&#125;// 提醒及时观看连续剧,通过中介者完成多个组件的依赖调用private void doAlarmEvent() &#123; alarmFunction.alarm(); musicDemand.playMusic(); videoDemand.playVideo();&#125;private void doVideoEvent() &#123; // TODO 看视频设置音量 &#125;&#125; 123456789101112131415public class Example &#123;public static void main(String[] args) &#123; AlarmFunction alarmFunction = new AlarmFunction(); MusicFunction musicFunction = new MusicFunction(); VideoFunction videoFunction = new VideoFunction(); Mobile mobile = new Mobile(alarmFunction, musicFunction, videoFunction); // 闹钟提醒观看连续剧（通过中介者操作其他组件） alarmFunction.onEvent(mobile); &#125;&#125;// 输出闹钟触发时间到播放音乐播放视频 源码解析 JDK JDK的反射机制我们非常熟悉，Method#invoke可以反射调用某个对象的方法，实际上调用的是第一个参数obj对象的一个成员方法，而非Method实现方法。Method在反射中起到中介作用 责任链 链式执行 定义 为了避免请求发送者与多个请求处理者耦合在一起，于是将所有请求的处理者通过前一对象记住其下一个对象的引用而连成一条链 当有请求发生时，可将请求沿着这条链传递，直到有对象处理它为止 适用场景 当程序需要使用不同方式处理不同种类请求， 而且请求类型和顺序预先未知时， 可以使用责任链模式 当必须按顺序执行多个处理者时， 可以使用该模式 如果所需处理者及其顺序必须在运行时进行改变， 可以使用责任链模式 优缺点 优点 请求的发送者和接收者(请求的处理)解耦 责任链可以动态组合 缺点 责任链太长或者处理时间过长，影响性能 责任链有可能过多 与其他设计模式对比 责任链模式和状态模式 责任链模式：在责任链模式中，各个对象并不指定下一个处理的对象者是谁，只有在客户端中设置了处理对象的顺序，直到被某个对象处理或整个链条结束 状态模式：每个状态对象知道自己下一个处理的对象是谁 模拟场景 网站用户注册时，校验用户名是否为一个正确的邮箱格式，密码长度是否大于6位，否则就抛出异常让用户重新填写 123456789/** * 用户 */@Data@AllArgsConstructorpublic class User &#123; private String username; private String password;&#125; 1234567891011121314151617181920/** * 验证处理器 */public abstract class VerifyHandler &#123; private VerifyHandler next; public void verify(User user) &#123; handler(user); if (Objects.nonNull(this.next)) &#123; this.next.verify(user); &#125; &#125; // 处理 protected abstract void handler(User user); public void setNext(VerifyHandler next) &#123; this.next = next; &#125;&#125; 1234567891011121314151617181920/** * 邮箱验证 */public class EmailHandler extends VerifyHandler &#123; @Override protected void handler(User user) &#123; if (Objects.isNull(user.getUsername())) &#123; throw new NullPointerException(&quot;邮箱为空&quot;); &#125; String str = &quot;^([a-zA-Z0-9]*[-_]?[a-zA-Z0-9]+)*@([a-zA-Z0-9]*[-_]?[a-zA-Z0-9]+)+[\\\\.][A-Za-z]&#123;2,3&#125;([\\\\.][A-Za-z]&#123;2&#125;)?$&quot;; Pattern p = Pattern.compile(str); Matcher m = p.matcher(user.getUsername()); if (!m.matches()) &#123; throw new IllegalArgumentException(&quot;邮箱格式不合法&quot;); &#125; &#125;&#125; 123456789101112131415/** * 密码验证 */public class PasswordHandler extends VerifyHandler &#123; @Override protected void handler(User user) &#123; if (Objects.isNull(user.getPassword())) &#123; throw new NullPointerException(&quot;密码不能为空&quot;); &#125; if (user.getPassword().length() &lt; 6) &#123; throw new IllegalArgumentException(&quot;密码长度必须超过6位&quot;); &#125; &#125;&#125; 12345678910111213public class Example &#123; public static void main(String[] args) &#123; // 创建责任链 VerifyHandler emailHandler = new EmailHandler(); VerifyHandler passwordHandler = new PasswordHandler(); emailHandler.setNext(passwordHandler); User user = new User(&quot;test@test.com&quot;, &quot;123456&quot;); emailHandler.verify(user); // 输出 Exception in thread &quot;main&quot; java.lang.IllegalArgumentException: 密码长度必须超过6位 &#125;&#125; 源码解析 Spring MVC Spring MVC 源码中大量使用了责任链模式，比如请求-&gt;方法调用的映射，还有自定义拦截器HandlerInterceptor DispatcherServlet#doDispatch在处理请求时会先获取拦截器链 Spring security Spring security的权限验证体系也是使用责任链完成的 访问者 低频使用 数据的操作被重定向到访问者 数据结构与数据操作分离 定义 它能将算法与其所作用的对象隔离开来 适用场景 如果你需要对一个复杂对象结构中的所有元素执行某些操作， 可使用访问者模式 当某个行为仅在类层次结构中的一些类中有意义， 而在其他类中没有意义时， 可使用该模式 数据结构与数据操作分离 优缺点 优点 增加新的操作很容易,即增加一个新的访问者 缺点 增加新的数据结构困难 具体元素变更比较麻烦 角色说明 元素接口：声明了一个方法来 “接收” 访问者。 该方法必须有一个参数被声明为访问者接口类型 具体元素：必须实现接收方法。 该方法的目的是根据当前元素类将其调用重定向到相应访问者的方法 访问者接口：声明了一系列以对象结构的具体元素为参数的访问者方法。 如果编程语言支持重载， 这些方法的名称可以是相同的， 但是其参数一定是不同的 具体访问者：会为不同的具体元素类实现相同行为的几个不同版本 与其他设计模式对比 访问者模式和迭代器模式 共同点：都是在某种数据结构上进行一些处理 访问者：主要对保存在数据结构中的元素进行某种特定的处理，重点是处理 迭代器：主要是逐个遍历保存在数据结构中的一些元素，重点是遍历 模拟场景 计算机零件有键盘、鼠标、内存、磁盘。电脑可以访问这些零件执行一些操作 1234567/** * 电脑零件（元素接口） */public interface ComputerPart &#123; // 接受访问 void accept(Visitor visitor);&#125; 12345678910/** * 键盘（具体元素） */public class Keyboard implements ComputerPart &#123; @Override public void accept(Visitor visitor) &#123; // 调用访问者进行访问 visitor.visit(this); &#125;&#125; 12345678910/** * 鼠标（具体元素） */public class Mouse implements ComputerPart &#123; @Override public void accept(Visitor visitor) &#123; // 调用访问者进行访问 内存 visitor.visit(this); &#125;&#125; 12345678910/** * 内存（具体元素） */public class Memory implements ComputerPart &#123; @Override public void accept(Visitor visitor) &#123; // 调用访问者进行访问 visitor.visit(this); &#125;&#125; 12345678910/** * 磁盘（具体元素） */public class Disk implements ComputerPart &#123; @Override public void accept(Visitor visitor) &#123; // 调用访问者进行访问 visitor.visit(this); &#125;&#125; 1234567891011121314151617/** * 访问者（访问者接口） */public interface Visitor &#123; // 访问键盘 void visit(Keyboard keyboard); // 访问鼠标 void visit(Mouse mouse); // 访问内存 void visit(Memory memory); // 访问磁盘 void visit(Disk disk);&#125; 123456789101112131415161718192021222324/** * 电脑（具体访问者） */public class Computer implements Visitor &#123; @Override public void visit(Keyboard keyboard) &#123; System.out.println(&quot;访问键盘 亮起键盘灯&quot;); &#125; @Override public void visit(Mouse mouse) &#123; System.out.println(&quot;访问鼠标 亮起鼠标灯&quot;); &#125; @Override public void visit(Memory memory) &#123; System.out.println(&quot;访问内存 写入内存数据&quot;); &#125; @Override public void visit(Disk disk) &#123; System.out.println(&quot;访问磁盘 写入文件&quot;); &#125;&#125; 12345678910111213141516171819202122public class Example &#123; public static void main(String[] args) &#123; List&lt;ComputerPart&gt; list = new ArrayList&lt;&gt;(); list.add(new Keyboard()); list.add(new Mouse()); list.add(new Memory()); list.add(new Disk()); // 访问者，实现访问不同数据结构的操作 Visitor visitor = new Computer(); for (ComputerPart computerPart : list) &#123; computerPart.accept(visitor); &#125;// 输出// 访问键盘 亮起键盘灯// 访问鼠标 亮起鼠标灯// 访问内存 写入内存数据// 访问磁盘 写入文件 &#125;&#125; 源码解析 JDK JDK nio包下的FileVisitor文件访问者接口，提供文件的访问和遍历操作 Spring Spring的BeanDefinitionVisitor用于遍历bean中的属性值和构造参数值，解析bean的元数据值 状态 将状态封装成对象并处理特定逻辑，上下文简化调用。实际业务处理由上下文重定向到状态对象 定义 允许一个对象在其内部状态改变时,改变它的行为 适用场景 如果对象需要根据自身当前状态进行不同行为， 同时状态的数量非常多且与状态相关的代码会频繁变更的话， 可使用状态模式 如果某个类需要根据成员变量的当前值改变自身行为， 从而需要使用大量的条件语句时， 可使用该模式 当相似状态和基于条件的状态机转换中存在许多重复代码时， 可使用状态模式 优缺点 优点 将不同的状态隔离 把各种状态的转换逻辑，分布到State的子类中，减少相互间依赖 增加新的状态非常简单 缺点 状态多的业务场景导致类数目增加，系统变复杂 角色说明 状态接口：声明特定于状态的方法。 这些方法应能被其他所有具体状态所理解， 因为你不希望某些状态所拥有的方法永远不会被调用 具体状态：会自行实现特定于状态的方法。 为了避免多个状态中包含相似代码， 你可以提供一个封装有部分通用行为的中间抽象类 上下文：保存了对于一个具体状态对象的引用， 并会将所有与该状态相关的工作委派给它。 上下文通过状态接口与状态对象交互， 且会提供一个设置器用于传递新的状态对象 与其他设计模式对比 状态模式和享元模式 状态模式和享元模式在特定场景可以配合使用，使用享元模式在多个上下文之间共享状态实例 场景模拟 地铁自定义有五种状态：运行、停止、开门、关门、异常、，每个状态执行不同的方法能够改变状态，并且不同的状态只能能执行特定的方法 123456789101112131415161718192021222324252627282930313233343536373839/** * 基础状态（状态接口） */public abstract class State &#123; // 状态持有上下文，当状态改变时才能委托给上下文 private Subway context; public void setContext(Subway context) &#123; this.context = context; &#125; // 以下方法都是帮助上下文切换状态并触发上下文执行 public void run() &#123; // 切换上下文状态 this.context.setState(Subway.RUN_STATE); // 将业务逻辑委托给上下文，上下文再委托给当前状态 this.context.run(); &#125; public void stop() &#123; this.context.setState(Subway.STOP_STATE); this.context.stop(); &#125; public void open() &#123; this.context.setState(Subway.OPEN_STATE); this.context.open(); &#125; public void close() &#123; this.context.setState(Subway.CLOSE_STATE); this.context.close(); &#125; public void error() &#123; this.context.setState(Subway.ERROR_STATE); this.context.error(); &#125;&#125; 123456789101112131415161718/** * 地铁运行状态（具体状态） */public class RunState extends State &#123; @Override public void run() &#123; System.out.println(&quot;地铁运行，前往下一个地铁站&quot;); &#125; /** * 控制 run -&gt; open 不能切换,如果切换则跳转到异常状态 */ @Override public void open() &#123; super.context.setState(Subway.ERROR_STATE); super.context.open(); &#125;&#125; 123456789/** * 地铁停止状态（具体状态） */public class StopState extends State &#123; @Override public void stop() &#123; System.out.println(&quot;地铁进站&quot;); &#125;&#125; 123456789/** * 地铁开门状态（具体状态） */public class OpenState extends State &#123; @Override public void open() &#123; System.out.println(&quot;地铁开门&quot;); &#125;&#125; 123456789/** * 地铁停止状态（具体状态） */public class CloseState extends State &#123; @Override public void close() &#123; System.out.println(&quot;地铁关门&quot;); &#125;&#125; 123456789101112131415161718192021222324252627282930/** * 地铁异常状态（具体状态） */public class ErrorState extends State &#123; @Override public void run() &#123; System.out.println(&quot;异常警告！，不能启动&quot;); &#125; @Override public void stop() &#123; System.out.println(&quot;异常警告！，不能停止&quot;); &#125; @Override public void open() &#123; System.out.println(&quot;异常警告！，不能开门&quot;); &#125; @Override public void close() &#123; System.out.println(&quot;异常警告！，不能关门&quot;); &#125; @Override public void error() &#123; System.out.println(&quot;异常警告！&quot;); &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243/** * 地铁 （上下文) */public class Subway &#123; // 定义状态 public final static RunState RUN_STATE = new RunState(); public final static StopState STOP_STATE = new StopState(); public final static OpenState OPEN_STATE = new OpenState(); public final static CloseState CLOSE_STATE = new CloseState(); public final static ErrorState ERROR_STATE = new ErrorState(); // 当前地铁状态 private State state; public void setState(State state) &#123; // 切换上下文状态 this.state = state; // 设置上下文 state.setContext(this); &#125; // 以下方法上下文都会委托给当前状态处理 public void run() &#123; this.state.run(); &#125; public void stop() &#123; this.state.stop(); &#125; public void open() &#123; this.state.open(); &#125; public void close() &#123; this.state.close(); &#125; public void error() &#123; this.state.error(); &#125;&#125; 12345678910111213141516171819public class Example &#123; public static void main(String[] args) &#123; // 初始化上下文 Subway context = new Subway(); context.setState(Subway.STOP_STATE); // 上下文状态切换，具体实现由对应的State实现 context.open(); context.close(); context.run(); context.open(); // 输出 // 地铁开门 // 地铁关门 // 地铁运行，前往下一个地铁站 // 异常警告！，不能开门 &#125;&#125; 源码解析 状态模式一般结合业务模式开发","categories":[],"tags":[]},{"title":"java源码","slug":"java源码","date":"2022-04-13T14:55:40.000Z","updated":"2023-08-28T11:03:47.495Z","comments":true,"path":"2022/04/13/java源码/","link":"","permalink":"https://wugengfeng.cn/2022/04/13/java%E6%BA%90%E7%A0%81/","excerpt":"","text":"思维导图 Java 基础 Java 语言有哪些特点 面向对象 封装 对象隐藏内部的属性和实现细节，对外部提供统一的访问方法 继承 子类继承父类的成员和方法，使子类也具有父类相同的行为 多态 对象类型和引用类型之间具有继承（类）/实现（接口）的关系 重写 重载 跨平台 JVM虚拟机 字节码 可靠性 强类型检查 异常处理 内存管理 安全性 字节码校验 访问控制（public,private） 安全沙箱 JDK、 JRE、 JVM JDK java开发环境 JRE Java运行环境 开发工具 编译器 调试器 JRE java 运行环境 包含java虚拟机 java类库和Java命令（javac） JVM java 虚拟机，用于运行java字节码 字节码 字节码：通过 javac 命令编译后的 .class 文件就是字节码，字节码是介于源码和机器码之间的一种编码，面向JVM 好处： java 是解释型语言，先把源码编译为字节码能够提高jvm的执行效率 字节码是跨平台的组成部分，面向jvm执行，jvm是跨平台实现的根本 java为什么不直接编译为机器码 和java的生态有关，java作为一种灵活的代码，得益于 CGLIB 和 ASM 动态字节码技术，如果全部编译为机器码就会失去这种灵活性 编译与解释并存 java源码 --javac–&gt; 字节码 --加载–&gt; JVM --解释–&gt; 机器码 --执行–&gt; cpu 编译：将java源码编译为JVM可执行的字节码（.class文件） 解释：JVM加载字节码后一行行进行通过Java解释器解释成系统能够运行的机器码 continue、break 和 return 的区别 continue：用于跳出本次循环进入下次循环 break ：用于跳出整个循环体 return：用于跳出所在方法 如何实现跨平台 将Java源码编译成字节码文件 使用Java虚拟机运行字节码文件 不同平台有不同的Java虚拟机实现 重载 发生在同一个类中（或者父类和子类之间） 方法名称相同 参数类型不同，个数不同，顺序不同，返回值和访问修饰符可以不同 12345678910public class Test &#123; public void hello() &#123; System.out.println(&quot;hello&quot;); &#125; // 重载 hello() 方法 public void hello(String name) &#123; System.out.println(&quot;hello: &quot; + name); &#125;&#125; 重写 子类覆盖父类的方法，有重新的实现 两同两小一大 方法名称相同，参数列表相同 子类方法返回值类型要小于等于父类方法返回值类型；子类方法声明抛出的异常要小于等于父类方法抛出的异常 子类方法的访问修饰符要大于等于父类方法的访问修饰符 123456789101112131415public class Parent &#123; protected Integer add(int a, int b) throws Exception &#123; return a + b; &#125;&#125;public class Son extends Parent &#123; @Override public Integer add(int a, int b) throws RuntimeException &#123; return 0; &#125;&#125; 数据类型 基本类型 位数 字节 默认值 取值范围 byte 8 1 0 -128 ~ 127 short 16 2 0 -32768 ~ 32767 int 32 4 0 -2147483648 ~ 2147483647 long 64 8 0L -9223372036854775808 ~ 9223372036854775807 char 16 2 ‘u0000’ 0 ~ 65535 float 32 4 0f 1.4E-45 ~ 3.4028235E38 double 64 8 0d 4.9E-324 ~ 1.7976931348623157E308 boolean 1 false true、false 封装类型 成员变量包装类型不赋值就是 null ，而基本类型有默认值且不是 null 包装类型可用于泛型，而基本类型不可以 Byte,Short,Integer,Long 这 4 种包装类默认创建了数值 [-128，127] 的相应类型的缓存数据 装箱、拆箱 装箱：将基本类型用它们对应的引用类型包装起来； 拆箱：将包装类型转换为基本数据类型； 12345// 自动装箱,相当于 Integer a = Integer.valueOf(10)Integer a = 10;// 自动拆箱, 相当于 int b = a.intValue()int b = a; 注意：如果频繁拆装箱的话，也会严重影响系统的性能。我们应该尽量避免不必要的拆装箱操作 解决浮点精度丢失 BigDecimal 可以实现对浮点数的运算，不会造成精度丢失，必须使用其字符串类型的构造函数 1BigDecimal a = new BigDecimal(&quot;1.0&quot;); 超过 long 整型的数据应该如何表示 BigInteger 内部使用 int[] 数组来存储任意大小的整形数据 123BigInteger a=new BigInteger(“23”);BigInteger b=new BigInteger(“34”);a. add(b); 相对于常规整数类型的运算来说，BigInteger 运算的效率会相对较低 隐式转换（自动转换） 数值型数据的转换：byte→short→int→long→float→double 字符型转换为整型：char→int 两种数据类型彼此兼容 低级类型数据转换成高级类型数据 1234567891011short a = 1;// 类型自动转换为int，因为1是intshort b = (short) (a + 1);// += -= 存在类型自动转换short d = a += 1;int c = a + 1;// 数据溢出处理int i = 900000 * 100000;// 隐式转换转换为longlong k = 900000 * 100000L; 显式转换（强制转换） 两种数据类型不兼容，高级类型向低级类型转换，自动转换将无法进行，这时就需要进行强制类型转换 123int a = 3;double b = 5.0;a = (int)b; 对象的相等和引用相等的区别 对象的相等一般比较的是内存中存放的内容是否相等 使用 equals 判断两个对象的内容是否相等 引用相等一般比较的是他们指向的内存地址是否相等 使用 == 判断两个变量是否指向同一个内存地址 构造方法 如果不声明构造方法，默认会生成一个无参的构造方法 如果我们自己添加了类的构造方法（无论是否有参），Java 就不会再添加默认的无参数的构造方法了 主要作用是完成对象的初始化工作，在对象被创建后执行 特点 名字与类名相同 没有返回值，但不能用 void 声明构造函数 生成类的对象时自动执行，无需调用 构造方法不能被 override（重写）,但是可以 overload（重载） 接口和抽象类有什么共同点和区别 相同 都不能被实例化 都可以包含抽象方法 都可以有默认实现的方法（Java 8 可以用 default 关键字在接口中定义默认方法） 区别 一个类只能继承一个类，但是可以实现多个接口 抽象类可以有构造方法，接口不可以 接口只能定义常量，抽象类可以定义变量 浅拷贝和深拷贝 浅拷贝 浅拷贝：浅拷贝会在堆上创建一个新的对象（区别于引用拷贝的一点），不过，如果原对象内部的属性是引用类型的话，浅拷贝会直接复制内部对象的引用地址，也就是说拷贝对象和原对象共用同一个内部对象 12345678910public class Test implements Cloneable &#123; private Integer age; @Override protected Object clone() throws CloneNotSupportedException &#123; Test clone = (Test) super.clone(); return clone; &#125;&#125; 深拷贝 深拷贝 ：深拷贝会完全复制整个对象，包括这个对象所包含的内部对象 12345678910111213@Datapublic class Test implements Cloneable &#123; private Integer age; @Override protected Object clone() throws CloneNotSupportedException &#123; Integer age = new Integer(this.getAge()); Test clone = new Test(); clone.setAge(age); return clone; &#125;&#125; 深拷贝可以使用序列化和反序列化实现 Object 方法 == 和 equals() 的区别 对于基本数据类型来说，== 比较的是值 对于引用数据类型来说，== 比较的是对象的内存地址 类没有重写 equals()方法 ：通过equals()比较该类的两个对象时，等价于通过“==”比较这两个对象 类重写了 equals()方法 ：一般我们都重写 equals()方法来比较两个对象中的属性是否相等 为什么重写 equals() 时必须重写 hashCode() 方法 java约定 两个对象相等其hashCode必定相等 两个对象的hashCode相等对象不一定相等（hash冲突） hashCode被广泛应用于Java核心类库的集合类中（HashSet，HashMap）。为了维护哈希表的正常运作并维持对象的一致性，确保通过hashCode能够找到正确的对象（HashMap源码有体现，先判断HashCode，存在冲突再遍历对象使用equals方法找出正确对象），因此重写 equals() 时必须重写 hashCode() 方法 字符串 String String不可变 String 是不可变的 (每次变量拼接会生成新的对象，底层由StringBuilder#append实现)，线程不安全的 String 类中使用 final 关键字修饰字符数组来保存字符串 1234public final class String implements java.io.Serializable, Comparable&lt;String&gt;, CharSequence &#123; private final char value[]; //...&#125; 保存字符串的数组被 final 修饰且为私有的，并且String 类没有提供/暴露修改这个字符串的方法。 String 类被 final 修饰导致其不能被继承，进而避免了子类破坏 String 不可变。 intern() String.intern()方法设计的初衷就是：重用字符串对象，以便节省内存 JDK1.8, 先判断常量池中当前字符串是否存在 如果不存在：不会将当前字符串复制到常量池，而是将当前字符串的引用复制到常量池 如果存在：不会改变常量池已经存在的引用，并直接返回常量池中字符串引用 123String str1 = &quot;java&quot;;String str2 = new String(&quot;Ja&quot;) + new String(&quot;va&quot;);str1 == str2.intern(); //false 思路分析：两者分别是堆中的两个对象 123String str1 = &quot;java&quot;;String str2 = new String(&quot;ja&quot;) + new String(&quot;va&quot;);str1 == str2.intern(); // true 思路分析：String str1 = “java” 会将字符串分配到常量池，str2.intern()直接返回常量池的引用。因此它们属于常量池的同一个引用 String 运算 12String str = &quot;R&quot;; 一共创建了几个对象 思路分析：&quot;R&quot;是一个字面量，会放在字符串常量池子中 如果字符串常量池中已经存在“R“，那么创建0个对象，直接返回字符串常量池引用 如果字符串常量池不存在&quot;R&quot;，那么在常量池中创建1个对象 12new String(&quot;R&quot;); 创建了几个对象 思路分析： new 关键字一定会在内存中创建一个对象，还要把字面量“R“提取出来分析 如果字符串常量池中已经存在&quot;R&quot;，那么创建1个对象(堆中1个) 如果字符串常量池不存在“R“，那么在常量池中创建2个对象(字符串常量池1个，堆中1个) 123String str = &quot;Java&quot;; // 常量池没有则在常量池创建一个 Java 对象,有则返回字符串的引用String str2 = &quot;Java&quot;; // 直接返回常量池引用为什么 str == str2 为true 思路分析：java对于常量（字面量）会存储在字符串常量池中，str1第一次赋值时在常量池中添加了“Java“并返回其引用, str2第二次赋值直接引用字符串常量池。两个都是常量池 “Java” 的引用 123String str = &quot;Java&quot;;String str2 = &quot;Ja&quot; + &quot;va&quot;; // 编译器对字面量优化 str2 = &quot;Java&quot;为什么 str == str2 为true 思路分析：参考上面，常量拼接还是常量(编译期优化) 123String str = &quot;Java&quot;;String str2 = new String(&quot;Java&quot;);为什么 str == str2 为false 思路分析：String str = “Java”; 内存存储区域在字符串常量池，new String(“Java”);内存存储区域在堆 1234String str1 = &quot;java&quot;;String str2 = &quot;ja&quot;;String str3 = str2 + &quot;va&quot;; // 编译器无法优化变量拼接，底层用的是 StringBuilder为什么 str1 == str3 为false 思路分析：因为两者属于不同对象，第一个对象在常量池中，第二个对象在堆中 str2 + “va”; 编译器无法对变量拼接优化。字符串拼接底层使用的是 StringBuilder#append，最终使用StringBuilder#toString方法返回String对象,对象在堆里 1234public String toString() &#123; // 创建一个新的String对象 return new String(value, 0, count);&#125; 拼接运算 12345678910111213String str1 = &quot;str&quot;;String str2 = &quot;ing&quot;;String str3 = &quot;str&quot; + &quot;ing&quot;;String str4 = str1 + str2;String str5 = &quot;string&quot;;String str6 = str1 + &quot;ing&quot;;System.out.println(str3 == str4);//falseSystem.out.println(str3 == str5);//trueSystem.out.println(str4 == str5);//falseSystem.out.println(str4.intern() == str6.intern()); // trueSystem.out.println(str5 == str6); // falseSystem.out.println(str4.intern() == str5); //true StringBuilder 当对字符串进行修改的时候，需要使用 StringBuffer 和 StringBuilder 类 因为 StringBuffer 和 StringBuilder 都是可变的，调用append 不会产生新的String对象 线程不安全，性能最好 默认16个字节 扩容：2倍 + 2个字节 toString 1234567char[] value;@Overridepublic String toString() &#123; // Create a copy, don&#x27;t share the array return new String(value, 0, count);&#125; toString 直接使用缓冲区 char数组 StringBuffe 和StringBuilder一致，不同的是使用了 Synchronized 保证了线程安全，性能差于StringBuilder 线程安全 默认16个字节 扩容：2倍 + 2个字节 toString 12345678@Overridepublic synchronized String toString() &#123;if (toStringCache == null) &#123; // 并发场景，会先拷贝一次缓冲区 toStringCache = Arrays.copyOfRange(value, 0, count);&#125;return new String(toStringCache, true);&#125; 异常 Throwable 所有的异常都有一个共同的祖先 Exception 和 Error 有什么区别 Exception :程序本身可以处理的异常，可以通过 catch 来进行捕获。Exception 又可以分为 Checked Exception (受检查异常，必须处理) 和 Unchecked Exception (不受检查异常，可以不处理) Checked Exception 即 受检查异常 ，Java 代码在编译过程中，如果受检查异常没有被 catch或者throws 关键字处理的话，就没办法通过编译 比如 SQLException IOException Unchecked Exception 即 不受检查异常 ，Java 代码在编译过程中 ，我们即使不处理不受检查异常也可以正常通过编译 RuntimeException 及其子类都统称为非受检查异常 NullPointerException(空指针错误) IllegalArgumentException(参数错误比如方法入参类型错误) NumberFormatException（字符串转换为数字格式错误，IllegalArgumentException的子类） ArrayIndexOutOfBoundsException（数组越界错误） ClassCastException（类型转换错误） Error ：属于程序无法处理的错误，例如 Java 虚拟机运行错误（Virtual MachineError）、虚拟机内存不够错误(OutOfMemoryError) 这些异常发生时，Java 虚拟机（JVM）一般会选择线程终止 try-catch-finally 如何使用 try块 ： 用于捕获异常。其后可接零个或多个 catch 块，如果没有 catch 块，则必须跟一个 finally 块。 catch块 ： 用于处理 try 捕获到的异常。 finally 块 ： 无论是否捕获或处理异常，finally 块里的语句都会被执行。当在 try 块或 catch 块中遇到 return 语句时，finally 语句块将在方法返回之前被执行。 12345678try &#123; System.out.println(&quot;Try to do something&quot;); throw new RuntimeException(&quot;RuntimeException&quot;);&#125; catch (Exception e) &#123; System.out.println(&quot;Catch Exception -&gt; &quot; + e.getMessage());&#125; finally &#123; System.out.println(&quot;Finally&quot;);&#125; 输出： 123Try to do somethingCatch Exception -&gt; RuntimeExceptionFinally 注意：不要在 finally 语句块中使用 return! 当 try 语句和 finally 语句中都有 return 语句时，try 语句块中的 return 语句会被忽略。这是因为 try 语句中的 return 返回值会先被暂存在一个本地变量中，当执行到 finally 语句中的 return 之后，这个本地变量的值就变为了 finally 语句中的 return 返回值 try具有缓存功能 在try语块中返回基本类型，会将return的值进行缓存，如果在finally语块修改返回值则不生效（基本类型） 12345678910 public static Integer test() &#123; int a = 0; try &#123; a = 1; return a; &#125; finally &#123; a = 10; &#125; &#125;// 返回值还是1 catch语块的多种用法 catch块可以捕获多种类型的异常，但是需要注意以下几点： 异常的捕获顺序：在多个catch块中，应该将具体的异常类型放在前面，将父类异常放在后面。如果将父类异常放在前面，那么子类异常永远不会被捕获 123456789try &#123; // 代码块&#125; catch (ArrayIndexOutOfBoundsException e) &#123; // 处理数组越界异常&#125; catch (IndexOutOfBoundsException e) &#123; // 处理下标越界异常&#125; catch (Exception e) &#123; // 处理其他异常&#125; 同时处理多个异常类型：在一个catch块中，可以同时捕获多个异常类型，多个异常类型之间使用管道符号 | 分隔 12345try &#123; // 代码块&#125; catch (IOException | SQLException e) &#123; // 处理IO异常和SQL异常&#125; finally 中的代码一定会执行吗 不一定的！在某些情况下，finally 中的代码不会被执行。 就比如说 finally 之前虚拟机被终止运行的话，finally 中的代码就不会被执行。 12345678910try &#123; System.out.println(&quot;Try to do something&quot;); throw new RuntimeException(&quot;RuntimeException&quot;);&#125; catch (Exception e) &#123; System.out.println(&quot;Catch Exception -&gt; &quot; + e.getMessage()); // 终止当前正在运行的Java虚拟机 System.exit(1);&#125; finally &#123; System.out.println(&quot;Finally&quot;);&#125; Java SPI SPI详解 SPI 全称为 Service Provider Interface（接口服务提供者），它是一种服务发现机制。SPI的本质是将接口的实现类配置在文件中，并由服务加载器读取配置文件，进行加载和实例化。 SPI机制的设计初衷是为了解决接口的扩展性问题。在传统的编程模型中，接口的实现类通常是由开发者手动指定或硬编码在代码中的。这种方式存在一个问题，即当需要替换或新增接口的实现类时，必须修改源代码并重新编译、部署应用程序。这种修改源代码的方式增加了耦合性，同时也不够灵活。 而SPI机制通过将接口的实现类配置在文件中，并由服务加载器动态加载，可以实现在不修改源代码的情况下，替换或新增接口的实现类。这样就提高了系统的扩展性和灵活性。 具体来说，SPI机制的使用步骤如下： 定义接口：首先需要定义一个接口，接口定义了一组方法或规范，表示一种功能或服务的抽象。 123public interface MyService &#123; void doSomething();&#125; 实现接口：根据接口的定义，编写实现类来实现接口中的方法，提供具体的功能实现。 123456public class MyServiceImpl implements MyService &#123; @Override public void doSomething() &#123; System.out.println(&quot;Doing something...&quot;); &#125;&#125; 创建配置文件：在resources/META-INF/services目录下创建一个以接口的全限定名命名的文件，文件内容为接口实现类的全限定名，每行一个。 1com.example.MyServiceImpl 加载实现类：通过服务加载器(ServiceLoader)加载配置文件中的实现类，并进行实例化。 1ServiceLoader&lt;MyService&gt; serviceLoader = ServiceLoader.load(MyService.class); // ServiceLoader 是一个可迭代对象，可以加在1-N个服务 使用服务：通过服务加载器获取已加载的实现类的实例，并调用其方法。 123for (MyService service : serviceLoader) &#123; service.doSomething();&#125; 序列化 序列化 ：将对象转为字节流的过程，让对象能够保存在本地文件或者在网络中传输 反序列化 ：将字节流转回Java对象的过程，根据字节流的信息将流数据转成一个Java对象的过程 序列化详解 Serializable 1234567@Datapublic class Article implements Serializable &#123; private static final long serialVersionUID = 1L; private Integer id; private String title; //文章标题 private String content; // 文章内容&#125; Externlizable 1234567891011121314151617@Datapublic class ExternalizableDemo implements Externalizable &#123; private static final long serialVersionUID = 1L; private String name; private int number; @Override public void writeExternal(ObjectOutput out) throws IOException &#123; out.writeUTF(name); out.writeInt(number); &#125; @Override public void readExternal(ObjectInput in) throws IOException, ClassNotFoundException &#123; this.name = in.readUTF(); this.number = in.readInt();&#125; 异同 序列化内容，Externalizable自定义序列化可以控制序列化的过程和决定哪些属性不被序列化 Serializable反序列化时不会调用默认的构造器，而Externalizable反序列化时会调用默认构造器的 使用Externalizable时，必须按照写入时的确切顺序读取所有字段状态。否则会产生异常。例如，如果更改ExternalizableDemo类中的number和name属性的读取顺序，则将抛出java.io.EOFException。而Serializable接口没有这个要求 transient修饰符 transient修饰符用于类属性、变量。表示该类的序列化过程用transient修饰该变量，可避免该变量被序列化 1private transient String password; 为什么 Java 中只有值传递 基本类型：传递的是实际值的拷贝 引用类型： 传递的是内存地址值的拷贝 比较器 Comparable 内部比较器 类继承 Comparable 接口并且实现 compareTo 方法就可以使用 Collections.sort 进行排序 12345678910111213141516171819202122232425262728293031323334 @Data @ToString @AllArgsConstructor public static class User implements Comparable&lt;User&gt; &#123; private String name; private Integer age; @Override public int compareTo(User o) &#123; // 倒叙 return o.getAge() - this.getAge(); // 正序 //return this.getAge() - o.getAge(); &#125; &#125; public static void main(String[] args) &#123; List&lt;User&gt; userList = new ArrayList&lt;&gt;(); userList.add(new User(&quot;test-1&quot;, 12)); userList.add(new User(&quot;test-2&quot;, 15)); userList.add(new User(&quot;test-3&quot;, 18)); userList.add(new User(&quot;test-4&quot;, 14)); userList.add(new User(&quot;test-5&quot;, 13)); // 使用内部比较器排序 Collections.sort(userList);// userList.forEach(System.out::println);// OrderService.User(name=test-3, age=18)// OrderService.User(name=test-2, age=15)// OrderService.User(name=test-4, age=14)// OrderService.User(name=test-5, age=13)// OrderService.User(name=test-1, age=12) &#125; 外部比较器 Comparator 如果类没有实现 Comparable 接口又要进行排序可使用外部比较器 Comparator 外部比较器重写 compare 方法 12345678910111213141516171819202122232425262728293031323334353637 @Data @ToString @AllArgsConstructor public static class User &#123; private String name; private Integer age; &#125; public static void main(String[] args) &#123; List&lt;User&gt; userList = new ArrayList&lt;&gt;(); userList.add(new User(&quot;test-1&quot;, 12)); userList.add(new User(&quot;test-2&quot;, 15)); userList.add(new User(&quot;test-3&quot;, 18)); userList.add(new User(&quot;test-4&quot;, 14)); userList.add(new User(&quot;test-5&quot;, 13)); // 类内有实现Comparable 但又需要进行排序, 使用外部比较器 Comparator&lt;User&gt; comparator = new Comparator&lt;User&gt;() &#123; @Override public int compare(User o1, User o2) &#123; return o2.getAge() - o1.getAge(); // 正序 //return o1.getAge() - o2.getAge(); &#125; &#125;; // 排序 Collections.sort(userList, comparator); userList.forEach(System.out::println);// OrderService.User(name=test-3, age=18)// OrderService.User(name=test-2, age=15)// OrderService.User(name=test-4, age=14)// OrderService.User(name=test-5, age=13)// OrderService.User(name=test-1, age=12) &#125; static 静态内部类：不需要依赖外部类的实例而实例化 静态代码块：类初始化时执行 静态方法：使用类名调用 静态变量：类变量分配到方法区 泛型 类型擦除 泛型信息只存在于代码编译阶段，但是在java的运行期(已经生成字节码文件后)与泛型相关的信息会被擦除掉，专业术语叫做类型擦除 1234ArrayList&lt;Integer&gt; l1 = new ArrayList(); ArrayList&lt;String&gt; l2 = new ArrayList(); System.out.println(l1.getClass()==l2.getClass()); //运行代码，结果为True 这是因为 ArrayList&lt;String&gt; 和 ArrayList&lt;Integer&gt; 在 jvm 中的 Class 都是 List.class，二者在 jvm 中等同于 List&lt;Object&gt;，所有的泛型在编译后都会变成Object Java的泛型其实是伪泛型，因为编译后都会进行类型擦除，它的作用就是在编码阶段对使用的类型作限制 泛型上下限 无边界擦除 12345678910List&lt;Integer&gt; list = new ArrayList&lt;&gt;();list.add(1);List&lt;?&gt; list2 = list;// 获得类型为ObjectObject o = list2.get(0);// 编译不通过list2.add(2); 相当于 List&lt;? extends Object&gt;，知道父类是Object 可以get获取值，返回的是Object 不清楚使用的是哪种具体类型，因此不能写入数据 上界通配 &lt;? extends 父类类型&gt; 可以确定父类型，所以返回的数据是父类型（向上转型） 不能写入数据 123456789List&lt;Animal&gt; list = new ArrayList&lt;&gt;();list.add(new Cat());List&lt;? extends Animal&gt; list2 = list;// 返回的是父类型（向上转型）Animal animal = list2.get(0);// 不能写数据，编译不通过list2.add(new Dog()); 下界通配 &lt;? super 子类类型&gt; 可以确定子类类型，但不确定其有多少父类，所以返回Object 由于知道子类，所以可以写入父类对象（向上转型是安全的） 12345678910List&lt;Animal&gt; list = new ArrayList&lt;&gt;();list.add(new Cat());list.add(new Dog());List&lt;? super Cat&gt; list2 = list;// 只知道子类，所以返回的是ObjectObject o = list2.get(0);// 能写数据list2.add(new Cat()); 集合 fail-fast机制 多线程环境保证集合安全性和一致性的错误机制 fail-fast(快速失败)机制是java集合(Collection)中的一种错误机制 当多个线程对同一个集合的内容进行操作时，就可能产生fail-fast事件 例如：线程A通过Iterator迭代器去遍历某个集合的过程中，如果集合对象的内容被其他线程进行了修改（增加，删除，修改），则会抛出java.util.ConcurrentModificationException异常，产生 fail-fast事件 java.util包下的集合都是快速失败的，不能在多线程并发修改 fail-safe机制 fail-safe(安全失败)机制在遍历时不是在集合内容上访问的，而是先复制原有集合内容，在拷贝的集合上进行遍历，因此它不会抛出java.util.ConcurrentModificationException异常 juc包下的集合都是fail-safe安全失败的，可以在多线程并发下使用，并发修改 迭代器 Iterator 支持 fail-fast 机制，更加安全 Iterator是JDK1.2添加的接口，Collection 集合类使用 速度比Enumeration慢 允许从集合中移除元素（从循环中删除元素） Enumeration 不支持 fail-fast 机制 Enumeration 是JDK 1.0添加的接口。使用到它的函数包括Vector、Hashtable等类，已过时 速度比Iterator快 基础的实现，不支持移除元素 List 提供排序 sort 方法，传入 Comparator List转Array：调用toArray方法 Array转List：new ArrayList(Arrays.asList(array)) List遍历：普通 for，增强 for，迭代器，Stream.foreach Arrays.asList 它的add/remove/clear方法会抛出UnsupportedOperationException() 返回的是 Arrays.ArrayList 内部类 要正常使用必须使用 new ArrayList() 包裹 ArrayList 数据结构：Object[] 初始容量：10 最大容量：Integer.MAX_VALUE - 8 扩容倍数：oldCapacity + (oldCapacity &gt;&gt; 1) 原容量 + （原容量/2) 1.5倍 扩容实现：System.arraycopy() 数组复制 非线程安全 支持随机访问 存空间占用：扩容后数组元素可能没有完全利用 特点：数据检索快，数据插入、删除慢（需要移动数组元素） Vector 数据结构：Object[] 初始容量：10 最大容量：Integer.MAX_VALUE - 8 扩容倍数：2 支持随机访问 历史遗留的集合类，已经不推荐使用 线程安全：可以看作线程安全的ArrayList 线程安全保证：方法使用synchronized修饰 性能比ArrayList差 线程安全推荐使用 JUC的 CopyOnWriteArrayList LinkedList 数据结构：双向链表，节点保存两个指针，前驱和后继 最大容量：理论上没有，取决于内存大小 头节点不存放数据，允许元素为null 不支持随机访问（链表不允许） 内存空间占用：维护额外的前驱后继指针 特点：数据插入删除快，数据检索效率不高 Map HashMap 数据结构：数组 + 单向链表 + 红黑树(hash冲突严重用于优化查询性能) 负载因子：0.75 初始容量：16 扩容条件：当前数组大小 &gt; 当前数组大小 * 0.75 (默认12) 扩容倍数：2 扩容过程： 不必将所有Key都重新计算一次hash 将key的hash与数组长度做高位与运算（2n-1），结果为0位置不变，为1位置变为当前下标 + 扩容步长 比如（7 + 16） 链表转红黑树：数组大小&gt;= 64, 链表长度&gt;=8 红黑树退化链表：链表长度&lt;=6 允许一个key为null 非线程安全 数据添加无序 put：(n-1) &amp; hash,计算出数组下标位置获取链表进行添加, (n为数组长度，hash为key的hash) get: (n-1) &amp; hash，计算素组下标位置获取链表进行遍历，先判断hash是否相等，相等则判断key JDK1.8的变化： 链表和红黑树转换 hash碰撞，1.7会在链表头部插入；1.8会在链表尾部插入 Entry被Node替代 HashTable 方法使用Synchronized修改，线程安全 初始容量：11 扩容倍数：2n + 1 key,value都不能为null 不支持链表和红黑树互转 考虑线程安全使用ConcurrentHashMap Node锁效率更高 LinkedHashMap 特点：保证插入顺序，使用双向链表保证 数据结构：数组加双向链表（前驱后继保证插入顺序） 使用before,after指针保证插入顺序 HashMap的Entry是一个单向链表，用于解决Hash冲突 123456static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final int hash; final K key; V value; Node&lt;K,V&gt; next; ... LinkedHashMap的Entry继承HashMap的Node, Entry是一个双向链表，用于保证插入顺序 123456static class Entry&lt;K,V&gt; extends HashMap.Node&lt;K,V&gt; &#123; Entry&lt;K,V&gt; before, after; Entry(int hash, K key, V value, Node&lt;K,V&gt; next) &#123; super(hash, key, value, next); &#125; &#125; TreeMap 数据结构：红黑树 实现key的自然排序 可以在构造函数指定Comparator比较器对key排序 WeakHashMap Java存在的引用 WeakHashMap：一个weak key的Map 它能让Map释放其所持有的对象 如果某个对象除了在Map当中充当键之外 ,在其他地方都没有引用的话，那它将被当作垃圾回收 使用场景：二三级缓存 也存在内存溢出可能 GC回收时进行回收 1234567891011121314151617181920212223242526272829/** * WeakHashMap：一个weak key的Map， * 是为某些特殊问题而设计的。它能让Map释放其所持有的对象。 * 如果某个对象除了在Map当中充当键之外， * 在其他地方都没有引用的话，那它将被当作垃圾回收。 * 使用场景：二三级缓存 */public class WeakHashMapTest &#123; public static WeakHashMap&lt;Integer, Integer&gt; map = new WeakHashMap&lt;&gt;(); public static void main(String[] args) throws InterruptedException &#123; for (int i = 0; i &lt; 1000; i++) &#123; map.put(i, i); &#125; System.out.println(map.size()); System.gc(); TimeUnit.SECONDS.sleep(5); System.out.println(map.size()); &#125;&#125;/** * 输出 * 1000 * 128 */ IdentityHashMap HashMap判断key是否相等是根据k1.hashcode==k2.hashcode &amp;&amp; k1.equals(k2) IdentityHashMap是只判断内存地址是否相等 k1=k2,换而言之就是同个Class的不同对象都可以存放在map里 12345678910public static void main(String[] args) &#123; // 不会判断key的hashcode是否相等，只判断了key的内存地址是否相等 IdentityHashMap&lt;Integer, Object&gt; map = new IdentityHashMap&lt;&gt;(); map.put(new Integer(1), 1); map.put(new Integer(1), 2); System.out.println(map); // 输出 // &#123;1=2, 1=1&#125;&#125; Set HashSet 底层采用HashMap实现，value为空 LinkedHashSet 底层采用LinkedHashMap实现，value为空 TreeSet 底层采用TreeMap实现，value为空 PriorityQueue 优先级队列使用 优先级队列 数据结构：利用了二叉堆的数据结构来实现的，底层使用可变长的数组来存储数据 初始容量：11 扩容倍数：1.5 其与 Queue 的区别在于元素出队顺序是与优先级相关的，即总是优先级最高的元素先出队 构造器允许传入 Comparator 实现自定义优先级 是非线程安全的，且不支持存储 NULL 和 non-comparable 的对象 默认是小顶堆，但可以接收一个 Comparator 作为构造参数 ArrayDeque 双端队列使用 双端队列，支持两边进两边出 数据结构：可变长数组加双指针（头，尾） 扩容倍数：2 初始容量：16 不允许数据为null（因为需要比较） 先进先出 addFirst() 方法 配合pollLast() 方法 addLast() 方法 配合 pollFirst()方法 先进后出（栈） addFirst() 方法配合 pollFirst()方法 addLast()方法配合pollLast()方法 CAS Compare and Swap（比较并交换），是一种无锁 原子算法，也是一条CPU的原子指令 MESI协议 Atomic 内部使用了 volatile 保证了自身的可见性 CAS涉及到三个属性 内存读写位置 旧值 需要写入的新值 CAS具体执行时，当内存中的值等于传入的旧值，就将内存值修改为新值，否则不替换 Unsafe是CAS的核心类，Java无法直接访问底层操作系统，而是通过本地（native）方法来访问。不过尽管如此，JVM还是开了一个后门：Unsafe，它提供了硬件级别的原子操作，一条汇编指令完成数据比较和交换保证了其原子性 1234567891011public static void main(String[] args) &#123; AtomicInteger ac = new AtomicInteger(0); /** * ac: 需要写入的内存位置 * 第一个参数：预期值 * 第二个参数：修改值 */ ac.compareAndSet(0, 1); System.out.println(ac.get()); // 1 &#125; ABA 如果一个值原来是A，变成了B，然后又变成了A，那么在CAS检查的时候会发现没有改变，但是实质上它已经发生了改变，这就是所谓的ABA问题 对于ABA问题其解决方案是加上版本号，即在每个变量都加上一个版本号，每次改变时加1，即A —&gt; B —&gt; A，变成1A —&gt; 2B —&gt; 3A ABA解决 AtomicMarkableReference 12345678910111213141516171819202122232425262728293031323334public class AtomicMarkableReferenceTest &#123; /** * 用boolean 值做版本戳 */ private static AtomicMarkableReference&lt;Integer&gt; mark = new AtomicMarkableReference(1, false); public static void main(String[] args) throws InterruptedException &#123; AtomicBoolean first = new AtomicBoolean(true); for (int i = 0; i &lt; 10; i++) &#123; new Thread(() -&gt; &#123; for (int j = 0; j &lt; 5; j++) &#123; if (first.getAndSet(false)) &#123; // CAS需要版本号 mark.compareAndSet(1, 10, false, false); &#125; else &#123; // 版本戳不一致，CAS不会成功 mark.compareAndSet(10, 100, true, true); &#125; &#125; &#125;).start(); &#125; TimeUnit.SECONDS.sleep(2); System.out.println(mark.getReference()); &#125;&#125;/** * 输出10 */ AtomicStampedReference 1234567891011121314151617181920212223242526272829303132/** * 根据一个版本戳解决ABA问题 */public class AtomicStampedReferenceTest &#123; /** * 设置版本戳为1 */ private static AtomicStampedReference&lt;Integer&gt; ref = new AtomicStampedReference(10, 1); public static void main(String[] args) throws InterruptedException &#123; AtomicBoolean first = new AtomicBoolean(true); for (int i = 0; i &lt; 10; i++) &#123; new Thread(() -&gt; &#123; if (first.getAndSet(false)) &#123; // 增加 2 个参数 一个是期望版本戳 一个是新版本戳 ref.compareAndSet(10, 100, ref.getStamp(), ref.getStamp() + 1); &#125; else &#123; // 版本戳不一直， CAS失败 ref.compareAndSet(10, 100, 0, 0); &#125; &#125;).start(); &#125; TimeUnit.SECONDS.sleep(2); System.out.println(ref.getReference()); &#125;&#125;/** * 输出 100 */ AtomicBoolean API文档 需要注意的方法 public final void set(boolean newValue); 无条件设置值，不保证原子性 public final void lazySet(boolean newValue); 懒设置值，不会像set一样马上将值刷到主内存中，但最终还是会刷新到主内存，非原子 public boolean weakCompareAndSet(boolean expect,boolean update)；在正常情况下weak版本比compareAndSet 更高效，但是不同的是任何给定的weakCompareAndSet方法的调用都可能会返回一个虚假的失败( 无任何明显的原因 )。一个失败的返回意味着，操作将会重新执行如果需要的话 AtomicInteger API文档 public final int getAndUpdate(IntUnaryOperator updateFunction)；实现IntUnaryOperator 接口的applyAsInt 可以实现复杂的原子性算法 getAndAccumulate(int x, IntBinaryOperator accumulatorFunction)；给指定值x,和实现IntBinaryOperator函数式接口实现当前值和给定值x的运算结果，最终通过cas重新设置新值 AtomicIntegerArray API文档 public final int getAndUpdate(int i,IntUnaryOperator updateFunction); 和AtomicInteger的类似，只是多了个下标 public final int getAndAccumulate(int i,int x,IntBinaryOperator accumulatorFunction)；和AtomicInteger的类似，只是多了个下标 初始化 new AtomicIntegerArray(new int[]{1, 2}); 1234567891011121314151617181920212223242526272829303132333435363738394041public class AtomicIntegerArrayTest &#123; public static AtomicIntegerArray array = new AtomicIntegerArray(new int[]&#123;1, 2&#125;); public static void main(String[] args) throws InterruptedException &#123; for (int i = 0; i &lt; 10; i++) &#123; new Thread(() -&gt; &#123; // 第一个参数是数组下标 // 第二个参数是指定值 // 第三个参数是一个函数式接口，根据数组下标当前值和执行值运算返回的新值 // 最后将计算出来的新值设置到对应的数组下标中 while (array.accumulateAndGet(0, 1, (x, y) -&gt; x + y) &lt; 10) &#123; &#125; &#125;).start(); new Thread(() -&gt; &#123; while (array.accumulateAndGet(1, 1, (x, y) -&gt; x + y) &lt; 10) &#123; &#125; &#125;).start(); &#125; TimeUnit.SECONDS.sleep(2); System.out.println(array.get(0)); System.out.println(array.get(1)); &#125;&#125;/** * 输出 * 19 * 19 * &lt;p&gt; * 分析，第一个线程累加1-10，在这累加的过程中其他9条线程也在进行cas操作，最后执行完成 * 第一个线程的执行结果为10 * 第二个线程cas执行结果后为 11 * ... * 第十个线程cas执行结果后为 19 * 原因是 array.accumulateAndGet(0, 1, (x, y) -&gt; x + y) &lt; 10 这个判断不是一个原子操作，在 * 多线程并发不能保证数据的准确性 */ AtomicIntegerFieldUpdater API文档 主要对对象里的 public volatile int 变量进行原子性操作，其API和AtomicInteger一致 12345678910111213141516171819202122232425262728293031323334353637383940414243444546public class AtomicIntegerFieldUpdaterTest &#123; // 通过反射创建 private static AtomicIntegerFieldUpdater&lt;Test&gt; fieldUpdater = AtomicIntegerFieldUpdater.newUpdater(Test.class, &quot;num&quot;); public static void main(String[] args) throws InterruptedException &#123; // 创建对象 Test test = new Test(); for (int i = 0; i &lt; 10; i++) &#123; new Thread(() -&gt; &#123; for (int j = 0; j &lt; 10; j++) &#123; fieldUpdater.incrementAndGet(test); &#125; &#125;).start(); &#125; TimeUnit.SECONDS.sleep(2); System.out.println(test.num); System.out.println(test.getStr()); &#125;&#125;/** * 输出 * 100 * null */class Test &#123; // 一定要使用 volatile 保证其他线程的可见性 // 一定要使用public保证外界只直接访问 // 必须为int类型，Integer封装类型也不允许 public volatile int num; private String str; public String getStr() &#123; return str; &#125; public void setStr(String str) &#123; this.str = str; &#125;&#125; AtomicLong API文档 和AtomicInteger使用一致，累加建议使用 LongAdder AtomicLongArray API文档 和AtomicIntegerArray 使用一致 AtomicReference API文档 在低并发情况下可以得到正确的结果,但是高并发情况下就会出现差异.因为自定义的对象在访问时用的是set,get没有CAS,所以导致线程不安全. atomic包中提供AtomicReferenceFieldUpdater、AtomicIntegerFieldUpdater、AtomicLongFieldUpdater，原子性的更新某一个类实例的指定的某一个字段 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849/** * 保证对象的原子性 */public class AtomicReferenceTest &#123; private static AtomicReference&lt;Simple&gt; ref = new AtomicReference&lt;&gt;(new Simple(20, &quot;&quot;)); public static void main(String[] args) throws InterruptedException &#123; for (int i = 0; i &lt; 100; i++) &#123; new Thread(() -&gt; &#123; for (int j = 0; j &lt; 100; j++) &#123; ref.compareAndSet(ref.get(), new Simple(ref.get().getAge() + 1, &quot;&quot;)); &#125; &#125;).start(); &#125; TimeUnit.SECONDS.sleep(5); System.out.println(ref.get().getAge()); &#125;&#125;/** * 不一定输出 10020 */class Simple &#123; public Simple(int age, String name) &#123; this.age = age; this.name = name; &#125; private int age; private String name; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125;&#125; AtomicReferenceFieldUpdater API文档 AtomicReferenceFieldUpdater 是基于反射的工具类，用来将指定类型的指定的volatile引用字段进行原子更新，对应的原子引用字段不能是private的。通常一个类volatile成员属性获取值、设定为某个值两个操作时非原子的，若想将其变为原子的，则可通过AtomicReferenceFieldUpdater来实现 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253 private static AtomicReferenceFieldUpdater&lt;Simple, Integer&gt; updater = AtomicReferenceFieldUpdater.newUpdater(Simple.class, Integer.class, &quot;age&quot;); public static void main(String[] args) throws InterruptedException &#123; Simple simple = new Simple(20, &quot;&quot;); for (int i = 0; i &lt; 100; i++) &#123; new Thread(() -&gt; &#123; for (int j = 0; j &lt; 100; j++) &#123; // 自旋修改 while (!updater.compareAndSet(simple, simple.getAge(),simple.getAge() + 1)); // 存在ABA问题， 最终不等于 10020, 原因是simple.getAge()只保证可见性，不保证原子性,多个线程会读到同一个值 // 如果要修改的值不依赖于 simple.getAge() 则可以直接使用 updater.getAndSet //Integer temp1 = simple.getAge(); //Integer temp2 = updater.getAndSet(simple, simple.getAge() + 1); //if (temp1 &gt;= temp2) &#123; // System.out.println(&quot;ABA&quot;); // System.out.println(&quot;temp1=&quot; + temp1); // System.out.println(&quot;temp2=&quot; + temp2); //&#125; &#125; &#125;).start(); &#125; TimeUnit.SECONDS.sleep(5); System.out.println(simple.getAge()); &#125;&#125;class Simple &#123; public Simple(Integer age, String name) &#123; this.age = age; this.name = name; &#125; public volatile Integer age; private String name; public Integer getAge() &#123; return age; &#125; public void setAge(Integer age) &#123; this.age = age; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125;&#125; LongAdder JDK1.8新增一个原子性操作类LongAdder，用于代替AtomicLong的功能，因为在非常高并发的请求下，AtomicLong的性能是一个很大的瓶颈，因为AtomicLong采用的CAS算法失败后还是通过无限循环的自旋锁不断的尝试 为了解决这个问题，JDK的开发组就创建了LongAdder，性能要高于AtomicLong很多 缺点：在统计的时候，如果有并发更新，可能会导致统计数据有些误差 实现 LongAdder内部维护一个Cell[] as数组，每个Cell里面有一个初始值为0的long型变量，在同等并发量的情况下，争夺单个变量的线程会减少，这是变相的减少了争夺共享资源的并发量，另外多个线程在争夺同一个原子变量时候，如果失败并不是自旋CAS重试，而是尝试获取其他原子变量的锁，最后当获取当前值时候是把所有变量的值累加后再加上base的值返回的。 1234567891011121314151617181920 // 底层使用cell数组，每个数组元素都是一个原子资源，高并发只要获得其中一个元素后进行自增即可// 调用get时，返回的是所有cell数组的累加值// 缺点：数值存在偏差public void add(long x) &#123; Cell[] as; long b, v; int m; Cell a; if ((as = cells) != null || !casBase(b = base, b + x)) &#123; boolean uncontended = true; if (as == null || (m = as.length - 1) &lt; 0 || (a = as[getProbe() &amp; m]) == null || !(uncontended = a.cas(v = a.value, v + x))) longAccumulate(x, null, uncontended); &#125; &#125; /** * Equivalent to &#123;@code add(1)&#125;. */ public void increment() &#123; add(1L); &#125; JUC 非阻塞 Queue 单端队列 ConcurrentLinkedQueue API文档 基于链接节点的无界线程安全queue FIFO，先进先出 数据结构：单向链表 采用cas算法保证线程安全，底层时Unsafe 元素不能为空（cas） add过程 如果链表为空，将当前节点通过CAS设置为链表的 head 和 tail 如果链表不为空，则通过CAS将当前节点追加到链表尾部 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162/** * 基于链接节点的无界线程安全queue 。 这个队列排列元素FIFO（先进先出）， * 像大多数其他并发集合实现一样，此类不允许使用null元素,否则会空指针异常 */public class ConcurrentLinkedQueueTest &#123; private static ConcurrentLinkedQueue&lt;Integer&gt; query = new ConcurrentLinkedQueue&lt;&gt;(); public static void main(String[] args) &#123; // 生产者线程 new Thread(() -&gt; &#123; Random random = new Random(); for (int i = 0; i &lt; 10; i++) &#123; int num = random.nextInt(50); try &#123; TimeUnit.MICROSECONDS.sleep(num); System.out.println(String.format(&quot;%s 生产产品编号：%s&quot;, Thread.currentThread().getName(), num)); query.add(num); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;).start(); // 消费者线程 new Thread(() -&gt; &#123; int count = 0; while (count &lt; 100) &#123; // 队列为空则返回null Integer num = query.poll(); if (Objects.nonNull(num)) &#123; System.out.println(String.format(&quot;%s 消费产品编号：%s&quot;, Thread.currentThread().getName(), num)); count++; &#125; &#125; &#125;).start(); &#125;&#125;/** * 输出 * Thread-0 生产产品编号：1 * Thread-0 生产产品编号：36 * Thread-1 消费产品编号：1 * Thread-1 消费产品编号：36 * Thread-0 生产产品编号：39 * Thread-1 消费产品编号：39 * Thread-0 生产产品编号：34 * Thread-1 消费产品编号：34 * Thread-0 生产产品编号：36 * Thread-1 消费产品编号：36 * Thread-0 生产产品编号：49 * Thread-1 消费产品编号：49 * Thread-0 生产产品编号：32 * Thread-1 消费产品编号：32 * Thread-0 生产产品编号：28 * Thread-1 消费产品编号：28 * Thread-0 生产产品编号：27 * Thread-1 消费产品编号：27 * Thread-0 生产产品编号：13 * Thread-1 消费产品编号：13 */ List 线程安全的List CopyOnWriteArrayList API文档 一个线程安全的变体ArrayList ，其中所有可变操作（ add ， get ，等等）通过对底层数组的最新副本实现 底层数据结构采用Object[] 实现 List 接口 使用和ArrayList类似 线程安全：使用 ReentrantLock 保证 CopyOnWrite 简称 COW，先复制再写入，就是在添加元素的时候，先把原List列表复制一份，再添加新的元素，添加元素时，先枷锁，再进行复制替换操作，最后释放锁 适合读多写少场景，写操作代价昂贵 1234567891011121314151617public boolean add(E e) &#123; // 加锁 final ReentrantLock lock = this.lock; lock.lock(); try &#123; Object[] elements = getArray(); int len = elements.length; // 复制副本 Object[] newElements = Arrays.copyOf(elements, len + 1); newElements[len] = e; // 数组替换 setArray(newElements); return true; &#125; finally &#123; lock.unlock(); &#125;&#125; 1234 // get 不加锁public E get(int index) &#123; return get(getArray(), index); &#125; Set 单端 线程安全的Set CopyOnWriteArraySet API文档 线程安全的 Set 底层数据：CopyOnWriteArrayList，Object[] ReentrantLock 保证线程安全 去重：每次插入先遍历元素是否存在 双端 ConcurrentSkipListSet API文档 key有序：可以当作线程安全的 TreeSet 提供获取第一个或最后一个元素的能力 底层数据结构：ConcurrentSkipListMap 12345678910 public boolean add(E e) &#123; return al.addIfAbsent(e); &#125;public boolean addIfAbsent(E e) &#123; Object[] snapshot = getArray(); return indexOf(e, snapshot, 0, snapshot.length) &gt;= 0 ? false : // 加锁新增 addIfAbsent(e, snapshot); &#125; Map ConcurrentHashMap API文档 在线源码 线程安全，使用Node锁替换分段锁 如果链表为空采用cas设置链表头 否则使用synchronized锁住整个桶 放弃分段锁: 分段锁会锁几条链表（数组的几个元素），锁粒度太大，不能很好支持并发 支持链表和红黑树互转 懒加载，Map创建不会初始化，当put第一个值才会初始化 initTable() sizeCtl -1：正在初始化 -n：正在扩容 n：下次扩容的阈值 扩容时机 桶数量 &gt;= 数组长度 * 0.75 扩容 ForwardingNode 转义节点 存放 nextTable 新扩容的Tab 转义节点Key的 hash = -1 完成扩容的旧桶放置转义节点，这样做时为了在扩容期间其他线程调用get转义节点能跳转到新Tab获取数据 多线程协助扩容 当调用Put方法定位桶，桶是一个转义节点时协助扩容 每个协助扩容线程最少负责16个桶 流程 在老的Tab里分配最少16个要扩容的桶 如果老桶为空，在老Tab插入一个转义节点，用于告诉其他线程当前在扩容 如果当前是转义节点说明这个桶已经完成扩容，跳过 如果老桶不为空也不是转义节点，开始扩容 对当前桶加锁 便利链表，进行高位与运算 在先Tab里分裂为2个链表 扩容完成，当前桶设置为转义节点 扩容时get get不需要加锁 如果桶不为空，直接加锁链表后遍历i获取数据 如果为转义节点，则通过转义节点去新的Tab获取数据 如果桶正在扩容，那么它被其他线程加锁，则等待 进化红黑树 hash值为-2，说明当前hash位置下挂的是红黑树 数组长度&gt;=64 链表长度大于8 退化红黑树 红黑树节点数量 &lt;= 6 K,V 不能为空 计数器优化，思路和 LongAdder 类似，底层采用 CountCell[] HASH_BITS 第一位是0， 01111111… 保证运算出来的hash一定是个正数，负数在Map中有特殊含义 分段锁 ReentrantLook + N个桶 Node锁 JDK1.8放弃分段锁的原因是因为粒度太大，影响并发性能 node锁 如果是链表第一个元素，则采用cas算法 否则就锁链表的第一个node节点 synchroinzed 扩容场景 hash冲突严重，链表节点数超过8，此时先判断数组是否超过64，没有超过就先扩容，超过就转红黑树 putAll方法，放入一个长度大于本身的Map put方法，数组长度超过扩容的阈值 ConcurrentSkipListMap 1、ConcurrentSkipListMap 的key是有序的。 2、构造函数支持 Comparator比较器自定义实现key排序 3、ConcurrentSkipListMap 支持更高的并发。ConcurrentSkipListMap 的存取时间是log（N），和线程数几乎无关。也就是说在数据量一定的情况下，并发的线程越多，ConcurrentSkipListMap越能体现出他的优势。 Deque 双端队列 ConcurrentLinkedDeque API文档 基于链表实现的无界双端队列 双端对列可以做 FIFO对列，也可以做栈 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778/** * 基于链接节点的无界并发deque 。 并发插入，删除和访问操作可以跨多个线程安全执行 * 像大多数其他并发集合实现一样，此类不允许使用null元素 * 迭代器和分配器是weakly consistent */public class ConcurrentLinkedDequeTest &#123; private static ConcurrentLinkedDeque&lt;Integer&gt; deque = new ConcurrentLinkedDeque(); private static final Random random = new Random(); public static void main(String[] args) &#123; // 生产者线程 new Thread(() -&gt; &#123; for (int i = 0; i &lt; 10; i++) &#123; deque.add(i); &#125; &#125;).start(); // 队列头部消费线程 new Thread(() -&gt; &#123; int count = 0; while (count &lt; 5) &#123; int stop = random.nextInt(100); try &#123; TimeUnit.MILLISECONDS.sleep(stop); // 获取队列头部元素 Integer num = deque.pollFirst(); if (Objects.nonNull(num)) &#123; System.out.println(String.format(&quot;头部线程消费元素：%d&quot;, num)); count++; &#125; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;).start(); // 队列尾部消费线程 new Thread(() -&gt; &#123; int count = 0; while (count &lt; 5) &#123; int stop = random.nextInt(100); try &#123; TimeUnit.MILLISECONDS.sleep(stop); // 获取队列尾部元素 Integer num = deque.pollLast(); if (Objects.nonNull(num)) &#123; System.out.println(String.format(&quot;尾部线程消费元素：%d&quot;, num)); count++; &#125; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;).start(); &#125;&#125;/** * 输出 * 尾部线程消费元素：9 * 头部线程消费元素：0 * 头部线程消费元素：1 * 头部线程消费元素：2 * 头部线程消费元素：3 * 尾部线程消费元素：8 * 尾部线程消费元素：7 * 头部线程消费元素：4 * 尾部线程消费元素：6 * 尾部线程消费元素：5 */ 阻塞 put: 队列满了阻塞 take: 队列空了阻塞 Queue ArrayBlockingQueue API文档 由数组创建的有界的单端对列 有界队列 底层数据类型：Object[] 采用ReentrantLock保证多线程安全 FIFO 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263/** * 一个有限的blocking queue由数组支持。 这个队列排列元素FIFO（先进先出） * 底层采用的数据结构是数组，所以在初始化时需要指定长度,创建后，容量无法更改 * 尝试put成满的队列的元件将导致在操作阻挡; * 尝试take从空队列的元件将类似地阻塞。 */public class ArrayBlockingQueueTest &#123; private static ArrayBlockingQueue&lt;Integer&gt; queue = new ArrayBlockingQueue&lt;&gt;(2); public static void main(String[] args) &#123; // 生产者线程 new Thread(() -&gt; &#123; for (int i = 0; i &lt; 10; i++) &#123; try &#123; // 用add 队列满了会抛出异常 queue.put(i); System.out.println(String.format(&quot;生产线程生产元素：%d&quot;, i)); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;).start(); // 消费者线程 new Thread(() -&gt; &#123; for (int i = 0; i &lt; 10; i++) &#123; // 底层采用ReentrantLock实现线程安全 try &#123; // 队列为空会阻塞 TimeUnit.MILLISECONDS.sleep(100); int num = queue.take(); System.out.println(String.format(&quot;消费线程消费元素：%d&quot;, num)); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;).start(); &#125;&#125;/** * 输出 * 生产线程生产元素：0 * 生产线程生产元素：1 * 消费线程消费元素：0 * 生产线程生产元素：2 * 生产线程生产元素：3 * 消费线程消费元素：1 * 消费线程消费元素：2 * 生产线程生产元素：4 * 生产线程生产元素：5 * 消费线程消费元素：3 * 生产线程生产元素：6 * 消费线程消费元素：4 * 消费线程消费元素：5 * 生产线程生产元素：7 * 生产线程生产元素：8 * 消费线程消费元素：6 * 消费线程消费元素：7 * 生产线程生产元素：9 * 消费线程消费元素：8 * 消费线程消费元素：9 */ 12345678910111213public void put(E e) throws InterruptedException &#123; checkNotNull(e); final ReentrantLock lock = this.lock; lock.lockInterruptibly(); try &#123; // 对列满了则阻塞 while (count == items.length) notFull.await(); enqueue(e); &#125; finally &#123; lock.unlock(); &#125;&#125; 123456789101112public E take() throws InterruptedException &#123; final ReentrantLock lock = this.lock; lock.lockInterruptibly(); try &#123; // 对列空了阻塞 while (count == 0) notEmpty.await(); return dequeue(); &#125; finally &#123; lock.unlock(); &#125;&#125; LinkedBlockingQueue API文档 基于链表实现的有界阻塞的单端对列 采用ReentrantLock保证多线程安全 底层数据结构：单向链表 FIFO 12345678910111213141516171819202122232425262728293031323334353637383940414243444546/** * 基于链接节点的可选限定的blocking queue 。 这个队列排列元素FIFO（先进先出） * &lt;p&gt; * LinkedBlockingQueue() * 创建一个 LinkedBlockingQueue ，容量为 Integer.MAX_VALUE 。 * &lt;p&gt; * LinkedBlockingQueue(Collection&lt;? extends E&gt; c) * 创建一个 LinkedBlockingQueue ，容量为 Integer.MAX_VALUE ，最初包含给定集合的元素，以集合的迭代器的遍历顺序添加。 * &lt;p&gt; * LinkedBlockingQueue(int capacity) * 创建一个具有给定（固定）容量的 LinkedBlockingQueue 。 * &lt;p&gt; * 队列满了继续往队列添加元素则阻塞，队列空继续往队列获取元素则阻塞 */public class LinkedBlockingQueueTest &#123; private static LinkedBlockingQueue&lt;Integer&gt; query = new LinkedBlockingQueue&lt;&gt;(2); public static void main(String[] args) &#123; // 生产线程 new Thread(() -&gt; &#123; for (int i = 0; i &lt; 10; i++) &#123; try &#123; // 队列满了则阻塞 query.put(i); System.out.println(String.format(&quot;生产线程生成元素：%d&quot;, i)); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;).start(); // 消费线程 new Thread(() -&gt; &#123; for (int i = 0; i &lt; 10; i++) &#123; // 队列为空则阻塞 try &#123; TimeUnit.MILLISECONDS.sleep(100); int num = query.take(); System.out.println(String.format(&quot;消费线程消费元素：%d&quot;, num)); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;).start(); &#125;&#125; 1234567891011121314151617181920212223public void put(E e) throws InterruptedException &#123; if (e == null) throw new NullPointerException(); int c = -1; Node&lt;E&gt; node = new Node&lt;E&gt;(e); final ReentrantLock putLock = this.putLock; final AtomicInteger count = this.count; putLock.lockInterruptibly(); try &#123; while (count.get() == capacity) &#123; notFull.await(); &#125; enqueue(node); c = count.getAndIncrement(); if (c + 1 &lt; capacity) notFull.signal(); &#125; finally &#123; putLock.unlock(); &#125; if (c == 0) signalNotEmpty(); &#125; 123456789101112131415161718192021public E take() throws InterruptedException &#123; E x; int c = -1; final AtomicInteger count = this.count; final ReentrantLock takeLock = this.takeLock; takeLock.lockInterruptibly(); try &#123; while (count.get() == 0) &#123; notEmpty.await(); &#125; x = dequeue(); c = count.getAndDecrement(); if (c &gt; 1) notEmpty.signal(); &#125; finally &#123; takeLock.unlock(); &#125; if (c == capacity) signalNotFull(); return x;&#125; PriorityBlockingQueue API文档 无边界具有优先级的阻塞队列 具有优先级的队列，可根据Comparator自定义优先级排序 采用ReentrantLock保证多线程安全 底层数据结构：Object[] 优先级队列可用于实现大顶堆和小顶堆 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889/** * 无边界具有优先级的阻塞队列 * 元素加入队列会根据自实现的Comparator比较器自动排序实现优先级 */public class PriorityBlockingQueueTest &#123; public static void main(String[] args) &#123; // 实现正序比较器，按照年龄排序 Comparator&lt;User&gt; comparator = (User u1, User u2) -&gt; &#123; if (u1 == u2) &#123; return 0; &#125; return u1.getAge() - u2.getAge(); &#125;; // 设置初始长度和比较器 PriorityBlockingQueue&lt;User&gt; queue = new PriorityBlockingQueue(2, comparator); // 生产者线程 new Thread(() -&gt; &#123; Random random = new Random(); for (int i = 0; i &lt; 10; i++) &#123; int num = random.nextInt(50); User user = new User(String.valueOf(num), num); queue.add(user); &#125; &#125;).start(); // 消费者线程 new Thread(() -&gt; &#123; for (int i = 0; i &lt; 10; i++) &#123; User user = null; try &#123; user = queue.take(); System.out.println(String.format(&quot;消费元素：%s&quot;, user)); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;).start(); &#125;&#125;class User &#123; public User(String name, int age) &#123; this.name = name; this.age = age; &#125; private String name; private int age; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; @Override public String toString() &#123; return &quot;User&#123;&quot; + &quot;name=&#x27;&quot; + name + &#x27;\\&#x27;&#x27; + &quot;, age=&quot; + age + &#x27;&#125;&#x27;; &#125;&#125;/** * 输出 * 消费元素：User&#123;name=&#x27;2&#x27;, age=2&#125; * 消费元素：User&#123;name=&#x27;6&#x27;, age=6&#125; * 消费元素：User&#123;name=&#x27;15&#x27;, age=15&#125; * 消费元素：User&#123;name=&#x27;17&#x27;, age=17&#125; * 消费元素：User&#123;name=&#x27;21&#x27;, age=21&#125; * 消费元素：User&#123;name=&#x27;22&#x27;, age=22&#125; * 消费元素：User&#123;name=&#x27;23&#x27;, age=23&#125; * 消费元素：User&#123;name=&#x27;32&#x27;, age=32&#125; * 消费元素：User&#123;name=&#x27;34&#x27;, age=34&#125; * 消费元素：User&#123;name=&#x27;45&#x27;, age=45&#125; */ DelayQueue API文档 具有延迟效果的单端队列，只有元素过期才能被获取 延迟队列，延迟时间到达才能释放元素 底层数据结构：PriorityQueue（优先级队列） ReentrantLock 实现线程安全 元素要实现 Delayed 接口 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108/** * delayQueue 延迟队列 * DelayQueue是一个无界的BlockingQueue，用于放置实现了Delayed接口的对象， * 其中的对象只能在其到期时才能从队列中取走。这种队列是有序的，即队头对象 * 的延迟到期时间最长。注意：不能将null元素放置到这种队列中。 */public class DelayQueueTest &#123; private static DelayQueue&lt;Order&gt; queue = new DelayQueue&lt;&gt;(); public static void main(String[] args) &#123; // 生产者线程 new Thread(() -&gt; &#123; for (int i = 0; i &lt; 10; i++) &#123; Order order = new Order(i, System.currentTimeMillis() + new Random().nextInt(1000 * 5)); queue.put(order); try &#123; TimeUnit.MILLISECONDS.sleep(300); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;).start(); // 消费者线程 new Thread(() -&gt; &#123; for (int i = 0; i &lt; 10; i++) &#123; try &#123; Order order = queue.take(); System.out.println(String.format(&quot;释放订单：%s&quot;, order)); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;).start(); &#125;&#125;class Order implements Delayed &#123; // 超时时间为20秒 public static final int TIME_OUT = 10 * 1000; private int orderId; private long payTime; public Order(int orderId, long payTime) &#123; this.orderId = orderId; this.payTime = payTime; &#125; public int getOrderId() &#123; return orderId; &#125; public void setOrderId(int orderId) &#123; this.orderId = orderId; &#125; public long getPayTime() &#123; return payTime; &#125; public void setPayTime(long payTime) &#123; this.payTime = payTime; &#125; /** * 用来判断是否到了截止时间 * * @param unit * @return */ @Override public long getDelay(TimeUnit unit) &#123; return payTime + TIME_OUT - System.currentTimeMillis(); &#125; /** * 排序,这个方法很重要,根据下单时间正序排序，保证队列头部一定是最需要释放的元素 * * @param o * @return */ @Override public int compareTo(Delayed o) &#123; Order order = (Order) o; return (int) (this.getPayTime() - order.getPayTime()); &#125; @Override public String toString() &#123; return &quot;Order&#123;&quot; + &quot;orderId=&quot; + orderId + &quot;, payTime=&quot; + payTime + &#x27;&#125;&#x27;; &#125;&#125;/** * 输出 * 释放订单：Order&#123;orderId=0, payTime=1595155223599&#125; * 释放订单：Order&#123;orderId=1, payTime=1595155223901&#125; * 释放订单：Order&#123;orderId=2, payTime=1595155224201&#125; * 释放订单：Order&#123;orderId=3, payTime=1595155224501&#125; * 释放订单：Order&#123;orderId=4, payTime=1595155224801&#125; * 释放订单：Order&#123;orderId=5, payTime=1595155225102&#125; * 释放订单：Order&#123;orderId=6, payTime=1595155225402&#125; * 释放订单：Order&#123;orderId=7, payTime=1595155225702&#125; * 释放订单：Order&#123;orderId=8, payTime=1595155226002&#125; * 释放订单：Order&#123;orderId=9, payTime=1595155226303&#125; */ SynchronousQueue API文档 具有单个元素容量的阻塞队列 SynchronousQueue 是一个特殊的队列，它的内部同时只能够容纳单个元素 底层数据结构：单向链表或栈 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960/** * SynchronousQueue 是一个特殊的队列，它的内部同时只能够容纳单个元素。 * 如果该队列已有一元素的话，试图向队列中插入一个新元素的线程将会阻塞 * 直到另一个线程将该元素从队列中抽走。同样，如果该队列为空，试图向队列 * 中抽取一个元素的线程将会阻塞，直到另一个线程向队列中插入了一条新的元素 */public class SynchronousQueueTest &#123; private static SynchronousQueue&lt;Integer&gt; queue = new SynchronousQueue&lt;&gt;(); public static void main(String[] args) &#123; // 生产队列 new Thread(() -&gt; &#123; for (int i = 0; i &lt; 10; i++) &#123; try &#123; // 用add会抛队列满异常 queue.put(i); System.out.println(String.format(&quot;生产线程生产元素：%d&quot;, i)); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;).start(); // 消费线程 new Thread(() -&gt; &#123; for (int i = 0; i &lt; 10; i++) &#123; try &#123; int num = queue.take(); System.out.println(String.format(&quot;消费线程消费元素：%d&quot;, num)); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;).start(); &#125;&#125;/** * 输出 * 生产线程生产元素：0 * 消费线程消费元素：0 * 消费线程消费元素：1 * 生产线程生产元素：1 * 生产线程生产元素：2 * 消费线程消费元素：2 * 消费线程消费元素：3 * 生产线程生产元素：3 * 生产线程生产元素：4 * 消费线程消费元素：4 * 消费线程消费元素：5 * 生产线程生产元素：5 * 消费线程消费元素：6 * 生产线程生产元素：6 * 消费线程消费元素：7 * 生产线程生产元素：7 * 消费线程消费元素：8 * 生产线程生产元素：8 * 消费线程消费元素：9 * 生产线程生产元素：9 */ LinkedTransferQueue LinkedTransferQueue采用一种预占模式。意思就是消费者线程取元素时，如果队列不为空，则直接取走数据，若队列为空，那就生成一个节点（节点元素为null）入队，然后消费者线程被等待在这个节点上，后面生产者线程入队时发现有一个元素为null的节点，生产者线程就不入队了，直接就将元素填充到该节点，并唤醒该节点等待的线程，被唤醒的消费者线程取走元素，从调用的方法返回。我们称这种节点操作为“匹配”方式 Deque LinkedBlockingDeque API文档 可以做 先进先出的队列，也可以做先进后出的栈 数据结构：双向链表 默认容量：Integer.MAX_VALUE ReentrantLock 保证线程安全 AQS API文档 在线源码 什么是AQS? AQS指的是Java中的AbstractQueuedSynchronizer类，翻译过来的意思就是抽象队列同步器，它是Java并发用来构建锁和其他同步组件的基础框架，它的核心是一个双端队列，cas算法和一个使用volatile修改的int类型的共享变量state, 当state为0时，表示可以申请锁，为1时表示其他线程获得锁 锁的模式：共享锁，排他锁 加锁过程：使用cas算法将state修改为1，并设置同步器的线程归属 未获得锁线程：被封装为一个node节点，并加入等待队列然后调用LockSupport.park挂起线程 解锁过程：将state修改为0，将同步器的线程归属置空，唤醒队列后继节点获取锁 为什么AQS是从后往前唤醒节点的 12345678910111213141516171819// 唤醒节点private void unparkSuccessor(Node node) &#123; int ws = node.waitStatus; if (ws &lt; 0) compareAndSetWaitStatus(node, ws, 0); Node s = node.next; if (s == null || s.waitStatus &gt; 0) &#123; s = null; // 丛队列尾部向前开始查找第一个需要唤醒的节点 for (Node t = tail; t != null &amp;&amp; t != node; t = t.prev) if (t.waitStatus &lt;= 0) s = t; &#125; if (s != null) LockSupport.unpark(s.thread);&#125; 1234567891011121314151617// 添加节点，node是当前节点private Node enq(final Node node) &#123; for (;;) &#123; Node t = tail; if (t == null) &#123; // Must initialize if (compareAndSetHead(new Node())) tail = head; &#125; else &#123;//将当前节点追加到队列尾部 //将新节点的前驱指向上一个节点;AQS节点从后往前遍历就是因为先赋值了prev指针，如果从前往后在高并发情况会发生next指针来不及指向而漏掉后面节点的扫描 node.prev = t; if (compareAndSetTail(t, node)) &#123;//cas设置队尾节点 t.next = node;//将上一节点的后继指向当前节点 return t; &#125; &#125; &#125;&#125; 原因是AQS在添加节点的时候，如果队列存在节点 先将插入节点的 prev 指针指向队列尾部节点 然后CAS将插入节点替换为队列的 tail 将原 tail 节点的 next 指针指向当前节点 由此可知 prev 指针比 next 指针有更高的赋值优先级，并发情况下，如果节点从前往后扫描，如果此时新增节点来没来得及对上一个节点的 next 指针赋值，AQS可能出现后继节点漏扫描情况 ReentrantLock API文档 在线源码 可重入锁：每次重入 AQS 的 state + 1 底层实现：在AQS基础上构建 锁模式 公平锁 非公平锁（默认） JUC包下很多同步机制都使用ReentrantLock实现，比如阻塞队列，CopyOnWriteArrayList等 可重入 可重入是如果当前线程获得了锁，那么在获得锁的情况下继续调用lock方法获得锁不会进行阻塞 调用了多少次lock方法就必须调用多少次unlock方法，因为每次调用lock方法时，state值都会+1，只有state = 0 才是无锁状态 公平锁 tryAcquire 尝试获取锁 公平锁调用lock方法时会调用tryAcquire方法尝试获取锁 如果state = 0，尝试获取锁 如果队列存在节点，则排队 123456789// 默认为非公平锁public ReentrantLock() &#123; sync = new NonfairSync(); &#125;// 传入 fair 获取公平或非公平锁public ReentrantLock(boolean fair) &#123; sync = fair ? new FairSync() : new NonfairSync(); &#125; 非公平锁 nonfairTryAcquire 尝试获取锁 非公平锁调用lock时会通过cas先尝试是否能获取锁，不能才调用nonfairTryAcquire方法 排队前插队，再次尝试通过cas将 state由0修改为1，尝试获取锁 获取不到则排队 用法 123456789101112private static ReentrantLock lock = new ReentrantLock();private static volatile Integer a = 0;public static void main(String[] args) &#123; for (int i = 0; i &lt; 15; i++) &#123; new Thread(() -&gt; &#123; lock.lock(); log.info(&quot;Thread = &#123;&#125;, &#123;&#125;&quot;, Thread.currentThread().getName(), a++); lock.unlock(); &#125;).start(); &#125;&#125; ReentrantLock.Condition API文档 Condition是一个多线程间协调通信的工具类，使得某个，或者某些线程一起等待某个条件（Condition）,只有当该条件具备( signal 或者 signalAll方法被带调用)时 ，这些等待线程才会被唤醒，从而重新争夺锁 作用：有条件地 唤醒线程 休眠线程 Condition.await() 让线程休眠，并自动释放Condition关联的锁 Condition.signal() 唤醒休眠线程，如果线程能够恢复，则说明当前线程获得Condition关联的锁 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108 // 锁 private static ReentrantLock lock = new ReentrantLock(); // 读条件 private static Condition read = lock.newCondition(); // 写条件 private static Condition write = lock.newCondition(); private static volatile Object[] arr = new Object[10]; // 读写索引，数组元素总量 private static int putIdx, takeIdx, count; // 写操作 public static void put(Object obj) throws InterruptedException &#123; // 获取锁 lock.lock(); try &#123; TimeUnit.MILLISECONDS.sleep(100); while (count == arr.length) &#123; // 数组满了，等待 write.await(); &#125; arr[putIdx] = obj; log.info(&quot;生产者生产了：&#123;&#125;&quot;, obj); // 如果数组写到最后一个元素后则索引复位 if (++putIdx == arr.length) &#123; putIdx = 0; &#125; count++; // 唤醒读取线程 read.signal(); &#125; finally &#123; lock.unlock(); &#125; &#125; // 读操作 public static Object take() throws InterruptedException &#123; // 获取锁 lock.lock(); try &#123; TimeUnit.MILLISECONDS.sleep(200); // 数组为空 while (count == 0) &#123; read.await(); &#125; Object result = arr[takeIdx]; // 读取到最后一个元素则复位 if (++takeIdx == arr.length) &#123; takeIdx = 0; &#125; count--; // 唤醒写线程 write.signal(); log.info(&quot;消费者消费了：&#123;&#125;&quot;, result); return result; &#125; finally &#123; lock.unlock(); &#125; &#125; public static void main(String[] args) throws InterruptedException &#123; for (int i = 0; i &lt; 2; i++) &#123; new Thread(() -&gt; &#123; try &#123; while (true) &#123; put(System.currentTimeMillis()); &#125; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;).start(); &#125; new Thread(() -&gt; &#123; try &#123; while (true) &#123; take(); &#125; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;).start(); Thread.currentThread().join(); &#125;// 01:04:49.824 [Thread-0] INFO com.macro.mall.controller.MinioController - 生产者生产了：1661706289722// 01:04:49.929 [Thread-0] INFO com.macro.mall.controller.MinioController - 生产者生产了：1661706289829// 01:04:50.029 [Thread-0] INFO com.macro.mall.controller.MinioController - 生产者生产了：1661706289929// 01:04:50.129 [Thread-0] INFO com.macro.mall.controller.MinioController - 生产者生产了：1661706290029// 01:04:50.229 [Thread-0] INFO com.macro.mall.controller.MinioController - 生产者生产了：1661706290129// 01:04:50.329 [Thread-1] INFO com.macro.mall.controller.MinioController - 生产者生产了：1661706289722// 01:04:50.429 [Thread-1] INFO com.macro.mall.controller.MinioController - 生产者生产了：1661706290329// 01:04:50.529 [Thread-1] INFO com.macro.mall.controller.MinioController - 生产者生产了：1661706290429// 01:04:50.629 [Thread-1] INFO com.macro.mall.controller.MinioController - 生产者生产了：1661706290529// 01:04:50.729 [Thread-1] INFO com.macro.mall.controller.MinioController - 生产者生产了：1661706290629// 01:04:51.029 [Thread-2] INFO com.macro.mall.controller.MinioController - 消费者消费了：1661706289722// 01:04:51.229 [Thread-2] INFO com.macro.mall.controller.MinioController - 消费者消费了：1661706289829// 01:04:51.429 [Thread-2] INFO com.macro.mall.controller.MinioController - 消费者消费了：1661706289929 ReentrantReadWriteLock ReentrantReadWriteLock API文档 ReentrantReadWriteLock.ReadLock API文档 ReentrantReadWriteLock.WriteLock API文档 适合读多写少场景 读写锁底层有两把锁，ReadLock和WriteLock。读写锁之间是互斥的 123456789101112131415161718192021222324public class Counter &#123; private final ReadWriteLock rwlock = new ReentrantReadWriteLock(); private final Lock rlock = rwlock.readLock(); private final Lock wlock = rwlock.writeLock(); private int[] counts = new int[10]; public void inc(int index) &#123; wlock.lock(); // 加写锁 try &#123; counts[index] += 1; &#125; finally &#123; wlock.unlock(); // 释放写锁 &#125; &#125; public int[] get() &#123; rlock.lock(); // 加读锁 try &#123; return Arrays.copyOf(counts, counts.length); &#125; finally &#123; rlock.unlock(); // 释放读锁 &#125; &#125;&#125; 共享锁（读锁） 多个线程可以获得同一把锁，不阻塞，但是数据不能修改 (不能写是代码上规范的，共享锁不是真的不让写数据) 排他锁（写锁） 多只能一个线程获得锁，其他线程阻塞 读写互斥 当获得共享锁时，需要获得排他锁就必须等待共享锁释放，反之亦然 底层实现 ReentrantReadWriteLock 还是基于 AQS 实现的，还是对state进行操作，拿到锁资源就去干活，如果没有拿到，依然去AQS队列中排队 将 AQS 中 int (32位)类型的 state 变量标记为读锁或写锁 读锁操作：基于state的高16位进行操作 写锁操作：基于state的低16为进行操作 ReentrantReadWriteLock依然是可重入锁 写锁重入：和 ReentrantLock一致，依然是对 state 进行 +1 操作，只不过范围变小了（低16位） 读锁重入：因为读锁是共享锁。读锁在获取锁资源操作时，是要对state的高16位进行 + 1操作。因为读锁是共享锁，所以同一时间会有多个读线程持有读锁资源。这样一来，多个读操作在持有读锁时，无法确认每个线程读锁重入的次数（多个线程操作 state）。为了去记录读锁重入的次数，每个读操作的线程，都会有一个 ThreadLocal 记录锁重入的次数，当 ThreadLocal 的重入次数为0则释放读锁 写锁的饥饿问题：读锁是共享锁，当有线程持有读锁资源时，再来一个线程想要获取读锁，直接对state修改即可。在读锁资源先被占用后，来了一个写锁资源，然后，大量的需要获取读锁的线程来请求锁资源，如果可以绕过写锁，直接拿资源，会造成写锁长时间无法获取到写锁资源 读锁在拿到锁资源后，如果再有读线程需要获取读锁资源，需要去AQS队列排队。如果队列的前面需要写锁资源的线程，那么后续读线程是无法拿到锁资源的。持有读锁的线程，只会让写锁线程之前的读线程拿到锁资源 StampedLock 参考 廖雪峰文章 StampedLock是对读写锁的一种优化，先通过获取乐观锁后获取数据，然后再进行版本号验证。如果版本号验证通过则说明获取的数据是安全的，否则申请悲观锁重新获取数据，是乐观锁和悲观锁结合的解决方案，能够有效提高并发（不支持锁的重入）。非常适合在读多写少的场景下使用。底层使用Unsafe的内存屏障保证数据读写的一致性 StampedLock和ReadWriteLock相比，改进之处在于：读的过程中也允许获取写锁后写入！这样一来，我们读的数据就可能不一致，所以，需要一点额外的代码来判断读的过程中是否有写入，这种读锁是一种乐观锁 先获取乐观锁，在读写冲突时，读取数据先获取乐观锁，然后再通过乐观锁版本号判断读取的数据是否最新版本 版本比较一致，说明当前数据为最新版本，直接返回数据 版本比较不一致，说明数据被修改，获取悲观锁后重新获取最新版本数据 多线程 并发三大特性 原子性 JMM规定所有变量都会存储在主内存中，在操作的时候，需要从主内存中复制一份到线程内存(CPU内存)，在线程内部做计算。然后再写回主内存中 定义：原子性指一个操作是不可分割的，不可中断的，一个线程在执行时，另一个线程不会影响到他 synchronized 排它锁，某一时刻只有一个线程能获得锁 CAS Compare And Swap也就是比较和交换，他是一条CPU的并发原语 Lock 底层是CAS + AQS排队 JUC 原子类，比如AtomicInteger 可见性 MESI协议 可见性问题是基于CPU位置出现的，CPU处理速度非常快，相对CPU来说，去主内存获取数据这个事情太慢了，CPU就提供了L1，L2，L3的三级缓存，每次去主内存拿完数据后，就会存储到CPU的三级缓存，每次去三级缓存拿数据，效率肯定会提升 这就带来了问题，现在CPU都是多核，每个线程的工作内存(CPU三级缓存)都是独立的会告知每个线程中做修改时，只改自己的工作内存，没有及时的同步到主内存，导致数据不一致问题 volatile 读屏障：将对应的CPU缓存置为无效，强制去主内存读取共享变量 写屏障：将写入缓存中的数据更新写入主内存，让其他线程可见 synchronized 获取锁：将内部（同步块、方法）涉及的变量重新去主内存中获取 释放锁：立即将缓存同步到主内存 final 常量不允许修改，因此可以保证可见性 有序性 指令重排序 volatile 内存屏障，内存屏障是一条CPU指令 通过插入内存屏障防止内存屏障前后发生指令重排序 线程 线程状态 新建（new）：当线程对象被创建时，线程处于新建状态。此时，该线程还没有启动，也没有分配系统资源 就绪（Runnable)：当线程调用start()方法后，线程进入就绪状态。此时，线程已经分配到了系统资源，但还没有开始执行。在就绪状态下，线程可能会等待其他线程的执行 运行（Running）：当线程获取到CPU资源后，开始执行run()方法，线程处于运行状态。线程会不断地执行run()方法中的代码，直到线程被阻塞或者执行完毕 阻塞（Blocked）：线程在某些情况下可能会被阻塞，例如等待某个资源的释放（synchronized）或者线程调用了sleep()方法暂停执行。在阻塞状态下，线程会暂时停止执行，直到满足某个条件后进入就绪状态 等待（Waiting）：线程在等待某个特定条件发生时，会进入等待状态。例如，线程调用了wait()方法，线程会一直等待直到其他线程调用了notify()或者notifyAll()方法唤醒它们 超时等待（Timed Waiting）：在等待状态中，线程可以设置等待的超时时间。一旦超过时间限制，线程会自动唤醒并进入就绪状态。例如，线程调用了sleep()方法，线程会在指定的时间内暂停执行 终止状态（TERMINATED）：线程任务执行完毕，线程被终止 API文档 创建线程的几种方式 继承Thread类 , 重写run方法 123456789101112static class MyThread extends Thread &#123; @Override public void run() &#123; System.out.println(&quot;myThread&quot;); &#125;&#125;public static void main(String[] args) throws InterruptedException &#123; MyThread myThread = new MyThread(); myThread.start(); myThread.join();&#125; 实现Runnable接口，实现run方法 1234567891011public static void main(String[] args) throws InterruptedException &#123; Thread thread = new Thread(new Runnable() &#123; @Override public void run() &#123; System.out.println(&quot;myThread&quot;); &#125; &#125;); thread.start(); thread.join();&#125; Future / Callable 配合线程池方式 123456789101112131415161718192021222324252627282930static class MyCallable implements Callable&lt;Integer&gt; &#123; @Override public Integer call() throws Exception &#123; int a = 0; for (int i = 0; i &lt; 10; i++) &#123; TimeUnit.MILLISECONDS.sleep(100); a ++; &#125; return a; &#125; &#125; public static void main(String[] args) throws ExecutionException, InterruptedException &#123; ExecutorService executorService = Executors.newFixedThreadPool(2); MyCallable myCallable = new MyCallable(); MyCallable myCallable2 = new MyCallable(); Future&lt;Integer&gt; submit = executorService.submit(myCallable); Future&lt;Integer&gt; submit2 = executorService.submit(myCallable2); // 阻塞等待结果 Integer integer = submit.get(); Integer integer2 = submit2.get(); System.out.println(integer + integer2); // 20 // 关闭线程池 executorService.shutdown(); &#125; 通过线程池创建 123456789101112public static void main(String[] args) throws ExecutionException, InterruptedException &#123; ExecutorService executorService = Executors.newFixedThreadPool(2); executorService.execute(() -&gt; &#123; for (int i = 0; i &lt; 5; i++) &#123; System.out.println(i); &#125; &#125;); // 关闭线程池 executorService.shutdown();&#125; 构造函数 父子关系 线程的创建是由另一个线程完成的 被创建线程的父线程是创建它的线程 ThreadGroup 构建一个线程如果没有指定ThreadGroup，则它会用父线程的ThreadGroup Runnable Thread负责线程相关职责和控制 Runnable负责执行逻辑单元 stackSize -Xss 线程的堆栈大小，如果没有设置则为0 设置了更小的堆栈大小，JVM能支持同时存活更多的线程，如果设置大了单个线程拥有更大的递归深度，但是同时存活的线程更少了 守护线程 守护线程有自动退出特性，当JVM没有非守护线程运行时，守护线程会自动退出 设置线程为守护线程 Thread.setDaemon(boolean)，必须在 start()之前设置 守护线程不能持有任何需要关闭的资源，例如打开文件等，因为虚拟机退出时，守护线程没有任何机会来关闭文件，这会导致数据丢失 123456789101112131415161718192021 public static void main(String[] args) throws InterruptedException &#123; Thread thread = new Thread(()-&gt; &#123; try &#123; for (;;) &#123; TimeUnit.MILLISECONDS.sleep(500); System.out.println(1); &#125; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;); thread.setDaemon(true); thread.start(); TimeUnit.SECONDS.sleep(2); &#125;//输出1111 1234public Thread(ThreadGroup group, Runnable target, String name, long stackSize) &#123; init(group, target, name, stackSize); &#125; sleep 线程休眠 sleep不会释放锁，阻塞线程 睡眠的时间不是一个准确时间，和cpu任务调度有关 可以使用TimeUnit.SECONDS.sleep(3);更优雅睡眠线程 一个线程调用sleep,另一个线程调用 interrupt 能捕获中断信号 yield 让出CPU执行时间片 yield 不会释放对象锁 让出cpu资源（不一定会让），让同等优先级的线程拥有可执行机会 让执行中的线程返回就绪状态，和同等优先级线程竞争执行机会 另外线程调用interrupt不能捕获到中断信号 suspend / resume (已废弃) suspend 线程会被挂起，但不会释放锁 不推荐使用，在同步块或同步方法等调用suspend 会造成死锁 resume 恢复suspend挂起的线程 死锁场景 1234567891011121314151617181920212223242526272829303132333435private static String message = null; private static int i=0; public static void main(String[] args) &#123; Thread consumer = new Thread(() -&gt; &#123; while (true) &#123; synchronized (Object.class) &#123; while (message == null) &#123; System.out.println(&quot;等待接受消息&quot;); Thread.currentThread().suspend(); // 这里消费者挂起线程没有释放锁 &#125; System.out.println(&quot;接受消息 =&gt; &quot; + message); message = null; &#125; &#125; &#125;); consumer.start(); Thread producer = new Thread(() -&gt; &#123; while (true) &#123; synchronized (Object.class) &#123; // 这里死锁，一直获取不到锁 try &#123; Thread.sleep(100); message = &quot;Hello , this is &quot; + i++; consumer.resume(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125;); producer.start(); &#125; // 输出 等待接受消息 stop 暴力停止，不推荐使用 一个线程几乎可以在任何地方抛出一个ThreadDeath异常，导致逻辑执行不完整 释放该线程所持有的所有的锁 join 串行执行 当一个线程调用另一个线程的join()方法时，调用线程会被阻塞，直到被调用线程执行完毕或者超时。这意味着join()方法可以用来实现线程的串行执行 被调用join()线程执行完毕，当前线程再继续往下执行 12345678910111213141516171819202122232425262728public class Test &#123; public static void main(String[] args) throws InterruptedException &#123; Thread thread = new Thread(() -&gt; &#123; System.out.println(&quot;等待执行完毕后，主线程退出&quot;); try &#123; for (int i = 0; i &lt; 5; i++) &#123; System.out.println(i); TimeUnit.MILLISECONDS.sleep(100); &#125; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;); thread.start(); // 等待调用join 方法的线程执行完后再执行 thread.join(); System.out.println(&quot;主线程继续执行&quot;); &#125;&#125;等待执行完毕后，主线程退出01234主线程继续执行 interrupt 用于打断阻塞线程，打断调用wait、sleep、join的线程 调用interrupt 方法后，在该线程的run方法会接收到 InterruptedException 异常（信号） 在线程内部可用 interrupted 判断当前线程是否被中断 12345678910111213141516171819202122232425public static void main(String[] args) throws InterruptedException &#123; Thread thread = new Thread(() -&gt; &#123; try &#123; // 休眠60秒 TimeUnit.SECONDS.sleep(60); &#125; catch (InterruptedException e) &#123; // 外部调用interrupt，线程体内部会触发InterruptedException e.printStackTrace(); &#125; int index = 0; while (true) &#123; try &#123; System.out.println(index++); &#125; catch (InterruptedException e) &#123; System.out.println(&quot;线程被中断&quot;); &#125; &#125; &#125;); thread.start(); TimeUnit.SECONDS.sleep(3); // 线程提内部会触发 InterruptedException thread.interrupt(); &#125; 实际上线程休眠3秒后就被主线程打断继续执行 interrupted interrupt具有清除状态的功能，线程中断后，连续两次调用，第二次会返回false setPriority 取值范围 1~10 设置线程优先级不一定会让线程拥有更多的执行机会 默认线程优先级都是5，因为Main线程的优先级就是5，其他线程优先级都是派生mail线程 线程的优先级设置可以理解为线程抢占CPU时间片的概率，虽然概率比较大，但是它不一定就是按照优先级的顺序去抢占CPU时间片的，具体的执行顺序还是要根据谁先抢到了CPU的时间片，谁就先来执行 12345public static void main(String[] args) &#123; Thread thread = new Thread(() -&gt; &#123;&#125;); thread.setPriority(8); thread.start();&#125; suspend和wait的区别 wait（Object方法） 暂停当前线程执行并释放锁，进入对象等待池(被监视对象的对象等待池)，调用wait方法必须获取对象锁，否则抛出IllegalMonitorStateException异常 调用wait 必须是获得对象锁的情况下才能调用，只有在获取该对象的锁才能使用 notify（Object方法） 调用notify会获取对象锁，只有在synchronized方法或块中才能调用，调用notify方法必须获取对象锁，否则抛出IllegalMonitorStateException异常 在对象（被监视对象）等待池中随机移走一个线程并放到锁标志等待池 notify all notify all 被唤醒的线程会再次去竞争对象锁，必须在synchronized方法或数据块中调用 从对象（被监视对象）等待池中移走所有等待那个对象的线程并放到锁标志等待池中 死锁 1234567891011121314151617public static void main(String[] args) throws InterruptedException &#123; Thread thread = new Thread(() -&gt; &#123; for (;;) &#123; try &#123; TimeUnit.SECONDS.sleep(1); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(1); &#125; &#125;); thread.start(); thread.wait(); //抛出 java.lang.IllegalMonitorStateException，需要获得对象锁才能调用 TimeUnit.SECONDS.sleep(3); thread.notify(); TimeUnit.SECONDS.sleep(3); &#125; 正确用法 1234567891011121314151617181920212223242526272829303132public static void main(String[] args) throws InterruptedException &#123; final Object object = new Object(); Thread t1 = new Thread() &#123; public void run() &#123; synchronized (object) &#123; try &#123; System.out.println(&quot;使用wait必须获取对象锁，因为要释放对象锁&quot;); object.wait(); System.out.println(&quot;继续执行&quot;); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125;; Thread t2 = new Thread() &#123; public void run() &#123; synchronized (object) &#123; System.out.println(&quot;获取对象锁 另外的线程调用 notify 唤醒线程&quot;); object.notify(); &#125; &#125; &#125;; t1.start(); t2.start(); &#125;输出使用wait必须获取对象锁，因为要释放对象锁获取对象锁 另外的线程调用 notify 唤醒线程继续执行 区别 supend和resume是Thread的方法，wait和notify是Object的方法 supend不释放对象锁，容易造成死锁，wait和notify必须要获取对象锁才能调用，并且会主动释放对象锁，不会造死锁 正确关闭线程 线程结束生命周期正常结束 等待线程体执行完任务，正常退出 捕获中断信号结束线程 12345678910111213141516171819202122public static void main(String[] args) throws InterruptedException &#123; Thread thread = new Thread(() -&gt; &#123; try &#123; // 接收线程中断信号 while (!Thread.interrupted()) &#123; TimeUnit.SECONDS.sleep(1); System.out.println(Thread.currentThread().getName()); &#125; &#125; catch (InterruptedException e) &#123; // 必须在循环外捕获 InterruptedException System.out.println(&quot;线程中断&quot;); &#125; &#125;); thread.start(); TimeUnit.SECONDS.sleep(3); // 中断线程 thread.interrupt(); &#125; 使用 volatile 开关控制 使用 volatile修饰是要保证多线程环境下的可见性 由于interrupted有清除状态的功能，可以使用一个flag来控制线程结束 1234567891011121314151617181920212223242526272829303132public class Test &#123; public static void main(String[] args) throws InterruptedException &#123; TestThread thread = new TestThread(); thread.start(); TimeUnit.SECONDS.sleep(10); thread.closedThread(); &#125;&#125;class TestThread extends Thread &#123; private volatile boolean closed = false; @Override public void run() &#123; while (!closed) &#123; try &#123; TimeUnit.SECONDS.sleep(1); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(this.getName()); &#125; &#125; public void closedThread() &#123; this.closed = true; &#125;&#125; 线程通信 单线程间通信 join / yield 单线程通信可以使用 synchronized 、wait、notify 完成线程间通信 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115public class Test &#123; /** * 模拟异步非阻塞事件处理 * @param args * @throws InterruptedException */ public static void main(String[] args) throws InterruptedException &#123; EventQuery eventQuery = new EventQuery(); new Thread(() -&gt; &#123; for (int i=0; i&lt;100; i++) &#123; eventQuery.offer(new EventQuery.Event(i)); &#125; &#125;).start(); new Thread(() -&gt; &#123; for (int i=0; i&lt;100; i++) &#123; try &#123; TimeUnit.MILLISECONDS.sleep(20); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; eventQuery.take(); &#125; &#125;).start(); &#125;&#125;/** * 事件队列 */class EventQuery &#123; private final static int MAX_NUM = 10; private int maxSize; private final LinkedList&lt;Event&gt; taskQuery = new LinkedList&lt;&gt;(); static class Event &#123; private int eventId; public Event(int eventId) &#123; this.eventId = eventId; &#125; @Override public String toString() &#123; return &quot;Event&#123;&quot; + &quot;eventId=&quot; + eventId + &#x27;&#125;&#x27;; &#125; &#125; public EventQuery() &#123; this(MAX_NUM); &#125; public EventQuery(int maxSize) &#123; this.maxSize = maxSize; &#125; /** * 提交事件 * offer 单线程通信 * @param event */ public void offer(Event event) &#123; synchronized (this.taskQuery) &#123; if (this.taskQuery.size() &gt;= this.maxSize) &#123; System.out.println(&quot;this query full&quot;); try &#123; this.taskQuery.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; System.out.println(&quot;this event commit &quot; + event); this.taskQuery.add(event); // 这一步很重要，实现自旋 this.taskQuery.notify(); &#125; &#125; /** * 消费事件 * take 单线程通信 * @return */ public Event take() &#123; Event event = null; synchronized (this.taskQuery) &#123; if (CollectionUtils.isEmpty(this.taskQuery)) &#123; System.out.println(&quot;this query is empty&quot;); try &#123; this.taskQuery.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; // 多线程下，可能会执行空的removeFirst,原因是eventQuery 两个线程同时 // wait,另外一个线程执行offer后顺利执行 take,然后又唤起了wait的take线程 event = this.taskQuery.removeFirst(); System.out.println(&quot;this event take &quot; + event); // 这一步很重要，实现自旋 this.taskQuery.notify(); &#125; return event; &#125;&#125; 多线程间通信 synchronized 、wait、notify、notifyAll ReentrantLock + Condition CountDownLatch 计数器 CyclicBarrier 同步屏障 Semaphore 信号量 join / yield BlockingQueue … 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156public class Test &#123; /** * 模拟异步非阻塞事件处理 * * @param args * @throws InterruptedException */ public static void main(String[] args) throws InterruptedException &#123; EventQuery eventQuery = new EventQuery(); new Thread(() -&gt; &#123; for (int i = 0; i &lt; 100; i++) &#123; eventQuery.offer(new EventQuery.Event(i)); &#125; &#125;).start(); new Thread(() -&gt; &#123; for (int i = 100; i &lt; 200; i++) &#123; eventQuery.offer(new EventQuery.Event(i)); &#125; &#125;).start(); new Thread(() -&gt; &#123; for (int i = 200; i &lt; 300; i++) &#123; eventQuery.offer(new EventQuery.Event(i)); &#125; &#125;).start(); new Thread(() -&gt; &#123; for (int i = 0; i &lt; 100; i++) &#123; try &#123; TimeUnit.MILLISECONDS.sleep(20); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; eventQuery.take(); &#125; &#125;).start(); new Thread(() -&gt; &#123; for (int i = 100; i &lt; 200; i++) &#123; try &#123; TimeUnit.MILLISECONDS.sleep(20); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; eventQuery.take(); &#125; &#125;).start(); new Thread(() -&gt; &#123; for (int i = 200; i &lt; 300; i++) &#123; try &#123; TimeUnit.MILLISECONDS.sleep(20); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; eventQuery.take(); &#125; &#125;).start(); &#125;&#125;/** * 事件队列 */class EventQuery &#123; private final static int MAX_NUM = 10; private int maxSize; private final LinkedList&lt;Event&gt; taskQuery = new LinkedList&lt;&gt;(); static class Event &#123; private int eventId; public Event(int eventId) &#123; this.eventId = eventId; &#125; @Override public String toString() &#123; return &quot;Event&#123;&quot; + &quot;eventId=&quot; + eventId + &#x27;&#125;&#x27;; &#125; &#125; public EventQuery() &#123; this(MAX_NUM); &#125; public EventQuery(int maxSize) &#123; this.maxSize = maxSize; &#125; /** * 提交事件 * offer 单线程通信 * * @param event */ public void offer(Event event) &#123; synchronized (this.taskQuery) &#123; // 多线程的场景下，线程每次被重新唤醒必须重新校验一次 while (this.taskQuery.size() &gt;= this.maxSize) &#123; System.out.println(&quot;this query full&quot;); try &#123; this.taskQuery.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; System.out.println(&quot;this event commit &quot; + event); this.taskQuery.add(event); // 唤醒所有线程进行锁竞争 this.taskQuery.notifyAll(); &#125; &#125; /** * 消费事件 * take 单线程通信 * * @return */ public Event take() &#123; Event event = null; synchronized (this.taskQuery) &#123; // 多线程的场景下，线程每次被重新唤醒必须重新校验一次 while (CollectionUtils.isEmpty(this.taskQuery)) &#123; System.out.println(&quot;this query is empty&quot;); try &#123; this.taskQuery.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; // 多线程下，可能会执行空的removeFirst,原因是eventQuery 两个线程同时 // wait,另外一个线程执行offer后顺利执行 take,然后又唤起了wait的take线程 event = this.taskQuery.removeFirst(); System.out.println(&quot;this event take &quot; + event); // 这一步很重要，实现自旋 this.taskQuery.notifyAll(); &#125; return event; &#125;&#125; CountDownLatch 1234567891011121314151617181920212223242526272829/** * 多线程计数器 */public class CountDownLatchTest &#123; private static int num = 0; public static void main(String[] args) throws InterruptedException &#123; CountDownLatch latch = new CountDownLatch(10); for (int i = 0; i &lt; 10; i++) &#123; new Thread(() -&gt; &#123; for (int j = 0; j &lt; 1000; j++) &#123; synchronized (CountDownLatchTest.class) &#123; num++; &#125; &#125; // 线程完成计数器减1 latch.countDown(); &#125;).start(); &#125; // 阻塞去监听计数器的值是否为0，是则往下执行 latch.await(); System.out.println(&quot;所有子线程执行完毕&quot;); System.out.println(num); &#125;&#125; 底层实现 底层使用 AQS 计数器的数量其实就是 AQS 的 state 值 当计数器为0（state = 0），调用await()的线程被唤醒 CyclicBarrier 主要用于阻塞子线程，让多个子线程在同一时刻开始执行 12345678910111213141516171819202122232425262728293031323334353637383940414243444546public static void main(String[] args) throws InterruptedException &#123; CyclicBarrier cyclicBarrier = new CyclicBarrier(3); new Thread(() -&gt; &#123; System.out.println(&quot;员工进入会议室&quot;); try &#123; cyclicBarrier.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; catch (BrokenBarrierException e) &#123; e.printStackTrace(); &#125; System.out.println(&quot;会议开始&quot;); &#125;).start(); new Thread(() -&gt; &#123; System.out.println(&quot;组长进入会议室&quot;); try &#123; cyclicBarrier.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; catch (BrokenBarrierException e) &#123; e.printStackTrace(); &#125; System.out.println(&quot;会议开始&quot;); &#125;).start(); new Thread(() -&gt; &#123; System.out.println(&quot;CEO进入会议室&quot;); try &#123; cyclicBarrier.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; catch (BrokenBarrierException e) &#123; e.printStackTrace(); &#125; System.out.println(&quot;会议开始&quot;); &#125;).start();// 员工进入会议室 // 组长进入会议室// CEO进入会议室// 会议开始// 会议开始// 会议开始 &#125; 底层实现 123 private final ReentrantLock lock = new ReentrantLock(); private final Condition trip = lock.newCondition();private int count; 底层使用 ReentrantLock 和 Condition 实现 线程都会调用 Condition 的 await 进行阻塞 当 count 被线程减到0会调用 Condition 的 signalAll() 唤醒所有阻塞线程 Semaphore 互联网项目不能使用 Semaphore 限流，单机版使用 Guava RateLimiter Semaphore也叫信号量，在JDK1.5被引入，可以用来控制同时访问特定资源的线程数量，通过协调各个线程，以保证合理的使用资源 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465package com.example.luckdraw.test;import java.util.concurrent.CountDownLatch;import java.util.concurrent.Semaphore;/** * 信号量有限的资源共享 */public class SemaphoreTest &#123; /** * 每次只有2个线程有许可证获取共享资源 */ private static Semaphore semaphore = new Semaphore(2); private static CountDownLatch latch = new CountDownLatch(10); private static int num; public static void main(String[] args) throws InterruptedException &#123; for (int i = 0; i &lt; 10; i++) &#123; new Thread(() -&gt; &#123; try &#123; // 获取凭证 semaphore.acquire(); System.out.println(String.format(&quot;%s 线程获取资源&quot;, Thread.currentThread().getName())); num++; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; // 释放凭证 System.out.println(String.format(&quot;%s 线程释放资源&quot;, Thread.currentThread().getName())); semaphore.release(); latch.countDown(); &#125; &#125;).start(); &#125; latch.await(); System.out.println(num); &#125;&#125;/** * 输出 * Thread-2 线程获取资源 * Thread-0 线程获取资源 * Thread-2 线程释放资源 * Thread-0 线程释放资源 * Thread-6 线程获取资源 * Thread-1 线程获取资源 * Thread-6 线程释放资源 * Thread-1 线程释放资源 * Thread-3 线程获取资源 * Thread-4 线程获取资源 * Thread-4 线程释放资源 * Thread-3 线程释放资源 * Thread-5 线程获取资源 * Thread-7 线程获取资源 * Thread-5 线程释放资源 * Thread-7 线程释放资源 * Thread-8 线程获取资源 * Thread-9 线程获取资源 * Thread-8 线程释放资源 * Thread-9 线程释放资源 * 10 * 底层实现 AQS 的 state 进行控制 当线程获得凭证后，state 会进行 -1 操作 当 state = 0 时，不允许线程申请凭证（阻塞） 当线程释放凭证时，state 会进行 +1 操作 Hook线程（钩子） 不建议在线程池中使用异常回调 用于处理线程异常回调和优雅停机 线程异常回调UncaughtExceptionHandler JVM退出监控 Hook线程 UncaughtExceptionHandler 线程异常回调需要实现的接口，它是一个函数式接口 1234567891011121314public static void main(String[] args) throws InterruptedException &#123; Thread thread = new Thread(() -&gt; &#123; int a = 10 / 0; &#125;); // 对单独线程设置异常回调 thread.setUncaughtExceptionHandler((Thread t, Throwable e) -&gt; &#123; System.out.println(t.getName()); System.out.println(e.getMessage()); &#125;); thread.start();&#125; DefaultUncaughtExceptionHandler 设置线程全局异常回调 123456789101112public static void main(String[] args) &#123; // 注入全局线程设置异常接口回调 Thread.setDefaultUncaughtExceptionHandler((Thread t, Throwable e) -&gt; &#123; System.out.println(t.getName()); System.out.println(e.getMessage()); &#125;); new Thread(() -&gt; &#123; int a = 10 / 0; &#125;).start(); &#125; 注入钩子线程[JVM] 实现优雅停机 JVM之所以运行是因为存在活跃的非守护进程，当JVM的所有非守护进程结束，程序退出，这时候就会触发Hook线程 123456789101112131415161718192021222324public static void main(String[] args) &#123; // 注入Hook线程 Runtime.getRuntime().addShutdownHook(new Thread(() -&gt; &#123; try &#123; TimeUnit.SECONDS.sleep(1); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(&quot;The Hook Thread 1 is Run&quot;); &#125;)); // Hook线程可以注入多个 Runtime.getRuntime().addShutdownHook(new Thread(() -&gt; &#123; try &#123; TimeUnit.SECONDS.sleep(2); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(&quot;The Hook Thread 2 is Run&quot;); &#125;)); &#125; ThreadGroup钩子 ​ 如果没有在线程注入异常回调接口，线程异常又会如何处理？ getUncaughtExceptionHandler 方法先会判断自身是否存在回调接口 若线程本身不存在则获取所在ThreadGroup的UncaughtExceptionHandler 若该ThreadGroup有parent则获取parent的UncaughtExceptionHandler 一直向上递归，若没有在ThreadGroup找到回调就找Thread的DefaultUncaughtExceptionHandle，若没有就输出异常堆栈 Thread 获取回调接口源码 1234public UncaughtExceptionHandler getUncaughtExceptionHandler() &#123; return uncaughtExceptionHandler != null ? uncaughtExceptionHandler : group; &#125; ThreadGroup获取回调接口源码 123456789101112131415public void uncaughtException(Thread t, Throwable e) &#123; if (parent != null) &#123; parent.uncaughtException(t, e); &#125; else &#123; Thread.UncaughtExceptionHandler ueh = Thread.getDefaultUncaughtExceptionHandler(); if (ueh != null) &#123; ueh.uncaughtException(t, e); &#125; else if (!(e instanceof ThreadDeath)) &#123; System.err.print(&quot;Exception in thread \\&quot;&quot; + t.getName() + &quot;\\&quot; &quot;); e.printStackTrace(System.err); &#125; &#125; &#125; 守护线程 守护线程，专门用于服务其他的线程，守护线程有自动退出特性，当JVM没有非守护线程运行时，守护线程会自动退出。比如垃圾回收线程，就是最典型的守护线程 123456789101112131415161718192021222324public static void main(String[] args) throws InterruptedException &#123; Thread thread = new Thread(() -&gt; &#123; while (true) &#123; try &#123; TimeUnit.MILLISECONDS.sleep(300); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(&quot;守护线程&quot;); &#125; &#125;); // 设置为守护线程 thread.setDaemon(true); thread.start(); // 主线程休眠1秒 TimeUnit.SECONDS.sleep(1); &#125;// 输出守护线程守护线程守护线程 线程池 ​ 为什么使用线程池 减少系统维护线程的开销 解耦，运行和创建分开 线程可复用 ThreadPoolExecutor 源码 1234567public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) 核心参数说明 参数名 作用 corePoolSize 核心线程池大小 maximumPoolSize 最大线程池大小 keepAliveTime 非核心线程最大存活时间 TimeUnit 非核心线程最大存活时间单位 workQueue 阻塞任务队列 threadFactory(可选) 新建线程工厂 RejectedExecutionHandler（可选） 当提交任务数超过maxmumPoolSize+workQueue之和时，任务会交给RejectedExecutionHandler来处理 拒绝策略 ​ 当提交的任务数大于corePoolSize时，会优先放到队列缓冲区，只有填满了缓冲区后，才会判断当前运行的任务是否大于maxPoolSize，小于时会新建线程处理，大于时就触发了拒绝策略 拒绝策略 说明 ThreadPoolExecutor.AbortPolicy (默认拒绝策略) 丢弃任务并抛出RejectedExecutionException异常 ThreadPoolExecutor.DiscardPolicy 也是丢弃任务，但是不抛出异常 ThreadPoolExecutor.DiscardOldestPolicy 丢弃队列最前面的任务，然后重新尝试执行任务（重复此过程） ThreadPoolExecutor.CallerRunsPolicy 由调用线程处理该任务 重点讲解 当线程池小于corePoolSize时，新提交任务将创建一个新线程执行任务，即使此时线程池中存在空闲线程。 当线程池达到corePoolSize时，新提交任务将被放入workQueue中，等待线程池中任务调度执行 当workQueue已满，且maximumPoolSize&gt;corePoolSize时，新提交任务会创建新线程执行任务 当提交任务数超过maximumPoolSize时，新提交任务由RejectedExecutionHandler处理 当线程池中超过corePoolSize线程，空闲时间达到keepAliveTime时，关闭空闲线程 非核心线程创建时机 ​ 当每个核心线程数都在执行时，等待队列已满，就会启用非核心线程处理堆积的任务 任务队列没满时就只有核心线程在执行 123456789101112131415161718 public static void main(String[] args) throws InterruptedException &#123; ThreadPoolExecutor pool = new ThreadPoolExecutor(1, 10, 100, TimeUnit.SECONDS, new LinkedBlockingQueue&lt;&gt;()); for (int i = 0; i &lt; 10; i++) &#123; pool.execute(() -&gt; &#123;System.out.println(Thread.currentThread().getName());&#125;); &#125;// pool-1-thread-1// pool-1-thread-1// pool-1-thread-1// pool-1-thread-1// pool-1-thread-1// pool-1-thread-1// pool-1-thread-1// pool-1-thread-1// pool-1-thread-1// pool-1-thread-1 &#125; 任务队列满了就创建额外线程执行 12345678910111213141516171819 public static void main(String[] args) throws InterruptedException &#123; ThreadPoolExecutor pool = new ThreadPoolExecutor(1, 3, 100, TimeUnit.SECONDS, new LinkedBlockingQueue&lt;&gt;(2) , Executors.defaultThreadFactory(), new ThreadPoolExecutor.CallerRunsPolicy()); for (int i = 0; i &lt; 10; i++) &#123; pool.execute(() -&gt; &#123;System.out.println(Thread.currentThread().getName());&#125;); &#125;// main// pool-1-thread-3// pool-1-thread-1// pool-1-thread-2// pool-1-thread-1// pool-1-thread-3// main// main// pool-1-thread-2// pool-1-thread-1 &#125; 线程池状态 1234567891011121314public class ThreadPoolExecutor extends AbstractExecutorService &#123; private final AtomicInteger ctl = new AtomicInteger(ctlOf(RUNNING, 0)); private static final int COUNT_BITS = Integer.SIZE - 3; private static final int CAPACITY = (1 &lt;&lt; COUNT_BITS) - 1; // 线程状态 // runState is stored in the high-order bits private static final int RUNNING = -1 &lt;&lt; COUNT_BITS; private static final int SHUTDOWN = 0 &lt;&lt; COUNT_BITS; private static final int STOP = 1 &lt;&lt; COUNT_BITS; private static final int TIDYING = 2 &lt;&lt; COUNT_BITS; private static final int TERMINATED = 3 &lt;&lt; COUNT_BITS; ... ctl：一个Integr占4个字节，也就是32位，前三位用来表示线程池的状态，后29位用来表示线程池的工作线程数 RUNNING 状态说明：线程池处在RUNNING状态时，能够接收新任务，以及对已添加的任务进行处理 状态切换：线程池的初始化状态是RUNNING。线程池被一旦被创建，就处于RUNNING状态，并且线程池中的任务数为0 SHUTDOWN 状态说明：线程池处在SHUTDOWN状态时，不接收新任务，但能处理已添加的任务 状态切换：调用线程池的 shutdown() 方法时，线程池由RUNNING -&gt; SHUTDOWN STOP 状态说明：线程池处在STOP状态时，不接收新任务，不处理已添加的任务，并且会中断正在处理的任务 状态切换：调用线程池的 shutdownNow() 方法时，线程池由(RUNNING or SHUTDOWN ) -&gt; STOP TIDYING 状态说明：当所有的任务已终止，ctl记录的工作线程数为0，线程池会变为TIDYING状态。当线程池变为TIDYING状态时，会执行钩子函数 terminated()。terminated()在ThreadPoolExecutor类中是空的，若用户想在线程池变为TIDYING时，进行相应的处理；可以通过重载terminated()函数来实现 状态切换：当线程池在SHUTDOWN状态下，阻塞队列为空并且线程池工作线程也为空，就会由 SHUTDOWN -&gt; TIDYING，当线程池在STOP状态下，线程池中执行的任务为空时，就会由STOP -&gt; TIDYING TERMINATED 状态说明：线程池彻底终止，就变成TERMINATED状态 状态切换：线程池处在TIDYING状态时，执行完terminated()之后，就会由 TIDYING -&gt; TERMINATED 线程池执行流程 12345678910111213141516171819public void execute(Runnable command) &#123; if (command == null) // 健壮性，任务判空 throw new NullPointerException(); int c = ctl.get(); if (workerCountOf(c) &lt; corePoolSize) &#123; // 判断当前线程数是否少于核心线程数 if (addWorker(command, true)) // 如果当前线程数少于核心线程则创建一个新的核心线程执行当前任务，并把线程放入池子 return; c = ctl.get(); &#125; if (isRunning(c) &amp;&amp; workQueue.offer(command)) &#123; // 如果当前核心线程满了就将任务放入队列 int recheck = ctl.get(); if (! isRunning(recheck) &amp;&amp; remove(command)) // 如果线程池状态停止运行，则行拒绝策略 reject(command); else if (workerCountOf(recheck) == 0) // 工作线程数量为0 addWorker(null, false); // 创建一个任务为空的非核心线程 &#125; else if (!addWorker(command, false)) // 任务队列满了就尝试创建一个非核心线程执行任务，否则就执行拒绝策略 reject(command); &#125; addWorker(null, false) 为什么要添加一个任务为空的非核心线程？ 线程池的核心线程数是允许为0的 那么就存在线程饥饿问题，就是任务队列里有任务，但是线程池里没有线程处理 创建一个空任务线程就是防止线程饥饿，先处理堆积任务 添加工作线程逻辑 校验线程池的状态以及工作线程个数 添加工作线程并且启动工作线程 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566private boolean addWorker(Runnable firstTask, boolean core) &#123; // 提交的任务以及是否核心线程 retry: // 外层for的标记，方便在内层for跳出外层for循环 for (;;) &#123; // 外层for在校验线程池的状态 int c = ctl.get(); int rs = runStateOf(c); // 拿到ctl的高3位 // Check if queue empty only if necessary. if (rs &gt;= SHUTDOWN &amp;&amp; ! (rs == SHUTDOWN &amp;&amp; //判断线程池是否调用了 shutdown firstTask == null &amp;&amp; ! workQueue.isEmpty())) return false; for (;;) &#123; // 内层for在校验工作线程的个数 int wc = workerCountOf(c); //获取当前线程数 if (wc &gt;= CAPACITY || wc &gt;= (core ? corePoolSize : maximumPoolSize)) // 判断当前是否核心线程，当前线程数是否小于核心线程 return false; // 判断是否大于核心线程数或最大线程数 if (compareAndIncrementWorkerCount(c)) // cas 线程数 + 1 break retry; // 跳出循环 c = ctl.get(); // Re-read ctl if (runStateOf(c) != rs) continue retry; // else CAS failed due to workerCount change; retry inner loop &#125; &#125; boolean workerStarted = false; boolean workerAdded = false; Worker w = null; try &#123; w = new Worker(firstTask); // 创建一个工作线程 final Thread t = w.thread; if (t != null) &#123; // 判断 Worker 线程是否为空，担心线程工厂有问题 final ReentrantLock mainLock = this.mainLock; mainLock.lock(); // 加锁，存储工作线程的对象是HashSet try &#123; // Recheck while holding lock. // Back out on ThreadFactory failure or if // shut down before lock acquired. int rs = runStateOf(ctl.get()); // 再次判断线程池运行状态 if (rs &lt; SHUTDOWN || (rs == SHUTDOWN &amp;&amp; firstTask == null)) &#123; if (t.isAlive()) // precheck that t is startable throw new IllegalThreadStateException(); workers.add(w); // 将当前线程加入线程池 int s = workers.size(); if (s &gt; largestPoolSize) largestPoolSize = s; workerAdded = true; // 工作线程创建成功 &#125; &#125; finally &#123; mainLock.unlock(); &#125; if (workerAdded) &#123; t.start(); // 开始执行线程 workerStarted = true; // 工作线程启动成功 &#125; &#125; &#125; finally &#123; if (! workerStarted) addWorkerFailed(w); // 如果工作线程启动失败，则进行补偿处理 &#125; return workerStarted; // 返回工作线程是否启动成功 &#125; Executors java官方提供用于创建线程池的工具类 这个类阿里巴巴的规范不推荐使用，原因是容易造成资源浪费和存在大量任务的时候会OOM 12345public interface Executor &#123; // 提交一个无返回值的任务以供执行 void execute(Runnable command);&#125; ExecutorService Executors创建线程池返回的对象，是ThreadPoolExecutor的实现接口 123456789101112131415161718192021222324252627282930313233343536373839404142434445public interface ExecutorService extends Executor &#123; // 启动有序关机，执行以前提交的任务，但不接受新任务 void shutdown(); // 尝试停止所有正在执行的任务，停止正在等待的任务的处理，并返回正在等待执行的任务的列表 List&lt;Runnable&gt; shutdownNow(); // 如果此执行器已关闭，则返回true boolean isShutdown(); // 如果关闭后所有任务都已完成，则返回true boolean isTerminated(); // 阻塞，直到所有任务在关闭请求后完成执行，或超时发生，或当前线程中断（以先发生的为准） boolean awaitTermination(long timeout, TimeUnit unit) throws InterruptedException; // 提交一个有返回值的任务以供执行，并返回一个表示任务执行结果的Future， Future的get()方法将在成功完成后返回任务的结果 &lt;T&gt; Future&lt;T&gt; submit(Callable&lt;T&gt; task); // 提交一个无返回值的任务和返回结果，Future的get()方法得到的结果是传入的result &lt;T&gt; Future&lt;T&gt; submit(Runnable task, T result); // 提交一个无返回值的任务，Future的get()方法得到的结果是null Future&lt;?&gt; submit(Runnable task); // 提交一个任务的集合，返回一个Future的集合 &lt;T&gt; List&lt;Future&lt;T&gt;&gt; invokeAll(Collection&lt;? extends Callable&lt;T&gt;&gt; tasks) throws InterruptedException; // 提交一个任务的集合，返回一个Future的集合，多了任务执行超时功能 &lt;T&gt; List&lt;Future&lt;T&gt;&gt; invokeAll(Collection&lt;? extends Callable&lt;T&gt;&gt; tasks, long timeout, TimeUnit unit) throws InterruptedException; // 执行集合中任务，返回任意一个任务执行结果 &lt;T&gt; T invokeAny(Collection&lt;? extends Callable&lt;T&gt;&gt; tasks) throws InterruptedException, ExecutionException; // 执行集合中任务，返回任意一个任务执行结果，多了任务执行超时功能 &lt;T&gt; T invokeAny(Collection&lt;? extends Callable&lt;T&gt;&gt; tasks, long timeout, TimeUnit unit) throws InterruptedException, ExecutionException, TimeoutException;&#125; execute方法 [无返回值] 实现原理 ThreadPoolExecutor维护一组工作线程 Worker execute 提交的 Runnable会被加入到任务队列中 Worker run 方法阻塞循环获取任务队列，然后在其run方法中执行Runnable的 run方法（非start） 最后启动线程池中 Worker（实现Runnable）线程 submit方法 [有返回值] Callable提交到线程池 线程池转为FutureTask并返回，把任务提交到队列中进行计算 FutureTask 继承了Runnable并且重写了run方法，run方法调用了Callable的call方法能够获取返回值，其本身就定义了一个返回值成员变量，当调用get的时候就时自旋判断是否有返回值，没有则调用LockSuppot挂起线程 调用get方法，此时如果线程池已计算完任务结果则直接返回，否则睡眠调用线程，直到任务计算完毕返回结果 1234@FunctionalInterfacepublic interface Callable&lt;V&gt; &#123; V call() throws Exception;&#125; 123456public &lt;T&gt; Future&lt;T&gt; submit(Callable&lt;T&gt; task) &#123; if (task == null) throw new NullPointerException(); RunnableFuture&lt;T&gt; ftask = newTaskFor(task); // 封装成 FutureTask execute(ftask); // FutureTask 继承了Runnable并且重写了run方法，其本身就定义了一个返回值成员变量，当调用get的时候就时自旋判断是否有返回值，没有则调用LockSuppot挂起线程 return ftask; &#125; 123456789101112131415161718192021222324252627282930313233public void run() &#123; if (state != NEW || !UNSAFE.compareAndSwapObject(this, runnerOffset, null, Thread.currentThread())) return; try &#123; Callable&lt;V&gt; c = callable; if (c != null &amp;&amp; state == NEW) &#123; V result; boolean ran; try &#123; result = c.call(); ran = true; &#125; catch (Throwable ex) &#123; result = null; ran = false; setException(ex); &#125; if (ran) // 调用set回写返回值 set(result); &#125; &#125; finally &#123; // runner must be non-null until state is settled to // prevent concurrent calls to run() runner = null; // state must be re-read after nulling runner to prevent // leaked interrupts int s = state; if (s &gt;= INTERRUPTING) handlePossibleCancellationInterrupt(s); &#125; &#125; 12345678protected void set(V v) &#123; // 修改状态回写返回值 if (UNSAFE.compareAndSwapInt(this, stateOffset, NEW, COMPLETING)) &#123; outcome = v; UNSAFE.putOrderedInt(this, stateOffset, NORMAL); // final state finishCompletion(); &#125;&#125; FutureTask#get() 实现 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364public V get() throws InterruptedException, ExecutionException &#123; int s = state; if (s &lt;= COMPLETING) s = awaitDone(false, 0L); return report(s);&#125;private int awaitDone(boolean timed, long nanos) throws InterruptedException &#123; // 计算等待截止时间 final long deadline = timed ? System.nanoTime() + nanos : 0L; WaitNode q = null; boolean queued = false; // 自旋 for (;;) &#123; // 1. 判断阻塞线程是否被中断,如果被中断则在等待队列中删除该节点并抛出InterruptedException异常 if (Thread.interrupted()) &#123; removeWaiter(q); throw new InterruptedException(); &#125; // 2. 获取当前状态，如果状态大于COMPLETING // 说明任务已经结束(要么正常结束，要么异常结束，要么被取消) // 则把thread显示置空，并返回结果 int s = state; if (s &gt; COMPLETING) &#123; if (q != null) q.thread = null; return s; &#125; // 3. 如果状态处于中间状态COMPLETING // 表示任务已经结束但是任务执行线程还没来得及给outcome赋值 // 这个时候让出执行权让其他线程优先执行 else if (s == COMPLETING) // cannot time out yet Thread.yield(); // 4. 如果等待节点为空，则构造一个等待节点 else if (q == null) q = new WaitNode(); // 5. 如果还没有入队列，则把当前节点加入waiters首节点并替换原来waiters else if (!queued) queued = UNSAFE.compareAndSwapObject(this, waitersOffset, q.next = waiters, q); else if (timed) &#123; // 如果需要等待特定时间，则先计算要等待的时间 // 如果已经超时，则删除对应节点并返回对应的状态 nanos = deadline - System.nanoTime(); if (nanos &lt;= 0L) &#123; removeWaiter(q); return state; &#125; // 6. 阻塞等待特定时间 LockSupport.parkNanos(this, nanos); &#125; else // 6. 阻塞等待直到被其他线程唤醒 LockSupport.park(this); &#125; // 等待节点，链表节点 static final class WaitNode &#123; volatile Thread thread; volatile WaitNode next; WaitNode() &#123; thread = Thread.currentThread(); &#125; &#125;&#125; newFixedThreadPool 固定数量的线程池 12345public static ExecutorService newFixedThreadPool(int nThreads) &#123; return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;());&#125; 创建一个核心线程数和最大线程数一样 核心线程最大存活时间无限 （容易造成资源浪费） 使用无边界的链表队列存放待执行的任务（如果存在大量任务容易造成任务堆积导致OOM） 使用 1234567891011121314151617181920212223242526272829public static void main(String[] args) &#123; ExecutorService executorService = Executors.newFixedThreadPool(2); for (int i = 0; i &lt; 10; i++) &#123; Runnable runnable = () -&gt; &#123; try &#123; TimeUnit.SECONDS.sleep(1); System.out.println(String.format(&quot;%s %s&quot;, Thread.currentThread().getName(), DateFormatUtils.format(new Date(), &quot;yyyy-MM-dd hh:mm:ss&quot;))); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;; // 提交任务 executorService.execute(runnable); &#125;&#125;// 输出pool-1-thread-1 2020-07-22 01:25:06pool-1-thread-2 2020-07-22 01:25:06pool-1-thread-2 2020-07-22 01:25:07pool-1-thread-1 2020-07-22 01:25:07pool-1-thread-2 2020-07-22 01:25:08pool-1-thread-1 2020-07-22 01:25:08pool-1-thread-2 2020-07-22 01:25:09pool-1-thread-1 2020-07-22 01:25:09pool-1-thread-2 2020-07-22 01:25:10pool-1-thread-1 2020-07-22 01:25:10 newSingleThreadExecutor 创建一个单线程化的线程池，它只会用唯一的工作线程来执行任务,提交的任务按照FIFO顺序执行 创建一个核心线程数，最大线程数为1的线程池 任何时候都只有一个活跃的线程 适用于单个任务顺序执行场景, 但是使用了无边界的BlockingQueue还是存在OOM风险 1234567public static ExecutorService newSingleThreadExecutor(ThreadFactory threadFactory) &#123; return new FinalizableDelegatedExecutorService (new ThreadPoolExecutor(1, 1, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;(), threadFactory)); &#125; 使用 123456789101112131415161718192021222324252627282930public static void main(String[] args) &#123; ExecutorService executorService = Executors.newSingleThreadExecutor(); for (int i = 0; i &lt; 10; i++) &#123; Runnable runnable = () -&gt; &#123; try &#123; TimeUnit.SECONDS.sleep(1); System.out.println(String.format(&quot;%s %s&quot;, Thread.currentThread().getName(), DateFormatUtils.format(new Date(), &quot;yyyy-MM-dd hh:mm:ss&quot;))); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;; // 提交任务 executorService.execute(runnable); &#125;&#125;// 输出pool-1-thread-1 2020-07-22 01:27:05pool-1-thread-1 2020-07-22 01:27:06pool-1-thread-1 2020-07-22 01:27:07pool-1-thread-1 2020-07-22 01:27:08pool-1-thread-1 2020-07-22 01:27:09pool-1-thread-1 2020-07-22 01:27:10pool-1-thread-1 2020-07-22 01:27:11pool-1-thread-1 2020-07-22 01:27:12pool-1-thread-1 2020-07-22 01:27:13pool-1-thread-1 2020-07-22 01:27:14 ​ newCachedThreadPool 具有缓存功能的线程池，无限制创建线程 创建一个核心线程数为0，没有最大线程数的线程池 线程的活跃时间为60秒，线程池会根据实际需求创建线程 如果线程池中有空闲线程未被销毁，则新提交的任务会重用这些线程 适用于高并发，任务处理时间短场景。长期空闲的池子不会消耗系统资源 使用SynchronousQueue任务队列只能存储一个任务，其他全部通过创建新的非核心线程执行，大量任务涌入会OOM 12345public static ExecutorService newCachedThreadPool() &#123; return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue&lt;Runnable&gt;());&#125; 使用 1234567891011121314151617181920212223242526272829public static void main(String[] args) &#123; ExecutorService executorService = Executors.newCachedThreadPool(); for (int i = 0; i &lt; 10; i++) &#123; Runnable runnable = () -&gt; &#123; try &#123; TimeUnit.SECONDS.sleep(1); System.out.println(String.format(&quot;%s %s&quot;, Thread.currentThread().getName(), DateFormatUtils.format(new Date(), &quot;yyyy-MM-dd hh:mm:ss&quot;))); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;; // 提交任务 executorService.execute(runnable); &#125;&#125;// 输出pool-1-thread-7 2020-07-22 01:25:53pool-1-thread-6 2020-07-22 01:25:53pool-1-thread-9 2020-07-22 01:25:53pool-1-thread-8 2020-07-22 01:25:53pool-1-thread-5 2020-07-22 01:25:53pool-1-thread-1 2020-07-22 01:25:53pool-1-thread-2 2020-07-22 01:25:53pool-1-thread-4 2020-07-22 01:25:53pool-1-thread-3 2020-07-22 01:25:53pool-1-thread-10 2020-07-22 01:25:53 newScheduledThreadPool 创建一个定长线程池，支持定时及周期性任务执行 创建一个可配置的核心线程数延迟队列 队列是按延时时间升序排序，不是按照submit时间排序的了以延时时间作为优先级排序，延时时间短的，优先级高，放在前面先执行 123public static ScheduledExecutorService newScheduledThreadPool(int corePoolSize) &#123; return new ScheduledThreadPoolExecutor(corePoolSize);&#125; 使用 123456789101112131415161718192021222324252627282930313233343536373839404142434445public static void main(String[] args) &#123; ScheduledExecutorService executorService = Executors.newScheduledThreadPool(2); for (int i = 0; i &lt; 10; i++) &#123; final int flag = i; Runnable runnable = () -&gt; &#123; try &#123; TimeUnit.SECONDS.sleep(1); System.out.println(String.format(&quot;%s %s %s&quot;, Thread.currentThread().getName(), flag, DateFormatUtils.format(new Date(), &quot;yyyy-MM-dd hh:mm:ss:SSS&quot;))); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;; /** * 第一个参数： Runnable * 第二个参数： initialDelay 初始延迟时间 * 第三个参数： period 间隔时长 * 第四个参数： unit 时间单位 */ executorService.scheduleAtFixedRate(runnable, 10, 10, TimeUnit.MILLISECONDS); &#125;&#125;//输出pool-1-thread-2 1 2020-07-22 01:36:17:944pool-1-thread-1 0 2020-07-22 01:36:17:937pool-1-thread-2 2 2020-07-22 01:36:19:057pool-1-thread-1 3 2020-07-22 01:36:19:057pool-1-thread-1 4 2020-07-22 01:36:20:057pool-1-thread-2 5 2020-07-22 01:36:20:057pool-1-thread-1 6 2020-07-22 01:36:21:058pool-1-thread-2 7 2020-07-22 01:36:21:058pool-1-thread-1 8 2020-07-22 01:36:22:058pool-1-thread-2 9 2020-07-22 01:36:22:058pool-1-thread-1 0 2020-07-22 01:36:23:059pool-1-thread-2 1 2020-07-22 01:36:23:059pool-1-thread-2 3 2020-07-22 01:36:24:059pool-1-thread-1 2 2020-07-22 01:36:24:059pool-1-thread-1 5 2020-07-22 01:36:25:059pool-1-thread-2 4 2020-07-22 01:36:25:059pool-1-thread-1 6 2020-07-22 01:36:26:062pool-1-thread-2 7 2020-07-22 01:36:26:063 CompletableFuture API文档 example1 example2 CompletionService CompletionService的实现目标是任务先完成可优先获取到，即结果按照完成先后顺序排序 API文档(https://www.matools.com/file/manual/jdk_api_1.8_google/java/util/concurrent/CompletionService.html) example Fork Join 核心类 RecursiveTask 递归任务类 ForkJoinPool 线程池 Future 获取结果 Fork/Join是Java中用于并行计算的框架，它基于工作窃取算法，用于将一个大任务切分为多个子任务，并行地执行，最后合并子任务的执行结果得到最终结果 工作窃取算法 ​ 工作窃取算法是指某个线程从其他队列里窃取任务来执行。对于一个比较大的任务，可以把它分割为若干个互不依赖的子任务，为了减少线程间的竞争，把这些子任务分别放到不同的队列里，并为每个队列创建一个单独的线程来执行队列里的任务，线程和队列一一对应。但是，有的线程会先把自己队列里的任务干完，而其他线程对应的队列里还有任务需要处理，于是它就去其他线程的队列里窃取一个任务来执行。由于此时它们访问同一个队列，为了减小竞争，通常会使用双端队列。被窃取任务的线程永远从双端队列的头部获取任务，窃取任务的线程永远从双端队列的尾部获取任务 窃取算法优缺点 优点：充分利用线程进行并行计算，减少了线程间的竞争 缺点：双端队列只存在一个任务时会导致竞争，会消耗更多的系统资源，因为需要创建多个线程和多个双端队列 Fork/Join 框架的异常处理 ​ ForkJoinTask 在执行的时候可能抛出异常，但没有办法在主线程中直接捕获异常，所以 ForkJoinTask 提供了 isCompletedAbnormally() 方法检查任务是否已经抛出异常或已经被取消。getException() 方法返回 Throwable 对象，如果任务被取消了则返回 CancellationException，如果任务没有完成或者没有抛出异常则返回 null fork() 方法的实现原理 ​ 当调用 ForkJoinTask 的 fork() 方法时，程序会调用 ForkJoinPool.WorkQueue 的 push() 方法异步地执行这个任务，然后立即返回结果。代码如下： 12345678public final ForkJoinTask&lt;V&gt; fork() &#123; Thread t; if ((t = Thread.currentThread()) instanceof ForkJoinWorkerThread) ((ForkJoinWorkerThread)t).workQueue.push(this); else ForkJoinPool.common.externalPush(this); return this;&#125; ​ push() 方法把当前任务存放在一个 ForkJoinTask 数组队列里，然后再调用 ForkJoinPool 的 signalWork() 方法唤醒或创建一个工作线程来执行任务。代码如下: 123456789101112131415final void push(ForkJoinTask&lt;?&gt; task) &#123; ForkJoinTask&lt;?&gt;[] a; ForkJoinPool p; int b = base, s = top, n; if ((a = array) != null) &#123; // ignore if queue removed int m = a.length - 1; // fenced write for task visibility U.putOrderedObject(a, ((m &amp; s) &lt;&lt; ASHIFT) + ABASE, task); U.putOrderedInt(this, QTOP, s + 1); if ((n = s - b) &lt;= 1) &#123; if ((p = pool) != null) p.signalWork(p.workQueues, this); &#125; else if (n &gt;= m) growArray(); &#125;&#125; join() 方法的实现原理 ​ 当调用 ForkJoinTask 的 join() 方法时，程序会调用 doJoin() 方法，通过 doJoin() 方法来判断返回什么结果 12345678910111213141516171819202122public final V join() &#123; int s; if ((s = doJoin() &amp; DONE_MASK) != NORMAL) reportException(s); return getRawResult();&#125;private int doJoin() &#123; int s; Thread t; ForkJoinWorkerThread wt; ForkJoinPool.WorkQueue w; return (s = status) &lt; 0 ? s : ((t = Thread.currentThread()) instanceof ForkJoinWorkerThread) ? (w = (wt = (ForkJoinWorkerThread)t).workQueue). tryUnpush(this) &amp;&amp; (s = doExec()) &lt; 0 ? s : wt.pool.awaitJoin(w, this, 0L) : externalAwaitDone();&#125;private void reportException(int s) &#123; if (s == CANCELLED) throw new CancellationException(); if (s == EXCEPTIONAL) rethrow(getThrowableException());&#125;public abstract V getRawResult(); 1~10累加 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849/** * RecursiveTask 递归任务 */public class Calculator extends RecursiveTask&lt;Integer&gt; &#123; // 任务递归分解停止的阈值 private static final int THRESHOLD = 1; // 最小计算元素 private int start; // 最大计算元素 private int end; public Calculator(int start, int end) &#123; System.out.println(&quot;min:&quot; + start + &quot; max:&quot; + end); this.start = start; this.end = end; &#125; @Override protected Integer compute() &#123; int sum = 0; // 当数据分解的范围小于设定的阈值后才会真正执行计算求sum if ((end - start) &lt; THRESHOLD) &#123; for (int i = start; i &lt;= end; i++) &#123; sum += i; &#125; // 任务拆解范围如果还大于阈值，线程工作不参与求值计算，而是进行任务分解后求sum &#125; else &#123; int middle = (end + start) / 2; Calculator left = new Calculator(start, middle); Calculator rigth = new Calculator(middle + 1, end); //分解 left.fork(); rigth.fork(); //合并 sum = left.join() + rigth.join(); &#125; return sum; &#125; public static void main(String[] args) throws ExecutionException, InterruptedException &#123; ForkJoinPool forkJoinPool = new ForkJoinPool(4); Future&lt;Integer&gt; result = forkJoinPool.submit(new Calculator(0, 10)); System.out.println(result.get()); &#125;&#125; ThreadLocal 在线参考 ThreadLocal是Java中的一个线程局部变量，用于在多线程环境下实现线程的数据隔离。它主要解决的问题是让每个线程都拥有自己的局部变量副本，避免了线程间的数据共享问题 ThreadLocal数据存储 数据直接存储在线程对象中 1234567891011121314class Thread implements Runnable &#123; ... // ThreadLocalMap用于支持一个线程使用多个ThreadLocal /* ThreadLocal values pertaining to this thread. This map is maintained * by the ThreadLocal class. */ ThreadLocal.ThreadLocalMap threadLocals = null; /*可继承的ThreadLocalMap，用于在父子线程间传递，在Thread构造函数中，如果父线程的这个属性不为空，子线程会复制一个一样的Map * InheritableThreadLocal values pertaining to this thread. This map is * maintained by the InheritableThreadLocal class. */ ThreadLocal.ThreadLocalMap inheritableThreadLocals = null;... 用法","categories":[],"tags":[]},{"title":"Mybatis","slug":"Mybatis","date":"2022-04-07T14:24:25.000Z","updated":"2023-07-14T06:49:15.080Z","comments":true,"path":"2022/04/07/Mybatis/","link":"","permalink":"https://wugengfeng.cn/2022/04/07/Mybatis/","excerpt":"","text":"友情连接 http://coderead.cn/ JDBC 完整的JDBC查询过程 加载驱动 连接数据库 预编译sql 执行操作 获取结果 12345678910111213141516171819202122232425262728293031@Slf4jpublic class JdbcTest &#123; public static void main(String[] args) throws ClassNotFoundException, SQLException &#123; // 加载驱动 Class.forName(&quot;com.mysql.cj.jdbc.Driver&quot;); // 获取连接 Connection connection = DriverManager.getConnection(&quot;jdbc:mysql://localhost:3306/test?useUnicode=true&amp;characterEncoding=UTF-8&amp;serverTimezone=Asia/Shanghai&quot;, &quot;root&quot;, &quot;root&quot;); // 预编译sql PreparedStatement preparedStatement = connection.prepareStatement(&quot;select * from tb_user where id = ?&quot;); // 设置参数，防止sql注入 preparedStatement.setInt(1, 1); // 执行查询 ResultSet resultSet = preparedStatement.executeQuery(); // 获取查询数据 while (resultSet.next()) &#123; int id = resultSet.getInt(&quot;id&quot;); String name = resultSet.getString(&quot;name&quot;); int age = resultSet.getInt(&quot;age&quot;); log.info(&quot;id: &#123;&#125;&quot;, id); log.info(&quot;name: &#123;&#125;&quot;, name); log.info(&quot;age: &#123;&#125;&quot;, age); &#125; &#125;&#125; Mybtais 核心组件介绍 Configuration 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889public class Configuration &#123; // &lt;environment&gt;节点的信息 protected Environment environment; // 以下为&lt;settings&gt;节点中的配置信息 protected boolean safeRowBoundsEnabled; protected boolean safeResultHandlerEnabled = true; protected boolean mapUnderscoreToCamelCase; protected boolean aggressiveLazyLoading; protected boolean multipleResultSetsEnabled = true; protected boolean useGeneratedKeys; protected boolean useColumnLabel = true; protected boolean cacheEnabled = true; protected boolean callSettersOnNulls; protected boolean useActualParamName = true; protected boolean returnInstanceForEmptyRow; protected String logPrefix; protected Class&lt;? extends Log&gt; logImpl; protected Class&lt;? extends VFS&gt; vfsImpl; protected LocalCacheScope localCacheScope = LocalCacheScope.SESSION; protected JdbcType jdbcTypeForNull = JdbcType.OTHER; protected Set&lt;String&gt; lazyLoadTriggerMethods = new HashSet&lt;&gt;(Arrays.asList(&quot;equals&quot;, &quot;clone&quot;, &quot;hashCode&quot;, &quot;toString&quot;)); protected Integer defaultStatementTimeout; protected Integer defaultFetchSize; protected ResultSetType defaultResultSetType; protected ExecutorType defaultExecutorType = ExecutorType.SIMPLE; protected AutoMappingBehavior autoMappingBehavior = AutoMappingBehavior.PARTIAL; protected AutoMappingUnknownColumnBehavior autoMappingUnknownColumnBehavior = AutoMappingUnknownColumnBehavior.NONE; // 以上为&lt;settings&gt;节点中的配置信息 // &lt;properties&gt;节点信息 protected Properties variables = new Properties(); // 反射工厂 protected ReflectorFactory reflectorFactory = new DefaultReflectorFactory(); // 对象工厂 protected ObjectFactory objectFactory = new DefaultObjectFactory(); // 对象包装工厂 protected ObjectWrapperFactory objectWrapperFactory = new DefaultObjectWrapperFactory(); // 是否启用懒加载，该配置来自&lt;settings&gt;节点 protected boolean lazyLoadingEnabled = false; // 代理工厂 protected ProxyFactory proxyFactory = new JavassistProxyFactory(); // #224 Using internal Javassist instead of OGNL // 数据库编号 protected String databaseId; // 配置工厂，用来创建用于加载反序列化的未读属性的配置。 protected Class&lt;?&gt; configurationFactory; // 映射注册表 protected final MapperRegistry mapperRegistry = new MapperRegistry(this); // 拦截器链（用来支持插件的插入） protected final InterceptorChain interceptorChain = new InterceptorChain(); // 类型处理器注册表，内置许多，可以通过&lt;typeHandlers&gt;节点补充 protected final TypeHandlerRegistry typeHandlerRegistry = new TypeHandlerRegistry(); // 类型别名注册表，内置许多，可以通过&lt;typeAliases&gt;节点补充 protected final TypeAliasRegistry typeAliasRegistry = new TypeAliasRegistry(); // 语言驱动注册表 protected final LanguageDriverRegistry languageRegistry = new LanguageDriverRegistry(); // 映射的数据库操作语句 protected final Map&lt;String, MappedStatement&gt; mappedStatements = new StrictMap&lt;MappedStatement&gt;(&quot;Mapped Statements collection&quot;) .conflictMessageProducer((savedValue, targetValue) -&gt; &quot;. please check &quot; + savedValue.getResource() + &quot; and &quot; + targetValue.getResource()); // 缓存 protected final Map&lt;String, Cache&gt; caches = new StrictMap&lt;&gt;(&quot;Caches collection&quot;); // 结果映射，即所有的&lt;resultMap&gt;节点 protected final Map&lt;String, ResultMap&gt; resultMaps = new StrictMap&lt;&gt;(&quot;Result Maps collection&quot;); // 参数映射，即所有的&lt;parameterMap&gt;节点 protected final Map&lt;String, ParameterMap&gt; parameterMaps = new StrictMap&lt;&gt;(&quot;Parameter Maps collection&quot;); // 主键生成器映射 protected final Map&lt;String, KeyGenerator&gt; keyGenerators = new StrictMap&lt;&gt;(&quot;Key Generators collection&quot;); // 载入的资源，例如映射文件资源 protected final Set&lt;String&gt; loadedResources = new HashSet&lt;&gt;(); // SQL语句片段，即所有的&lt;sql&gt;节点 protected final Map&lt;String, XNode&gt; sqlFragments = new StrictMap&lt;&gt;(&quot;XML fragments parsed from previous mappers&quot;); // 暂存未处理完成的一些节点 protected final Collection&lt;XMLStatementBuilder&gt; incompleteStatements = new LinkedList&lt;&gt;(); protected final Collection&lt;CacheRefResolver&gt; incompleteCacheRefs = new LinkedList&lt;&gt;(); protected final Collection&lt;ResultMapResolver&gt; incompleteResultMaps = new LinkedList&lt;&gt;(); protected final Collection&lt;MethodResolver&gt; incompleteMethods = new LinkedList&lt;&gt;(); // 用来存储跨namespace的缓存共享设置 protected final Map&lt;String, String&gt; cacheRefMap = new HashMap&lt;&gt;(); public Configuration(Environment environment) &#123; this(); this.environment = environment; &#125;.... ​ Mybatis 上下文配置，项目启动会根据配置的mapperLocations加载解析XML文件，最终将解析好的XML配置信息装载到自身属性中，在SqlSessionFactoryBean的afterPropertiesSet方法中被创建 interceptorChain 拦截器链 typeHandlerRegistry 类型处理器 typeAliasRegistry 别名处理器 resultMaps 返回结果映射 keyGenerators主键生成映射 mappedStatements 数据库语句映射（mapper 文件sql） sqlFragments SQL语句片段，即所有的节点 caches 缓存（一级缓存） … SqlSessionFactory 12345678910111213141516171819public interface SqlSessionFactory &#123; //8个方法可以用来创建SqlSession实例 SqlSession openSession(); //自动提交 SqlSession openSession(boolean autoCommit); //连接 SqlSession openSession(Connection connection); //事务隔离级别 SqlSession openSession(TransactionIsolationLevel level); //执行器的类型 SqlSession openSession(ExecutorType execType); SqlSession openSession(ExecutorType execType, boolean autoCommit); SqlSession openSession(ExecutorType execType, TransactionIsolationLevel level); SqlSession openSession(ExecutorType execType, Connection connection); // 持有全局配置 Configuration getConfiguration();&#125; ​ SqlSession会话工厂，用于获取SqlSession。通过SqlSessionFactoryBean的getObject方法创建，创建SqlSessionFactory的前提是解析Mapper配置文件创建Configuration对象后创建，因为DefaultSqlSessionFactory构造函数需要Configuration参数 SqlSession 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169public interface SqlSession extends Closeable &#123; /** * Retrieve a single row mapped from the statement key * 根据指定的SqlID获取一条记录的封装对象 */ &lt;T&gt; T selectOne(String statement); /** * Retrieve a single row mapped from the statement key and parameter. * 根据指定的SqlID获取一条记录的封装对象，只不过这个方法容许我们可以给sql传递一些参数 * 一般在实际使用中，这个参数传递的是pojo，或者Map或者ImmutableMap */ &lt;T&gt; T selectOne(String statement, Object parameter); /** * Retrieve a list of mapped objects from the statement key and parameter. * 根据指定的sqlId获取多条记录 */ &lt;E&gt; List&lt;E&gt; selectList(String statement); /** * Retrieve a list of mapped objects from the statement key and parameter. * 获取多条记录，这个方法容许我们可以传递一些参数 */ &lt;E&gt; List&lt;E&gt; selectList(String statement, Object parameter); /** * Retrieve a list of mapped objects from the statement key and parameter, * within the specified row bounds. * 获取多条记录，这个方法容许我们可以传递一些参数，不过这个方法容许我们进行 * 分页查询。 * * 需要注意的是默认情况下，Mybatis为了扩展性，仅仅支持内存分页。也就是会先把 * 所有的数据查询出来以后，然后在内存中进行分页。因此在实际的情况中，需要注意 * 这一点。 * * 一般情况下公司都会编写自己的Mybatis 物理分页插件 */ &lt;E&gt; List&lt;E&gt; selectList(String statement, Object parameter, RowBounds rowBounds); /** * The selectMap is a special case in that it is designed to convert a list * of results into a Map based on one of the properties in the resulting * objects. * Eg. Return a of Map[Integer,Author] for selectMap(&quot;selectAuthors&quot;,&quot;id&quot;) * 将查询到的结果列表转换为Map类型。 */ &lt;K, V&gt; Map&lt;K, V&gt; selectMap(String statement, String mapKey); /** * The selectMap is a special case in that it is designed to convert a list * of results into a Map based on one of the properties in the resulting * objects. * 将查询到的结果列表转换为Map类型。这个方法容许我们传入需要的参数 */ &lt;K, V&gt; Map&lt;K, V&gt; selectMap(String statement, Object parameter, String mapKey); /** * The selectMap is a special case in that it is designed to convert a list * of results into a Map based on one of the properties in the resulting * objects. * 获取多条记录,加上分页,并存入Map */ &lt;K, V&gt; Map&lt;K, V&gt; selectMap(String statement, Object parameter, String mapKey, RowBounds rowBounds); void select(String statement, Object parameter, ResultHandler handler); /** * Retrieve a single row mapped from the statement * using a &#123;@code ResultHandler&#125;. * 获取一条记录,并转交给ResultHandler处理。这个方法容许我们自己定义对 * 查询到的行的处理方式。不过一般用的并不是很多 */ void select(String statement, ResultHandler handler); /** * Retrieve a single row mapped from the statement key and parameter * using a &#123;@code ResultHandler&#125; and &#123;@code RowBounds&#125; * 获取一条记录,加上分页,并转交给ResultHandler处理 */ void select(String statement, Object parameter, RowBounds rowBounds, ResultHandler handler); /** * Execute an insert statement. * 插入记录。一般情况下这个语句在实际项目中用的并不是太多，而且更多使用带参数的insert函数 */ int insert(String statement); /** * Execute an insert statement with the given parameter object. Any generated * autoincrement values or selectKey entries will modify the given parameter * object properties. Only the number of rows affected will be returned. * 插入记录，容许传入参数。 注意返回的是受影响的行数 */ int insert(String statement, Object parameter); /** * Execute an update statement. The number of rows affected will be returned. * 更新记录。返回的是受影响的行数 */ int update(String statement); /** * Execute an update statement. The number of rows affected will be returned. * 更新记录 返回的是受影响的行数 */ int update(String statement, Object parameter); /** * Execute a delete statement. The number of rows affected will be returned. * 删除记录 返回的是受影响的行数 */ int delete(String statement); /** * Execute a delete statement. The number of rows affected will be returned. * 删除记录 返回的是受影响的行数 */ int delete(String statement, Object parameter); //以下是事务控制方法,commit,rollback void commit(); void commit(boolean force); void rollback(); void rollback(boolean force); /** * Flushes batch statements. * 刷新批处理语句,返回批处理结果 */ List&lt;BatchResult&gt; flushStatements(); /** * Closes the session * 关闭Session */ @Override void close(); /** * Clears local session cache * 清理Session缓存 */ void clearCache(); /** * Retrieves current configuration * 得到配置 */ Configuration getConfiguration(); /** * Retrieves a mapper. * 得到映射器 * 这个巧妙的使用了泛型，使得类型安全 * 到了MyBatis 3，还可以用注解,这样xml都不用写了 */ &lt;T&gt; T getMapper(Class&lt;T&gt; type); /** * Retrieves inner database connection * 得到数据库连接 */ Connection getConnection();&#125; ​ 数据库会话，使用外观模式提供操作数据库的方法，具体实现底层调用Executor，设计这一层的目的是对,外屏蔽Executor底层调用的复杂性,DefaultSqlSession中包含Executor对象 功能 数据库操作 RURD 事务管理 提交回滚事务 获取连接 1234567@Test public void sqlSessionTest() &#123; SqlSession sqlSession = sqlSessionFactory.openSession(ExecutorType.SIMPLE); List&lt;Object&gt; admin = sqlSession.selectList(&quot;com.wgf.modules.sys.dao.SysUserDao.queryByUserName&quot;, &quot;admin&quot;); admin = sqlSession.selectList(&quot;com.wgf.modules.sys.dao.SysUserDao.queryByUserName&quot;, &quot;admin&quot;); System.out.println(admin); &#125; Executor 执行器 Executor是MyBatis执行者接口，执行器的功能包括： 基本功能：改、查，没有增删的原因是，所有的增删操作在JDBC都可以归结到改 缓存维护：这里的缓存主要是为一级缓存服务，功能包括创建缓存Key、清理缓存、判断缓存是否存在 事务管理：提交、回滚、关闭 批处理刷新 单元测试公共代码 12345678910111213141516171819 @Autowired SqlSessionFactory sqlSessionFactory; Configuration configuration; Connection connection; JdbcTransaction jdbcTransaction; @Before public void init() throws SQLException &#123; connection = DriverManager.getConnection(&quot;jdbc:mysql://localhost:3306/test?useUnicode=true&amp;characterEncoding=UTF-8&amp;serverTimezone=Asia/Shanghai&quot; , &quot;root&quot; , &quot;root&quot;); configuration = sqlSessionFactory.getConfiguration(); jdbcTransaction = new JdbcTransaction(connection); &#125;&#125; BaseExecutor ​ 将Executor的共性抽取出一个公共的父类 ​ 基础执行器主要是用于维护缓存和事务。事务是通过会话中调用commit、rollback进行管理。重点在于缓存这块它是如何处理的? (这里的缓存是指一级缓存）,它实现了Executor中的query与update方法。会话中SQL请求，正是调用的这两个方法。query方法中处理一级缓存逻辑，即根据SQL及参数判断缓存中是否存在数据，有就走缓存。否则就会调用子类的doQuery() 方法去查询数据库,然后在设置缓存。在doUpdate() 中主要是用于清空缓存 共性 一级缓存 事务管理 获取、提交、回滚、关闭 数据库基本操作 query,update (除了查询为的操作可归并为update) 12345678910111213141516171819/** * 两条相同的sql只执行一次查询，原因是第二次直接从BaseExecutor中获取数据 BaseExecutor.query方法存在一级缓存逻辑 * 执行逻辑，调用BaseExecutor.query 是否一级缓存命中，以及缓存又命中则再调用子类的doQuery进行数据库查询 * c.w.m.s.dao.SysUserDao.queryByUserName : ==&gt; Preparing: select * from sys_user where username = ? * c.w.m.s.dao.SysUserDao.queryByUserName : ==&gt; Parameters: admin(String) * c.w.m.s.dao.SysUserDao.queryByUserName : &lt;== Total: 1 * @throws SQLException */ @Test public void baseExecutor() throws SQLException &#123; SimpleExecutor simpleExecutor = new SimpleExecutor(configuration, jdbcTransaction); MappedStatement mappedStatement = configuration.getMappedStatement(&quot;com.wgf.modules.sys.dao.SysUserDao.queryByUserName&quot;); // 相同的sql 会执两次 List&lt;Object&gt; objects = simpleExecutor.query(mappedStatement, &quot;admin&quot;, RowBounds.DEFAULT, SimpleExecutor.NO_RESULT_HANDLER); objects = simpleExecutor.query(mappedStatement, &quot;admin&quot;, RowBounds.DEFAULT, SimpleExecutor.NO_RESULT_HANDLER); System.out.println(objects); &#125; SimpleExecutor SimpleExecutor是默认执行器，它的行为是每处理一次会话当中的SQl请求都会通过对应的StatementHandler 构建一个新个Statement，这就会导致即使是相同SQL语句也无法重用Statement,所以就有了（ReuseExecutor）可重用执行器 12345678910111213141516171819@Test public void simpleExecutor() throws SQLException &#123; SimpleExecutor simpleExecutor = new SimpleExecutor(configuration, jdbcTransaction); MappedStatement mappedStatement = configuration.getMappedStatement(&quot;com.wgf.modules.sys.dao.SysUserDao.queryByUserName&quot;); List&lt;Object&gt; objects = simpleExecutor.doQuery(mappedStatement, &quot;admin&quot;, RowBounds.DEFAULT, SimpleExecutor.NO_RESULT_HANDLER, mappedStatement.getBoundSql(&quot;admin&quot;)); objects = simpleExecutor.doQuery(mappedStatement, &quot;admin&quot;, RowBounds.DEFAULT, SimpleExecutor.NO_RESULT_HANDLER, mappedStatement.getBoundSql(&quot;admin&quot;)); System.out.println(objects); &#125; /** * 日志中预编译了2次sql, 输出两次Preparing * 2022-04-03 14:52:31.416 DEBUG 15120 --- [ main] c.w.m.s.dao.SysUserDao.queryByUserName : ==&gt; Preparing: select * from sys_user where username = ? * 2022-04-03 14:52:31.416 DEBUG 15120 --- [ main] c.w.m.s.dao.SysUserDao.queryByUserName : ==&gt; Parameters: admin(String) * 2022-04-03 14:52:31.431 DEBUG 15120 --- [ main] c.w.m.s.dao.SysUserDao.queryByUserName : &lt;== Total: 1 * 2022-04-03 14:52:31.431 DEBUG 15120 --- [ main] c.w.m.s.dao.SysUserDao.queryByUserName : ==&gt; Preparing: select * from sys_user where username = ? * 2022-04-03 14:52:31.431 DEBUG 15120 --- [ main] c.w.m.s.dao.SysUserDao.queryByUserName : ==&gt; Parameters: admin(String) * 2022-04-03 14:52:31.431 DEBUG 15120 --- [ main] c.w.m.s.dao.SysUserDao.queryByUserName : &lt;== Total: 1 * @throws SQLException */ ReuseExecutor ​ 重用的是 PreparedStatemen对象，减少预编译sql次数 ​ ReuseExecutor 区别在于他会将在会话期间内的Statement进行缓存，并使用SQL语句作为Key。所以当执行下一请求的时候，不在重复构建Statement，而是从缓存中取出并设置参数，然后执行 12345678910111213141516171819@Test public void reuseExecutor() throws SQLException &#123; ReuseExecutor reuseExecutor = new ReuseExecutor(configuration, jdbcTransaction); MappedStatement mappedStatement = configuration.getMappedStatement(&quot;com.wgf.modules.sys.dao.SysUserDao.queryByUserName&quot;); // 相同的sql 会执两次,但是sql预编译只执行一次 List&lt;Object&gt; objects = reuseExecutor.doQuery(mappedStatement, &quot;admin&quot;, RowBounds.DEFAULT, ReuseExecutor.NO_RESULT_HANDLER, mappedStatement.getBoundSql(&quot;admin&quot;)); objects = reuseExecutor.doQuery(mappedStatement, &quot;admin&quot;, RowBounds.DEFAULT, SimpleExecutor.NO_RESULT_HANDLER, mappedStatement.getBoundSql(&quot;admin&quot;)); System.out.println(objects); &#125; /** * 从日志看出只有一次预编译 Preparing * 2022-04-03 13:10:40.042 DEBUG 18364 --- [ main] c.w.m.s.dao.SysUserDao.queryByUserName : ==&gt; Preparing: select * from sys_user where username = ? * 2022-04-03 13:10:40.042 DEBUG 18364 --- [ main] c.w.m.s.dao.SysUserDao.queryByUserName : ==&gt; Parameters: admin(String) * 2022-04-03 13:10:40.042 DEBUG 18364 --- [ main] c.w.m.s.dao.SysUserDao.queryByUserName : &lt;== Total: 1 * 2022-04-03 13:10:40.042 DEBUG 18364 --- [ main] c.w.m.s.dao.SysUserDao.queryByUserName : ==&gt; Parameters: admin(String) * 2022-04-03 13:10:40.058 DEBUG 18364 --- [ main] c.w.m.s.dao.SysUserDao.queryByUserName : &lt;== Total: 1 * @throws SQLException */ BatchExecutor ​ sql语句相同，顺序连贯(为保证sql执行的准确性)，才能合并 ​ BatchExecutor 顾名思议，它就是用来作批处理的。但会将所 有SQL请求集中起来，最后调用Executor.flushStatements() 方法时一次性将所有请求发送至数据库 1234567891011121314151617181920/** * sql 只编译了一次，参数合并起来了。并不是所有相同的sql都合并的，必须满足sql一样和sql语句的顺序连贯才能合并 * c.w.m.sys.dao.SysUserDao.updateNameById : ==&gt; Preparing: update sys_user set username = ? where user_id = ? * c.w.m.sys.dao.SysUserDao.updateNameById : ==&gt; Parameters: admin(String), 1(Integer) * c.w.m.sys.dao.SysUserDao.updateNameById : ==&gt; Parameters: admin(String), 1(Integer) * @throws SQLException */@Testpublic void batchExecutor() throws SQLException &#123; BatchExecutor reuseExecutor = new BatchExecutor(configuration, jdbcTransaction); MappedStatement mappedStatement = configuration.getMappedStatement(&quot;com.wgf.modules.sys.dao.SysUserDao.updateNameById&quot;); Map&lt;String, Object&gt; param = new HashMap&lt;&gt;(); param.put(&quot;arg0&quot;, 1); param.put(&quot;arg1&quot;, &quot;admin&quot;); reuseExecutor.doUpdate(mappedStatement, param); reuseExecutor.doUpdate(mappedStatement, param); reuseExecutor.flushStatements(); //刷新声明才会批量执行&#125; CachingExecutor ​ 查看Executor 的子类还有一个CachingExecutor,这是用于处理二级缓存的。为什么不把它和一级缓存一起处理呢？因为二级缓存和一级缓存相对独立的逻辑，而且二级缓存可以通过参数控制关闭，而一级缓存是不可以的。综上原因把二级缓存单独抽出来处理。抽取的方式采用了装饰者设计模式，即在CachingExecutor 对原有的执行器进行包装，处理完二级缓存逻辑之后，把SQL执行相关的逻辑交给实至的Executor处理 12345678910111213141516171819202122232425262728/** * 缓存执行器 * * 第一次查询二级缓存命中率为 0 * com.wgf.modules.sys.dao.SysUserDao : Cache Hit Ratio [com.wgf.modules.sys.dao.SysUserDao]: 0.0 * c.w.m.s.dao.SysUserDao.queryByUserName : ==&gt; Preparing: select * from sys_user where username = ? * c.w.m.s.dao.SysUserDao.queryByUserName : ==&gt; Parameters: admin(String) * c.w.m.s.dao.SysUserDao.queryByUserName : &lt;== Total: 1 * * 第二次查询二级缓存命中率为 0.5 说明使用了缓存 * com.wgf.modules.sys.dao.SysUserDao : Cache Hit Ratio [com.wgf.modules.sys.dao.SysUserDao]: 0.5 * @throws SQLException */ @Test public void cachingExecutor() throws SQLException &#123; SimpleExecutor simpleExecutor = new SimpleExecutor(configuration, jdbcTransaction); // 装饰者模式，传入包装的Executor CachingExecutor cachingExecutor = new CachingExecutor(simpleExecutor); MappedStatement mappedStatement = configuration.getMappedStatement(&quot;com.wgf.modules.sys.dao.SysUserDao.queryByUserName&quot;); // 相同的sql 会执两次 List&lt;Object&gt; objects = cachingExecutor.query(mappedStatement, &quot;admin&quot;, RowBounds.DEFAULT, Executor.NO_RESULT_HANDLER); // 一级缓存和二级缓存不同，二级缓存是线程共享的，必须是事务提交后才刷新到缓存中去 // 执行顺序 二级缓存 &gt; 一级缓存 cachingExecutor.commit(true); objects = cachingExecutor.query(mappedStatement, &quot;admin&quot;, RowBounds.DEFAULT, Executor.NO_RESULT_HANDLER); System.out.println(objects); &#125; mybatis 开启二级缓存 配置文件 mybatis-config.xml 1234&lt;settings&gt; &lt;!--显示的开启全局缓存 (默认开启二级缓存)--&gt; &lt;setting name=&quot;cacheEnabled&quot; value=&quot;true&quot;/&gt;&lt;/settings&gt; 在 Mapper.xml 文件中添加cache标签 在要使用二级缓存的Mapper.xml文件中添加cache标签 123456&lt;cache eviction=&quot;FIFO&quot; flushInterval=&quot;60000&quot; size=&quot;512&quot; readOnly=&quot;true&quot;/&gt;eviction：清除策略为FIFO缓存，先进先出原则，默认的清除策略是 LRUflushInterval：属性可以被设置为任意的正整数，设置的值应该是一个以毫秒为单位的合理时间量size：最多可以存储结果对象或列表的引用数readOnly：只读属性，可以被设置为 true 或 false。 MappedStatement ​ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152/** * 表示一个 SQL 节点（SELECT \\ UPDATE \\ DELETE \\ INSERT） */public final class MappedStatement &#123; private String resource; private Configuration configuration; //节点中的id属性加要命名空间 private String id; private Integer fetchSize; //SQL超时时间 private Integer timeout; // Statement的类型，STATEMENT/PREPARE/CALLABLE-对应SQL执行的几种方式 private StatementType statementType; // 结果集类型， 是一个枚举类型 private ResultSetType resultSetType; // SqlSource 对象， 对应一条 SQL private SqlSource sqlSource; // 缓存 private Cache cache; // 参数 private ParameterMap parameterMap; // 结果集 private List&lt;ResultMap&gt; resultMaps; // 刷新缓存 private boolean flushCacheRequired; // 是否使用缓存 private boolean useCache; private boolean resultOrdered; private SqlCommandType sqlCommandType; //和SELECTKEY标签有关 private KeyGenerator keyGenerator; private String[] keyProperties; private String[] keyColumns; // 是否有嵌套的结果集 private boolean hasNestedResultMaps; private String databaseId; private Log statementLog; private LanguageDriver lang; private String[] resultSets; MappedStatement() &#123; // constructor disabled &#125; /** * 静态内部类， 又是建造者模式 */ public static class Builder &#123; private MappedStatement mappedStatement = new MappedStatement(); ... MappedStatement维护了一条&lt;select|update|delete|insert&gt;节点的封装，也就是说一个Mapper类的方法会产生一个MappedStatement对象，它存储在Configuration的mappedStatementsmap存储，key为 Mapper全类名.方法名 StatementHandler ​ 对JDBC的Statement进行封装,负责操作 Statement 对象与数据库进行交流，在工作时还会使用 ParameterHandler 和 ResultSetHandler 对参数进行映射，对结果进行实体类的绑定 ​ Mapper XML 配置文件中可以手动指定StatementHandler 12&lt;select id=&quot;page&quot; resultMap=&quot;voMap&quot; statementType=&quot;PREPARED&quot;&gt; 取值 STATEMENT,PREPARED,CALLABLE BaseStatementHandler SimpleStatementHandler ​ 对JDBC的StatementImpl进行封装,添加返回结果映射到java实体,不支持SQL预编译 PreparedStatementHandler ​ 支持通过占位符实现SQL预编译的处理器,可以防止SQL注入,默认使用这个处理器 CallableStatementHandler ​ 支持执行存储过程的StatementHandler ParameterHandler ResultSetHandler TypeHandler SqlSource BoundSql 缓存体系 mybatis的缓存体系是框架内部实现的，和数据库没有任何关系，myBatis中存在两个缓存，一级缓存和二级缓存 一级缓存 ​ 也叫做会话级缓存，生命周期仅存在于当前会话，不可以直接关关闭。但可以通过flushCache和是 localCacheScope对其做相应控制，BaseExecutore 实现了一级缓存，一级缓存是sqlSession级别的缓存，线程不共享，默认开启，会话关闭一级缓存失效 二级缓存 CachingExecutor实现了二级缓存，二级缓存是sqlSessionFactory级别的缓存，线程共享 如果开启了二级缓存，执行顺序二级缓存 &gt; 一级缓存 &gt; 数据库获取 一级缓存 由 BaseExecutore 内部的 PerpetualCache 对象存储，PerpetualCache 内部使用map存储 缓存key -&gt; 数据的映射关系 缓存命中参数 SQL语句与参数相同 同一个会话 (sqlSession) 相同的MapperStatement ID RowBounds行范围相同 (分页对象) 触发清空缓存 手动调用clearCache 1sqlSession.clearCache(); 执行事务 提交回滚 执行update (update标签里面是select语句也会) 配置flushCache=true 1@Options(flushCache = Options.FlushCachePolicy.TRUE) 修改缓存作用域为Statement 1&lt;setting name=&quot;localCacheScope&quot; value=&quot;STATEMENT&quot;/&gt; 一级缓存失效 在使用Mybatis生成的Mapper代理类执行相关的sql操作时，如果多个sql操作不在同一个事务内部，则无法使用一级缓存（缓存失效），原因是不在一个事务内，每次调用Mapper代理类操作数据库都会获取新的会话 缓存失效代码 12345678910111213141516171819/** * Mapper 缓存失效， 走两次查询 * c.w.m.sys.dao.SysUserDao.selectById : ==&gt; Preparing: SELECT user_id,username,password,salt,email,mobile,status,create_user_id,create_time FROM sys_user WHERE user_id=? * c.w.m.sys.dao.SysUserDao.selectById : ==&gt; Parameters: 1(Integer) * c.w.m.sys.dao.SysUserDao.selectById : &lt;== Total: 0 * c.w.m.sys.dao.SysUserDao.selectById : ==&gt; Preparing: SELECT user_id,username,password,salt,email,mobile,status,create_user_id,create_time FROM sys_user WHERE user_id=? * c.w.m.sys.dao.SysUserDao.selectById : ==&gt; Parameters: 1(Integer) * c.w.m.sys.dao.SysUserDao.selectById : &lt;== Total: 0 */@Testpublic void invalidCache() &#123; // sysUserDao 自动注入获得 // 调用获取新的SqlSession sysUserDao.selectById(1); // 调用获取新的SqlSession sysUserDao.selectById(1);&#125; 缓存生效 12345678910111213/** * 使用事务后一级缓存生效 * o.s.t.c.transaction.TransactionContext : Began transaction (1) for test context [DefaultTestContext@142269f2 testClass = ExecutorTest, testInstance = com.wgf.ExecutorTest@248e31a1, testMethod = tran@ExecutorTest, testException = [null], merge * c.w.m.sys.dao.SysUserDao.selectById : ==&gt; Preparing: SELECT user_id,username,password,salt,email,mobile,status,create_user_id,create_time FROM sys_user WHERE user_id=? * c.w.m.sys.dao.SysUserDao.selectById : ==&gt; Parameters: 1(Integer) * c.w.m.sys.dao.SysUserDao.selectById : &lt;== Total: 0 */ @Test @Transactional(readOnly = true) public void tran() &#123; sysUserDao.selectById(1); sysUserDao.selectById(1); &#125; 缓存失效原因 为了对事务的支持，Mybatis的Spring模块对SqlSession进行了封装，通过SqlSessionTemplae ，使得如果不在一个事务内调用Mapper，每次都会重新构建一个SqlSession，具体参见SqlSessionInterceptor ，解决的方法是让多个操作在一个事务内部就可以共享同一个SqlSession，这样就能使用一级缓存 二级缓存 ​ 经典的装饰器加责任链模式 这样设计有以下优点： 职责单一：各个节点只负责自己的逻辑，不需要关心其它节点。 扩展性强：可根据需要扩展节点、删除节点，还可以调换顺序保证灵活性。 松耦合：各节点之间不没强制依赖其它节点。而是通过顶层的Cache接口进行间接依赖。 二级缓存的使用 启用 xml 1&lt;cache eviction=&quot;FIFO&quot; flushInterval=&quot;60000&quot; size=&quot;512&quot; readOnly=&quot;true&quot;/&gt; 注解 123456类上使用@CacheNamespace Mapper开启二级缓存@CacheNamespaceRef 引入其他Mapper的二级缓存方法上使用的@Options 设置缓存大小，过期时间等 XML配置的二级缓存和java注解配置的缓存不能同时存在 二级缓存的作用范围： NameSpace 使用代码 先开启二级缓存 1234567@Mapper@CacheNamespacepublic interface SysUserDao extends BaseMapper&lt;SysUserEntity&gt; &#123; @Select(&quot;select * from sys_user where username = #&#123;arg0&#125;&#125;&quot;) public SysUserEntity selectUsername(String username);&#125; 二级缓存为什么要提交才生效 ​ 二级缓存体系中存在一个事务缓存管理器 TransactionalCacheManager和事务缓存 TransactionalCach，缓存暂存区的意义在于当前事务执行了update但是还没提交事务（事务可能回滚），此时由缓存区先存着，等事务真正提交再把缓存区数据刷进二级缓存能够防止二级缓存的脏读 二级缓存生效场景 提交事务 关闭SqlSession 二级缓存流程 SqlSessionFactoryBean Mybatis和Spring融合 其他问题 什么是Mybatis Mybatis是一个半自动ORM（对象关系映射）框架，它内部封装了JDBC，面向Sql开发，不需要花费精力去处理加载驱动、创建连接、创建statement等繁杂的过程 MyBatis 可以使用 XML 或注解来配置和映射实体和表的映射关系，避免JDBC手动设置参数和获取结果的复杂性 提供XML和注解的方式将Sql和业务代码剥离，提供访问数据的DAO层 总结：半自动ORM， 面向Sql开发，对JDBC二次封装，屏蔽JDBC复杂性减少代码量，提供动态Sql标签 Mybaits的优点 面向Sql编程，提供XML和注解方式将Sql和代码玻璃，Sql便于统一管理和复用 与JDBC相比，减少了获取连接，设置参数，获取结果等最少50%代码量 能很好兼容常见的关系型数据库 提供映射标签，支持对象与数据库的ORM映射 提供DAO层访问数据库 提供动态SQL标签，支持灵活实现SQL和SQL复用 总结：面向SQL编程，与JDBC比较，数据库兼容性，提供DAO层，映射标签，动态SQL标签 Mybatis的缺点 半自动ORM框架，面向Sql开发，Sql开发量大 SQL语句依赖于数据库，导致数据库移植性差，不能随意更换数据库 总结：SQL开发量大，数据库可移植性差 MyBatis框架适用场合 MyBatis专注于SQL本身，是一个足够灵活的DAO层解决方案 对性能要求高，并且业务数据复杂的系统，比如互联网，ERP等 总结：灵活的DAO层解决方案，性能高 MyBatis与Hibernate有哪些不同 相同点 都是对JDBC进行二次封装，提供DAO层解决方案 不同点 mybatis是一个半自动化的ORM框架，配置的是java对象与SQL语句执行结果的映射，多表关联配置简单 Hibernate是个全自动化的ORM框架，配置Java对象与数据库表的对应关系 ，多表关联配置复杂 Hibernate可以使用HQL面向对象编程 Hibernate对比MySQL数据库可移植性更强，底层采用HQL屏蔽数据库差异性 总结：半自动/自动ORM，HQL，数据库移植性 ORM是什么 ORM（Object Relational Mapping） ,对象关系映射 。ORM是用于描述对象与数据库之间的映射元数据，将程序中的对自动的持久化到关系型数据库中 总结：对象与数据库的映射，对象持久化到数据库 Mybatis为什么不是全自动ORM 全自动ORM是将对象属性与数据库表字段进行一一对应，并将表的关联关系具体体现在实体关系中 Mybatsi使用映射标签将对象字段与SQL执行结果映射而不是和表映射，需要自己配置映射关系 总结：参数全自动ORM，SQL执行结果映射 传统JDBC开发存在什么问题 频繁创建数据量连接对象，自己管理事务，代码量大，影响系统性能 SQL语句定义，参数填充，结果获取存在硬编码，代码不够灵活 结果集处理存在重复代码，可维护性差 总结：连接难维护，SQL硬编码，参数设置死板，处理结果集代码冗余 MyBatis是如何解决JDBC不足之处 数据库链接创建 ,Mybatis允许配置连接池来管理数据库连接 SQL语句剥离，使用XML和注解将SQL语句和代码剥离，统一管理 动态SQL，提供动态标签解决JDBC参数设置不灵活缺点 结果映射，通过灵活配置将结果集映射到实体上 对JDBC进行封装，屏蔽底层复杂性 总结：支持连接池管理连接，SQL统一管理，动态SQL标签，结果映射，屏蔽JDBC复杂性 MyBatis编程步骤 创建SqlSessionFactory 通过SqlSessionFactory创建SqlSession 通过sqlsession执行数据库操作 调用session.commit()提交事务 调用session.close()关闭会话 **总结：**Mybatis通过门面设计模式封装了SqlSession会话层，对外屏蔽了Executor调用的复杂性 请说说MyBatis的工作原理 读取 MyBatis 配置文件：mybatis-config.xml 为 MyBatis 的全局配置文件，配置了 MyBatis 的运行环境等信息，例如数据库连接信息 加载映射文件。映射文件即 SQL 映射文件，该文件中配置了操作数据库的 SQL 语句，需要在 MyBatis 配置文件 mybatis-config.xml 中加载。mybatis-config.xml 文件可以加载多个映射文件，每个文件对应数据库中的一张表 构造会话工厂：通过 MyBatis 的环境等配置信息构建会话工厂 SqlSessionFactory 创建会话对象：由会话工厂创建 SqlSession 对象，该对象中包含了执行 SQL 语句的所有方法 Executor 执行器：MyBatis 底层定义了一个 Executor 接口来操作数据库，它将根据 SqlSession 传递的参数动态地生成需要执行的 SQL 语句，同时负责查询缓存的维护 MappedStatement 对象：在 Executor 接口的执行方法中有一个 MappedStatement 类型的参数，该参数是对映射信息的封装，用于存储要映射的 SQL 语句的 id、参数等信息 输入参数映射：输入参数类型可以是 Map、List 等集合类型，也可以是基本数据类型和 POJO 类型。输入参数映射过程类似于 JDBC 对 preparedStatement 对象设置参数的过程 输出结果映射：输出结果类型可以是 Map、 List 等集合类型，也可以是基本数据类型和 POJO 类型。输出结果映射过程类似于 JDBC 对结果集的解析过程 MyBatis的功能架构 API接口层：提供给外部使用的接口API，开发人员通过这些本地API来操纵数据库。接口层一接收到调用请求就会调用数据处理层来完成具体的数据处理 数据处理层：负责具体的SQL查找、SQL解析、SQL执行和执行结果映射处理等。它主要的目的是根据调用的请求完成一次数据库操作 基础支撑层：负责最基础的功能支撑，包括连接管理、事务管理、配置加载和缓存处理，这些都是共用的东西，将他们抽取出来作为最基础的组件。为上层的数据处理层提供最基础的支撑 为什么需要预编译 **定义：**SQL 预编译指的是数据库驱动在发送 SQL 语句和参数给数据库之前对 SQL 语句进行编译，这样 数据库执行 SQL 时，就不需要重新编译 为什么 防止SQL注入 JDBC 中使用对象 PreparedStatement 来抽象预编译语句，使用预编译。预编译阶段可以优化 SQL 的执行。预编译之后的 SQL 多数情况下可以直接执行，数据库不需要再次编译，越复杂的SQL，编译的复杂度将越大，预编译阶段可以合并多次操作为一个操作。同时预编译语句对象可以重复利用 **** 总结：防止SQL注入，减少数据库编译工作 #{}和${}的区别是什么 #{}是预编译处理 ${}是字符串替换 体类中的属性名和表中的字段名不一样怎么办 通过在查询的sql语句中定义字段名的别名，让字段名的别名和实体类的属性名一致 通过 来映射字段名和实体类属性名的一一对应的关系 总结：SQL字段起别名，&lt;resultMap&gt;br&gt;添加映射 模糊查询like语句该怎么写 在代码中添加通配符 1String name = &quot;%wgf%&quot; 在sql语句中拼接通配符，会引起sql注入 1select * from foo where bar like &#x27;%$&#123;bar&#125;%&#x27; 使用 concat 函数，推荐 1select * from where name like concat(&#x27;%&#x27;, #&#123;name&#125;) 总结：java字符串拼接，$字符串拼接，concat函数 Mybatis都有哪些Executor执行器 BaseExecutor: 将Executor共性抽取出一个父类，提供一级缓存，事务，数据库基本操作 CachingExecutor: 提供二级缓存功能 SimpleExecutor: 默认执行器，每次执行query或update都创建一个新的Statement对象，用完立即关闭Statement对象 ReuseExecutor: 使用Sql作为Key,将Statement对象缓存起来，重复使用Statement减少SQL预编译次数（SqlSession范围重用） BatchExecutor: 执行update（没有select，JDBC批处理不支持select），将所有sql都添加到批处理中（addBatch()），等待统一执行（executeBatch()），会合并sql语句 Mybatis中如何指定使用哪一种Executor 在Mybatis配置文件中，在设置（settings）可以指定默认的ExecutorType执行器类型 在yml文件中配置 12mybatis: executor-type: batch 在调用DefaultSqlSessionFactory的openSession方法传入ExecutorType参数 总结：mybatis XML setting配置，yml 文件配置， DefaultSqlSessionFactory 参数传入 Mapper接口里的方法能重载吗 Mapper接口允许多个方法重载，但是映射只能有一个，否则报错 Mybatis源码Configuration类中获取MappedStatement信息是通过Mapper全类名加方法名作为key获取的，底层数据结构是一个Map，因此不能存在多个映射 总结：允许重载，只能有一个映射 Mybatis是如何进行分页的，分页插件原理 分页 ​ Mybatis 使用 RowBounds 对象进行分页，也可以直接编写 sql 实现分页，也可以使用Mybatis 的分页插件 插件原理 ​ 实现 Mybatis 提供的接口，实现自定义插件，在插件的拦截方法内拦截待执行的 sql，然后重写 sql 总结： RowBounds，编写SQL，使用分页插件，自定义插件，拦截sql且重写 Mybatis是如何将sql执行结果封装为目标对象并返回的？都有哪些映射形式？ 第一种是使用 标签，逐一定义数据库列名和对象属性名之间的映射关系 第二种是使用sql列的别名功能，将列的别名书写为对象属性名 有了列名与属性名的映射关系后，Mybatis通过反射创建对象，同时使用反射给对象的属性逐一赋值并返回，那些找不到映射关系的属性，是无法完成赋值的 总结： &lt;resultMap&gt;，sql别名和字段名对应，反射创建对象并赋值 如何获取自动生成的(主)键值 insert 方法总是返回一个int值 ，这个值代表的是插入的行数 update 方法返回一个int值，是受影响行数 xml中设置 useGeneratedKeys=“true” keyProperty=“id” 总结：useGeneratedKeys，keyProperty 在mapper中如何传递多个参数 下表占位符 #{arg0}, #{arg1} @Param注解 使用Map传参 使用实体传参 总结：#&#123;arg0&#125;，@Param Mybatis是否支持延迟加载？原理是什么 Mybatis仅支持association关联对象和collection关联集合对象的延迟加载 ,association指的就是一对一，collection指的就是一对多查询,可以配置是否启用延迟加载lazyLoadingEnabled=true|false 原理：它的原理是，使用CGLIB创建目标对象的代理对象，当调用目标方法时，进入拦截器方法，比如调用a.getB().getName()，拦截器invoke()方法发现a.getB()是null值，那么就会单独发送事先保存好的查询关联B对象的sql，把B查询上来，然后调用a.setB(b)，于是a的对象b属性就有值了，接着完成a.getB().getName()方法的调用。这就是延迟加载的基本原理 总结：关联关系允许懒加载 Mybatis动态sql有什么用 Mybatis动态sql可以在Xml映射文件内，以标签的形式编写动态sql，执行原理是根据表达式的值完成逻辑判断并动态拼接sql的功能 通过动态sql标签灵活装配SQL Mybatis提供了9种动态sql标签： trim| where| set| foreach| if|choose| when| otherwise| bind Mybatis的Xml映射文件中，不同的Xml映射文件，id是否可以重复 不同的Xml映射文件，如果配置了namespace，那么id可以重复；如果没有配置namespace，那么id不能重复 原因是Mybatis的配置中MapperStatement是根据 namespace+id 作为作为Map &lt;String,MapperStatement&gt;的key使用的 ，保证namespace+id不重复就行 总结：MapperStatement -&gt; namespace + id 一对一、一对多的关联查询 一对一使用associate，一个类根据关联字段对应着一个类，实体类里声明另一个实体类 一对多使用collection，一个类根据关联字段对应着多个类，实体类里声明另一个实体类的List 多对一使用associate，多个类根据关联字段对应着一个类，实体类里声明另一个实体类 总结：associate，collection Mybatis的一级、二级缓存 基于 PerpetualCache 的 HashMap 本地缓存，其存储作用域为 Session，当 Session flush 或 close 之后，该 Session 中的所有 Cache 就将清空，默认打开一级缓存 二级缓存与一级缓存其机制相同，默认也是采用 PerpetualCache，HashMap 存储，不同在于其存储作用域为 Mapper(Namespace)，并且可自定义存储源 总结：一级缓存会话独享，二级缓存会话共享，一级缓存是SqlSession级别缓存，二级缓存是SqlSessionFactory级别缓存 开发一个Mybatis插件 实现 Mybatis 的 Interceptor 接口并复写 intercept()方法，然后在给插件编写注解，指定 要拦截哪一个接口的哪些方法即可，在配置文件中配置插件 总结：实现 Interceptor接口 ，重写 intercept方法，注解配置拦截方法，配置文件配置插件","categories":[],"tags":[]},{"title":"jvm","slug":"jvm","date":"2022-03-29T14:19:52.000Z","updated":"2023-09-22T06:30:40.421Z","comments":true,"path":"2022/03/29/jvm/","link":"","permalink":"https://wugengfeng.cn/2022/03/29/jvm/","excerpt":"","text":"JVM 内存区域 线程共享区域 堆（Heap） 堆是JVM中最大的一块内存区域，用于存放对象实例，几乎所有的对象实例都在堆中分配。堆是线程共享的，每个线程都可以访问堆中的对象 堆又分为新生代和老年代，新生代又分为Eden区、Survivor区（一般有两个，称为From区和To区） 方法区（元空间） 方法区是用于存放类的元信息、常量池、静态变量等数据。在JDK1.8及以前的版本，方法区是位于永久代的，而在JDK1.8及以后的版本，方法区被移动到了元空间 方法区主要存储以下内容： 类的Class对象：类加载器加载类后，会在方法区中为该类创建一个Class对象。 方法信息：存放类中声明的每个方法的信息，包括方法名、返回值类型、参数类型等。 字符串常量池：用于存放字符串常量。 静态变量：存放类的静态变量。 非线程共享区域 虚拟机栈 栈帧 Java调用一个方法就是压入一个栈帧的过程 局部变量表 ​ 用于存放方法参数和方法内部定义的变量（局部变量必须赋予初始值） 操作数栈 ​ 虚拟机计算的临时存储区域，JVM虚拟机对数据的操作是通过指定的压栈和弹栈完成的 动态链接 ​ 指定了栈帧所属方法的引用 方法出口 ​ 指定了方法执行完毕或者异常后需要被调用的位置，程序才能继续执行 本地方法栈 ​ java跨语言调用 native方法（本地C类库方法）的相关信息 程序计数器 ​ 记录字节码执行位置。多线程环境（避免线程上下文切换忘记之前代码执行到哪里） 方法区详解 JDK1.7 及之后版本的 JVM 已经将运行时常量池从方法区中移了出来，在 Java 堆（Heap）中开辟了一块区域存放运行时常量池。符号引用转移到直接内存(native heap) 元空间：JDK1.8开始，取消了方法区，取而代之的是位于直接内存的元空间（metaSpace） 字符串常量池 静态变量 符号引用：符号引用是一个字符串，它给出了被引用内容的描述，类的引用必须是全类名组成的，符号描述能精准定位被引用的内容（类，方法，字符） 作用：帮助JVM快速定位加载类 12类：Java.lang.String对应描述符：Ljava/lang/String; 方法信息 类中声明的每一个方法的信息，包括方法名、返回值类型、参数类型、修饰符、异常、方法的字节码 类的Class对象 类加载将二进制流转为方法区运行时数据会将加载类的Class对象分配到方法区 元空间 Java7之前，方法区位于永久代，永久代和堆虽然是内存隔离的。但是本质上使用的还是JVM内存，如果JVM内存设置过小，永久代就会有内存溢出风险，因此Java8之后废弃了永久代，使用了元空间，元空间不再使用JVM内存，而是使用本地内存，减少了OOM风险 类加载器 类加载器是Java虚拟机（JVM）的一部分，用于将类的字节码加载到内存中，并在运行时创建类的对象 Bootstrap ClassLoader ​ 启动类加载器：最顶层的加载类，主要加载核心类库，也就是我们环境变量下面%JRE_HOME%\\lib下的rt.jar、resources.jar、charsets.jar和class等 Extention ClassLoader ​ 加载目录%JRE_HOME%\\lib\\ext目录下的jar包和class文件 Application ClassLoader ​ 加载当前应用的classpath的所有类 自定义类加载器 定义一个类，继承 ClassLoader 重写findClass 方法 使用defineClass() 实例化Class对象 12345678910111213141516171819202122public class MyClassLoad extends ClassLoader &#123; @Override protected Class&lt;?&gt; findClass(String name) throws ClassNotFoundException &#123; Class&lt;?&gt; clazz = null; if (name.equals(&quot;com.example.luckdraw.test.TestObject&quot;)) &#123; try &#123; InputStream inputStream = new FileInputStream(&quot;C:\\\\Desktop\\\\TestObject.class&quot;); byte[] buff = new byte[inputStream.available()]; inputStream.read(buff); clazz = defineClass(name, buff, 0, buff.length); &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; else &#123; clazz = super.findClass(name); &#125; return clazz; &#125;&#125; 双亲委派原则 委派：当类加载器收到加载任务会先委派给父加载器进行加载（自下向顶为委派） 加载：当父类加载不到此类则丢给子类加载器加载（自顶向下加载） 作用：保证核心类被正确加载（java源码级别的类） 好处：因为类加载后会被缓存，所以保证类的一致性和避免类的冲突 双亲委派原则只是一种规范或约定，并不是Java虚拟机的强制要求。在特定的情况下，用户也可以自定义类加载器，来实现自己的加载逻辑 破坏双亲委派 JDBC 根据类加载机制，当被加载的类引用了另外一个类的时候，虚拟机就会使用加载第一个类的类加载器加载被引用的类 JDBC驱动程序通常是由第三方提供的，由Application ClassLoader加载，而且这些驱动程序需要访问JDBC API的内部实现 JDBC API是Java的核心类，由Bootstrap ClassLoader加载，因此需要破坏双亲委派 Tomcat 破坏双亲委派：每个Tomcat的WebApp ClassLoader加载自己的目录下的class文件，不会传递给父类加载器 对于各个 webapp中的 class和 lib，需要相互隔离，不能出现一个应用中加载的类库会影响另一个应用的情况 热部署 类加载过程 加载 将类的字节码文件加载到内存，并方法区对应生成一个Class对象 链接 对加载的类进行校验 验证 验证二进制流是否符合规范，是否存在安全问题 准备 为静态变量开辟内存空间并赋予初始值 解析 将类的符号引用改为直接引用 符号引用：以一组符号来描述所引用的目标，符号可以是任何形式的字面量 12类：Java.lang.String对应描述符：Ljava/lang/String; 直接引用：直接指向目标的指针 初始化 ​ 当一个类被主动使用时，Java虚拟机就会对其初始化 只对static修饰的变量或静态代码块进行初始化 如果存在父类，则优先初始化父类 如果同时包含多个静态变量和静态代码块，则按照自上而下的顺序依次执行 类加载时机 类没有被主动使用，没有被加载 1234567891011121314151617public class Constant &#123; public static int a; static &#123; System.out.println(&quot;a=10&quot;); a = 10; &#125;&#125;public class Test &#123; public static void main(String[] args) &#123; // 类被加载但是没有被主动使用，所以没有执行初始化 System.out.println(Constant.class); &#125;&#125;输出 class com.example.demo.model.Constant 主动使用类，JVM会调用类的构造器收集 static修饰的变量和语句进行初始化 12345678910111213141516171819public class Constant &#123; public static int a; static &#123; System.out.println(&quot;a=10&quot;); a = 10; &#125;&#125;public class Test &#123; public static void main(String[] args) &#123; // 主动使用类，JVM会调用类的构造器收集 static修饰的变量和语句进行初始化 System.out.println(Constant.a); &#125;&#125;输出a=1010 加载时机 创建某个类的实例 访问某个类的静态方法 使用某个类的静态字段 初始化某个类的子类 使用反射机制访问类 程序的启动类（包含main方法） 对象创建过程 使用 new 关键字或者反射机制 根据new的参数在 常量池中定位一个类的符号引用 如果没有找到这个类的符号引用，说明类没有被加载，则进行类的加载，解析和初始化 虚拟机为对象分配内存空间 指针碰撞（内存相对规整的情况，也就是垃圾回收器带内存整理压缩功能的） 空闲链表 对齐填充，对象的大小始终为8的整数倍，原因是便于内存寻址 将分配的内存初始化为0 设置对象头，包括分代年龄，对象的HashCode 调用对象的构造方法 对象结构 Head 头信息 hash值：对象的hashCode 分代年龄：默认为0，用于分代年龄计算 锁状态标识位：标记当前对象是否上锁 持有锁的线程：当前对象锁属于哪个线程所有 类型指针：标记当前对象属于哪个Class 对象实例数据 对齐填充：无意义，JVM要求对象大小必须是8字节的整数倍 ClassLoad和Class.forName ClassLoad只是将类加载到JVM Class.forName除了类加载还会执行静态代码块 Class.forName还可以对类进行实例化 指令重排序 在代码实际运行时，代码指令可能不按照代码语句顺序执行的。只要程序的结果和顺序性执行代码的结果一致，那么指令的执行顺序就可以不和代码顺序一致，这就是指令重排序 原因：现代处理器架构采用乱序执行方法，在条件允许的情况下，直接执行后面的指令，通过乱序执行技术提高处理器的执行效率 编译器优化的重排序。编译器在不改变单线程程序的语义前提下，可以重新安排语句的执行顺序 指令级并行的重排序。现在处理器采用了指令集并行技术，来将多条指令重叠执行。如果不存在依赖性，处理器可以改变语句对应的机器指令的执行顺序 内存系统的重排序。由于处理器使用缓存和读写缓冲区，这使得加载和存储操作看上去可能是在乱序执行 内存屏障 内存屏障（Memory Barrier）是指一组指令，用于控制处理器和内存之间的访问顺序和可见性 作用： 阻止屏障两侧的指令重排序 强制缓冲区数据刷新到主内存或强制从主内存加载最新数据到缓冲区 分类： 读屏障：在指令前插入Load Barrier，可以让高速缓存中的数据失效，强制从新从主内存加载数据 写屏障：在指令后插入Store Barrier，能让写入缓存中的最新数据更新写入主内存，让其他线程可见 volatile语义中的内存屏障 可见性：对一个volatile变量的写操作对其他线程的读操作是可见的，即写操作立即刷新到主内存中，读操作从主内存中获取最新的值。 有序性：对一个volatile变量的写操作不会与之前的写操作发生重排序，也不会与之后的写操作发生重排序。保证了volatile写操作的有序性。 禁止指令重排序：volatile变量的读写操作都会插入内存屏障，禁止指令重排序。 具体来说，volatile语义的内存屏障包括以下几种屏障： 在每个volatile写操作前插入StoreStore屏障，保证在写操作前的所有写操作对其他线程/CPU可见。 在每个volatile写操作后插入StoreLoad屏障，保证volatile写操作对其他线程/CPU的读操作可见。 在每个volatile读操作前插入LoadLoad屏障，保证volatile读操作之前的读操作完成。 在每个volatile读操作后插入LoadStore屏障，保证volatile读操作后的所有读操作都能看到volatile读操作之前的写操作。 final语义中的内存屏障 防止指令重排序 对于final域，编译器和CPU会遵循两个排序规则： 新建对象过程中，构造体中对final域的初始化写入和这个对象赋值给其他引用变量，这两个操作不能重排序；（废话嘛） 初次读包含final域的对象引用和读取这个final域，这两个操作不能重排序；（晦涩，意思就是先赋值引用，再调用final值） 总之上面规则的意思可以这样理解，必需保证一个对象的所有final域被写入完毕后才能引用和读取。这也是内存屏障的起的作用： 写final域：在编译器写final域完毕，构造体结束之前，会插入一个StoreStore屏障，保证前面的对final写入对其他线程/CPU可见，并阻止重排序。 读final域：在上述规则2中，两步操作不能重排序的机理就是在读final域前插入了LoadLoad屏障。 X86处理器中，由于CPU不会对写-写操作进行重排序，所以StoreStore屏障会被省略；而X86也不会对逻辑上有先后依赖关系的操作进行重排序，所以LoadLoad也会变省略 @Contended @sun.misc.Contended 是 Java 8 新增的一个注解，对某字段加上该注解则表示该字段会单独占用一个 缓存行（Cache Line） 这里的缓存行是指 CPU 缓存的存储单元，常见的缓存行大小为 64 字节 注意：JVM 添加 -XX:-RestrictContended 参数后 @sun.misc.Contended 注解才有效 单独使用一个缓存行避免伪共享 为了提高读取速度，每个 CPU 有自己的缓存，CPU 读取数据后会存到自己的缓存里。而且为了节省空间，一个缓存行可能存储着多个变量，即 伪共享。但是这对于共享变量，会造成性能问题： 当一个 CPU 要修改某共享变量 A 时会先 锁定 自己缓存里 A 所在的缓存行，并且把其他 CPU 缓存上相关的缓存行设置为 无效 但如果被锁定或失效的缓存行里，还存储了其他不相干的变量 B，其他线程此时就访问不了 B，或者由于缓存行失效需要重新从内存中读取加载到缓存里，这就造成了 开销。所以让共享变量 A 单独使用一个缓存行就不会影响到其他线程的访问。 适用场景：主要适用于频繁写的共享数据上。如果不是频繁写的数据，那么 CPU 缓存行被锁的几率就不多，所以没必要使用了，否则不仅占空间还会浪费 CPU 访问操作数据的时间。 synchronized synchronized 用的锁标志是存在Java对象头里的 底层实现 在Java中，每个对象都关联一个 monitor（监视器锁，锁的粒度是对象，如果是静态资源则对应Class对象），在HotSpot虚拟机中它是由ObjectMonitor实现的（C++实现）。当线程进入synchronized代码块时，它会尝试获取对象的监视器锁。如果该锁未被其他线程占用，则该线程会立即获得该锁，并继续执行synchronized代码块。如果该锁已经被其他线程占用，则当前线程会进入该对象的等待队列中，等待其他线程释放该锁。当其他线程释放该锁时，等待队列中的线程会被唤醒，然后它们会再次尝试获取该锁。 123456789101112131415161718ObjectMonitor() &#123; _header = NULL; // 用于存储对象监视器的头部信息，初始化为NULL _count = 0; // 线程获取锁的次数 _waiters = 0, // 当前等待获取该对象监视器的线程数，初始化为0 _recursions = 0; // 当前线程获取该对象监视器的重入次数，初始化为0 _object = NULL; // 被监视的对象，初始化为NULL _owner = NULL; // 指向持有ObjectMonitor对象的线程地址 _WaitSet = NULL; // 等待获取该对象监视器的线程组成的双向循环链表，初始化为NULL，表示没有等待线程 _WaitSetLock = 0 ; // 保护等待线程链表的自旋锁，初始化为0 _Responsible = NULL ; // 如果某个线程为了获取该对象监视器而阻塞，在唤醒时这个字段指向阻塞线程的Successor _succ = NULL ; // 当前线程阻塞时的Successor，初始化为NULL _cxq = NULL ; // 多线程竞争锁进入时的单向链表，初始化为NULL FreeNext = NULL ; // 用于释放已经不再需要的对象监视器的单向链表，初始化为NULL _EntryList = NULL ; // 等待被_owner线程唤醒时的线程节点组成的双向循环链表，初始化为NULL _SpinFreq = 0 ; // 用于自旋次数的计数，初始化为0 _SpinClock = 0 ; // 用于自旋的时钟，初始化为0 OwnerIsThread = 0 ; // 持有者是否为线程的标记，初始化为0 &#125; 加锁过程 当一个线程尝试进入一个synchronized块时，它会先尝试获取对象监视器的锁 如果锁没有被其他线程持有，那么当前线程立即获得锁，可以继续执行synchronized块中的代码 如果锁已经被其他线程持有，那么当前线程会进入阻塞状态，被放入锁的等待队列中（_WaitSet） 当一个线程释放锁时，ObjectMonitor会将等待队列中（_WaitSet）唤醒一个线程，让其获得锁 释放锁 线程执行完synchronized代码块中的代码，或者发生了异常导致代码块提前退出 执行一个monitor.exit指令，该指令会释放当前线程持有的锁 在释放锁之前，线程会将持有锁的计数减1，如果计数为0，则表示锁被完全释放；如果计数大于0，表示锁被重入了多次 线程释放锁后，JVM 会从等待队列中选择一个线程唤醒，让其获得锁 使用 Synchroinzed 用法 synchronized修饰实例方法 synchronized修饰静态方法 synchronized修饰实例方法的代码块 synchronized修饰静态方法的代码块 JDK1.6之后对 synchronized 做了下面三点优化，在之后的JDK版本里性能和 ReentrantLock 不相上下 锁升级 无锁 ：对象头开辟 25bit 的空间用来存储对象的 hashcode ，4bit 用于存放对象分代年龄，1bit 用来存放是否偏向锁的标识位，2bit 用来存放锁标识位为01 偏向锁： 在偏向锁中划分更细，还是开辟 25bit 的空间，其中23bit 用来存放线程ID，2bit 用来存放 Epoch，4bit 存放对象分代年龄，1bit 存放是否偏向锁标识， 0表示无锁，1表示偏向锁，锁的标识位还是01 轻量级锁：在轻量级锁中直接开辟 30bit 的空间存放指向栈中锁记录的指针，2bit 存放锁的标志位，其标志位为00 重量级锁： 在重量级锁中和轻量级锁一样，30bit 的空间用来存放指向重量级锁的指针，2bit 存放锁的标识位，为11 无锁 无锁（也称为乐观锁）是指当多个线程同时访问共享资源时，没有线程进行阻塞，通常是使用CAS机制实现的 偏向锁 当一个线程第一次访问共享资源时，JVM会将锁的对象头设置为偏向模式，并将线程ID记录在对象头中。在接下来的访问中，线程只需要检查对象头中的线程ID是否是自己，如果是，则无需进行锁的获取操作 轻量级锁（自旋锁） 轻量级锁是指当锁是偏向锁的时候，却被另外的线程所访问，此时偏向锁就会升级为轻量级锁，其他线程会通过自旋的形式尝试获取锁，线程不会阻塞，从而提高性能 轻量级锁的获取主要由两种情况 当关闭偏向锁功能时 -XX:-UseBiasedLocking=false 由于多个线程竞争偏向锁导致偏向锁升级为轻量级锁 当多个线程同时访问同一个共享资源时，JVM会将锁的对象头设置为轻量级锁模式，并尝试使用CAS + 自旋操作将对象头中的锁指针指向当前线程，如果CAS操作成功，则当前线程获得锁 重量级锁 重量级锁显然，此忙等是有限度的(自旋，有个计数器记录自旋次数，默认允许循环10次，可以通过虚拟机参数更改)。如果锁竞争情况严重，某个达到最大自旋次数的线程，会将轻量级锁升级为重量级锁，当后续线程尝试获取锁时，发现被占用的锁是重量级锁，则直接将自己挂起并加入等待队列，等待锁释放后被唤醒 重量级锁是指当有一个线程获取锁之后，其余所有等待获取该锁的线程都会处于阻塞状态 锁消除 锁消除是指虚拟机即时编译器（JIT）在运行时，对一些代码上要求同步，但是被检测到不可能存在共享数据竞争的锁进行消除。锁消除的主要判定依据来源于逃逸分析的数据支持，如果判断在一段代码中，堆上的所有数据都不会逃逸出去从而被其他线程访问到，那就可以把它们当做栈上数据对待，认为它们是线程私有的，同步加锁自然就无须进行 比如一个使用 synchronized 修饰的方法，它并没有访问共享资源，不存在并发问题，此时没有资源竞争自然就不需要加锁 12345678910111213141516171819202122public class Synchroinzed &#123; /** * 逃逸分析 * 1.没有共享数据竞争 * 2.没有返回值，方法体内创建的对象直接在栈上内存分配，其他线程无法访问到，不存在线程安全问题 */ public synchronized void test() &#123; System.out.println(Thread.currentThread().getName() + &quot;:&quot; + System.currentTimeMillis()); try &#123; TimeUnit.SECONDS.sleep(500); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; public static void main(String[] args) &#123; for (int i = 0; i &lt; 10; i++) &#123; new Thread(() -&gt; new Synchroinzed().test()).start(); &#125; &#125;&#125; 锁粗化 将多次连续的同步块合并为一次更大的同步块，从而减少锁竞争的次数，提高程序的执行效率 如果JVM检测到有连续的操作都是对同一对象的加锁，将会扩大加锁同步的范围（即锁粗化）到整个操作序列的外部 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354public class Synchroinzed &#123; public void test() &#123; System.out.println(Thread.currentThread().getName() + &quot;:&quot; + System.currentTimeMillis()); try &#123; TimeUnit.MILLISECONDS.sleep(50); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; public static void main(String[] args) &#123; Synchroinzed sync = new Synchroinzed(); // 创建10个线程 for (int i = 0; i &lt; 5; i++) &#123; new Thread(() -&gt; &#123; for (int j = 0; j &lt; 5; j++) &#123; // 连续对同一对象加锁解锁 synchronized (sync) &#123; sync.test(); &#125; &#125; &#125;).start(); &#125; &#125;&#125;线程编号输出相对规整，说明做了锁粗话，但还是建议放在循环体外面Thread-0:1650645031133Thread-0:1650645031183Thread-0:1650645031233Thread-0:1650645031283Thread-0:1650645031333Thread-4:1650645031383Thread-4:1650645031433Thread-4:1650645031483Thread-4:1650645031533Thread-4:1650645031583Thread-3:1650645031633Thread-3:1650645031683Thread-3:1650645031733Thread-3:1650645031783Thread-3:1650645031833Thread-2:1650645031883Thread-2:1650645031933Thread-2:1650645031983Thread-2:1650645032033Thread-2:1650645032083Thread-1:1650645032133Thread-1:1650645032183Thread-1:1650645032233Thread-1:1650645032283Thread-1:1650645032333 JIT 即时编译器 JIT编译器（Just-In-Time Compiler）是Java虚拟机（JVM）中的一种编译器，它的主要作用是将Java字节码（bytecode）实时编译为本地机器码（native code），以提高程序的执行效率。 JIT编译器的工作原理如下： 解释器解释字节码：当Java程序被执行时，JVM会先将字节码交给解释器进行解释执行。解释器将逐条解释执行字节码指令，将其转换为对应的机器指令并执行。这种方式的好处是可移植性强，适用于任何平台。 监控热点代码：JIT编译器会监控程序的执行情况，识别出频繁执行的热点代码。热点代码通常是被多次执行的方法或循环体。 编译热点代码：一旦识别出热点代码，JIT编译器会将其编译为本地机器码。与解释执行相比，本地机器码的执行速度更快。 优化编译：在编译过程中，JIT编译器会进行一些优化操作。例如，方法内联、循环展开、去除无用代码等。这些优化措施可以进一步提高程序的执行效率。 热点代码： 被多次调用的方法 被多次执行的循环体 缺点： 内存占用：JIT编译器需要将编译后的本地机器码保存在内存中，这会增加内存的占用。 编译时间：JIT编译器需要一定的时间来进行编译，这可能会导致程序的启动时间延长。 推荐文章 美团技术沙龙 GC (Garbage Collectors) 内存区域划分 Eden 基本上所有新建的对象都在这个区域 因为还存在大对象直接进入老年代和栈上内存分配情况 Eden:Survivor_from:Survivor_to 8:1:1 Survivor 被Minor GC 后移入的存活区, 分为Survivor_from和Survivor_to两个区域，Survivor区域在GC时采用复制算法进行对象淘汰，当对象多次GC（默认15次）后存在的仍然存活就会进入老年代 Eden:Survivor_from:Survivor_to 8:1:1 Old 老年代，长期存活的对象存放的区域，只有Full GC才会清理这个区域的对象 新生代:老年代 1：2 MinorGC、MajorGC 、FullGC和Mixed GC Minor GC：从年轻代（Eden和survivor区域）回收内存，这个区域的对象生存周期短，发生gc的次数比较频繁，回收速度比较快，一般采用复制算法 Major GC： 是清理老年代 Full GC：清理整个堆空间，包含年轻代，老年代，永久代（方法区和元空间）的垃圾回收，一般耗时比较长，因此必须降低FullGC的频率 老年代空间不足，Survivor晋升对象大小大于老年代剩余空间 Minor GC晋升到旧生代的平均大小大于老年代的剩余空间 Metaspace区内存达到阈值（20M） 达到收集器收集的阈值（90%） Mixed GC：对新生代和老年代同时进行垃圾回收，将新生代和部分老年代一起进行回收，以减少SWT的时间 stop the word 现象 GC线程执行垃圾对象回收而挂起所有工作线程，程序会出现卡顿现象 内存分配策略 优先分配到Eden 基本上几乎所有新建的对象都会被分配到 Eden区域，Eden区域每次发生GC都会被清除 大对象直接分配到老年代 12-XX:PretenureSizeThreshold=6M 指定多大的对象直接放进老年区生效的垃圾收集器 Serial,ParNew,G1 长期存活的对象分配到老年代 12默认是15次GC后进入老年代，可以有参数设置，也非必要15次Minor GC才能进入老年代，因为还有动态对象年龄分配-XX:MaxTenuringThreshold=10 动态对象年龄判断 1Survivor区中相同年龄的对象大小的总和大于Survivor空间的一半，大于或等于该年龄的对象可以直接进入老年代 空间分配担保 触发场景：Eden区域有对象要进入Survivor区域，Survivor区域满了 121.6后默认开启，不用手动设置老年代的连续空间大于新生代对象总大小或者历次新生代对象晋升老年代平均大小就会进行Minor GC（将survivor区域复制到老年代），否则将进行Full GC 逃逸分析与栈上分配 123-XX:+DoEscapeAnalysis 1.6后默认开启逃逸分析：对象仅在当前作用于下有效（成员变量会发生逃逸）为了提高GC的回收效率，对象实例的内存分配不一定必须存在于堆区中，还可采用堆外分配。而最常见的堆外分配就是采用逃逸分析筛选出未发生逃逸的对象，在栈帧中分配内存空间 判断对象是否死亡 堆中几乎放着所有的对象实例，对堆垃圾回收前的第一步就是要判断哪些对象已经死亡，当一个对象不再被任何存活的对象继续引用的时候，就可以宣判为死亡了 分代内存区域划分 Eden : Survivor1 : Survivor2 = 8 : 1 : 1 新生代：老年代 = 1：3 引用计数法 引用计数算法（Reference Counting）比较简单，对每个对象保存一个整型的引用计数器属性。用于记录对象被引用的情况。 对于一个对象A，只要有任何一个对象引用了A，则A的引用计数器就加1；当引用失效时，引用计数器就减1。只要对象A的引用计数器的值为0，即表示对象A不可能再被使用，可进行回收 优点：实现简单，垃圾对象便于辨识；判定效率高，回收没有延迟性。 缺点： 它需要单独的字段存储计数器，这样的做法增加了存储空间的开销。 每次赋值都需要更新计数器，伴随着加法和减法操作，这增加了时间开销。 引用计数器有一个严重的问题，即无法处理循环引用的情况。这是一条致命缺陷，导致在Java的垃圾回收器中没有使用这类算法。 在 Netty 的直接内存缓冲区中有实际落地场景 可达性分析算法 可达性分析算法：也可以称为根搜索算法、追踪性垃圾收集 相对于引用计数算法而言，可达性分析算法不仅同样具备实现简单和执行高效等特点，更重要的是该算法可以有效地解决在引用计数算法中循环引用的问题，防止内存泄漏的发生。 相较于引用计数算法，这里的可达性分析就是Java、C#选择的。这种类型的垃圾收集通常也叫作追踪性垃圾收集（Tracing Garbage Collection） 实现思路 可达性分析算法是以根对象集合（GCRoots）为起始点，按照从上至下的方式搜索被根对象集合所连接的目标对象是否可达。 使用可达性分析算法后，内存中的存活对象都会被根对象集合直接或间接连接着，搜索所走过的路径称为引用链（Reference Chain） 如果目标对象没有任何引用链相连，则是不可达的，就意味着该对象己经死亡，可以标记为垃圾对象。 在可达性分析算法中，只有能够被根对象集合直接或者间接连接的对象才是存活对象。 GC ROOT 虚拟机栈中局部变量表引用的对象 方法区中静态变量引用的对象 比如：Java类的引用类型静态变量 方法区中常量引用的对象 比如：字符串常量池（StringTable）里的引用 所有被同步锁synchronized持有的对象 系统类加载器 总之只要不会被GC所回收的对象都能被当作GC ROOT 除了这些固定的GC Roots集合以外，根据用户所选用的垃圾收集器以及当前回收的内存区域不同，还可以有其他对象“临时性”地加入，共同构成完整GC Roots集合 （比如Minor GC 使用老年代对象作为 GC Root） 注意 如果要使用可达性分析算法来判断内存是否可回收，那么分析工作必须在一个能保障一致性的快照中进行。这点不满足的话分析结果的准确性就无法保证。 这点也是导致GC进行时必须Stop The World的一个重要原因。即使是号称（几乎）不会发生停顿的CMS收集器中，枚举根节点时也是必须要停顿的。 三色标记 Java的三色标记法是一种垃圾收集算法，它的核心思想是将对象分为三种颜色来标记其状态，从而决定哪些对象可以被回收。 白色（White，垃圾对象）：表示尚未扫描的对象。在初始标记阶段，所有的对象都是白色的。 灰色（Gray）：没有完全扫描的对象。 黑色（Black）：完全扫描的对象，对象是可达的，不会被垃圾回收。 但仅仅将对象划分成三个颜色还不够，真正关键的是：实现根可达算法的时候，将整个过程拆分成了初始标记、并发标记、重新标记、并发清除四个阶段 初始标记阶段，指的是标记 GCRoots 直接引用的节点，将它们标记为灰色，这个阶段需要 STW 并发标记阶段，指的是从灰色节点开始，去扫描整个引用链，然后将它们标记为黑色，这个阶段不需要 STW 重新标记阶段，指的是去校正并发标记阶段的错误，这个阶段需要（漏标） STW 并发清除，指的是将已经确定为垃圾的对象清除掉，这个阶段不需要 STW 多标 多标问题指的是原本应该回收的对象，被多余地标记为黑色存活对象，从而导致该垃圾对象没有被回收 原因：多标问题会出现，是因为在并发标记阶段，有可能之前已经被标记为存活的对象，其引用被删除，从而变成了不可达对象 多标问题会导致内存产生浮动垃圾，但好在其可以再下次 GC 的时候被回收，因此问题还不算很严重 漏标 原本存活的对象，被遗漏标记为黑色，从而导致该对象被错误回收 原因：并发标记和对象引用关系的变化导致的 条件 至少一个黑色对象引用了白色对象 所有的灰色对象在扫描完成之前断开对白色对象的引用 解决漏标 CMS采用的是 增量更新 G1采用的是 原始快照 对象的 finalization 机制 Finalization机制是一种对象销毁前的回调函数，通常通过重写对象的 finalize() 方法来实现，finalize()只会被GC线程调用一次 当一个垃圾对象被回收前，总会先调用这个对象的finalize()方法。 它允许在对象被垃圾回收之前执行一些特定的清理操作（已不推荐使用）。 12// 等待被重写protected void finalize() throws Throwable &#123; &#125; 不主动调用finalize()方法 在 finalize() 方法中可能导致对象复活，使其从垃圾状态恢复为存活状态。 finalize() 方法的执行时机是不确定的，由垃圾回收线程决定 一个糟糕的 finalize() 方法可能严重影响垃圾回收的性能。比如finalize是个死循环 对象的生命周期 Finalization机制存在的情况下，Java对象可以处于三种状态： 可触及的（Reachable）：从根节点出发，可以访问到这个对象，它处于活动状态 可复活的（Finalizable）：对象的所有引用都被释放，但对象有可能在 finalize() 方法中复活，它处于一种临时状态 不可触及的（Unreachable）：对象的 finalize() 方法已被调用，且没有复活，对象进入不可触及状态，不可再被引用或复活 对象的自救 要判断一个对象是否可回收，需要经历两次标记过程： 如果对象无法从任何根节点访问到，首先进行第一次标记 进行筛选，判断对象是否需要执行 finalize() 方法： 如果对象没有重写 finalize() 方法，或者 finalize() 方法已经被调用过，视为不需要执行，对象被判定为不可触及 如果对象重写了 finalize() 方法且未执行过，对象被插入到Finalizer队列中，由Finalizer线程触发其finalize()方法执行 finalize() 方法是对象逃脱死亡的最后机会。如果在 finalize() 方法中与引用链上的任何一个对象建立了联系，对象将被移出&quot;即将回收&quot;集合。然后，对象会再次处于没有引用的状态，此时 finalize() 方法不会再次被调用，对象变为不可触及状态 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455/** * 测试Object类中finalize()方法，即对象的finalization机制。 * */public class CanReliveObj &#123; public static CanReliveObj obj;//类变量，属于 GC Root //此方法只能被调用一次 @Override protected void finalize() throws Throwable &#123; super.finalize(); System.out.println(&quot;调用当前类重写的finalize()方法&quot;); obj = this;//当前待回收的对象在finalize()方法中与引用链上的一个对象obj建立了联系 &#125; public static void main(String[] args) &#123; try &#123; obj = new CanReliveObj(); // 对象第一次成功拯救自己 obj = null; System.gc();//调用垃圾回收器 System.out.println(&quot;第1次 gc&quot;); // 因为Finalizer线程优先级很低，暂停2秒，以等待它 Thread.sleep(2000); if (obj == null) &#123; System.out.println(&quot;obj is dead&quot;); &#125; else &#123; System.out.println(&quot;obj is still alive&quot;); &#125; System.out.println(&quot;第2次 gc&quot;); // 下面这段代码与上面的完全相同，但是这次自救却失败了 obj = null; System.gc(); // 因为Finalizer线程优先级很低，暂停2秒，以等待它 Thread.sleep(2000); if (obj == null) &#123; System.out.println(&quot;obj is dead&quot;); &#125; else &#123; System.out.println(&quot;obj is still alive&quot;); &#125; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;&#125;输出 第1次 gc调用当前类重写的finalize()方法obj is still alive第2次 gcobj is dead 如果注释掉finalize()方法,输出结果: 1234第1次 gcobj is dead第2次 gcobj is dead 垃圾回收算法 垃圾清除阶段 当成功区分出内存中存活对象和死亡对象后，GC接下来的任务就是执行垃圾回收，释放掉无用对象所占用的内存空间，以便有足够的可用内存空间为新对象分配内存。目前在JVM中比较常见的三种垃圾收集算法是 标记-清除算法（Mark-Sweep） 复制算法（Copying） 标记-压缩算法（Mark-Compact） 标记-清除算法 空闲链表 空间碎片 执行过程 用于在堆中清理不再使用的对象。它的执行过程可以分为两个阶段 标记 阶段和 清除 阶段 标记阶段（Mark Phase）： 在标记清除算法的第一阶段，垃圾收集器从根对象开始，遍历整个对象图，并标记所有可以从根对象访问到的对象。 垃圾收集器会将已标记的对象标记为&quot;存活&quot;，而未标记的对象被视为&quot;垃圾&quot;。 清除阶段（Sweep Phase）： 在清除阶段，垃圾收集器遍历特定的内存区域，将未标记的对象（即垃圾对象）进行回收，释放它们占用的内存空间。 清除操作通常是通过将对象所在的内存标记为空闲来实现的，使其可以用于后续的内存分配。 缺点 效率低下：需要找到未被标记的对象进行回收 停顿时间长：垃圾回收期间停止整个应用程序的运行（STW，Stop-The-World），以便进行标记和清除操作 内存碎片：使用空闲链表标记已回收的内存空间，内存不连续，使用效率低 复制算法 指针碰撞 需要两倍内存空间 复制算法被广泛应用于新生代垃圾收集器中。它的核心思想是： 将内存分成两个相等大小的区域 然后将活跃对象复制到一个新的内存区域（Eden区和Survivor的From和To区） 最后将不活跃的对象清除掉 优缺点 优点 实现简单，运行高效 活跃对象被复制后，会直接释放整块内存区域，不存在内存碎片 缺点 需要两倍的内存空间（空间换时间） 其他对象要更新移动对象的引用关系，有额外的性能开销 标记-压缩算法 指针碰撞 整理空间碎片，效率更低 背景 复制算法的高效性是建立在存活对象少、垃圾对象多的前提下的。这种情况在新生代经常发生，但是在老年代，更常见的情况是大部分对象都是存活对象。如果依然使用复制算法，由于存活对象较多，复制的成本也将很高。因此，基于老年代垃圾回收的特性，需要使用其他的算法 标记-清除算法的确可以应用在老年代中，但是该算法不仅执行效率低下，而且在执行完内存回收后还会产生内存碎片，所以JVM的设计者需要在此基础之上进行改进。标记-压缩（Mark-Compact）算法由此诞生 1970年前后，G.L.Steele、C.J.Chene和D.s.Wise等研究者发布标记-压缩算法。在许多现代的垃圾收集器中，人们都使用了标记-压缩算法或其改进版本 实现 对象标记完成后，存活对象往内存的一端移动，非存活对象往反方向移动 移动存活对象时，需要更新所有引用这些对象的引用地址 找到存活对象的内存的临界点，释放临界点外的内存 比较 标记-压缩算法的最终效果等同于标记-清除算法执行完成后，再进行一次内存碎片整理，因此，也可以把它称为标记-清除-压缩（Mark-Sweep-Compact）算法 二者的本质差异在于标记-清除算法是一种非移动式的回收算法，标记-压缩是移动式的。是否移动回收后的存活对象是一项优缺点并存的风险决策 可以看到，标记的存活对象将会被整理，按照内存地址依次排列，而未被标记的内存会被清理掉。如此一来，当我们需要给新对象分配内存时，JVM只需要持有一个内存的起始地址即可，这比维护一个空闲列表显然少了许多开销 优缺点 优点 消除了标记-清除算法当中存在内存碎片问题 消除了复制算法当中，内存使用翻倍成本 缺点 效率相对较低，标记-整理算法要低于复制算法 移动对象的同时，如果对象被其他对象引用，则还需要调整对象引用的地址 移动过程中，需要全程暂停用户应用程序，造成STW 分代收集算法 分代收集算法并不是一种新的算法，而是根据场景不同使用不同的算法，比如新生代中使用复制算法，老年代中使用标记-清除，标记-压缩算法 目前几乎所有的GC都采用分代收集算法执行垃圾回收的 在HotSpot中，基于分代的概念，GC所使用的内存回收算法必须结合年轻代和老年代各自的特点 年轻代（Young Gen） 年轻代特点：区域相对老年代较小，对象生命周期短、存活率低，回收频繁 这种情况复制算法的回收整理，速度是最快的。复制算法的效率只和当前存活对象大小有关，因此很适用于年轻代的回收。而复制算法内存利用率不高的问题，通过hotspot中的两个survivor的设计得到缓解 老年代（Tenured Gen） 老年代特点：区域较大，对象生命周期长、存活率高，回收不及年轻代频繁 这种情况存在大量存活率高的对象，复制算法明显变得不合适。一般是由标记-清除或者是标记-清除与标记-整理的混合实现 标记阶段的开销与存活对象的数量成正比 清除阶段的开销与所管理区域的大小成正相关 压缩阶段的开销与存活对象的数据成正比 以HotSpot中的CMS回收器为例，CMS是基于标记-清除算法实现的，对于对象的回收效率很高。对于碎片问题，CMS采用基于标记-压缩算法的Serial Old回收器作为补偿措施：当内存回收不佳（碎片导致的Concurrent Mode Failure时），将采用Serial Old执行Full GC以达到对老年代内存的整理。 分代的思想被现有的虚拟机广泛使用。几乎所有的垃圾回收器都区分新生代和老年代​ ​ 垃圾回收算法小结 效率上来说，复制算法是当之无愧的老大，但是却浪费了太多内存 而为了尽量兼顾上面提到的三个指标，标记-整理算法相对来说更平滑一些，但是效率上不尽如人意，它比复制算法多了一个标记的阶段，比标记-清除多了一个整理内存的阶段 标记清除 标记整理 复制 速率 中等 最慢 最快 空间开销 少（但会堆积碎片） 少（不堆积碎片） 通常需要活对象的2倍空间（不堆积碎片） 移动对象 否 是 是 评估GC的性能指标 指标 说明 吞吐量 运行用户代码的时间占总运行时间的比例（总运行时间 = 程序的运行时间 + 内存回收的时间） 垃圾收集开销 吞吐量的补数，垃圾收集所用时间与总运行时间的比例 暂停时间 执行垃圾收集时，程序的工作线程被暂停的时间 收集频率 相对于应用程序的执行，收集操作发生的频率 内存占用 Java堆区所占的内存大小 快速 一个对象从诞生到被回收所经历的时间 吞吐量、暂停时间、内存占用这三者共同构成一个“不可能三角”。三者总体的表现会随着技术进步而越来越好。一款优秀的收集器通常最多同时满足其中的两项。 这三项里，暂停时间的重要性日益凸显。因为随着硬件发展，内存占用多些越来越能容忍，硬件性能的提升也有助于降低收集器运行时对应用程序的影响，即提高了吞吐量。而内存的扩大，对延迟反而带来负面效果。 简单来说，主要抓住两点： 吞吐量 暂停时间 吞吐量（throughput） 吞吐量就是CPU用于运行用户代码的时间与CPU总消耗时间的比值，即吞吐量=运行用户代码时间 /（运行用户代码时间+垃圾收集时间） 比如：虚拟机总共运行了100分钟，其中垃圾收集花掉1分钟，那吞吐量就是99%。 这种情况下，应用程序能容忍较高的暂停时间，因此，高吞吐量的应用程序有更长的时间基准，快速响应是不必考虑的 吞吐量优先，意味着在单位时间内，STW的时间最短：0.2+0.2=0.4 暂停时间（pause time） 暂停时间是指一个时间段内应用程序线程暂停，让GC线程执行的状态。 例如，GC期间100毫秒的暂停时间意味着在这100毫秒期间内没有应用程序线程是活动的 暂停时间优先，意味着尽可能让单次STW的时间最短：0.1+0.1 + 0.1+ 0.1+ 0.1=0.5，但是总的GC时间可能会长 垃圾回收器 回收器 特点 线程模型 新/老年代 回收算法 Serial 单线程串行收集器 串行收集器 新生代 复制算法 ParNew 多线程并行Serial收集器 并行收集器 新生代 复制算法 Parallel Scavenge 并行吞吐量优先收集器 并行收集器 新生代 复制算法 Serial Old Serial单线程收集器老年代版本 串行收集器 老年代 标记-压缩 CMS(Concurrent Mark Sweep) 并行最短停顿时间收集器 并发收集器 老年代 标记-清除 Parallel Old Parallel Scavenge并行收集器老年代版本 并行收集器 老年代 标记-压缩 G1 面向局部收集和基于Region内存布局的新型低延时收集器 并发/并行收集器 新生代/老年代 三色标记 JDK8 默认垃圾收集器是Parallel Scavenge+Parallel Old Serial 新生代 串行执行 复制算法 它是一种单线程的垃圾回收器，意味着它在执行垃圾回收操作时只使用一个线程，因此在垃圾回收过程中会暂停应用程序的执行，吞吐量较差 单线程执行：它在执行垃圾回收操作时只使用一个线程。这使得它在多核处理器上的性能表现相对较差。因为它是串行的，所以它执行垃圾回收时会暂停应用程序的所有线程 复制算法： Serial垃圾回收器通常用于新生代（Young Generation）的垃圾回收。在新生代中，它使用复制算法回收垃圾对象 适用于客户端应用：由于Serial垃圾回收器的特点，它通常用于客户端应用程序，如桌面应用程序或移动应用程序，对吞吐量要求不高的场景 最悠久的垃圾回收器：Serial收集器是最基本、历史最悠久的垃圾收集器了，JDK1.3之前回收新生代唯一的选择 指定Serial收集器：-XX:+UseSerialGC Serial Old 串行执行 标记-整理算法 除了年轻代之外，Serial收集器还提供用于执行老年代垃圾收集的Serial Old收集器。Serial old收集器同样也采用了串行回收和&quot;Stop the World&quot;机制，只不过内存回收算法使用的是 标记-压缩算法 简单高效：对于单核CPU来说，Serial收集器没有线程切换的开销，专心做垃圾收集自然可以获得最高的单线程收集效率。运行在Client模式下的虚拟机是个不错的选择 标记-整理算法： Serial Old垃圾收集器使用标记-整理（Mark-Sweep-Compact）算法，有助于减少内存碎片，提高内存利用率 不适用于高吞吐量需求：Serial Old垃圾回收器是单线程的，不适合要求高吞吐量的大型服务器应用程序，因为它可能导致较长的垃圾回收停顿时间，影响应用程序的响应性 指定Serial收集器：-XX:+UseSerialGC 等价于新生代用Serial GC，且老年代用Serial Old GC Server模式下的用途 Serial Old是运行在 Client模式 下默认的老年代的垃圾回收器，Serial Old在 Server模式 下主要有两个用途 与新生代的Parallel Scavenge配合使用 作为老年代CMS收集器内存整理的方案 ParNew 新生代 多线程并行回收 复制算法 提高吞吐量 配合CMS使用 多线程执行： ParNew是Serial的多线程版本，利用多个处理器核心并行执行垃圾回收操作，从而提高了吞吐量 适用于多核处理器：ParNew是多线程的垃圾回收器，因此在多核处理器上表现良好，适用于需要更高吞吐量的应用程序 与CMS配合使用：早期这种组合旨在最大程度地减少应用程序的停顿时间，适用于需要低延迟的应用 启用选项： 设置新生代垃圾收集器，不影响老年代 -XX:+UseParNewGC 限制并行线程数，默认开启和CPU核心相同的线程数，-XX:ParallelGCThreads ParNew 回收器与 Serial 回收器比较 由于ParNew收集器基于并行回收，那么是否可以断定ParNew收集器的回收效率在任何场景下都会比Serial收集器更高效？ ParNew收集器运行在多CPU的环境下，由于可以充分利用多CPU、多核心等物理硬件资源优势，可以更快速地完成垃圾收集，提升程序的吞吐量。 但是在单个CPU的环境下，ParNew收集器不比Serial收集器更高效。虽然Serial收集器是基于串行回收，但是由于CPU不需要频繁地做任务切换，因此可以有效避免多线程交互过程中产生的一些额外开销。 除Serial外，目前只有ParNew GC能与CMS收集器配合工作 Parallel Scavenge 复制算法 并行回收 吞吐量优先 自适应调节策略 吞吐量 = 程序运行时间 /（程序运行时间 + GC时间） 注重提供高吞吐量和可控制的垃圾回收延迟 吞吐量优先 尽可能地减少垃圾回收停顿时间的同时，也更关注程序的总吞吐量 高吞吐量则可以高效率地利用CPU时间，尽快完成程序的运算任务，主要适合在后台运算而不需要太多交互的任务 自适应调节策略 开启后就不需要人工指定新生代的大小（-Xmn）、Eden 与 Survisor 区的比例（-XX:SurvivorRatio）、直接晋升老年代对象大小（-XX:PretenureSizeThreshold） -XX:+UseParallelGC: 手动指定年轻代使用Parallel并行收集器执行内存回收任务 -XX:ParallelGCThreads：设置年轻代并行收集器的线程数。一般地，最好与CPU数量相等，以避免过多的线程数影响垃圾收集性能 -XX:MaxGCPauseMillis: 设置垃圾收集器最大停顿时间（即STW的时间），单位是毫秒 （设置越小堆空间可能越小，GC越频繁） -XX:GCTimeRatio: 垃圾收集时间占总时间的比例，即等于 1 / (N+1) ，用于衡量吞吐量的大小 取值范围(0, 100)。默认值99，也就是垃圾回收时间占比不超过1 与前一个-XX:MaxGCPauseMillis参数有一定矛盾性，STW暂停时间越长，Radio参数就容易超过设定的比例 -XX:+UseAdaptiveSizePolicy 设置Parallel Scavenge收集器具有自适应调节策略 在这种模式下，年轻代的大小、Eden和Survivor的比例、晋升老年代的对象年龄等参数会被自动调整，已达到在堆大小、吞吐量和停顿时间之间的平衡点 Parallel Old 多线程 标记-压缩 适合场景：配合 Parallel Scavenge 运行在不太关注停顿时间的后台计算任务 标记-整理 自适应调节 调整垃圾回收线程 调整吞吐量 多线程执行：Java 1.6 用户代替老年代Serial Old单线程收集器 吞吐量优先：设计目标是提供高吞吐量。它在尽可能减少垃圾回收停顿时间的同时，更关注程序的总吞吐量 不适用于低延迟需求：由于Parallel Old更关注的是吞吐量（吞吐量优先），它不适合对低停顿时间有极高要求的应用程序 在Java8中，默认是此垃圾收集器。 参数 -XX:+UseParallelGC：手动指定年轻代使用Parallel并行收集器执行内存回收任务 -XX:+UseParallelOldGC：手动指定老年代都是使用并行回收收集器 分别适用于新生代和老年代 上面两个参数分别适用于新生代和老年代。默认jdk8是开启的。默认开启一个，另一个也会被开启。（互相激活） CMS 已经被ZGC干掉 最求停顿时间和用户体验 第一款并行并发回收器 主打停顿时间 三色标记 标记-清除 CMS 旨在减小垃圾回收的停顿时间（在JDK1.5时期推出），适用于那些需要低延迟和响应性能的应用程序。它是第一款真正意义上的并发收集器，第一次实现了垃圾收集线程与用户线程同时工作，并且使用的是 标记-清除 算法 优点 低停顿时间：CMS与用户线程并发工作，使得在标记阶段应用程序的停顿时间极短 并发执行：允许用户线程和垃圾回收线程同时执行，缩短SWT时间 可控停顿时间：可设置期望最大的停顿时间，控制程序的停顿时间，提高用户体验 适用于大堆：由于其并发特性，CMS能够高效地处理大规模内存，避免出现过长的停顿 缺点 内存碎片：标记-清除算法导致内存碎片，随着时间推移，可能会累积，降低内存的有效利用率 CPU开销：垃圾回收器线程与应用程序线程竞争CPU资源，这可能会影响应用程序的整体性能 无法处理浮动垃圾：在并发标记阶段，用户线程和垃圾收集线程同时运行或者交叉运行。如果产生新的垃圾对象，CMS将无法对其进行标记 需要整理内存碎片：内存碎片问题需要委托给Serial Old来处理，随着堆的增大，可能会导致较长的停顿时间 三色标记 使用三色标记判断对象是否垃圾对象的过程 三色 黑色（Black）：这些对象是被标记为可达的，不是垃圾对象。 灰色（Gray）：这些对象是初始标记和并发标记阶段被标记为可达的，但还未经过重新标记。它们需要重新标记后才能确定是否为垃圾对象。 白色（White）：这些对象是未被标记为可达的，被认定为垃圾对象，将在清理阶段被回收。 过程 初始标记：CMS首先对根对象进行标记，将它们标记为灰色。这是一个STW（Stop-The-World）的短暂停顿阶段，用于确定哪些对象是直接可达的。这些被标记的对象是灰色 并发标记：GC线程与用户线程并行执行，继续标记从根对象可达的对象，以及它们所引用的对象，直到无法再继续标记新对象为止。这些被标记的对象是黑色 重新标记：在重新标记阶段，CMS对并发标记期间发生状态变化的对象进行修正，例如在并发标记期间某些对象可能由于应用程序活动而变成了可达状态。这一阶段需要一次STW停顿，但持续时间通常很短 并发清理：GC线程与用户线程并行执行，清理白色对象 参数 参数 描述 -XX:+UseConcMarkSweepGC 手动启用CMS垃圾回收器，执行内存回收任务。开启此参数会自动启用ParNew（Young区）+CMS（Old区）+Serial Old（Old区备选方案）的组合。 -XX:CMSInitiatingOccupanyFraction 设置触发CMS回收的堆内存使用率阈值。JDK5及以前版本默认值为68%，JDK6及以上版本默认值为92%。可以根据内存增长情况调整，以优化性能。 -XX:+UseCMSCompactAtFullCollection 指定在执行完Full GC后对内存空间进行压缩整理，以避免内存碎片。注意，这会增加停顿时间。 -XX:CMSFullGCsBeforeCompaction 设置在执行多少次Full GC后对内存空间进行压缩整理。 -XX:ParallelCMSThreads 设置CMS的线程数量。 -XX:MaxGCPauseMillis 设置最大停顿时间。 其他信息 CMS默认启动的线程数是 (ParallelGCThreads + 3) / 4，其中ParallelGCThreads是年轻代并行收集器的线程数，这可能会影响应用程序性能。 漏标解决 漏标：在并发标记阶段，某些对象由于并发执行和对象状态的变化，未能被正确标记为可达对象 为了应对漏标问题，CMS采用了 增量更新（Incremental Update）方法来解决： 记录状态变化：并发标记阶段，如果某个对象从可达状态变为不可达状态（例如，被删除或修改），CMS会详细记录这个状态变化 重新标记阶段：重新标记阶段，CMS会根据记录的状态变化信息，有针对性地重新检查那些发生变化的对象，并确保它们被正确地标记为不可达状态 G1 G1详解 动态Region管理 无传统分代 优先级回收 可控停顿时间 三色标记 标记整理 G1垃圾收集器采用一种非传统的内存布局方式，它不再将堆内存严格划分为新生代和老年代，而是将整个堆内存划分为大小相等的逻辑内存块，称为&quot;Region&quot;。每个Region都是逻辑上连续的内存段，其具体大小会根据堆的实际大小而定，通常在1MB到32MB之间，而且必须是2的幂次方大小。与传统内存布局不同的是，G1不再要求相同类型的Region在物理内存上相邻，而是允许通过动态分配来实现逻辑上的连续性。 G1垃圾收集器的核心思想是跟踪每个Region中垃圾的堆积情况，并根据预设的垃圾回收时间目标来确定回收的优先级。这使得G1能够避免全堆垃圾回收，而是选择回收优先级最高的区域。这种方式能够显著减少程序停顿（stop-the-world）的时间，使其更加短暂和可预测。同时，G1垃圾收集器在有限的时间内能够获得最高的垃圾回收效率。 分区Region G1 垃圾收集器将堆内存划分为若干个 Region，每个区域只扮演其中一个角色，包括 Eden区、Survivor区 和 Old区。这种划分允许新生代的Region在需要时直接转化为老年代的Region H区（Humongous）：这个特殊区域专门用于存放巨大对象。G1垃圾回收器将对象大小超过Region容量的 50%以上 的对象标记为巨大对象。在其他垃圾回收器中，这类巨大对象通常会被分配到老年代，但这可能导老年代频繁GC 如果一个H区无法容纳一个巨型对象，G1会寻找连续的H分区来存储该对象。如果找不到足够的连续H区，G1可能会被迫触发 Full GC Remember Set 在分代算法的场景下，线程执行的过程中，对象可能存储在新生代，也可能在老年代。那么，如果对象之间的引用关系，大概率会存在对象 跨代引用 在触发新生代GC 的时候，由于 GC Roots 到 E 和 G 是不可达的，那么 E 和 G 将会被当作垃圾对象回收。导致 H 和 J 指向的地址不再是存放 E 和 G，并且是不确定的，这将会造成程序崩溃 Remember Set主要用于帮助G1垃圾回收器跟踪对象之间的引用关系 在G1垃圾回收器中，Remember Set 用于记录哪些非收集区域的对象引用了收集区域的对象。具体来说，在新生代GC中，Remember Set 记录哪些老年代对象（非收集区域）引用了新生代对象（收集区域） 作用 跟踪跨区域引用关系：记录其他区域对象对本区域对象的引用 辅助垃圾回收：使用Remembered Set，G1可以更快地确定哪些区域与其他区域有较少的交互引用，从而回收收益更大的内存区域 写屏障 写屏障是一个机制（类似AOP），它的目的是在写操作发生时，通知垃圾回收器，以便更新记忆集。当一个对象引用另一个对象，或者引用关系被取消时，写屏障会记录这些变化（记录原始快照） G1中写屏障的一般工作流程： 当对象引用发生变化时，例如，一个对象引用了另一个对象，JVM会在执行此引用变化的地方插入写屏障代码 写屏障代码负责将这个引用变化记录到相应的记忆集中，以便在垃圾回收时可以找到受影响的对象 垃圾回收器描这些记忆集，防止对象被错误标记 写屏障用于Rset记录对象间的引用，不直接修改Rset是因为存在并发修改，是把脏卡片放到队列中慢慢更新的 JVM注入的一小段代码，用于记录指针变化（类似AOP记录原始快照STAB） 12// 假设 A 是老年代，B是新生代的跨区引用A.b = B 当指针更新时 将A对应的 Card 标记为 Dirty Card 将 Card 存入 Dirty Card Queue 队列有白/绿/黄/红四个颜色 白色：RSet修改缓慢，一切正常 绿色：Refinement线程（优化线程池）开始被激活，开始更新RS 具体操作：正常的从队列中拿出DirtyCard，并更新到对应的RSet中 黄色：当产生藏卡片的速度非常快，所有的Refinement线程开始激活 目的：全力以赴的把队列排空，目标就是不要让队列太慢 红色：应用线程也参与排空（清理Dirty Card Queue）队列的操作 目的：GC线程和用户线程一起清空，使得Refinement能够得到及时的执行完毕 Card 拆分Region, 堆内存最小可用粒度 一个 Card Table 将一个 Region 在逻辑上划分为若干个固定大小（介于128到512字节之间）的连续区域，每个区域称之为卡片 Card。Card作用是跟踪区域内对象引用关系的变化，帮助确定哪些区域需要进行垃圾回收 具体而言，当一个对象引用了另一个对象时，G1垃圾回收器会将对应的卡片标记为 脏卡片，表示该卡片中的对象引用发生了变化。当垃圾回收器需要进行垃圾回收时，它可以快速检查哪些卡片是脏的，从而确定需要扫描和处理的区域 写屏障（记录原始快照）：Card还用于实现G1的写屏障机制。在对象引用关系变化时，将变化写入脏卡片后加入Dirty Card Queue，进行对象关系更新 并发标记：对象引用关系更新，对应的卡片会被标记为脏卡片，G1通过扫描卡片来判断哪些区域中的对象是活动的，以便在标记过程中准确地识别垃圾对象 Card Table Card Table是一种数据结构，由一组字节组成，每个字节对应一个Card。用于跟踪对象之间的引用关系。它的主要目的是在垃圾回收过程中快速确定哪些对象是活跃的，从而只对这些对象执行回收操作，以提高回收效率 每个Card代表一个固定大小的内存区域，通常是一个内存页。默认情况下，所有的Card都被标记为未引用状态 当一个线程修改某个Region内部的引用时，它会通知Card Table，并更新相应的Card。这意味着当引用发生变化时，相关的Card将被标记为已引用状态，对应的Card的字节值会被标记为&quot;1&quot;，表示该地址空间中的对象是活跃的 为了更高效地跟踪引用变化，G1垃圾回收器将内存划分为许多Region，并为每个Region维护一个对应的Card Table。这样，当进行垃圾回收时，只需要处理与已标记为引用的Card相关的对象，而无需扫描整个堆 Collect Set ​ Collect Set（CSet）是指，在回收阶段，由G1垃圾回收器选择的待回收的Region集合，G1的目标是在限定的停顿时间内回收尽可能多的垃圾。因此，它会基于每个区域的存活对象的数量和其他历史数据来选择哪些区域应该被包含在Collection Set中。优先选择那些包含大量垃圾的区域可以确保在回收时获得最大的空间回报 G1解决漏标 STAB解决漏标问题的基本思想是，在进行并发标记的同时，跟踪并记录在并发标记过程开始之后发生的所有写操作（通过写屏障）。当发生写操作时（对象引用发生变化），STAB会将相关的对象引用信息记录在一个日志中，该日志称为 Remembered Set（记忆集） 当G1回收器需要对某个Region进行垃圾回收时，它会首先检查该Region对应的Remembered Set中的引用信息，重新检查对象的引用情况。从而将漏标的对象进行正确标记，避免错误地回收 调参 选项/默认值 说明 -XX:+UseG1GC 使用 G1 (Garbage First) 垃圾收集器 -XX:MaxGCPauseMillis=n 设置最大GC停顿时间(GC pause time)指标(target). 这是一个软性指标(soft goal), JVM 会尽量去达成这个目标. -XX:InitiatingHeapOccupancyPercent=n 启动并发GC周期时的堆内存占用百分比. G1之类的垃圾收集器用它来触发并发GC周期,基于整个堆的使用率,而不只是某一代内存的使用比. 值为 0 则表示&quot;一直执行GC循环&quot;. 默认值为 45. -XX:NewRatio=n 新生代与老生代(new/old generation)的大小比例(Ratio). 默认值为 2. -XX:SurvivorRatio=n eden/survivor 空间大小的比例(Ratio). 默认值为 8. -XX:MaxTenuringThreshold=n 提升年老代的最大临界值(tenuring threshold). 默认值为 15. -XX:ParallelGCThreads=n 设置垃圾收集器在并行阶段使用的线程数,默认值随JVM运行的平台不同而不同 -XX:ConcGCThreads=n 并发垃圾收集器使用的线程数量. 默认值随JVM运行的平台不同而不同. -XX:G1ReservePercent=n 设置堆内存保留为假天花板的总量,以降低提升失败的可能性. 默认值是 10. -XX:G1HeapRegionSize=n 使用G1时Java堆会被分为大小统一的的区(region)。此参数可以指定每个heap区的大小. 默认值将根据 heap size 算出最优解. 最小值为 1Mb, 最大值为 32Mb. 收集过程 初始标记：这是一个短暂的停顿阶段，主要标记与GC Roots 直接关联的对象，例如Remembered Set 中的引用。这个阶段的目标是快速确定哪些对象是存活的。 并发标记：在这个阶段，G1垃圾回收器从GC Roots开始进行可达性分析，找出要回收的对象。与用户程序并发执行，不会导致长时间的停顿。它使用写屏障记录STAB（Snapshot-at-the-Beginning）信息到Remembered Set，以跟踪对象引用关系。 最终标记：这是一个短暂的停顿阶段，用于重新处理并发标记阶段结束后遗留下来的少量SATB（Snapshot-at-the-Beginning）记录。它确保了标记过程的完整性。 筛选回收：在这个阶段，G1垃圾回收器根据Region的回收价值和成本进行排序，并制定回收计划，以满足用户期望的停顿时间。然后，它将要回收的一部分Region中的存活对象复制到空的Region中，清理掉整个旧Region的全部空间。这个阶段也是与用户程序并发执行的。 Fully Yong GC STW（Stop The World） 构建CS【Collection Set】（Eden+Survivor） 扫描GC Roots Update RS：排空Dirty Card Queue，并更新Remember Set Process RS：在Remember Set中找到被哪些老年代的对象跨代引用的。 Object Copy：常规的对新生代进行标记复制算法（复制到空的 Region 中） Reference Processing：回收可以被回收的引用类型 调优 G1记录每个阶段的时间，用于自动调优 记录Eden/Survivor的数量和GC时间 根据暂停目标自动调整Region数量（如果达不到你设定的时间，则减少该Region的数量） 如果你设置了Eden区GC时间只能小于5ms，但是你一次回收了100ms，只能减少Eden区的数量来尽量满足你对该Region的GC最小暂停时间的设置 暂停目标越短，Eden数量越少 如果你设置的Eden区的Region过于短，那么可能会导致Eden区过于少，从而导致CPU大部分时间都在回收Eden区上。导致吞吐量下降（工作线程运行时长/运行总时长） 打印自适应的尺寸调节策略：-XX:+PrintAdaptiveSizePolicy 打印老年代的提升的分布：-XX:+PrintTenuringDistribution Old GC 当堆用量达到一定程度时除法 三色标记 -XX:IntiatingHeapOccupancyPercent=N 45 by default（默认是45） Old GC是并发(concurrent)进行的 ZGC TODO JVM调优篇 JVM调优参数 JVM参数设置 参数 描述 格式 说明 -Xms 初始化堆空间大小 -Xms64M 默认值：物理内存的1/64(&lt;1GB) -Xmx 最大堆空间大小 -Xmx128M 默认值：物理内存的1/4(&lt;1GB) -Xmn 新生代的空间大小 -Xmn32M 此处的大小是（eden+ 2 survivor space).与jmap -heap中显示的New gen是不同的相当于对-XX:newSize、-XX:MaxnewSize同时设置整个堆大小=新生代大小 + 老年代大小 + 持久代大小增大年轻代会较少老年代,可能影响性能,Sun官方推荐配置为整个堆的3/8 -XX:NewSize 新生代初始化内存的大小 -XX:NewSize=64M 注意：该值需要小于-Xms的值 -XX:MaxNewSize 新生代可被分配的内存的最大上限 -XX:MaxNewSize=1024M 默认：堆最大值的1/3 -XX:MetaspaceSize 元空间初始化空间大小 -XX:MetaspaceSize=256M 默认：物理内存的1/64 -XX:MaxMetaspaceSize 元空间可被分配内存的最大上限 -XX:MaxMetaspaceSize=1024M 最大：物理内存的1/4 -Xss 设置线程栈空间大小 -Xss512k JDK5.0以后每个线程堆栈大小为1M,以前每个线程堆栈大小为256K，这个值和线程数量成反比 -XX:ThreadStackSize 和Xss类似 -XX:ThreadStackSize=2M 设置线程对堆栈大小 -XX:NewRatio 年轻代(包括Eden和两个Survivor区)与年老代的比值(除去持久代) -XX:NewRatio=4 设置为4，则年轻代与年老代所占比值为1：4，年轻代占整个堆栈的1/5 -XX:SurvivorRatio Eden区与Survivor区的大小比值 -XX:SurvivorRatio=8 设置为8,则两个Survivor区与一个Eden区的比值为2:8,一个Survivor区占整个年轻代的1/10 -XX:MaxTenuringThreshold 垃圾最大年龄Serial、ParNew有效 -XX:MaxTenuringThreshold=15 默认：15如果设置为0的话,则年轻代对象不经过Survivor区,直接进入年老代. 对于年老代比较多的应用,可以提高效率.如果将此值设置为一个较大值,则年轻代对象会在Survivor区进行多次复制,这样可以增加对象在年轻代的存活时间,增加在年轻代即被回收的概率 该参数只有在串行GC时才有效. -XX:PretenureSizeThreshold 对象超过多大直接在老年代分配 -XX:PretenureSizeThreshold=1000000 默认值是0，意味着任何对象都会现在新生代分配内存 JVM辅助信息参数设置 参数 描述 说明 -XX:+PrintGC 打印GC信息 输出形式:[GC 118250K-&gt;113543K(130112K), 0.0094143 secs] [Full GC 121376K-&gt;10414K(130112K), 0.0650971 secs] -XX:+PrintGCDetails 打印GC明细 输出形式:[GC [DefNew: 8614K-&gt;781K(9088K), 0.0123035 secs] 118250K-&gt;113543K(130112K), 0.0124633 secs][GC [DefNew: 8614K-&gt;8614K(9088K), 0.0000665 secs][Tenured: 112761K-&gt;10414K(121024K), 0.0433488 secs] 121376K-&gt;10414K(130112K), 0.0436268 secs] -XX:PrintHeapAtGC 打印GC前后的详细堆栈信息 -verbose:gc 输出虚拟机GC详情 -XX:+PrintGCDetails 稳定版 -XX:+PrintCommandLineFlags 查看当前垃圾回收器 PrintGCDetails 日志详解 JVM参数疑问解答 JVM参数 -Xmn，-XX:NewSize / -XX:MaxNewSize，以及 -XX:NewRatio 这三组参数都可以影响年轻代的大小。在混合使用这些参数时，它们的优先级如下： 高优先级：-XX:NewSize / -XX:MaxNewSize：这两个参数具有最高优先级，因为它们直接设置年轻代的初始大小和最大大小。如果同时设置了这两个参数，它们会覆盖其他参数对年轻代大小的影响。 中优先级：-Xmn：如果你设置了 -Xmn 参数，它会以默认等效 -Xmn=-XX:NewSize=-XX:MaxNewSize 的方式影响年轻代的大小。这意味着 -Xmn 与 -XX:NewSize / -XX:MaxNewSize 具有相同的优先级。 低优先级：-XX:NewRatio：-XX:NewRatio 参数设置年轻代与老年代的大小比例。它具有较低的优先级，因为它会在已经设置了上述参数的情况下进一步影响年轻代大小。 推荐使用-Xmn参数，原因是这个参数简洁，相当于一次设定 NewSize/MaxNewSIze，而且两者相等，适用于生产环境 调优策略 优化JVM并不是首要选择，而应该优先考虑对应用程序本身的优化。JVM调优通常被视为一种最后的手段，用于解决性能问题或满足特定的需求。在程序优化中，以下是一般的优化顺序： 程序本身的优化：首先应该关注应用程序的代码和算法。通过改进代码结构、减少不必要的计算、使用更高效的算法等方式，可以显著提高应用程序的性能。 数据库和存储优化：如果应用程序涉及数据库操作或文件存储，优化数据库查询、索引和数据访问，以及存储方案可以提高性能。 JVM调优：只有在上述步骤都得到了满足，而且性能问题仍然存在，才考虑JVM调优。这可能包括调整内存分配、垃圾回收策略等JVM参数。 选择合适的垃圾回收器 场景 垃圾回收器 原因 高吞吐量任务，例如后台计算任务 Parallel Scavenge + Parallel Old 这个组合适用于高吞吐量场景，通过并行回收方式可以减少同一时间段的GC次数，虽然单次GC停顿时间较长，但适合计算密集型应用，能够充分利用多核CPU资源。 用户停顿时间敏感，面向用户应用程序，JVM内存小于等于4GB，JDK版本小于1.8 ParNew + CMS 在内存较小（4GB以内）且要求用户停顿时间敏感的场景下，可以选择ParNew和CMS组合。这两个垃圾回收器都支持多线程并行回收，有助于降低GC停顿时间。CMS使用标记-清除算法，可以降低老年代的回收时间。 用户停顿时间敏感，JDK版本大于等于1.8，JVM内存大于等于6GB G1 对于要求可控停顿时间、内存较大（6GB以上）的场景，推荐使用G1垃圾回收器。G1回收器可以提供可预测的停顿时间，同时在大内存下具有高回收效率，适合处理复杂的内存管理需求。 调整内存 现象：频繁的垃圾收集操作 原因：垃圾收集频繁的原因可能是堆内存过小，导致需要频繁进行垃圾收集以释放足够的内存来创建新的对象。因此，增加堆内存大小通常会显著减少垃圾收集的频率。 注意：值得注意的是，如果垃圾收集次数非常频繁，但每次垃圾收集只能回收很少的对象，那么问题可能不是堆内存大小太小，而可能是内存泄漏导致的对象无法被回收，从而导致频繁的垃圾收集。 12345678# 调整初始堆大小-Xms2g# 调整最大堆大小-Xmx4g# 如果新生代GC频繁，可以考虑分配更多内存给新生代-Xmn2g 设置符合预期的停顿时间 现象：程序出现间歇性的卡顿现象 原因：如果没有明确设置垃圾收集器的停顿时间目标，垃圾收集器通常以吞吐量为主要目标，这可能导致垃圾收集时间的不稳定性，从而引发程序卡顿。 注意：在设置停顿时间目标时，应避免设置过于不切实际的停顿时间，因为较短的停顿时间可能需要更频繁的垃圾收集操作，从而增加GC的次数，甚至影响性能。 适用垃圾收集器：建议使用Parallel Scavenge和G1垃圾收集器来实现可控制的停顿时间。 12# 设置垃圾收集器的最大停顿时间目标，需要进行反复测试以确定合适的值-XX:MaxGCPauseMillis=200 调整内存区域大小比率 现象：某一个区域的GC频繁，其他区域正常。 原因：可能的原因是对应区域的空间不足，导致需要频繁进行GC以释放空间。在JVM堆内存无法增加的情况下，可以考虑调整对应区域的大小比率。 注意：也有可能并非仅仅是空间不足，而是由于内存泄露导致内存无法被回收，从而引发频繁的GC。 12345// 调整新生代和老年代的比例。例如，设置为1表示新生代和老年代的大小相等。默认值是2，意味着老年代是新生代的两倍。-XX:NewRatio=1 // 调整survivor区和Eden区的大小比率。例如，设置为6表示Eden区的大小是一个Survivor区的6倍。-XX:SurvivorRatio=6 扩大新生代的比例: 对象生命周期短：如果应用主要产生的是短生命周期的对象，那么扩大新生代可以减少老年代的GC频率，因为对象会在新生代中被回收。 频繁的Full GC：如果老年代经常满，导致频繁的Full GC，增大新生代可能有助于减少对象到老年代的晋升速度。 频繁的Minor GC：如果新生代经常触发Minor GC，并且每次GC后新生代仍然很满，这可能意味着新生代的空间不足，需要扩大。 老年代使用不足：如果通过监控工具观察到老年代的使用率持续很低，而新生代经常接近满载，这可能是一个信号，表明可以安全地增大新生代的比例。 缩小新生代的比例: 对象生命周期长：如果应用产生大量的长生命周期对象或大对象，这些对象很快就会晋升到老年代，那么可能需要减少新生代的大小，为老年代分配更多空间。 老年代GC频繁：如果老年代经常触发GC，特别是Full GC，并且每次GC后老年代仍然很满，这可能意味着老年代的空间不足，需要为其分配更多的空间。 增大Survivor区的比率: 对象晋升过快：如果大量对象在经历了很少的GC周期后就晋升到老年代，可能需要增大Survivor区，以便对象在新生代中停留更长时间。 Survivor区溢出：如果Survivor区经常填满，导致对象提前晋升到老年代，增大Survivor区可以帮助缓解这个问题。 应用的对象生命周期：如果应用中有大量的中生命周期对象（即不是很短暂，也不是很长久的对象），增大Survivor区可能有助于提高性能。 缩小Survivor区的比率: 对象生命周期很短：如果大部分对象都是短暂的，并且很快在Eden区被回收，那么可以缩小Survivor区，为Eden区分配更多空间。 Survivor区使用率低：通过JVM监控工具，如果观察到Survivor区的使用率持续很低，这意味着大部分对象要么很快被回收，要么很快晋升，这时可以考虑缩小Survivor区。 频繁的Minor GC但Eden区回收效果不佳：如果Eden区经常触发Minor GC，但每次回收后仍然很满，这可能意味着需要为Eden区分配更多空间，从而缩小Survivor区的比率。 调整对象晋升老年代的年龄 现象：老年代频繁GC，每次回收的对象数量较多。 原因：如果晋升年龄设置得太小，新生代的对象会很快晋升到老年代，导致老年代对象数量增多。而这些对象可能在短时间内就可以被回收。为了解决老年代频繁GC的问题，可以考虑调整对象的晋升年龄，使对象不那么容易晋升到老年代。 注意：增加晋升年龄后，对象在新生代的停留时间会增加，这可能导致新生代GC的频率上升。同时，由于这些对象需要在新生代中经历更多的GC，Minor GC的暂停时间也可能增加。 12// 设置对象晋升到老年代的初始年龄阈值，默认值为15-XX:InitialTenuringThreshold=20 调整大对象的标准 现象：老年代频繁进行GC，每次回收的对象数量较多，且这些对象的体积都比较大。 原因：大对象如果直接分配到老年代，会导致老年代容易被填满，从而引发频繁的GC。可以通过设置一个阈值，使得超过此阈值的对象直接在老年代分配。 注意：当这些大对象被分配到新生代时，可能会增加新生代的GC频率和暂停时间。 12// 设置对象的大小阈值，超过此值的对象直接在老年代分配。单位是字节，0代表没有限制，全部分配在新生代。-XX:PretenureSizeThreshold=1000000 -XX:PretenureSizeThreshold 参数只在使用 Serial 和 ParNew 垃圾收集器时有效。 调整GC的触发时机 适用于：CMS、G1 现象：CMS 和 G1 经常进行 Full GC，导致程序卡顿严重。 原因：在 G1 和 CMS 的部分GC阶段，垃圾收集是并发进行的，这意味着业务线程和垃圾收集线程会同时运行。因此，业务线程在GC过程中会生成新的对象。为了容纳这些新对象，需要预留一部分内存空间。如果内存空间不足以容纳新产生的对象，JVM会停止并发收集并暂停所有业务线程（STW），以确保垃圾收集可以正常进行。为了避免这种情况，可以提前触发GC，从而预留出足够的空间来容纳业务线程创建的新对象。 注意：提前触发GC可能会增加老年代GC的频率。 12345// 当老年代使用率达到此比例时，开始CMS收集。默认值是68%。如果频繁发生Full GC卡顿，应该调小此值。-XX:CMSInitiatingOccupancyFraction=65// 在G1混合垃圾回收周期中，设置旧区域的占用率阈值。默认值为65%。-XX:G1MixedGCLiveThresholdPercent=65 调整 JVM本地内存大小 现象：虽然GC的次数、时间和回收的对象都正常，且堆内存空间充足，但仍然报出OOM错误。 原因：JVM除了堆内存之外，还有一块被称为 本地内存 或 直接内存 的区域。这块内存区域不会主动触发GC，只有在堆内存区域触发GC时才会尝试回收本地内存中的对象。如果本地内存分配不足，将直接抛出OOM异常。 1-XX:MaxDirectMemorySize=512m 如果不显式设置 -XX:MaxDirectMemorySize，那么默认的直接内存大小是基于Java堆的最大大小。像NIO等其他框架如果有使用直接内存，需要关注此设置。 调优实例 网站流量浏览量暴增后，网站反应页面响很慢 策略： 调整JVM内存配置 选择合适的垃圾回收器 问题推测：尽管在测试环境中响应速度快，但生产环境变慢。可能的原因是垃圾收集导致的业务线程停顿。 定位：使用jstat -gc指令在线上观察，发现JVM的GC频率很高，且GC所占用的时间也长。这表明GC频繁导致业务线程经常停顿，从而使页面响应变慢。 解决方案：由于高流量导致对象创建速度快，堆内存容易被填满，从而频繁触发GC。问题可能在于新生代内存设置得太小。为了解决这个问题，我们将JVM的内存从2G增加到6G。 第二个问题：虽然增加内存后常规请求变快了，但偶尔会出现更长时间的卡顿。 问题推测：考虑到之前的内存增加，可能是因为单次GC的时间变长导致的间歇性卡顿。 定位：通过jstat -gc指令再次观察，发现FGC（Full GC）的次数并不多，但每次FGC所花费的时间非常长，有时甚至达到几十秒。 解决方案：默认的JVM使用的是Parallel Scavenge + Parallel Old组合，这两者在标记和收集阶段都会导致STW（Stop-The-World）。内存增加后，每次GC所需的时间也变长。为了减少单次GC的时间，我们需要切换到并发收集器。考虑到当前的JDK版本是1.7，我们选择CMS收集器（如果是1.8，可以考虑使用G1）。根据之前的GC日志，我们还为CMS设置了一个预期的停顿时间。这样，网站的卡顿问题得到了解决。 后台导出数据引发的OOM 问题描述：公司后台系统偶尔出现OOM异常，导致堆内存溢出。 初步判断：由于问题是偶发性的，最初的猜测是堆内存不足。因此，我们将堆内存从4G增加到8G。 问题复现：尽管增加了堆内存，问题仍然存在。为了进一步定位问题，我们启用了-XX:+HeapDumpOnOutOfMemoryError参数，以在OOM时获取堆内存的dump文件。 堆分析：使用MAT工具对dump文件进行分析。通过MAT的内存泄露报告和Top Consumers功能，我们识别出了大量的大对象。 代码审查：进一步分析线程和代码，我们注意到一个正在运行的业务线程与“导出订单信息”方法有关。 问题定位：考虑到订单信息导出可能涉及数万条数据，这个方法首先从数据库查询订单信息，然后将其转换为Excel。这个过程会生成大量的String对象和其他临时数据。 前端交互问题：为了验证这个猜测，我们尝试在后台进行测试。令人惊讶的是，我们发现导出订单的按钮在前端没有禁用点击交互。因此，如果用户发现点击后页面没有反应，他们可能会连续点击多次。这导致了大量的请求涌入后台，生成了大量的订单和Excel对象。由于这些方法执行很慢，这些对象在短时间内无法被回收，从而导致内存溢出。 解决方案：了解问题原因后，我们决定不调整JVM参数。而是在前端的“导出订单”按钮上添加了禁用状态，直到后端响应完成后才重新启用。此外，我们还对导出接口进行了限流，以防止用户通过刷新页面连续点击，从而彻底解决了OOM问题。 CPU经常100% 问题定位 问题描述：CPU使用率经常达到100%，可能是由于锁竞争激烈或其他原因导致。 定位高CPU使用的进程： 使用top命令查看系统中各进程的资源占用情况。 1top 定位进程中的高CPU使用线程： 根据上一步找到的进程ID，列出该进程中各线程的资源占用情况。 1top -Hp 进程ID 获取线程的堆栈信息： 首先，将线程ID转换为16进制格式： 1printf &quot;%x\\n&quot; 线程ID 然后，使用 1jstack 命令打印出进程的所有线程堆栈信息，并从中找到上一步转换为16进制的线程ID对应的堆栈信息： 1jstack 进程ID 分析堆栈信息定位问题： 通过分析堆栈信息，查看是否有线程长时间处于WAITING或BLOCKED状态。 如果线程长期处于WAITING状态，关注waiting on xxxxxx部分，这表示线程正在等待某个锁。根据锁的地址，可以找到持有该锁的线程。 如果线程长期处于BLOCKED状态，这通常意味着线程正在尝试获取一个已被其他线程持有的锁。 最后，根据堆栈信息，您可以深入到具体的代码逻辑中，找到可能导致高CPU使用或锁竞争的部分，并进行相应的优化。 数据分析平台系统频繁 Full GC 问题背景：数据分析平台的主要功能是对用户在App中的行为进行定时分析统计，并提供报表导出功能。系统的老年代使用了CMS垃圾回收器。 问题描述：数据分析师在使用平台时发现，系统页面打开时经常出现卡顿。通过使用jstat命令监控，发现每次Young GC后，大约有10%的存活对象被晋升到老年代。 问题原因：经过分析，确定问题的原因是Survivor区域设置得过小。每次Young GC后，由于存活对象在Survivor区域放不下，这导致它们提前被晋升到老年代。 解决方案：为了解决这个问题，我们调整了Survivor区的大小，确保它可以容纳Young GC后的存活对象。这样，对象会在Survivor区经历多次Young GC，只有当它们达到一定的年龄阈值时才会被晋升到老年代。 效果：调整后，每次Young GC后进入老年代的存活对象数量大大减少，稳定运行时仅有几百Kb。这大大降低了Full GC的频率，从而提高了系统的响应速度。 MQ消费者 OOM 问题背景：系统主要功能是消费Kafka数据，进行数据处理和计算，然后将处理后的数据转发到另一个Kafka队列。 问题描述：系统在运行几小时后出现OOM（内存溢出）异常。即使重启系统，几小时后仍然会再次出现OOM。 问题定位：为了定位问题，我们使用jmap工具导出了系统的堆内存快照。然后，我们使用eclipse MAT工具对这个快照进行了分析。 问题原因：分析结果显示，大量的对象在内存中堆积，等待被同步打印到日志。进一步的代码审查发现，我们在代码中异步打印了某个业务Kafka的topic数据。由于这个业务的数据量非常大，这导致了大量的对象在内存中堆积，最终导致了OOM。 解决方案：为了解决这个问题，使用日志框架的异步打印方式，并设置异步队列的大小，确保不会有大量的对象在内存中堆积。此外，我们还增加了适当的流控制和内存监控，以确保系统在面对大量数据时仍能稳定运行。 JVM监控工具 jps jps (Java Virtual Machine Process Status Tool) 是 Java JDK 中的一个实用工具，用于列出正在运行的 JVM 进程。它可以帮助你快速查找 Java 进程的进程ID。 -l: 输出完整的包名和应用主类。 -m: 输出传递给 JVM 的参数。 -v: 输出传递给 JVM 的 JVM 参数。 -q: 只输出进程ID，不输出类名、jar名或参数。 jsp 查看启动类名和进程id jps –l 输出主类或者jar的完全路径名 jps –v 输出jvm参数 jstat jstat 是一个用于监控Java HotSpot VM性能的命令行工具。它可以提供有关类加载、即时编译和垃圾收集的统计信息。 使用方法 1jstat [option] [vmid] [interval] [count] 其中： option 是你想要的统计信息的类型。 vmid 是虚拟机的进程ID。 interval 和 count 是可选的，用于指定多久收集一次数据和总共收集多少次数据。 常用的选项： -class: 显示类加载器的行为统计。 -compiler: 显示即时编译的统计。 -gc: 显示垃圾收集的统计。 -gccapacity: 显示各个代的当前和最大容量。 -gcutil: 显示各个代的使用百分比。 -gcnew: 显示新生代的统计。 -gcnewcapacity: 显示新生代的容量。 -gcold: 显示老年代的统计。 -gcoldcapacity: 显示老年代的容量。 jstat -gc pid 12如下表示分析进程id为31736 的gc情况，每隔1000ms打印一次记录，打印10次停止，每3行后打印指标头部jstat -gc -h3 31736 1000 10 列名 描述 S0C 当前 Survivor 0 区的容量 (KB) S1C 当前 Survivor 1 区的容量 (KB) S0U 当前 Survivor 0 区已使用的空间 (KB) S1U 当前 Survivor 1 区已使用的空间 (KB) EC 当前 Eden 区的容量 (KB) EU 当前 Eden 区已使用的空间 (KB) OC 当前老年代的容量 (KB) OU 当前老年代已使用的空间 (KB) MC 元空间的容量 (KB) (Java 8及以后版本) MU 元空间已使用的空间 (KB) (Java 8及以后版本) CCSC 压缩类空间的容量 (KB) (仅在使用压缩Oops时) CCSU 压缩类空间已使用的空间 (KB) (仅在使用压缩Oops时) YGC 从应用启动到当前时刻发生的次要垃圾收集事件的次数 YGCT 从应用启动到当前时刻次要垃圾收集所花费的总时间 FGC 从应用启动到当前时刻发生的完全垃圾收集事件的次数 FGCT 从应用启动到当前时刻完全垃圾收集所花费的总时间 GCT 从应用启动到当前时刻垃圾收集所花费的总时间 (YGCT + FGCT) jstat -gcutil pid 显示各个代的使用百分比 参数 说明 S0 幸存1区当前使用比例 S1 幸存2区当前使用比例 E 伊甸园区使用比例 O 老年代使用比例 M 元数据区使用比例 CCS 压缩使用比例 YGC 年轻代垃圾回收次数 YGCT 年轻代垃圾回收消耗时间 FGC 老年代垃圾回收次数 FGCT 老年代垃圾回收消耗时间 GCT 垃圾回收消耗总时间 jstat -gcnew pid 新生代垃圾回收统计 参数 说明 S0C 第一个幸存区大小 S1C 第二个幸存区的大小 S0U 第一个幸存区的使用大小 S1U 第二个幸存区的使用大小 TT 对象在新生代存活的次数 MTT 对象在新生代存活的最大次数 DSS 期望的幸存区大小 EC 伊甸园区的大小 EU 伊甸园区的使用大小 YGC 年轻代垃圾回收次数 YGCT 年轻代垃圾回收消耗时间 jstat -gcold pid 老年代垃圾回收统计 参数 说明 MC 方法区大小 MU 方法区使用大小 CCSC 压缩类空间大小 CCSU 压缩类空间使用大小 OC 老年代大小 OU 老年代使用大小 YGC 年轻代垃圾回收次数 FGC 老年代垃圾回收次数 FGCT 老年代垃圾回收消耗时间 GCT 垃圾回收消耗总时间 jstat -gccapacity pid 堆内存统计 参数 说明 NGCMN 新生代最小容量 NGCMX 新生代最大容量 NGC 当前新生代容量 S0C 第一个幸存区大小 S1C 第二个幸存区的大小 EC 伊甸园区的大小 OGCMN 老年代最小容量 OGCMX 老年代最大容量 OGC 当前老年代大小 OC 当前老年代大小 MCMN 最小元数据容量 MCMX 最大元数据容量 MC 当前元数据空间大小 CCSMN 最小压缩类空间大小 CCSMX 最大压缩类空间大小 CCSC 当前压缩类空间大小 YGC 年轻代GC次数 FGC 老年代GC次数 jinfo jinfo 用于获取 Java 进程的配置信息和 JVM 参数。它可以显示和调整运行时的 JVM 参数 jinfo pid jstack 线程状态快照 死锁分析 jstack（Java Virtual Machine Stack Trace）用于生成 Java 线程的堆栈跟踪。这个工具对于诊断性能问题、锁竞争、死锁和其他线程相关的问题非常有用。 选项 作用 -F 当正常输出的请求不被响应时，强制输出线程堆栈 -m 如果调用到本地方法的话，可以显示C/C++的堆栈 -l 除堆栈外，显示关于锁的附加信息，在发生死锁时可以用jstack -l pid来观察锁持有情况 线程状态 New：创建后尚未启动的线程处于这种状态，不会出现在Dump中。 RUNNABLE：包括Running和Ready。线程开启start（）方法，会进入该状态，在虚拟机内执行的。 Waiting：无限的等待另一个线程的特定操作。 Timed Waiting：有时限的等待另一个线程的特定操作。 阻塞（Blocked）：在程序等待进入同步区域的时候，线程将进入这种状态，在等待监视器锁。 结束（Terminated）：已终止线程的线程状态，线程已经结束执行。 死锁排查 死锁代码 1234567891011121314151617181920212223242526272829303132333435public class LockTest &#123; private static Object lock1 = new Object(); private static Object lock2 = new Object(); private static void f1() &#123; synchronized (lock1) &#123; synchronized (lock2) &#123; System.out.println(&quot;线程&quot; + Thread.currentThread().getName() + &quot;获得两把锁&quot;); &#125; &#125; &#125; private static void f2() &#123; synchronized (lock2) &#123; synchronized (lock1) &#123; System.out.println(&quot;线程&quot; + Thread.currentThread().getName() + &quot;获得两把锁&quot;); &#125; &#125; &#125; public static void main(String[] args) &#123; new Thread(() -&gt; &#123; while (true) &#123; f1(); &#125; &#125;).start(); new Thread(() -&gt; &#123; while (true) &#123; f2(); &#125; &#125;).start(); &#125;&#125; 运行命令： jstack -l 244 CPU过载分析 参考 调优实例 CPU经常100% 问题定位 jmap java9后改用 jhsdb 用于生成 Java 堆转储 (heap dump) 和查询堆内存的详细信息。这个工具对于诊断内存泄漏、分析对象的内存占用和其他与内存相关的问题非常有用。 jmap [option] heap：打印Java堆概要信息，包括使用的GC算法、堆配置参数和各代中堆内存使用情况； histo[:live]： 打印Java堆中对象直方图，通过该图可以获取每个class的对象数目，占用内存大小和类全名信息，带上:live，则只统计活着的对象； finalizerinfo： 打印等待回收的对象信息 dump：以hprof二进制格式将Java堆信息输出到文件内，该文件可以用MAT、VisualVM或jhat等工具查看； jmap -heap pid 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566&gt; jmap -heap 10352jmap -heap 10352Attaching to process ID 10352, please wait...Debugger attached successfully.Server compiler detected.JVM version is 25.201-b09using thread-local object allocation.Parallel GC with 4 thread(s)Heap Configuration: //对应jvm启动参数-XX:MinHeapFreeRatio设置JVM堆最小空闲比率(defalut 40) MinHeapFreeRatio = 0 //对应jvm启动参数 -XX:MaxHeapFreeRatio设置JVM堆最大空闲比率(default 70) MaxHeapFreeRatio = 100 //对应jvm启动参数-XX:MaxHeapSize=设置JVM堆的最大大小 MaxHeapSize = 4280287232 (4082.0MB) //对应jvm启动参数-XX:NewSize=设置JVM堆的‘新生代’的默认大小 NewSize = 89128960 (85.0MB) //对应jvm启动参数-XX:MaxNewSize=设置JVM堆的‘新生代’的最大大小 MaxNewSize = 1426587648 (1360.5MB) //对应jvm启动参数-XX:OldSize=&lt;value&gt;:设置JVM堆的‘老年代’的大小 OldSize = 179306496 (171.0MB) //对应jvm启动参数-XX:NewRatio=:‘新生代’和‘老生代’的大小比率 NewRatio = 2 //对应jvm启动参数-XX:SurvivorRatio=设置年轻代中Eden区与Survivor区的大小比值 SurvivorRatio = 8 //对应jvm启动参数-XX:MetaspaceSize=&lt;value&gt;:设置JVM堆的‘元空间’的初始大小 // jdk1.8 永久代已经被元空间所取代 MetaspaceSize = 21807104 (20.796875MB) CompressedClassSpaceSize = 1073741824 (1024.0MB) //对应jvm启动参数-XX:MaxMetaspaceSize= :设置JVM堆的‘元空间’的最大大小 MaxMetaspaceSize = 17592186044415 MB G1HeapRegionSize = 0 (0.0MB)//堆内存分布Heap Usage://新生代的内存分布PS Young Generation//Eden区内存分布Eden Space: //Eden区总容量 capacity = 1425539072 (1359.5MB) //Eden区已使用 used = 28510792 (27.19001007080078MB) //Eden区剩余容量 free = 1397028280 (1332.3099899291992MB) //Eden区使用比率 2.0000007407724003% usedFrom Space: capacity = 524288 (0.5MB) used = 65536 (0.0625MB) free = 458752 (0.4375MB) 12.5% usedTo Space: capacity = 524288 (0.5MB) used = 0 (0.0MB) free = 524288 (0.5MB) 0.0% usedPS Old Generation capacity = 128974848 (123.0MB) used = 24006808 (22.894676208496094MB) free = 104968040 (100.1053237915039MB) 18.613557893086256% used13410 interned Strings occupying 1194568 bytes. jmap -histo pid 展示class的内存情况，类似MAT的直方图 jmap -dump:format=b,file=D:\\dump\\dump.hprof 11404 导出dump文件到指定文件夹 jhat jhat (Java Heap Analysis Tool) 用于分析 Java 堆转储文件。它可以帮助你诊断内存泄漏、分析对象的内存占用和其他与内存相关的问题。 jhat 的一个特点是它可以启动一个 Web 服务器，允许你通过 Web 浏览器浏览堆转储的内容。这为分析大型堆转储提供了一个方便的界面。 jhat [ options ] heap-dump-file jconsole Jconsole是JDK自带的监控工具，在JDK/bin目录下可以找到。它用于连接正在运行的本地或者远程的JVM，对运行在java应用程序的资源消耗和性能进行监控，并画出大量的图表，提供强大的可视化界面。而且本身占用的服务器内存很小，甚至可以说几乎不消耗。 命令行下数据启动命令 jconsole jvisualvm 下载 VisualVM 是一款免费的，集成了多个 JDK 命令行工具的可视化工具，它能为您提供强大的分析能力，对 Java 应用程序做性能分析和调优。这些功能包括生成和分析海量数据、跟踪内存泄漏、监控垃圾回收器、执行内存和 CPU 分析，同时它还支持在 MBeans 上进行浏览和操作 命令行下数据启动命令 jvisualvm 下载额外插件 MAT 内存数据分析 MAT中文文档 Arthas 诊断工具 使用参考 Arthas 阿尔萨斯 是一款线上监控诊断产品，通过全局视角实时查看应用 load、内存、gc、线程的状态信息，并能在不修改应用代码的情况下，对业务问题进行诊断，包括查看方法调用的出入参、异常，监测方法执行耗时，类加载信息等，大大提升线上问题排查效率 jvm 相关 dashboard - 当前系统的实时数据面板 getstatic - 查看类的静态属性 heapdump - dump java heap, 类似 jmap 命令的 heap dump 功能 jvm - 查看当前 JVM 的信息 logger - 查看和修改 logger mbean - 查看 Mbean 的信息 memory - 查看 JVM 的内存信息 ognl - 执行 ognl 表达式 perfcounter - 查看当前 JVM 的 Perf Counter 信息 sysenv - 查看 JVM 的环境变量 sysprop - 查看和修改 JVM 的系统属性 thread - 查看当前 JVM 的线程堆栈信息 vmoption - 查看和修改 JVM 里诊断相关的 option vmtool - 从 jvm 里查询对象，执行 forceGc class/classloader 相关 classloader - 查看 classloader 的继承树，urls，类加载信息，使用 classloader 去 getResource dump - dump 已加载类的 byte code 到特定目录 jad - 反编译指定已加载类的源码 mc - 内存编译器，内存编译.java文件为.class文件 redefine - 加载外部的.class文件，redefine 到 JVM 里 retransform - 加载外部的.class文件，retransform 到 JVM 里 sc - 查看 JVM 已加载的类信息 sm - 查看已加载类的方法信息 monitor/watch/trace 相关 注意 请注意，这些命令，都通过字节码增强技术来实现的，会在指定类的方法中插入一些切面来实现数据统计和观测，因此在线上、预发使用时，请尽量明确需要观测的类、方法以及条件，诊断结束要执行 stop 或将增强过的类执行 reset 命令。 monitor - 方法执行监控 stack - 输出当前方法被调用的调用路径 trace - 方法内部调用路径，并输出方法路径上的每个节点上耗时 tt - 方法执行数据的时空隧道，记录下指定方法每次调用的入参和返回信息，并能对这些不同的时间下调用进行观测 watch - 方法执行数据观测 内存溢出自动dump -Xmx2048m -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/data/oom 指定最大堆内存大小，当发生OOM异常时，将堆内存数据dump到指定的文件夹下 JMM 内存模型 参考 InfoQ Simon郎 文章 String问题 String str = “R”; 一共创建了几个对象 思路分析：&quot;R&quot;是一个字面量，会放在字符串常量池子中 两种情况 如果字符串常量池中已经存在“R“，那么创建0个对象 如果字符串常量池不存在“R“，那么在常量池中创建1个对象 new String(“R”); 创建了几个对象？ 思路分析： new 关键字一定会在内存中创建一个对象，还要把字面量“R“提取出来分析 两种情况 如果字符串常量池中已经存在“R“，那么创建1个对象(堆中1个) 如果字符串常量池不存在“R“，那么在常量池中创建2个对象(字符串常量池1个，堆中1个) 123String str = &quot;Java&quot;;String str2 = “Java“;为什么 str == str2 为true 思路分析：java对于常量（字面量）会存储在字符串常量池中，str1第一次赋值时在常量池中添加了“Java“并返回其引用, str2第二次赋值直接 引用字符串常量池。两个都是常量池 “Java” 的引用 123String str = &quot;Java&quot;;String str2 = &quot;Ja&quot; + &quot;va&quot;;为什么 str == str2 为true 思路分析：参考上面，常量拼接还是常量(编译期优化) 123String str = &quot;Java&quot;;String str2 = new String(&quot;Java&quot;);为什么 str == str2 为false 思路分析： String str = “Java”; 内存存储区域在字符串常量池，new String(“Java”);内存存储区域在堆 1234String str1 = &quot;java&quot;;String str2 = &quot;ja&quot;;String str3 = str2 + &quot;va&quot;;为什么 str1 == str3 为false 思路分析：因为两者属于不同对象，第一个对象在常量池中，第二个对象在堆中 str2 + “va”; 字符串拼接底层使用的是 StringBuilder#append，最终使用StringBuilder#toString方法返回String对象 1234public String toString() &#123;// 创建一个新的String对象 return new String(value, 0, count);&#125; String.inertn() String.intern()方法设计的初衷就是：重用字符串对象，以便节省内存 JDK1.8, 先判断常量池中当前字符串是否存在 如果不存在：不会将当前字符串复制到常量池，而是将当前字符串的引用复制到常量池 如果存在：不会改变常量池已经存在的引用，并直接返回常量池中字符串引用 123Strign str1 = &quot;java&quot;;String str2 = new String(&quot;Ja&quot;) + new String(&quot;va&quot;);str1 == str2.intern(); //false 思路分析：因为“java”已经创建并存在str1，所以str2.intern()返回的是常量池已存在的字符串str1的引用，两者并非同一字符串 12Strign str1 = new String(&quot;ja&quot;) + new String(&quot;va&quot;);str1 == str2.intern(); // true 思路分析：因为常量池不纯在时，不会将当前字符串复制到常量池，而是将当前字符串的引用复制到常量池，两者是同一对象 java存在的引用 引用类型 回收时间 用途 强引用 永不回收 普通对象引用 软引用 在内存不足回收 缓存对象 弱引用 垃圾回收时 缓存对象 虚引用 不确定 不确定 强引用 强引用：强应用就是我们平时对象的引用，JVM不会回收带有强引用的对象，即使内存不足导致OOM 1User user = new User(); 我们平时使用变量对对象的引用就是强引用，user持有这个对象的存储地址引用 软引用 软引用：如果一个对象只有软引用 在内存空间足够的情况下，垃圾回收器就不会回收它 如果内存空间不够了，就会对这些只有软引用的对象进行回收 1234567891011121314// 设置堆内存 -Xmx20M public static void main(String[] args) &#123; SoftReference&lt;byte[]&gt; softReference = new SoftReference&lt;&gt;(new byte[1024 * 1024 * 10]); System.out.println(softReference.get()); System.gc(); System.out.println(softReference.get()); byte[] bytes = new byte[1024 * 1024 * 10]; System.out.println(softReference.get()); &#125;输出[B@6956de9[B@6956de9null 弱引用 弱引用：在垃圾回收器扫描内存区域时，一旦发现了只具有弱引用的对象，不管当前内存空间是否足够，都会进行回收 123456789101112131415public static void main(String[] args) &#123; WeakReference&lt;String&gt; weakReference = new WeakReference&lt;&gt;(new String(&quot;java&quot;)); System.out.println(weakReference.get()); System.gc(); System.out.println(weakReference.get()); System.out.println(&quot;-------------&quot;); // 错误用法，因为这里的java会被放在常量池属于强引用 WeakReference&lt;String&gt; weakReference1 = new WeakReference&lt;&gt;(&quot;java&quot;); System.out.println(weakReference1.get()); System.gc(); System.gc(); System.gc(); System.out.println(weakReference1.get()); &#125; 123456789101112 public static void main(String[] args) &#123; // 使用WeakHashMap 作为缓存 WeakHashMap&lt;String, Object&gt; cache = new WeakHashMap&lt;&gt;(); // key不能使用常量 cache.put(new String(&quot;java&quot;), &quot;java&quot;); System.out.println(cache); System.gc();; System.out.println(cache); &#125;输出&#123;java=java&#125;&#123;&#125; 当WeakHashMap没有被正确使用就可能造成OOM,退化成hashMap 12345678910public static void main(String[] args) &#123; // 使用WeakHashMap 作为缓存 WeakHashMap&lt;Object, Object&gt; cache = new WeakHashMap&lt;&gt;(); // test会被放入常量池，变成强引用 cache.put(&quot;test&quot;, &quot;test&quot;); for (int i = 0; i &lt; 10; i++) &#123; System.gc(); &#125; System.out.println(cache); &#125; 虚引用 虚引用：顾名思义就是形同虚设，虚引用不决定对象的生命周期，如果一个对象仅持有虚引用那么它就和没有任何引用一样在任何时候都能被垃圾回收器回收 12345public static void main(String[] args) &#123; // 需要配合引用队列使用 ReferenceQueue&lt;String&gt; referenceQueue = new ReferenceQueue&lt;&gt;(); PhantomReference&lt;String&gt; phantomReference = new PhantomReference&lt;&gt;(new String(&quot;java&quot;), referenceQueue); &#125;","categories":[],"tags":[]},{"title":"spring","slug":"spring","date":"2022-03-23T15:11:21.000Z","updated":"2023-07-14T06:49:17.938Z","comments":true,"path":"2022/03/23/spring/","link":"","permalink":"https://wugengfeng.cn/2022/03/23/spring/","excerpt":"","text":"spring核心类分析 ​ 要想读懂spring源码就需要知道spring核心类的脉络，及核心类的作用。先有核心类的脉络体系知识后，带着脉络看源码调用流程事半功倍 ApplicationContext ​ ApplicationContext接口代表了Spring Ioc容器，它负责实例化、配置、组装bean。容器通过读取配置元数据获取对象的实例化、配置和组装的描述信息 ​ Spring提供几个开箱即用的ApplicationContext接口的实现类 ClassPathXmlApplicationContext 从classpath的XML配置文件中读取上下文 ApplicationContext context = new ClassPathXmlApplicationContext(“bean.xml”); FileSystemXmlApplicationContext 由文件系统中的XML配置文件读取上下文 ApplicationContext context = new FileSystemXmlApplicationContext(“bean.xml”); XmlWebApplicationContext 由Web应用的XML文件读取上下文 AnnotationConfigApplicationContext spring boot使用的基于注解的上下文 BeanFactory 和 ApplicationContext 尽量使用ApplicationContext除非你有更好的理由不用它 因为ApplicationContext包括了BeanFactory的所有功能，通常也优于BeanFactory,除非一些少数的场景，例如：在受资源约束的嵌入式设备上运行一个嵌入式应用，它的内存消耗可能至关重要，并且可能会产生字节。然而，对于大多数典型的企业级应用和系统来说，ApplicationContext才是你想使用的。Spring大量使用了BeanPostProcessor扩展点（以便使用代理等）。如果你仅仅只使用简单的BeanFactory，很多的支持功能将不会有效，例如：事务和AOP，但至少不会有额外的步骤。这可能会比较迷惑，毕竟配置又没有错。 Feature BeanFactory ApplicationContext Bean实例化/装配 是 是 BeanPostProcessor自动注册 否 是 BeanFactoryPostProcessor自动注册 否 是 MessageSource便捷访问（针对i18n） 否 是 ApplicationEvent 发布 否 是 区别： BeanFactory提供了ioc容器的基本功能，主要负责bean的实例化和装配，ApplicationContext代表了spring ioc容器，ApplicationContext继承于BeanFactory包含BeanFactory的所有功能，并且还提供了BeanPostProcessor，ApplicationEvent等功能，比BeanFaction功能更加强大 AnnotationConfigApplicationContext AnnotationConfigApplicationContext，在 Spring 3.0 中 是 新加入的,从此spring进入注解配置时代，支持注解配置 @Configuration @Bean @Component @Autowired … 从类图结构中看出，AbstractApplicationContext是所有开箱即用的ApplicationContext接口的实现类的中枢，图中出现另一个重要spring顶级接口体系AliasRegistry (别名注册表)及它的子接口BeanDefinitionRegistry (Bean 定义注册表) 注册表体系 注册表体系主要提供了bean别名注册，单例bean注册，bean循环依赖三级缓存 AliasRegistry bean别名注册表，作为bean定义的最顶层接口，这个接口定义了管理别名的一些方法，主要作用是将名字-别名映射存到内存中。提供查找和校验的接口 registerAlias 注册一个bean的别名 removeAlias 删除注册的别名 isAlias 确定一个名字是否是别名 getAliases 返回一个名字注册的别名列表。当传入bean的名称带有&amp;前缀时，转换为别名后也会返回带&amp;前缀的别名，这种特殊处理参考BeanFactory#FACTORY_BEAN_PREFIX BeanDefinition 在了解BeanDefinitionRegistry (Bean 定义注册表)之前必须知道BeanDefinition是什么 在 Spring 容器中，我们广泛使用的是一个一个的 Bean，BeanDefinition（接口） 从名字上就可以看出是关于 Bean 的定义 事实上就是这样，我们在 XML 文件中，java配置类中配置的 Bean 的各种属性，这些属性不仅仅是和对象相关，Spring 容器还要解决 Bean 的生命周期、销毁、初始化等等各种操作，我们定义的关于 Bean 的生命周期、销毁、初始化等操作总得有一个对象来承载，那么这个对象就是 BeanDefinition的实现类。文章后续会讲到spring如何将bean的配置转成BeanDefinition,又是如何使用BeanDefinitionRegistryPostProcessor后置处理器使用BeanDefinition对bean进行注册流程 首先一开始定义了两个变量用来描述 Bean 是不是单例的，后面的 setScope/getScope 方法可以用来修改/获取 scope 属性。 ROLE_xxx 用来描述一个 Bean 的角色，ROLE_APPLICATION 表示这个 Bean 是用户自己定义的 Bean；ROLE_SUPPORT 表示这个 Bean 是某些复杂配置的支撑部分；ROLE_INFRASTRUCTURE 表示这是一个 Spring 内部的 Bean，通过 setRole/getRole 可以修改。 setParentName/getParentName 用来配置 parent 的名称，这块可能有的小伙伴使用较少，这个对应着 XML 中的 &lt;bean parent=&quot;&quot;&gt; 配置。 setBeanClassName/getBeanClassName 这个就是配置 Bean 的 Class 全路径，对应 XML 中的 &lt;bean class=&quot;&quot;&gt; 配置。 setLazyInit/isLazyInit 配置/获取 Bean 是否懒加载，这个对应了 XML 中的 &lt;bean lazy-init=&quot;&quot;&gt; 配置。 setDependsOn/getDependsOn 配置/获取 Bean 的依赖对象，这个对应了 XML 中的 &lt;bean depends-on=&quot;&quot;&gt; 配置。 setAutowireCandidate/isAutowireCandidate 配置/获取 Bean 是否是自动装配，对应了 XML 中的 &lt;bean autowire-candidate=&quot;&quot;&gt; 配置。 setPrimary/isPrimary 配置/获取当前 Bean 是否为首选的 Bean，对应了 XML 中的 &lt;bean primary=&quot;&quot;&gt; 配置。 setFactoryBeanName/getFactoryBeanName 配置/获取 FactoryBean 的名字，对应了 XML 中的 &lt;bean factory-bean=&quot;&quot;&gt; 配置，factory-bean 松哥在之前的入门视频中讲过，小伙伴们可以参考这里:https://www.bilibili.com/video/BV1Wv41167TU。 setFactoryMethodName/getFactoryMethodName 和上一条成对出现的，对应了 XML 中的 &lt;bean factory-method=&quot;&quot;&gt; 配置，不再赘述。 getConstructorArgumentValues 返回该 Bean 构造方法的参数值。 hasConstructorArgumentValues 判断上一条是否是空对象。 getPropertyValues 这个是获取普通属性的集合。 hasPropertyValues 判断上一条是否为空对象。 setInitMethodName/setDestroyMethodName 配置 Bean 的初始化方法、销毁方法。 setDescription/getDescription 配置/返回 Bean 的描述。 isSingleton Bean 是否为单例。 isPrototype Bean 是否为原型。 isAbstract Bean 是否抽象。 getResourceDescription 返回定义 Bean 的资源描述。 getOriginatingBeanDefinition 如果当前 BeanDefinition 是一个代理对象，那么该方法可以用来返回原始的 BeanDefinition BeanDefinition实现类 BeanDefinitionRegistry BeanDefinitionRegistry类图 ​ 我们定义bean时不管是通过java config方式，还是xml配置文件的方式，最终都会解析成BeanDefinition，而这些BeanDefinition都需要注册到容器中，这个注册的过程是通过接口org.springframework.beans.factory.support.BeanDefinitionRegistry来定义的，也就是说BeanDefinitionRegistry子类具备注册bean的定义到ioc容器的能力 123456789101112131415161718192021222324252627&gt;org.springframework.beans.factory.support.BeanDefinitionRegistry&gt;// 定义bean定义的注册相关方法的接口public interface BeanDefinitionRegistry extends AliasRegistry &#123; // 在注册中心注册一个bean定义，必须支持RootBeanDefinition和ChildBeanDefinition void registerBeanDefinition(String beanName, BeanDefinition beanDefinition) throws BeanDefinitionStoreException; // 移除对应bean名称的bean定义 void removeBeanDefinition(String beanName) throws NoSuchBeanDefinitionException; // 返回给定bean名称对应的bean定义 BeanDefinition getBeanDefinition(String beanName) throws NoSuchBeanDefinitionException; // 检查注册中心是否包含指定名称的bean定义 boolean containsBeanDefinition(String beanName); // 返回注册中心中所有的bean定义的名称的数组 String[] getBeanDefinitionNames(); // 返回注册中心中bean定义的个数 int getBeanDefinitionCount(); // 返回beanName是否被占用，即已经在注册中心中注册了bean定义 boolean isBeanNameInUse(String beanName);&#125; bean注册的流程 sequenceDiagram 后置处理器 ->> 扫描配置 : xml、java config 扫描配置 ->> BeanDefinition : 解析生成bean定义对象 BeanDefinition ->> DefaultListableBeanFactory : BeanDefinition注册 DefaultListableBeanFactory ->> DefaultSingletonBeanRegistry : 单例bean实例化注册到ioc容器 SingletonBeanRegistry 单例bean注册表接口，这个是单例bean注册的顶级接口，主要是用于管理单例的注册、获取 12345678910111213141516171819202122232425public interface SingletonBeanRegistry &#123; // 以指定的名字 把Object放进去 // 1、给定的Object必须是被完全初始化了的(比如new出来的) // 2、此注册接口不会提供任何用以初始化的回调函数(比如：InitializingBean、afterPropertiesSet都是不会执行的) // 3、如果此接口的实现类是一个BeanFactory，最好最好最好将你的类注册成Bean Definition而不是直接使用对象(这样就可以使你定义的Bean收到initialization和destruction回调) void registerSingleton(String beanName, Object singletonObject); // 仅仅返回已经初始化完成的Bean，对于还没有初始化的Bean Definition不予以考虑 // 但是要注意，此方法**并不支持使用别名**对Bean进行查找，如果只有别名的话，要先通过BeanFactory的接口获取到Bean对应的全限定名称（transformedBeanName()） Object getSingleton(String beanName); // 检查此实例是否包含指定名字的并且！！！已经初始化完成的单例Bean（不支持别名查找） // BeanFactory#containsBean是containsSingleton(beanName) || containsBeanDefinition(beanName) boolean containsSingleton(String beanName); // 返回所有单例 Bean 的名称 String[] getSingletonNames(); // 返回已注册的单例 Bean 实例数量 int getSingletonCount(); // 返回当前使用的单例锁，主要提供给外部协作者使用 Object getSingletonMutex();&#125; *DefaultSingletonBeanRegistry DefaultSingletonBeanRegistry是整个ioc容器比较核心的类，因为它负责存储、提供单例的bean实例、维护bean之间关系以及维护已经注册的bean的名称 解决循环依赖的三级缓存,存储单例bean 注册单例bean 添加单例bean到ioc容器 获取单例bean 重要属性说明 123456789101112131415161718192021222324// 单例对象的缓存:从beanname到bean实例 一级缓存，存放完整的 Beanprivate final Map&lt;String, Object&gt; singletonObjects = new ConcurrentHashMap&lt;&gt;(256);// 单例工厂的缓存:从beanname到ObjectFactory 三级缓存，存放的是 Bean 工厂，主要是生产 Bean，解决循环private final Map&lt;String, ObjectFactory&lt;?&gt;&gt; singletonFactories = new HashMap&lt;&gt;(16);// 早期单例对象的缓存:从beanname到bean实例 二级缓存 解决循环依赖二级缓存，存放提前暴露的Bean，Bean 是不完整的，未完成属性注入和执行 init 方法private final Map&lt;String, Object&gt; earlySingletonObjects = new HashMap&lt;&gt;(16);// 一组已注册的单例，包含按注册顺序排列的beannameprivate final Set&lt;String&gt; registeredSingletons = new LinkedHashSet&lt;&gt;(256);// 正在创建的单例的beanName的集合private final Set&lt;String&gt; singletonsCurrentlyInCreation = Collections.newSetFromMap(new ConcurrentHashMap&lt;&gt;(16));// 当前不检查的bean的集合private final Set&lt;String&gt; inCreationCheckExclusions = Collections.newSetFromMap(new ConcurrentHashMap&lt;&gt;(16));// 异常集合private Set&lt;Exception&gt; suppressedExceptions;// 当前是否在销毁bean中private boolean singletonsCurrentlyInDestruction = false;// 具有销毁能力的bean实例，DisposableBean的实现类private final Map&lt;String, Object&gt; disposableBeans = new LinkedHashMap&lt;&gt;();// 内部bean和外部bean之间关系private final Map&lt;String, Set&lt;String&gt;&gt; containedBeanMap = new ConcurrentHashMap&lt;&gt;(16);// 指定bean与依赖指定bean的集合，比如bcd依赖a，那么就是key为a，bcd为valueprivate final Map&lt;String, Set&lt;String&gt;&gt; dependentBeanMap = new ConcurrentHashMap&lt;&gt;(64);// 指定bean与指定bean依赖的集合，比如a依赖bcd，那么就是key为a，bcd为valueprivate final Map&lt;String, Set&lt;String&gt;&gt; dependenciesForBeanMap = new ConcurrentHashMap&lt;&gt;(64); 重要方法说明 registerSingleton 这个是对 SingletonBeanRegistry （此注册方法不会提供任何用以初始化的回调函数） 接口方法的实现，调用这个方法注册bean的一般都是框架自身的bean，比如spring准备上下文需要的bean StandardServletEnvironment 1234567891011121314public void registerSingleton(String beanName, Object singletonObject) throws IllegalStateException &#123; Assert.notNull(beanName, &quot;Bean name must not be null&quot;); Assert.notNull(singletonObject, &quot;Singleton object must not be null&quot;); synchronized (this.singletonObjects) &#123; Object oldObject = this.singletonObjects.get(beanName); // 如果缓存有，说明已经注册过 if (oldObject != null) &#123; throw new IllegalStateException(&quot;Could not register object [&quot; + singletonObject + &quot;] under bean name &#x27;&quot; + beanName + &quot;&#x27;: there is already object [&quot; + oldObject + &quot;] bound&quot;); &#125; // 缓存没有，开始注册 addSingleton(beanName, singletonObject); &#125;&#125; addSingleton 单例bean加入到缓存 12345678910111213protected void addSingleton(String beanName, Object singletonObject) &#123; synchronized (this.singletonObjects) &#123; // 加入单例对象的缓存（一级缓存） this.singletonObjects.put(beanName, singletonObject); // 既然加入了单例对象的缓存，那singletonFactories和earlySingletonObjects就不再持有 // 删除三级级缓存 this.singletonFactories.remove(beanName); // 删除二级缓存 this.earlySingletonObjects.remove(beanName); // 加入已注册的bean this.registeredSingletons.add(beanName); &#125;&#125; addSingletonFactory 添加三级缓存 12345678910111213141516//增加单例工程的单例，取单例的时候调用getObject方法protected void addSingletonFactory(String beanName, ObjectFactory&lt;?&gt; singletonFactory) &#123; Assert.notNull(singletonFactory, &quot;Singleton factory must not be null&quot;); synchronized (this.singletonObjects) &#123; if (!this.singletonObjects.containsKey(beanName)) &#123; this.singletonFactories.put(beanName, singletonFactory); this.earlySingletonObjects.remove(beanName); this.registeredSingletons.add(beanName); &#125; &#125;&#125;@FunctionalInterfacepublic interface ObjectFactory&lt;T&gt; &#123; T getObject() throws BeansException;&#125; getSingleton 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192public Object getSingleton(String beanName) &#123; // 允许早期依赖 return getSingleton(beanName, true);&#125;protected Object getSingleton(String beanName, boolean allowEarlyReference) &#123; //如果一级缓存有直接返回 Object singletonObject = this.singletonObjects.get(beanName); //缓存没有的情况但正在创建 if (singletonObject == null &amp;&amp; isSingletonCurrentlyInCreation(beanName)) &#123; synchronized (this.singletonObjects) &#123; //如果早期缓存中有，说明正在加载，则不处理直接返回 //一级缓存拿不到尝试去二级缓存拿 singletonObject = this.earlySingletonObjects.get(beanName); //allowEarlyReference允许是否从singletonFactories读取 if (singletonObject == null &amp;&amp; allowEarlyReference) &#123; // 某些方法提前初始化的时候会调用addSingletonFactory，把ObjectFactory缓存在singletonFactories中 // 二级缓存拿不到到三级缓存拿 ObjectFactory&lt;?&gt; singletonFactory = this.singletonFactories.get(beanName); if (singletonFactory != null) &#123; //如果singletonFactories有，调用getObject方法返回 singletonObject = singletonFactory.getObject(); // singletonFactories产生的对象放入earlySingletonObjects中 this.earlySingletonObjects.put(beanName, singletonObject); // 已经产生过一次对象了，所以就不能再用了，后面直接用earlySingletonObjects获取 this.singletonFactories.remove(beanName); &#125; &#125; &#125; &#125; return singletonObject;&#125;public Object getSingleton(String beanName, ObjectFactory&lt;?&gt; singletonFactory) &#123; Assert.notNull(beanName, &quot;Bean name must not be null&quot;); synchronized (this.singletonObjects) &#123; // 已经创建过了，直接返回 Object singletonObject = this.singletonObjects.get(beanName); if (singletonObject == null) &#123; // 当前在销毁bean，不能创建 if (this.singletonsCurrentlyInDestruction) &#123; throw new BeanCreationNotAllowedException(beanName, &quot;Singleton bean creation not allowed while singletons of this factory are in destruction &quot; + &quot;(Do not request a bean from a BeanFactory in a destroy method implementation!)&quot;); &#125; if (logger.isDebugEnabled()) &#123; logger.debug(&quot;Creating shared instance of singleton bean &#x27;&quot; + beanName + &quot;&#x27;&quot;); &#125; // 创建前检查，记录正在加载状态 beforeSingletonCreation(beanName); boolean newSingleton = false; // 如果当前没有异常，初始化异常集合 boolean recordSuppressedExceptions = (this.suppressedExceptions == null); if (recordSuppressedExceptions) &#123; this.suppressedExceptions = new LinkedHashSet&lt;&gt;(); &#125; try &#123; // 通过ObjectFactory的getObject创建bean，实际是回调createBean方法 singletonObject = singletonFactory.getObject(); newSingleton = true; &#125; catch (IllegalStateException ex) &#123; // Has the singleton object implicitly appeared in the meantime -&gt; // if yes, proceed with it since the exception indicates that state. // 有可能是其他方式创建的bean singletonObject = this.singletonObjects.get(beanName); if (singletonObject == null) &#123; throw ex; &#125; &#125; catch (BeanCreationException ex) &#123; if (recordSuppressedExceptions) &#123; for (Exception suppressedException : this.suppressedExceptions) &#123; ex.addRelatedCause(suppressedException); &#125; &#125; throw ex; &#125; finally &#123; if (recordSuppressedExceptions) &#123; this.suppressedExceptions = null; &#125; // 创建后检查，移除加载状态 afterSingletonCreation(beanName); &#125; if (newSingleton) &#123; // 是新创建的bean，就加入到缓存中并移除其他缓存，如果是其他方式创建的bean，说明已经加入过缓存了，这边不再加入 addSingleton(beanName, singletonObject); &#125; &#125; return singletonObject; &#125;&#125; removeSingleton 移除单例bean 123456789101112protected void removeSingleton(String beanName) &#123; synchronized (this.singletonObjects) &#123; // 移除一级缓存 this.singletonObjects.remove(beanName); // 移除三级缓存 this.singletonFactories.remove(beanName); // 移除二级缓存 this.earlySingletonObjects.remove(beanName); // 移除已注册的beanName名单 this.registeredSingletons.remove(beanName); &#125;&#125; beforeSingletonCreation 创建单例bean之前的检查 123456protected void beforeSingletonCreation(String beanName) &#123; // 如果这个beanName要检查，看看add的时候返回什么，如果返回false，说明已经在创建了，抛异常 if (!this.inCreationCheckExclusions.contains(beanName) &amp;&amp; !this.singletonsCurrentlyInCreation.add(beanName)) &#123; throw new BeanCurrentlyInCreationException(beanName); &#125;&#125; afterSingletonCreation 创建单例bean之后的检查 123456protected void afterSingletonCreation(String beanName) &#123; // 如果这个beanName要检查，看看remove返回什么，如果返回false，说明已经创建完了。 if (!this.inCreationCheckExclusions.contains(beanName) &amp;&amp; !this.singletonsCurrentlyInCreation.remove(beanName)) &#123; throw new IllegalStateException(&quot;Singleton &#x27;&quot; + beanName + &quot;&#x27; isn&#x27;t currently in creation&quot;); &#125;&#125; registerDisposableBean 注册具有销毁业务的bean， 123456789public void registerDisposableBean(String beanName, DisposableBean bean) &#123; synchronized (this.disposableBeans) &#123; this.disposableBeans.put(beanName, bean); &#125;&#125;public interface DisposableBean &#123; void destroy() throws Exception;&#125; BeanFactory体系 BeanFactory BeanFactory为Spring的IoC功能提供了底层的基础，但是它仅仅被用于和第三方框架的集成，现在对于大部分的Spring用户来说都是历史了。BeanFactory及其相关的接口，例如：BeanFactoryAware，InitializingBean，DisposableBean，在Spring中仍然有所保留，目的就是为了让大量的第三方框架和Spring集成时保持向后兼容 BeanFactory和类加载器一样，也是有父BeanFactory的，具体体现在HierarchicalBeanFactory接口 12345678910111213141516171819202122232425262728293031323334package org.springframework.beans.factory;public interface BeanFactory &#123; /** * 用来引用一个实例，或把它和工厂产生的Bean区分开，就是说，如果一个FactoryBean的名字为a，那么，&amp;a会得到那个Factory */ String FACTORY_BEAN_PREFIX = &quot;&amp;&quot;; /* * 四个不同形式的getBean方法，获取实例 */ Object getBean(String name) throws BeansException; &lt;T&gt; T getBean(String name, Class&lt;T&gt; requiredType) throws BeansException; &lt;T&gt; T getBean(Class&lt;T&gt; requiredType) throws BeansException; Object getBean(String name, Object... args) throws BeansException; boolean containsBean(String name); // 是否存在 boolean isSingleton(String name) throws NoSuchBeanDefinitionException;// 是否为单实例 boolean isPrototype(String name) throws NoSuchBeanDefinitionException;// 是否为原型（多实例） boolean isTypeMatch(String name, Class&lt;?&gt; targetType) throws NoSuchBeanDefinitionException;// 名称、类型是否匹配 Class&lt;?&gt; getType(String name) throws NoSuchBeanDefinitionException; // 获取类型 String[] getAliases(String name);// 根据实例的名字获取实例的别名&#125; FACTORY_BEAN_PREFIX` String FACTORY_BEAN_PREFIX = &quot;&amp;&quot;; 根据beanName从spring容器获取实例，如果该实例不是FactoryBean类型，则直接返回该实例，这也是我们平时用的最多的、最普通的情况；如果该实例是FactoryBean类型，而name又是以&amp;开头，也直接返回该实例，说明我们想要的就是FactoryBean实例；如果name不是以&amp;开头，而该实例又是FactoryBean类型，则会调用该实例的getObject()来创建我们需要的目标实例 1234567891011121314151617181920212223// 定义UserFactoryBean并使用别名注册@Component(&quot;user&quot;)public class UserFactoryBean implements FactoryBean&lt;User&gt; &#123; @Override public User getObject() throws Exception &#123; // 假设User的实例化过程比较复杂，在此处进行User的实例化 return new User(); &#125; @Override public Class&lt;?&gt; getObjectType() &#123; return User.class; &#125; @Override public boolean isSingleton() &#123; return true; &#125;&#125;// 这种写法是获取 UserFactoryBean 实例applicationContext.getBean(BeanFactory.FACTORY_BEAN_PREFIX + &quot;user&quot;);// 这种写法是获取 User实例applicationContext.getBean(&quot;user&quot;); *AutowireCapableBeanFactory 定义BeanFactory自动装配接口 对于想拥有自动装配能力，并且想要把这种能力暴露给外部应用的beanfactory需要实现该接口 正常情况下，不要实现该接口，应该实现BeanFactory或者ListableBeanFactory接口 需要注意的是ApplicationContext并没实现该接口。如果需要用到自动装配功能的话，可以调用ApplicationContext.getAutowireCapableBeanFactory()方法，来获取此接口实例 如果一个接口实现了该接口，很大程度上，还要实现BeanFactoryWare接口，这样子就能在上下文中返回BeanFactory 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485public interface AutowireCapableBeanFactory extends BeanFactory &#123; //变量指明工厂没有自动装配的bean int AUTOWIRE_NO = 0; //根据名称自动装配 int AUTOWIRE_BY_NAME = 1; //根据类型自动装配 int AUTOWIRE_BY_TYPE = 2; //标识按照贪婪策略匹配出的最符合的构造方法来自动装配的常量 int AUTOWIRE_CONSTRUCTOR = 3; //根据bean的class内部自动装配，但是Spring3.0后被弃用 //先使用constructor进行装配，如果不成功就使用byType来装配 @Deprecated int AUTOWIRE_AUTODETECT = 4; //参数：bean的class类型 //返回：bean的实例 //功能：根据bean的class类型，创建对应的bean实例 &lt;T&gt; T createBean(Class&lt;T&gt; beanClass) throws BeansException; //使用autowireBeanProperties装配属性 void autowireBean(Object existingBean) throws BeansException; //自动装配属性 Object configureBean(Object existingBean, String beanName) throws BeansException; //------------------------------------------------------------------------- // Specialized methods for fine-grained control over the bean lifecycle //------------------------------------------------------------------------- //对bean的生命周期进行细粒度控制的专门方法 //执行完整的bean初始化（包括：initializedBean和BeanPostProcessors） //autowireMode：根据名字或类型来决定自动装配的方式 Object createBean(Class&lt;?&gt; beanClass, int autowireMode, boolean dependencyCheck) throws BeansException; //根据bean的类型和自动装配方式来自动装配 Object autowire(Class&lt;?&gt; beanClass, int autowireMode, boolean dependencyCheck) throws BeansException; //根据自动装配方式来装配bean的属性 void autowireBeanProperties(Object existingBean, int autowireMode, boolean dependencyCheck) throws BeansException; //将参数中指定了那么的Bean，注入给定实例当中 void applyBeanPropertyValues(Object existingBean, String beanName) throws BeansException; //初始化参数中指定的Bean，调用任何其注册的回调函数如setBeanName、setBeanFactory等。 Object initializeBean(Object existingBean, String beanName) throws BeansException; //调用参数中指定Bean的postProcessBeforeInitialization方法(如果实现了BeanPostProcessor接口) Object applyBeanPostProcessorsBeforeInitialization(Object existingBean, String beanName) throws BeansException; //调用参数中指定Bean的postProcessorsAfterInitialization方法(如果实现了BeanPostProcessor接口) Object applyBeanPostProcessorsAfterInitialization(Object existingBean, String beanName) throws BeansException; //销毁参数中指定的Bean，同时调用此Bean上的DisposableBean和DestructionAwareBeanPostProcessors方法 void destroyBean(Object existingBean); //------------------------------------------------------------------------- // Delegate methods for resolving injection points //------------------------------------------------------------------------- //销毁参数中指定的bean,同时调用此Bean上的DisposableBean和DestructionAwareBeanPostProcessors方法 &lt;T&gt; NamedBeanHolder&lt;T&gt; resolveNamedBean(Class&lt;T&gt; requiredType) throws BeansException; //查找唯一符合指定类的实例，如果有，则返回实例的名字和实例本身 Object resolveDependency(DependencyDescriptor descriptor, String requestingBeanName) throws BeansException; //查找唯一符合指定类的实例，如果有，则返回实例的名字和实例本身 Object resolveDependency(DependencyDescriptor descriptor, String requestingBeanName, Set&lt;String&gt; autowiredBeanNames, TypeConverter typeConverter) throws BeansException;&#125; ListableBeanFactory 提供一次性获取当前BeanFactory 所有bean信息能力 ​ ListableBeanFactory实现了BeanFactory接口， Listable意思是可列表的，ListableBeanFactory可以枚举它们的所有bean信息，而不用一个个通过bean的名称或类型一个个查找 (比如通过bean类型获取所有的bean) 。如果容器是有层级的，比如实现了HierarchicalBeanFactory接口，返回值不考虑层级的信息，只读取当前容器定义的信息 123456789101112131415161718192021222324252627282930313233343536373839404142434445public interface ListableBeanFactory extends BeanFactory &#123; //判断是否包含指定名称的BeanDefinition boolean containsBeanDefinition(String beanName); //获取BeanDefinition的数量 int getBeanDefinitionCount(); //获取所有BeanDefinition的名称 String[] getBeanDefinitionNames(); //获取指定类型的所有Bean的名称,包括子类 //通过bean 定义或者FactoryBean的getObjectType判断. String[] getBeanNamesForType(Class&lt;?&gt; type); //获取指定类型的所有Bean的名称,包括子类 //通过bean 定义或者FactoryBean的getObjectType判断. //includeNonSingletons 是否只要单例 //allowEagerInit 是否初始化懒加载的单例,FactoryBean初始化的类和工厂方法初始化的类.就是说执行这个方法会执行对应的初始化. String[] getBeanNamesForType(Class&lt;?&gt; type, boolean includeNonSingletons, boolean allowEagerInit); // 根据类型（包括子类）返回指定Bean名和Bean的Map &lt;T&gt; Map&lt;String, T&gt; getBeansOfType(Class&lt;T&gt; type) throws BeansException; //根据类型（包括子类）返回指定Bean名和Bean的Map //includeNonSingletons 是否只要单例 //allowEagerInit 是否初始化懒加载的单例,FactoryBean初始化的类和工厂方法初始化的类.就是说执行这个方法会执行对应的初始化. &lt;T&gt; Map&lt;String, T&gt; getBeansOfType(Class&lt;T&gt; type, boolean includeNonSingletons, boolean allowEagerInit) throws BeansException; //获取注解类型的Bean String[] getBeanNamesForAnnotation(Class&lt;? extends Annotation&gt; annotationType); // 找到使用注解的类 //并放在指定Bean名和Bean的Map Map&lt;String, Object&gt; getBeansWithAnnotation(Class&lt;? extends Annotation&gt; annotationType) throws BeansException; //查找一个类上的注解,如果找不到,父类,接口使用注解也算. // 根据指定Bean名和注解类型查找指定的Bean &lt;A extends Annotation&gt; A findAnnotationOnBean(String beanName, Class&lt;A&gt; annotationType) throws NoSuchBeanDefinitionException; &#125; HierarchicalBeanFactory HierarchicalBeanFactory继承BeanFactory并扩展使其支持层级结构 (对BeanFactory提供分层) 12345678public interface HierarchicalBeanFactory extends BeanFactory &#123; //返回本Bean工厂的父工厂 BeanFactory getParentBeanFactory(); //判断本地工厂是否包含这个Bean（忽略其他所有父工厂）。这也是分层思想的体现 boolean containsLocalBean(String name);&#125; ConfigurableBeanFactory ​ ConfigurableBeanFactory继承了HierarchicalBeanFactory, SingletonBeanRegistry两个接口。定义了一些对BeanFactory的配置功能，比如通过addBeanPostProcessor配置Bean后置处理器，setParentBeanFactory设置双亲Ioc容器 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188String SCOPE_SINGLETON = &quot;singleton&quot;;//单例 String SCOPE_PROTOTYPE = &quot;prototype&quot;;//原型 /** * 设置容器的父容器,获取父容器方法在父接口HierarchicalBeanFactory里 */ void setParentBeanFactory(BeanFactory parentBeanFactory) throws IllegalStateException; /** * 设置和获取类加载器,主要用于加载Bean,默认是线上上下文的loader */ void setBeanClassLoader(@Nullable ClassLoader beanClassLoader); @Nullable ClassLoader getBeanClassLoader(); /** * 设置获取临时类加载器 */ void setTempClassLoader(@Nullable ClassLoader tempClassLoader); @Nullable ClassLoader getTempClassLoader(); /** * 设置是否缓存给定BeanDefinition和确定Bean类型的元数据,默认是开启状态. * 关掉flag缓存会启用beanDefinition对象的热刷新.任何bean实例的创建都将重新查询bean class loader来确定这个Bean最新的类型 */ void setCacheBeanMetadata(boolean cacheBeanMetadata); boolean isCacheBeanMetadata(); /** * 为beanDefinition值中的表达式提供解决策略.默认的BeanFactory里是没有激活的表达式支持的. * 一个ApplicationContext通常会设置一个标准的表达式策略,以一种统一的EL兼容风格支持“#&#123;&#125;”表达式. */ void setBeanExpressionResolver(@Nullable BeanExpressionResolver resolver); @Nullable BeanExpressionResolver getBeanExpressionResolver(); /** * 设置和获取ConversionService接口,进行数据类型转换 * @param conversionService */ void setConversionService(@Nullable ConversionService conversionService); @Nullable ConversionService getConversionService(); /** * 添加一个PropertyEditorRegistrar应用于所有bean的创建过程. * 一个Registrar创建了一个新的PropertyEditor实例,并且会将他们注册到一个给定的Registry中,并尝试刷新每个bean的创建. * 这就避免了自定义Editor的同步应用需求.因此通常更倾向于使用这个方法来代替registerCustomEditor. */ void addPropertyEditorRegistrar(PropertyEditorRegistrar registrar); /** * 为所有给定类型的属性注册一个给定的自定义属性编辑器.通常在factory配置期间被调用. * 注意这个方法注册一个共享的自定义属性编辑器实例;为了线程安全,需要授权该实例去进行同步操作. * 通常更倾向于使用addPropertyEditorRegistrar来代替这个方法,这就避免了自定义编辑器同步的需要 */ void registerCustomEditor(Class&lt;?&gt; requiredType, Class&lt;? extends PropertyEditor&gt; propertyEditorClass); /** * 使用一个已经在BeanFactory里注册过的自定义属性编辑器来初始化给定的PropertyEditorRegistry. */ void copyRegisteredEditorsTo(PropertyEditorRegistry registry); /** * 设置或获取自定义的类型转换器,BeanFactory用它来对Bean的属性值,构造参数等进行转换.这将会覆盖默认的PropertyEditor机制, * 因此,使用无关的自定义Editor或自定义Editor Registrars.因为TypeConverter通常不是线程安全的,所以每次调用都会产生一个新的实例. * 如果默认的PropertyEditor机制被激活,获取typeConverter方法将会返回所有已被注册的自定义的typeConverter */ void setTypeConverter(TypeConverter typeConverter); TypeConverter getTypeConverter(); /** * 用来增加一个嵌入式的StringValueResolver,比如说注解的属性.可以参考SpringMVC中的ArgumentResolver. */ void addEmbeddedValueResolver(StringValueResolver valueResolver); /** * 确定是否有一个嵌入式的value resolver已经在这个bean factory中注册了,并且可以通过resolveEmbeddedValue函数来应用. */ boolean hasEmbeddedValueResolver(); /** * 决定一个给定的嵌入式的值,例如注解中的属性 */ @Nullable String resolveEmbeddedValue(String value); /** *添加一个新的BeanPostProcessor,通过这个工厂所创建的beans将会应用这个后置处理器. * 在工厂的配置期间调用.注意这里的Post-processor提交将会按着registration的顺序被依次应用. * 任何通过Ordered这个接口所实现的顺序语义将会被忽略.也要注意到自动检测的后置处理器将会在以编程方式注册的那些后置处理器之后执行. */ void addBeanPostProcessor(BeanPostProcessor beanPostProcessor); //获取已经注册的Bean后置处理器的个数 int getBeanPostProcessorCount(); /** *注册一个给定的scope,支持Scope的实现类. */ void registerScope(String scopeName, Scope scope); /** * 返回所有当前注册过的scope的名字.这个方法只返回明确注册过的scope的名字,内置的(Built-in)scopes像”singleton”和”prototype”不会被暴露. * 如果没有返回的是空数组. */ String[] getRegisteredScopeNames(); /** * 如果有的话,返回给定名字的Scope实现.和上一个函数一样,将只返回明确注册过的scope,内置的(Built-in)scopes像”singleton”和”prototype”不会被暴露. */ @Nullable Scope getRegisteredScope(String scopeName); /** * 提供一个与这个工厂相关的安全的访问控制上下文.这个绝不会为空. */ AccessControlContext getAccessControlContext(); /** * 从给定的工厂中拷贝所有相关的配置信息.应该包括了所有标准的配置,也包括了BeanPostProcessor,Scopes和factory-specific内置的一些配置. * 应该不包括任何真实Bean的metadata信息,像BeanDefinition对象和bean的别名等 */ void copyConfigurationFrom(ConfigurableBeanFactory otherFactory); /** * 给定一个bean的名字,创建它的别名.这个方法的典型应用是支持那些在XML的ids里是无效的名字(被用于Bean的命名). * 通常都是在factory的配置期间被调用,但是也可以用于别名的registration的运行时.所以一个实现了该函数的接口应该同步别名访问. */ void registerAlias(String beanName, String alias) throws BeanDefinitionStoreException; /** * 处理所有目标名称的别名和在这个工厂注册过的别名,然后为它们应用给定的StringValueResolver. * 这个value resolver是处理像目标bean名称甚或在别名名称里的占位符而设置的. */ void resolveAliases(StringValueResolver valueResolver); /** * 返回一个给定名字的合并后的BeanDefinition,如果有必要会将子BeanDefinition和父BeanDefinition进行合并. * 并且也会考虑祖先容器中的BeanDefinition */ BeanDefinition getMergedBeanDefinition(String beanName) throws NoSuchBeanDefinitionException; //判断给定名字的Bean是否是一个FactoryBean. boolean isFactoryBean(String name) throws NoSuchBeanDefinitionException; /** * 设置Bean的当前创建状态 * @param beanName * @param inCreation Bean是否正在创建中 */ void setCurrentlyInCreation(String beanName, boolean inCreation); boolean isCurrentlyInCreation(String beanName); /** * 为给定名称的Bean注册一个依赖Bean,并且该依赖Bean会在给定的Bean被销毁之前进行销毁 */ void registerDependentBean(String beanName, String dependentBeanName); /** *如果有的话,返回依赖于给定名字Bean的所有Bean名称. */ String[] getDependentBeans(String beanName); /** * 如果有的话,返回给定名字Bean所依赖的所有Bean名称. */ String[] getDependenciesForBean(String beanName); /** * 依据BeanDefinition,销毁给定Bean的实例,(通常会从这个工厂中获取一个原型实例). * 在销毁期间所抛出的任何异常都应该用捕获来取代往这个方法的调用者那里抛出. */ void destroyBean(String beanName, Object beanInstance); /** * 如果有的话,在当前目标Scope中销毁指定的ScopeBean. * 在销毁期间所抛出的任何异常都应该用捕获来取代往这个方法的调用者那里抛出 */ void destroyScopedBean(String beanName); /** * 销毁这个工厂中所有的singleton bean,包括一次性的已经注册的内嵌Bean.在工厂关闭的时候会被调用. * 在销毁期间所抛出的任何异常都应该用捕获来取代往这个方法的调用者那里抛出. */ void destroySingletons(); ConfigurableListableBeanFactory ​ ConfigurableListableBeanFactory继承了ListableBeanFactory, AutowireCapableBeanFactory, ConfigurableBeanFactory。在ConfigurableBeanFactory的基础上，并扩展了忽略依赖,自动装配判断,冻结bean的定义,枚举所有bean名称的功能 123456789101112131415161718192021222324252627282930313233public interface ConfigurableListableBeanFactory extends ListableBeanFactory, AutowireCapableBeanFactory, ConfigurableBeanFactory &#123; //在装配的时候忽略指定的依赖类型 void ignoreDependencyType(Class&lt;?&gt; type); //在装配的时候忽略指定的接口 void ignoreDependencyInterface(Class&lt;?&gt; ifc); //使用相应的自动装配值注册特殊依赖关系类型(ObjectFactory)。 void registerResolvableDependency(Class&lt;?&gt; dependencyType, @Nullable Object autowiredValue); //确定指定的bean是否有资格作为autowire候选者，注入到声明匹配类型依赖关系的其他bean中。 boolean isAutowireCandidate(String beanName, DependencyDescriptor descriptor) throws NoSuchBeanDefinitionException; //返回指定bean的已注册BeanDefinition，允许访问其属性值和构造函数参数值（可以在bean工厂后处理期间修改）。 BeanDefinition getBeanDefinition(String beanName) throws NoSuchBeanDefinitionException; //返回所有bean名称的迭代对象 Iterator&lt;String&gt; getBeanNamesIterator(); //清除合并的bean定义缓存，删除尚未被认为有资格进行完整元数据缓存的bean条目。 void clearMetadataCache(); //冻结所有bean定义，表明注册的bean定义不会被修改或进一步后处理。 void freezeConfiguration(); //返回是否冻结此工厂的bean定义 boolean isConfigurationFrozen(); //确保所有非lazy-init单例都被实例化 void preInstantiateSingletons() throws BeansException;&#125; AbstractBeanFactory AbstractAutowireCapableBeanFactory *DefaultListableBeanFactory *Bean初始化的全过程 常见问题 Spring中有哪些方式可以把Bean注入到IOC容器 使用xml文件的方式来定义bean,在spring启动的时候会解析这个xml，把bean装载到IOC容器 使用@ComponentScan注解来扫描声明了 @Controller、@Service、@Component注解的类 使用@Configuration注解声明配置类，并使用@Bean实现bean的定义 使用@Import注解导入配置类或普通的Bean 使用FactoryBean这一个工厂Bean来构建一个Bean的实例 实现ImportBeanDefinitionRegistrat这个接口，可以动态注入bean实例 实现ImportSelector接口。动态批量注入Bean对象 Spring为什么要用三级缓存来解决循环依赖 名称 描述 singletonObjects 一级缓存，存放完整的 Bean earlySingletonObjects 二级缓存，存放提前暴露的Bean，Bean 是不完整的，未完成属性注入和执行 init 方法 singletonFactories 三级缓存，存放的是 Bean 工厂，主要是生产 Bean，存放到二级缓存中 什么是依赖注入（DI）？什么是控制反转(IOC)？ 在 Spring 中，有几种依赖注入方式？ 依赖注入：对象的创建过程由框架管理；对象外部资源的获取由对象本身反转到Ioc容器 控制反转： 控制：Ioc容器控制了对象；IOC容器控制对象的创建和外部资源的获取 反转：对象的创建和外部资源的获取由程序本身反转到Ioc容器 spring的依赖注入有哪些方式 变量注入: @Autowired 构造器注入 set方法注入 Spring自动装配的方式 byName byType constructor autodetect：首先尝试使用constructor来自动装配，如果无法工作，则使用byType方式 spring是如何创建一个Bean对象的 bean -&gt; 推断构造函数 -&gt; 反射生成对象 -&gt; 依赖注入（属性填充）-&gt; 初始化前（@PostConstruct） -&gt; Aware接口 -&gt; 初始化（InitializingBean）-&gt; 初始化后（AOP）-&gt; 代理对象 -&gt; 放入一级缓存 Spring事务传播行为 传播属性 描述 REQUIRED 如果当前没有事务，就新建一个事务，如果已经存在一个事务中，加入到这个事务中（默认） REQUIRES_NEW 新建事务，如果当前存在事务，把当前事务挂起 SUPPORTS 支持当前事务，如果当前没有事务，就以非事务方式执行 NOT_SUPPORTED 不支持事务，如果当前存在事务就将事务挂起 MANDATORY 使用当前的事务，如果当前没有事务，就抛出异常 NEVER 不支持事务，如果当前存在事务，则抛出异常 NESTED 如果当前存在事务，则在嵌套事务内执行。如果当前没有事务，则执行与REQUIRED类似的操作 嵌套事务：（部分事务提交）内层事务的回滚不影响外层事务的提交 什么是单例池？作用是什么？ DefaultSingletonBeanRegistry#singletonObjects 这个 ConcurrentHashMap就是单例池 作用就是缓存单例bean对象 Bean对象和普通对象之间的区别是什么 区别：对象的创建流程不同，spring的bean创建包含了依赖注入，初始化前操作，初始化，初始化后操作（AOP） 推断构造方法是什么意思 spring 需要通过构造函数反射创建对象，这个时候就面临构造函数选择（如果有多个），默认使用无参构造函数 如果没有无参构造函数并且有多个有参构造函数，则抛出异常（不知如何选择），除非给其中一个构造函数加@Autowired注解（暗示spring使用它） 如果没有无参构造函数，并且只有一个有参构造函数，则使用它（没得选择） 单例Bean和单例模式之间有什么关系 单例bean：通过相同的依赖注入条件能获取到同一个对象，ioc容器能注册多个类型相同的单例bean.单例bean是相对依赖注入条件而言的 单例模式：整个JVM虚拟机中只允许存在这个类型的一个实例 什么是先bytype再byname 在构造函数注入中：假设在ioc容器中注册了多个类型相同的单例bean,根据类型能够获取多个单例bean,那么就再按照变量名称匹配 12345678910111213141516171819202122232425262728293031323334353637@Configurationpublic class UserConfig &#123; @Bean public SysUserEntity sysUserEntity() &#123; return new SysUserEntity().setUserId(1L); &#125; @Bean public SysUserEntity sysUserEntity2() &#123; return new SysUserEntity().setUserId(2L); &#125; @Bean public SysUserEntity sysUserEntity3() &#123; return new SysUserEntity().setUserId(3L); &#125;&#125;@Servicepublic class UserService &#123; SysUserEntity sysUserEntity; /** * spring在ioc容器中能匹配到多个 SysUserEntity 实例,分别为 * name userId * sysUserEntity userId = 1 * sysUserEntity2 userId = 2 * sysUserEntity3 userId = 3 * 那么再根据构造函数的形式参数名称判断，最终选择sysUserEntity2 * 这就是先bytype再byname */ public UserService(SysUserEntity sysUserEntity2) &#123; this.sysUserEntity = sysUserEntity; &#125;&#125; Spring AOP底层是怎么工作的 Spring事务底层是怎么工作的 同类方法调用为什么会事务失效","categories":[],"tags":[]},{"title":"Sort","slug":"Sort","date":"2022-03-11T03:58:13.000Z","updated":"2023-07-14T06:49:15.569Z","comments":true,"path":"2022/03/11/Sort/","link":"","permalink":"https://wugengfeng.cn/2022/03/11/Sort/","excerpt":"","text":"排序 堆排序 堆构建过程 什么是堆？ ​ 堆就是用数组实现的二叉树，所以它没有使用父指针或者子指针。堆根据“堆属性”来排序，“堆属性”决定了树中节点的位置 定义 通常堆是通过一维数组来实现的。在数组起始位置为0的情形中： 父节点i的左子节点在位置：2i+1 父节点i的右子节点在位置：2i+2 子节点i的父节点在位置：i/2 -1 大顶堆：每个节点的值都大于或等于其子节点的值，在堆排序算法中用于升序排列 小顶堆：每个节点的值都小于或等于其子节点的值，在堆排序算法中用于降序排列 堆的操作 在堆的数据结构中，堆中的最大值总是位于根节点（在优先队列中使用堆的话堆中的最小值位于根节点）。堆中定义以下几种操作： 最大堆调整（Max Heapify）：将堆的末端子节点作调整，使得子节点永远小于父节点 创建最大堆（Build Max Heap）：将堆中的所有数据重新排序 堆排序（HeapSort）：移除位在第一个数据的根节点，并做最大堆调整的递归运算 JUC中的优先级队列就是使用小顶堆来实现的 堆排序算法步骤 将待排序的数组初始化为大顶堆，该过程即建堆 将堆顶元素与最后一个元素进行交换，除去最后一个元素外可以组建为一个新的大顶堆 由于第二步堆顶元素跟最后一个元素交换后，新建立的堆不是大顶堆，需要重新建立大顶堆。重复上面的处理流程，直到堆中仅剩下一个元素 代码实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117public class HeapSort &#123; private int[] arr; public HeapSort(int[] arr) &#123; this.arr = arr; &#125; public void sort() &#123; int len = arr.length - 1; // 获取最后一个子节点 int beginIndex = (len &gt;&gt; 1) - 1; // 构建一个大顶堆 for (int i = beginIndex; i &gt;= 0; i--) &#123; maxHeap(i, len); &#125; // 将堆顶元素和队尾交换 for (int i = len; i &gt; 0; i--) &#123; // 将堆顶元素和最后一个元素交换 swap(0, i); // 将最大值给到堆尾后，破坏了最大堆，除去最后一个元素外继续构建大顶堆 maxHeap(0, i - 1); &#125; &#125; /** * 构建大顶堆 * * @param index */ public void maxHeap(int index, int len) &#123; // 求左子节点 int left = (index &lt;&lt; 1) + 1; // 求右子节点 int right = left + 1; // 当前堆的最大值，默认左子节点 (下标) int max = left; // 左节点越界，直接结束 if (left &gt; len) &#123; return; &#125; // 判断计算出来的右子节点是否越界, 获取子节点最大的 if (right &lt;= len &amp;&amp; arr[right] &gt; arr[left]) &#123; max = right; &#125; // 子节点和当前堆顶比较，对调堆顶 if (arr[max] &gt; arr[index]) &#123; swap(max, index); maxHeap(max, len); &#125; &#125; public void compare(int num) &#123; // 判断小顶堆堆顶是否小于当前值，是则替换 if (arr[0] &lt; num) &#123; arr[0] = num; // 重建小顶堆 sort(); &#125; &#125; /** * 构建小顶堆 * * @param index * @param len */// public void minHeap(int index, int len) &#123;//// // 求左子节点// int left = (index &lt;&lt; 1) + 1;// // 求右子节点// int right = left + 1;// // 当前堆的最大值，默认左子节点 (下标)// int min = left;//// // 左节点越界，直接结束// if (left &gt; len) &#123;// return;// &#125;//// // 判断计算出来的右子节点是否越界, 获取子节点最大的// if (right &lt;= len &amp;&amp; arr[right] &lt; arr[left]) &#123;// min = right;// &#125;//// // 子节点和当前堆顶比较，对调堆顶// if (arr[min] &lt; arr[index]) &#123;// swap(min, index);// minHeap(min, len);// &#125;// &#125; public void swap(int i, int j) &#123; int temp = arr[i]; arr[i] = arr[j]; arr[j] = temp; &#125; /** * 测试用例 * &lt;p&gt; * 输出： * [0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 6, 7, 7, 7, 8, 8, 8, 9, 9, 9] */ public static void main(String[] args) &#123; int[] arr = new int[]&#123;3, 5, 3, 0, 8, 6, 1, 5, 8, 6, 2, 4, 9, 4, 7, 0, 1, 8, 9, 7, 3, 1, 2, 5, 9, 7, 4, 0, 2, 6&#125;; new HeapSort(arr).sort(); System.out.println(arr); &#125;&#125; 经典 Top K 问题 除了排序外，经常还利用大顶堆、小顶堆解决 Top K 问题 大顶堆： 从N个数中找出最小的K个 小顶堆：从N个数中找出最大的K个 核心思想：最大K个，先获取K个值，建立小顶堆。然后遍历 K～N的元素，与堆顶比较，元素比堆顶大则进行替换。然后再重新构建小顶堆。当循环多次后，堆顶能够过滤掉大部分数据 Top K 也能使用一个长度为K的链表进行排序，记录最大值和最小值（链头，链尾），然后循环K～N的元素进行比较，如果元素最小值&lt;元素则从链表头部插入，如果元素&gt;最大值则从链表尾部插入，值相等不插入，当链表超过K长度，修剪链表头部 (海量数据TOP K 数据漏斗思想) 实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384/** * @description: * @author: ken 😃 * @create: 2022-03-11 10:26 * 从一亿的数组中获取最大的K个 **/public class Heap &#123; public static Random RANDOM = new Random(); public static int[] arr = new int[1000]; // 堆初始化 public static void init() &#123; int len = arr.length - 1; int lastSubNode = (len &gt;&gt; 1) - 1; for (int i = lastSubNode; i &gt;= 0; i--) &#123; minHeapIfy(i, len); &#125; &#125; // 构建小顶堆算法 public static void minHeapIfy(int currentIndex, int len) &#123; int left = (currentIndex &lt;&lt; 1) + 1; int right = left + 1; int subNodeMin = left; if (left &gt; len) &#123; return; &#125; if (right &lt;= len &amp;&amp; arr[right] &lt; arr[left]) &#123; subNodeMin = right; &#125; if (arr[subNodeMin] &lt; arr[currentIndex]) &#123; // 堆顶置换 swap(subNodeMin, currentIndex); // 递归让置换的直接点继续保持最小堆特性 minHeapIfy(subNodeMin, len); &#125; &#125; // 元素和堆顶比较 public static void compare(int num) &#123; if (arr[0] &lt; num) &#123; arr[0] = num; // 保持小顶堆 init(); &#125; &#125; public static void swap(int i, int k) &#123; int temp = arr[i]; arr[i] = arr[k]; arr[k] = temp; &#125; public static int getNum(int min, int max) &#123; return RANDOM.nextInt(max - min + 1) + min; &#125; public static void main(String[] args) &#123; // 数据初始化 int[] data = new int[100000000]; for (int i = 0; i &lt; data.length; i++) &#123; data[i] = getNum(0, 100000); &#125; // 随机设置个最大值，测试是否正确 data[getNum(0, 50000000)] = 999999999; // 获取前K个组成小顶堆 long start = System.currentTimeMillis(); arr = Arrays.copyOfRange(data, 0, 1000); for (int i = 1000; i &lt; data.length; i++) &#123; compare(data[i]); &#125; Arrays.stream(arr).forEach(System.out::println); System.out.println(String.format(&quot;耗时: %s&quot;, (System.currentTimeMillis() - start) / 1000.f)); &#125;&#125;","categories":[],"tags":[]},{"title":"Mysql","slug":"Mysql","date":"2022-02-26T02:51:54.000Z","updated":"2024-02-19T08:22:42.720Z","comments":true,"path":"2022/02/26/Mysql/","link":"","permalink":"https://wugengfeng.cn/2022/02/26/Mysql/","excerpt":"","text":"思维导图 数据库概念 关系型数据库（RDBMS） 概念 关系型数据库，如MySQL、Oracle、DB2、SQL Server等，是建立在关系模型的基础上的。这种数据库由多个相互关联的二维表组成。每个二维表，就像Excel的工作表一样，由行（代表记录）和列（代表数据字段）构成。表格中的每一行通常代表一个实体的实例，而列则表示实体的属性。关键的一点在于，这些表之间可以通过键（例如主键和外键）相互关联，使得不同表中的数据能够相互参照和整合。这样的设计允许数据库高效地组织、查询和维护大量 结构化 数据。 此外，关系型数据库与非关系型数据库（NoSQL）的主要区别在于数据存储和处理方式。关系型数据库依赖于预定义的模式和二维表来组织数据，而非关系型数据库可能使用更灵活的数据模型，如键值对、文档或图形结构，这使它们更适合处理非结构化或半结构化数据。 特点 结构化数据存储：使用规范化的表结构来组织数据，保证数据结构一致性和完整性。 SQL标准：通过统一的SQL语言进行数据查询和操作，实现高效的数据库管理。 事务处理支持：提供完整的事务处理机制，确保数据库操作的原子性和一致性。 索引和性能优化：利用索引来加速数据检索，采用各种优化策略提高数据库性能。 SQL语句分类 分类 全称 说明 DDL Data Definition Language 数据定义语言，用来定义数据库对象(数据库，表，字段) DML Data Manipulation Language 数据操作语言，用来对数据库表中的数据进行增删改 DQL Data Query Language 数据查询语言，用来查询数据库中表的记录 DCL Data Control Language 数据控制语言，用来创建数据库用户、控制数据库的访问权限 TCL Transaction Control Language 事务控制语言 MySql架构 连接层客户端连接管理：负责管理客户端和服务器之间的连接。它接收客户端请求，并建立、管理、终止客户端到服务器的连接。认证与安全：在此层进行用户认证，检查用户是否有权限连接到MySQL服务器，并对数据传输进行加密处理以保证安全。连接池：优化资源使用，复用现有连接。 服务层SQL解析：解析SQL查询，进行语法检查，生成解析树。查询优化：优化器对查询进行优化，选择最有效的查询路径。缓存：查询缓存可以存储SELECT语句及其结果，加速后续相同的查询过程。 引擎层存储引擎：MySQL支持多种存储引擎，每种引擎都有其特点。例如，InnoDB支持事务处理、行级锁定和外键；MyISAM提供高速存储、全文搜索功能等。事务管理：对于支持事务的存储引擎（如InnoDB），在此层进行事务的管理。数据处理：执行具体的数据读写、缓存、锁定等操作。 存储层数据文件：物理存储数据的文件，包括数据文件和索引文件。数据组织：数据按照存储引擎的格式在磁盘上进行组织，例如InnoDB存储引擎使用表空间来存储数据。数据备份与恢复：负责数据的备份和在系统崩溃后的数据恢复。 存储引擎 存储引擎是数据库管理系统中用于实现 数据存储、索引建立 以及 数据更新 和 查询 等功能的底层软件组件。存储引擎是基于表的，而非基于整个数据库（库）。这意味着在同一个数据库中，不同的表可以使用不同的存储引擎，根据它们各自的需求优化性能和功能。因此，存储引擎也可以被称为 表类型。 在创建表时，可以明确指定要使用的存储引擎。如果在创建表时没有指定存储引擎，MySQL将使用默认的存储引擎，这通常是InnoDB，因为它提供了全面的事务支持、数据完整性和性能优化。 指定存储引擎 MySql5.6以后，如果不指定存储引擎，则默认使用 InnoDB 12345CREATE TABLE 表名( 字段1 字段1类型 [ COMMENT 字段1注释 ] , ...... 字段n 字段n类型 [COMMENT 字段n注释 ]) ENGINE = INNODB [ COMMENT 表注释 ]; 查询当前数据库支持的存储引擎 1show engines; MyISAM 特性 不支持事务 不支持外键 仅支持表级锁 全文索引 数据压缩 高速读取：读取性能比写入性能好，特别是在读密集型的应用中 存储文件 .frm 存储表结构定义，如字段名称、数据类型和其他与表结构相关的细节。 .MYD 存储表的实际数据。.MYD 文件包含了表中的行数据，但不包含任何索引信息。 .MYI 专门用于存储表的索引。这个文件包含了所有用于加速查询的索引信息。 故障恢复 健康状态查询 check table table_name 表损坏修复 repair table table_name 存储格式 静态表（默认）：所有字段都有固定长度。常见的静态字段类型包括CHAR、INT、FLOAT等。 动态表：包含可变长度字段，如VARCHAR、BLOB和TEXT。 压缩表：MyISAM表可以转换为压缩表，以节省空间并提高读取效率，只能读取，不能插入。 读写互斥 写操作锁定：当执行写操作时，MyISAM 会锁定整个表。这意味着，在写操作完成之前，其他任何读或写操作都必须等待。 写锁优先级高于读锁：在默认情况下，MyISAM倾向于优先赋予写操作锁。这意味着，如果读写操作同时到达，写操作将优先获得锁，这是为了防止写锁饥饿。 全文索引 不支持中文 MyISAM存储引擎支持全文索引（Full-Text Indexes），这是一种特别为文本搜索设计的索引类型。全文索引提供了一种比传统的LIKE查询更快、更高效的方式来搜索大量文本数据。 特点 快速文本搜索：全文索引允许对文本内容进行快速搜索，尤其是在处理大量数据时，它比LIKE模糊查询更高效。 支持复杂查询：全文索引支持自然语言搜索、布尔搜索等复杂查询，能够处理词组、否定词等。 索引更新：当表中的文本数据被插入、更新或删除时，全文索引会自动更新。 12345# 查询最小和最大搜索长度show variables like &#x27;%ft%&#x27;;ft_max_word_len 84ft_min_word_len 4 最小搜索长度 (ft_min_word_len)：这个变量定义了全文索引中词的最小长度。长度小于这个值的词将不会被包含在全文索引中。默认值通常是4。 最大搜索长度 (ft_max_word_len)：这个变量定义了索引中词的最大长度。长度超过这个值的词将不会被包含在全文索引中。 12345678910111213141516171819CREATE TABLE `my_isam` ( `id` int NOT NULL AUTO_INCREMENT, `name` varchar(20) DEFAULT NULL, `age` int DEFAULT NULL, `email` varchar(30) DEFAULT NULL, `address` varchar(60) DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=MyISAM AUTO_INCREMENT=2 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;insert into my_isam(name, age, email, address)values (&#x27;test1&#x27;, 18, &#x27;test1@test.com&#x27;, &#x27;shenzhen&#x27;), (&#x27;test2&#x27;, 19, &#x27;test2@test.com&#x27;, &#x27;beijing&#x27;), (&#x27;test3&#x27;, 20, &#x27;test3@test.com&#x27;, &#x27;shanghai&#x27;), (&#x27;test4&#x27;, 21, &#x27;test4@test.com&#x27;, &#x27;shandong&#x27;), (&#x27;test5&#x27;, 22, &#x27;test5@test.com&#x27;, &#x27;hunan&#x27;), (&#x27;test6&#x27;, 23, &#x27;test6@test.com&#x27;, &#x27;changsha&#x27;);# 全文索引create fulltext index idx_full on my_isam(address) 自然语言全文索引 默认情况下，或者使用 in natural language mode 修饰符时，match() 函数对文本集合执行自然语言搜索 123456# match 可以多个字段，类似es的搜索select *from my_isamwhere match(address) against(&#x27;shen&#x27;)shen zhen 布尔全文索引 在布尔搜索中，我们可以在查询中自定义某个被搜索的词语的相关性，当编写一个布尔搜索查询时，可以通过一些前缀修饰符来定制搜索 + 必须包含该词 - 必须不包含该词 &gt; 提高该词的相关性，查询的结果靠前 &lt; 降低该词的相关性，查询的结果靠后 (*)星号 通配符，只能接在词后面 1234567select *from my_isamwhere match(address) against(&#x27;sh*&#x27; in boolean mode)shan dongshang haishen zhen InnoDB 事务 行锁 间隙锁 外键 MVCC 自动生成hash索引 从MySQL 5.5版本开始，InnoDB成为默认的存储引擎。InnoDB提供全面的 ACID 事务支持，是MySQL中实现这一关键数据库特性的首要存储引擎之一。此外，InnoDB支持 行级锁定（Row-level Locking）和 多版本并发控制（MVCC），这两种机制共同提高了数据库在高并发环境下的读写性能和一致性。 特性 ACID兼容事务：InnoDB提供完整的 ACID 事务支持。这意味着它可以保证即使在系统崩溃或宕机的情况下，数据库操作也能保持一致性和完整性。 行级锁定：行级锁定允许数据库在执行事务时只锁定影响的数据行，而不是整个表，这显著提高了多用户环境中的并发性能。 MVCC：MVCC支持高效的读写分离，减少了读写操作之间的冲突（快照读和当前写）。 外键约束：InnoDB支持外键约束，这有助于维护不同表之间的参照完整性，自动处理相关表的更新和删除操作。 崩溃恢复：通过使用redo log（重做日志）和undo log（回滚日志），InnoDB能够在系统崩溃后恢复到一致状态，保证数据不丢失。 聚簇索引：InnoDB使用聚簇索引，将数据行直接存储在主键索引的叶节点中。这种方式可以加快主键查询的速度和便于范围查找。 ACIDAtomicity（原子性）含义：原子性确保事务中的所有操作要么全部完成，要么全部不执行。事务是一个不可分割的工作单位，事务中的操作要么全部成功，要么全部失败回滚。例子：在银行系统中，从一个账户转账到另一个账户涉及两个操作：从第一个账户扣款和向第二个账户存款。这两个操作必须同时成功或同时失败。Consistency（一致性）含义：一致性确保事务从一个一致的状态转变到另一个一致的状态。一致状态的定义是基于业务规则，确保数据库的数据和结构在事务前后都保持一致。例子：继续上面的银行转账例子，一致性确保在转账过程中账户的总额保持不变。Isolation（隔离性）含义：隔离性确保并发事务的执行彼此独立，事务之间不会互相影响。这意味着事务对数据的修改在提交之前，对其他事务是不可见的。例子：当两个银行客户同时检查同一个账户余额时，他们看到的应该是不受对方事务影响的稳定视图。Durability（持久性）含义：持久性确保一旦事务提交，它对数据库的更改就是永久性的。即使发生系统故障，事务的结果也不会丢失。例子：在银行转账事务提交后，即使系统崩溃，转账的结果（资金的移动）也会被永久保存。 存储文件类型 .frm ：用于存储表的结构信息，包括表的定义，如字段名称、类型、大小等。 .ibd：是一个完整的表空间文件，其中最基本的单位为页(Page)，一个Page通常为16k(存放聚簇索引)。 redo log（重做日志）：通常为.ib_logfile0和.ib_logfile1，这些文件用于记录事务操作，以便在数据库崩溃后进行数据恢复。 undo log（回滚日志）：记录了数据的旧版本，用于回滚事务和MVCC。 表空间 inodb_data_file_path：用于定义InnoDB共享表空间文件的名称和大小，例如innodb_data_file_path=ibdata1:10M:autoextend innodb_data_home_dir：定义了InnoDB表空间文件的默认位置。 1show variables like &#x27;innodb_file_per_table&#x27;; 文件-per-table 模式：当innodb_file_per_table被设置为ON（或1），InnoDB为每个新创建的表单独创建一个.ibd文件。这个文件包含了该表的数据和索引。 共享表空间：如果innodb_file_per_table被设置为OFF（或0），所有的InnoDB表都会共享一个大的表空间文件（通常是ibdata1）。 1idb2sid xxx.idb 通过以上命令可以查看ibd文件内容，以JSON形式输出 逻辑存储结构 InnoDB存储引擎在MySQL中使用一种 层级化 的方式来组织数据，这种结构包括表空间、段、区、页和行。下面是这些组件的详细解释： 表空间（Tablespace） : 表空间是InnoDB存储结构的最顶层，可以看作是存储数据的容器。它可以是单个文件（如在文件-per-table模式下的.ibd文件），也可以是共享的大文件（如ibdata1）。表空间包含 数据、索引、插入缓冲区、undo log 等数据结构。 段 （Segment）: 段是表空间内的一个逻辑部分，用于管理特定类型的数据。例如，数据段用于存储实际的表数据，索引段用于存储索引数据。每个段由多个区（Extent）组成。 区（Extent） : 区是表空间的一个分配单元。在InnoDB中，一个区是一组连续的页，通常包含64个页。区的大小固定，由页的大小决定（默认为1MB，基于16KB大小的页） 页（Page） : 页是InnoDB存储结构中的基本单元，也是磁盘I/O操作的最小单位。每个页默认为16KB大小。页按照不同的类型存储不同的信息，如数据页、索引页、undo页等。 行（Row） : 行是InnoDB的最小数据单位。InnoDB是面向行的存储引擎，这意味着数据按行来组织和存储。每个数据页包含多个行记录。 三大特性 插入缓冲（Change Buffer ） 双写缓冲区（Double Write Buffer） 自适应哈希索引（Adaptive Hash Index） MEMORY Hash索引 字段定长 表级锁 不支持大字段 数据存储内存，重启丢失 Memory存储引擎使用存在内存中的内容来创建表，每个Memory表只实际对应一个磁盘文件，在磁盘中表现为.frm文件。Memory类型的表访问速度非常快，因为它的数据是放在内存中的，并且默认使用hash索引，但是一旦服务关闭，表中的数据就会丢失。 特性 基于内存的存储：MEMORY引擎将数据存储在内存中，提供快速的数据访问速度，尤其适用于需要高速读取的场景。 数据易失性：MEMORY表的内容在数据库服务器重启后会丢失，因此主要用于临时数据，如缓存或会话信息。 索引类型： 支持 HASH索引，默认使用，适合快速的等值查询。 支持 B+Tree索引，适用于范围查询。 固定长度：MEMORY引擎中所有字段都被视为固定长度。例如，即使定义为VARCHAR(10)，也会像CHAR(10)一样处理，占用固定空间。 不支持大型字段：不支持BLOB和TEXT类型的字段，因为这些类型的数据通常较大，不适合存储在内存中。 表级锁：MEMORY引擎使用表级锁定机制，这可能在高并发写操作场景中成为性能瓶颈。 表大小限制：表的最大大小由系统变量max_heap_table_size决定，默认值通常是16MB。这个限制只对新创建的表有效。 MERGE MyISAM表组合 MERGE存储引擎提供了一种将多个相同结构的MyISAM表合并为一个虚拟表的机制。这种引擎对于处理大量数据或分区数据特别有用。 特性 组合多个表：MERGE引擎允许多个具有相同表结构（即相同的列和索引）的MyISAM表被合并为一个单一的虚拟表。这些实际的表被称为子表，而合并后的表被称为MERGE表。 查询优化：通过合并表，可以在一个查询中同时访问多个子表的数据，这对于执行大规模查询非常有效。 数据管理和维护：MERGE表本身不存储数据，数据仍然存储在子表中。这意味着对子表的更新会直接反映在MERGE表上。 只读或可写：默认情况下，MERGE表是只读的，但是如果所有子表都具有相同的结构，并且MERGE表被显式地声明为可写，则可以在MERGE表上执行插入、更新和删除操作。 数据库基础 数据类型 MySQL中的数据类型有很多，主要分为三类：数值类型、字符串类型、日期时间类型。 数值类型 类型 大小 范围（有符号） 范围（无符号） 用途 TINYINT 1 Bytes (-128，127) (0，255) 小整数值 SMALLINT 2 Bytes (-32 768，32 767) (0，65 535) 大整数值 MEDIUMINT 3 Bytes (-8 388 608，8 388 607) (0，16 777 215) 大整数值 INT或INTEGER 4 Bytes (-2 147 483 648，2 147 483 647) (0，4 294 967 295) 大整数值 BIGINT 8 Bytes (-9,223,372,036,854,775,808，9 223 372 036 854 775 807) (0，18 446 744 073 709 551 615) 极大整数值 FLOAT 4 Bytes (-3.402 823 466 E+38，-1.175 494 351 E-38)，0，(1.175 494 351 E-38，3.402 823 466 351 E+38) 0，(1.175 494 351 E-38，3.402 823 466 E+38) 单精度 浮点数值 DOUBLE 8 Bytes (-1.797 693 134 862 315 7 E+308，-2.225 073 858 507 201 4 E-308)，0，(2.225 073 858 507 201 4 E-308，1.797 693 134 862 315 7 E+308) 0，(2.225 073 858 507 201 4 E-308，1.797 693 134 862 315 7 E+308) 双精度 浮点数值 DECIMAL 对DECIMAL(M,D) ，如果M&gt;D，为M+2否则为D+2 DECIMAL(M，D)表示列可以存储D位小数的M位数，DECIMAL(8,4)表示能存储8位数，其中包含四位小数 依赖于M和D的值 小数值 int(5)：对于 int(5) 这样的声明，数字5表示展示宽度，而不是最大值。展示宽度并不限制值的范围，而是影响值的显示格式。 zerofill：当字段声明为 ZEROFILL 时，MySQL会用零填充展示值，以使其符合指定的展示宽度。例如，INT(5) ZEROFILL 会将数值123格式化为00123。 无符号UNSIGNED：UNSIGNED 属性用于指定整数类型为无符号。无符号整数只能存储正数和零。例如，INT UNSIGNED 的范围是0到4294967295。 日期时间类型 类型 大小 ( bytes) 范围 格式 用途 DATE 3 1000-01-01/9999-12-31 YYYY-MM-DD 日期值 TIME 3 ‘-838:59:59’/‘838:59:59’ HH:MM:SS 时间值或持续时间 YEAR 1 1901/2155 YYYY 年份值 DATETIME 8 1000-01-01 00:00:00/9999-12-31 23:59:59 YYYY-MM-DD HH:MM:SS 混合日期和时间值 TIMESTAMP 4 1970-01-01 00:00:00/2038结束时间是第 2147483647 秒，北京时间 2038-1-19 11:14:07，格林尼治时间 2038年1月19日 凌晨 03:14:07 YYYYMMDD HHMMSS 混合日期和时间值，时间戳 字符串类型 类型 大小 用途 CHAR 0-255 bytes 定长字符串 VARCHAR 0-65535 bytes 变长字符串 TINYBLOB 0-255 bytes 不超过 255 个字符的二进制字符串 TINYTEXT 0-255 bytes 短文本字符串 BLOB 0-65 535 bytes 二进制形式的长文本数据 数据空洞 TEXT 0-65 535 bytes 长文本数据 数据空洞 MEDIUMBLOB 0-16 777 215 bytes 二进制形式的中等长度文本数据 MEDIUMTEXT 0-16 777 215 bytes 中等长度文本数据 LONGBLOB 0-4 294 967 295 bytes 二进制形式的极大文本数据 LONGTEXT 0-4 294 967 295 bytes 极大文本数据 CHAR：定长字符串，如果有空位则用空串补位，性能较好 VARCHAR：性能较差，需要计算长度 函数 MySQL中的函数可以被理解为一组预定义的操作，这些操作封装了特定的逻辑或计算过程，使得用户可以在SQL查询中直接调用它们。这意味着用户无需编写复杂的代码来执行常见的数据库任务。函数在数据库中被设计为易于使用，并且是为了解决特定的业务或数据处理需求。 数值函数 功能 描述 ABS 返回数字的绝对值 ACOS 返回数字的反余弦值 ASIN 返回数字的反正弦值 ATAN 返回一个或两个数字的反正切 ATAN2 返回两个数字的反正切 AVG 返回表达式的平均值 CEIL 返回&gt; =到数字的最小整数值 CEILING 返回&gt; =到数字的最小整数值 COS 返回数字的余弦值 COT 返回数字的余切 COUNT 返回select查询返回的记录数 DEGREES 将弧度值转换为度数 DIV 用于整数除法 EXP 返回e提升到指定数字的幂 FLOOR 返回&lt;=到数字的最大整数值 GREATEST 返回参数列表的最大值 LEAST 返回参数列表的最小值 LN 返回数字的自然对数 LOG 返回数字的自然对数，或数字的对数到指定的基数 LOG10 返回数字的自然对数到10 LOG2 返回数字2的自然对数 MAX 返回一组值中的最大值 MIN 返回一组值中的最小值 MOD 返回数字的余数除以另一个数字 PI 返回PI的值 POW 返回被提升到另一个数的幂的数字的值 POWER 返回被提升到另一个数的幂的数字的值 RADIANS 将度数值转换为弧度 RAND 返回一个随机数 ROUND 将数字舍入到指定的小数位数 SIGN 返回数字的符号 SIN 返回数字的正弦值 SQRT 返回数字的平方根 SUM 计算一组值的总和 TAN 返回数字的正切值 TRUNCATE 将数字截断为指定的小数位数 日期函数 功能 描述 ADDDATE 将时间/日期间隔添加到日期，然后返回日期 ADDTIME 将时间间隔添加到时间/日期时间，然后返回时间/日期时间 CURDATE 返回当前日期 CURRENT_DATE 返回当前日期 CURRENT_TIME 返回当前时间 CURRENT_TIMESTAMP 返回当前日期和时间 CURTIME 返回当前时间 DATE 从日期时间表达式中提取日期部分 DATEDIFF 返回两个日期值之间的天数 DATE_ADD 将时间/日期间隔添加到日期，然后返回日期 DATE_FORMAT 格式化日期 DATE_SUB 从日期中减去时间/日期间隔，然后返回日期 DAY 返回给定日期的月中的某天 DAYNAME 返回给定日期的工作日名称 DAYOFMONTH 返回给定日期的月中的某天 DAYOFWEEK 返回给定日期的工作日索引 DAYOFYEAR 返回给定日期的一年中的某一天 EXTRACT 从给定日期提取部分 FROM_DAYS 从数字日期值返回日期 HOUR 返回给定日期的小时部分 LAST_DAY 提取指定日期的月份的最后一天 LOCALTIME 返回当前日期和时间 LOCALTIMESTAMP 返回当前日期和时间 MAKEDATE 根据年份和天数值创建并返回日期 MAKETIME 根据小时，分钟和秒值创建并返回时间 MICROSECOND 返回时间/日期时间的微秒部分 MINUTE 返回时间/日期时间的分钟部分 MONTH 返回给定日期的月份部分 MONTHNAME 返回给定日期的月份名称 NOW 返回当前日期和时间 PERIOD_ADD 在一段时间内添加指定的月数 PERIOD_DIFF 返回两个句点之间的差异 QUARTER 返回给定日期值的一年中的季度 SECOND 返回时间/日期时间的秒部分 SEC_TO_TIME 返回基于指定秒数的时间值 STR_TO_DATE 返回基于字符串和格式的日期 SUBDATE 从日期中减去时间/日期间隔，然后返回日期 SUBTIME 从日期时间中减去时间间隔，然后返回时间/日期时间 SYSDATE 返回当前日期和时间 TIME 从给定的时间/日期时间中提取时间部分 TIME_FORMAT 按指定格式格式化时间 TIME_TO_SEC 将时间值转换为秒 TIMEDIFF 返回两个时间/日期时间表达式之间的差异 TIMESTAMP 返回基于日期或日期时间值的日期时间值 TO_DAYS 返回日期和日期“0000-00-00”之间的天数 WEEK 返回给定日期的周数 WEEKDAY 返回给定日期的工作日编号 WEEKOFYEAR 返回给定日期的周数 YEAR 返回给定日期的年份部分 YEARWEEK 返回给定日期的年和周数 字符串函数 功能 描述 ASCII 返回特定字符的ASCII值 CHAR_LENGTH 返回字符串的长度（以字符为单位） CHARACTER_LENGTH 返回字符串的长度（以字符为单位） CONCAT 一起添加两个或多个表达式 CONCAT_WS 将两个或多个表达式与分隔符一起添加 FIELD 返回值列表中值的索引位置 FIND_IN_SET 返回字符串列表中字符串的位置 FORMAT 将数字格式化为“＃，###，###。##”等格式，舍入到指定的小数位数 INSERT 在指定位置的字符串中插入一个字符串，并插入一定数量的字符 INSTR 返回第一次出现在另一个字符串中的字符串的位置 LCASE 将字符串转换为小写 LEFT 从字符串中提取多个字符（从左开始） LENGTH 返回字符串的长度（以字节为单位） LOCATE 返回字符串中第一次出现子字符串的位置 LOWER 将字符串转换为小写 LPAD 左边用另一个字符串填充一个字符串到一定长度 LTRIM 从字符串中删除前导空格 MID 从字符串中提取子字符串（从任何位置开始） POSITION 返回字符串中第一次出现子字符串的位置 REPEAT 按指定的次数重复一次字符串 REPLACE 使用新的子字符串替换字符串中所有出现的子字符串 REVERSE 反转字符串并返回结果 RIGHT 从字符串中提取多个字符（从右侧开始） RPAD 右边用另一个字符串填充一个字符串到一定长度 RTRIM 从字符串中删除尾随空格 SPACE 返回指定数量的空格字符的字符串 STRCMP 比较两个字符串 SUBSTR 从字符串中提取子字符串（从任何位置开始） SUBSTRING 从字符串中提取子字符串（从任何位置开始） SUBSTRING_INDEX 在指定数量的分隔符出现之前返回字符串的子字符串 TRIM 从字符串中删除前导和尾随空格 UCASE 将字符串转换为大写 UPPER 将字符串转换为大写 聚合函数 函数 说明 COUNT() 没有数据时默认值是0 SUN() 没有数据时，默认值是null MAX() 求最大值 MIN() 求最小值 AVG() 求平均值 GROUP_CONCAT() 这个函数把来自同一个组的某一列（或者多列）的数据连接起来成为一个字符串(逗号分隔) JSON_ARRAYAGG() select json_arrayagg(degree) from score group by cno将结果集聚合为单个JSON数组 JSON_OBJECTAGG(key,value) select JSON_OBJECTAGG(sno, degree) from score group by sno第一个用作键，第二个用作值，并返回包含键值对的JSON对象 HAVING 对分组后的结果再进行条件过滤 WITH ROLLUP select cno, count(0) from score group by cno with rollup对分类聚合后的结果再汇总，再结果追加一条所有汇总数据,(count(0) 再汇总) 流程函数 功能 描述 IF 如果条件为TRUE则返回值，如果条件为FALSE则返回另一个值 IFNULL 如果表达式为NULL，则返回指定的值，否则返回表达式 其他常用函数 功能 描述 DATABASE 返回当前数据库的名称 VERSION 返回MySQL数据库的当前版本 USER 返回当前的MySQL用户名和主机名 UUID 生成UUID MD5 MD5加密 CASE 语法 123456CASE WHEN condition1 THEN result1 WHEN condition2 THEN result2 WHEN conditionN THEN resultN ELSE resultEND; CASE第一种用法，指定字段 123456select CASE siteWHEN &#x27;MX&#x27; THEN &#x27;墨西哥&#x27;WHEN &#x27;ES&#x27; THEN &#x27;西班牙&#x27;WHEN &#x27;US&#x27; THEN &#x27;美国&#x27;ELSE &#x27;其他&#x27; ENDfrom site CASE第二种用法，满足条件 12345select CASEWHEN site_name = &#x27;墨西哥&#x27; THEN &#x27;MX&#x27;WHEN site_name = &#x27;西班牙&#x27; THEN &#x27;ES&#x27;ELSE &#x27;其他&#x27; ENDfrom site SQL语句优先级 from join on where group by with rollup having select distinct order by limit WITH ROLLUPWITH ROLLUP 用于在 GROUP BY 操作中生成额外的汇总行，它不仅返回常规的分组聚合结果，还会为每个分组级别以及总计提供聚合数据。使用WITH ROLLUP可以方便地获取数据的层级汇总，从而避免编写额外的查询来获取这些汇总信息。WITH … AS …如果一整句查询中多个子查询都需要使用同一个子查询的结果，那么就可以用with as，将共用的子查询提取出来，加个别名。后面查询语句可以直接用，对于大量复杂的SQL语句起到了很好的优化作用注意：相当于一个临时表，但是不同于视图，不会存储起来，要与select配合使用。同一个select前可以有多个临时表，写一个with就可以，用逗号隔开，最后一个with语句不要用逗号。with子句要用括号括起来。12345# 查看 3-105 课程的最高分和最低分with score_temp as (select * from score where cno = &#x27;3-105&#x27;)select max(degree) as degree from score_tempunion allselect min(degree) as degree from score_temp 约束 约束（CONSTRAINT ）是数据库表中字段上应用的规则，用于确保数据库表中数据的有效性和完整性。 约束类型 描述 关键字 非空约束 确保字段中的值不能为NULL，即字段必须有值。 NOT NULL 唯一约束 保证表中一个字段或字段组合的值是唯一的，防止重复记录的产生。 UNIQUE 主键约束 同时具备非空和唯一性约束，用于唯一标识表中的每条记录。 PRIMARY KEY 默认约束 为字段指定一个默认值，如果插入记录时未提供值，则使用此默认值。 DEFAULT 检查约束 确保字段值必须满足特定的条件，只有满足条件的值才被允许插入或更新。 CHECK 外键约束 用于在两个表之间建立关联，确保一个表中的数据引用另一个表中的数据，从而维护跨表的数据完整性。 FOREIGN KEY 检查约束检查约束在MySQL中直到8.0.16版本才被完全支持。12ALTER TABLE studentsADD CONSTRAINT age_check CHECK (age &gt;= 18 AND age &lt;= 30);添加一个检查约束，年龄只能在[18,30]之间。 DDL DDL（Data Definition Language，数据定义语言）是SQL语言中用于定义和修改数据库结构的一部分。DDL主要包括用于创建、修改、删除 数据库、表、索引 等数据库对象的命令。 数据库操作查询所有数据库1show databases;查询当前数据库1select database();创建数据库1234567#create database [ if not exists ] 数据库名 [ default charset 字符集 ] [ collate 排序规则 ];# 排序规则决定了字符串之间如何比较# 排序规则影响ORDER BY子句的结果# 使用默认的utf8_general_ci，不区分大小写# 若要区分大小写，需要修改为utf8_bincreate database test_db default charset utf8mb4_general_ci;删除数据库1drop database [ if exists ] 数据库名;切换数据库1use 数据库名; 表操作查询当前数据库所有表1show tables;查看指定表结构通过这条指令，我们可以查看到指定表的字段，字段的类型、是否可以为NULL，是否存在默认值等信息1desc 表名;查询指定表的建表语句1show create table 表名;创建表结构1234CREATE TABLE 表名( 字段1 字段1类型 [ COMMENT 字段1注释 ], ... ) [ COMMENT 表注释 ];123456create table tb_user( id int comment &#x27;编号&#x27;, name varchar(50) comment &#x27;姓名&#x27;, age int comment &#x27;年龄&#x27;, gender varchar(1) comment &#x27;性别&#x27;) comment &#x27;用户表&#x27;;修改表名1ALTER TABLE 表名 RENAME TO 新表名;删除表1DROP TABLE [ IF EXISTS ] 表名;删除指定表, 并重新创建表1TRUNCATE TABLE 表名;TRUNCATE TABLE 命令用于快速删除表中的所有行，同时保留表结构。这个命令相比于使用 DELETE 命令删除所有行更为高效，特别是在处理大型表时。和 DELETE 比较：性能：对于大型表，TRUNCATE TABLE 比 DELETE 快。事务和日志：DELETE 是事务安全的，可以回滚；而 TRUNCATE TABLE 不是事务安全的，不能针对单个行操作进行回滚。自增计数器：DELETE 不会重置自增计数器，而 TRUNCATE TABLE 会。 字段操作添加字段1ALTER TABLE 表名 ADD 字段名 类型 (长度) [ COMMENT 注释 ] [ 约束 ];1ALTER TABLE emp ADD nickname varchar(20) COMMENT &#x27;昵称&#x27;;修改字段类型1ALTER TABLE 表名 MODIFY 字段名 新数据类型 (长度);修改字段名和字段类型1ALTER TABLE 表名 CHANGE 旧字段名 新字段名 类型 (长度) [ COMMENT 注释 ] [ 约束 ];1ALTER TABLE emp CHANGE nickname username varchar(30) COMMENT &#x27;昵称&#x27;;删除字段1ALTER TABLE emp DROP username; DML DML（Data Manipulation Language，数据操纵语言）是SQL（Structured Query Language，结构化查询语言）的一部分，用于修改数据。 添加数据（INSERT） 修改数据（UPDATE） 删除数据（DELETE） 隐式事务： 默认行为：在InnoDB存储引擎中，默认情况下，事务是隐式的。这意味着当执行一个 INSERT、UPDATE 或 DELETE 操作时，如果当前没有活跃的事务，则MySQL自动为这个操作开始一个新的事务。 自动提交：每个独立的SQL语句都被视为一个事务。除非手动开始一个新的事务，否则执行完这些语句后，事务会自动提交。 自动回滚：如果在执行过程中遇到错误，事务可能会被自动回滚。 添加数据给指定字段添加数据1INSERT INTO 表名 (字段名1, 字段名2, ...) VALUES (值1, 值2, ...);给表所有的字段添加数据1INSERT INTO 表名 VALUES (值1, 值2, ...);批量添加数据1234INSERT INTO 表名 (字段名1, 字段名2, ...) VALUES (值1, 值2, ...), (值1, 值2, ...),(值1, 值2, ...);1234INSERT INTO 表名 VALUES (值1, 值2, ...),(值1, 值2, ...),(值1, 值2, ...);查询添加12INSERT INTO table2SELECT * FROM table1;12INSERT INTO table2(field1,field2,...) SELECT field1,field2,... FROM table1 ... 修改数据修改数据的具体语法123UPDATE table_nameSET column1 = value1, column2 = value2, ...WHERE condition;联表修改联表操作不建议使用，A join B B join A 会产生死锁123update course c join teacher t on (c.tno = t.tno)set c.cname = &#x27;计算机导论&#x27;where t.tname = &#x27;王萍&#x27; 删除数据删除数据1DELETE FROM table_name WHERE condition;联表删除123delete t # 定义起作用的表from teacher t join course c on (t.tno = c.tno)where c.cname = &#x27;JAVA&#x27;; DQL DQL（Data Query Language，数据查询语言）是SQL（Structured Query Language，结构化查询语言）的一个组成部分，专门用于查询数据库中的数据。在实际应用中，DQL基本上是由SELECT语句组成的，它允许用户从数据库中检索数据，而不会对数据本身进行任何修改。 查询关键字: SELECT 基本语法1234567891011121314SELECT 字段列表FROM 表名列表WHERE 条件列表GROUP BY 分组字段列表HAVING 分组后条件列表ORDER BY 排序字段列表LIMIT 分页参数 基础查询在基本查询的DQL语句中，不带任何的查询条件，查询的语法如下：查询多个字段1SELECT 字段1, 字段2, 字段3 ... FROM 表名;1SELECT * FROM 表名 ;字段设置别名1SELECT 字段1 [ AS 别名1 ] , 字段2 [ AS 别名2 ] ... FROM 表名;1SELECT 字段1 [ 别名1 ] , 字段2 [ 别名2 ] ... FROM 表名; 1去除重复记录1SELECT DISTINCT 字段列表 FROM 表名; 条件查询语法1SELECT 字段列表 FROM 表名 WHERE 字段1 [条件1] [查询条件1] [逻辑运算符] 字段2 [条件2] [查询条件2];条件比较运算符功能&gt;大于&gt;=大于等于&lt;小于&lt;=小于等于=等于&lt;&gt; 或 !=不等于BETWEEN … AND …在某个范围之内，区间 [min, max]IN(…)多值查询LIKE 占位符模糊查询IS NULL判空查询常用的逻辑运算符逻辑运算符功能AND 或 &amp;&amp;并且 (多个条件同时成立)OR 或或者 (多个条件任意一个成立)NOT 或 !非 , 不是 聚合查询使用 聚合函数 将一列数据作为一个整体，进行纵向计算。1SELECT 聚合函数(字段列表) FROM 表名 ;1SELECT count(0) FROM user; 分组查询语法1SELECT 字段列表 FROM 表名 [ WHERE 条件 ] GROUP BY 分组字段名 [ HAVING 分组后过滤条件 ];where 与 having 区别执行时机不同where是分组之前进行过滤，不满足where条件，不参与分组having是分组之后对结果进行过滤判断条件不同where不能对聚合函数进行判断而having可以对聚合函数进行判断123456# 查询45元以下商品价位的总数，不展示少于3款产品的价位select price, count(*) price_count from product where price &lt; 45 group by price having price_count &gt;= 3;注意事项• 分组之后，查询的字段一般为聚合函数和分组字段，查询其他字段无任何意义• 执行顺序: where &gt; 聚合函数 &gt; having• 支持多字段分组, 具体语法为 : group by columnA，columnB 排序查询语法1SELECT 字段列表 FROM 表名 ORDER BY 字段1 排序方式1 , 字段2 排序方式2;排序方式ASC ：升序（默认值，可缺省）DESC：降序Rand()：随机排序 分页查询1SELECT 字段列表 FROM 表名 LIMIT [offset, rows];注意事项:offset：数据的偏移量，（查询页码 - 1）* 每页显示记录数rows：要从偏移量开始取多少条数据分页查询是数据库的方言，不同的数据库有不同的实现，MySQL中是LIMIT如果查询的是第一页数据，起始索引可以省略，直接简写为 limit 10 DCL DCL（Data Control Language，数据控制语言）主要用于管理数据库中的访问权限和安全性。DCL包括一系列的命令，允许数据库管理员控制用户对数据库对象的访问权限。 查询用户 1select * from mysql.user; Host列代表当前用户访问的主机, 如果为localhost, 仅代表只能够在当前本机访问，是不可以远程访问的。 User代表的是访问该数据库的用户名。在MySQL中需要通过Host和User来唯一标识一个用户。 用户操作在MySQL中需要通过用户名@主机名的方式，来唯一标识一个用户主机名可以使用 % 通配创建用户1CREATE USER &#x27;用户名&#x27;@&#x27;主机名&#x27; IDENTIFIED BY &#x27;密码&#x27;;12# 创建用户admin, 可以在任意主机访问该数据库, 密码123456;create user &#x27;admin&#x27;@&#x27;%&#x27; identified by &#x27;123456&#x27;;修改用户密码1ALTER USER &#x27;用户名&#x27;@&#x27;主机名&#x27; IDENTIFIED WITH mysql_native_password BY &#x27;新密码&#x27;;删除用户1DROP USER &#x27;用户名&#x27;@&#x27;主机名&#x27; ; 权限控制MySQL中定义了很多种权限，但是常用的就以下几种：权限说明ALL, ALL PRIVILEGES所有权限SELECT查询数据INSERT插入数据UPDATE修改数据DELETE删除数据ALTER修改表DROP删除数据库/表/视图CREATE创建数据库/表上述只是简单罗列了常见的几种权限描述，其他权限描述及含义，可以直接参考 官方文档查询权限1SHOW GRANTS FOR &#x27;用户名&#x27;@&#x27;主机名&#x27;;1show grants for &#x27;admin&#x27;@&#x27;%&#x27;;授予权限1GRANT 权限列表 ON 数据库名.表名 TO &#x27;用户名&#x27;@&#x27;主机名&#x27;;1grant all on itcast.* to &#x27;admin&#x27;@&#x27;%&#x27;;撤销权限1REVOKE 权限列表 ON 数据库名.表名 FROM &#x27;用户名&#x27;@&#x27;主机名&#x27;;1revoke all on itcast.* from &#x27;admin&#x27;@&#x27;%&#x27;;多个权限之间，使用逗号分隔授权时， 数据库名和表名可以使用 * 进行通配，代表所有 TCL TCL（Transaction Control Language，事务控制语言）用于管理数据库中的事务。开启事务1BEGIN;提交事务1COMMIT;回滚事务1ROLLBACK; 多表查询 多表查询是SQL中的一项关键技术，它允许从两个或多个表中组合数据。这种查询通常通过在这些表之间的共同字段上建立关联来实现。通过多表查询，可以获得跨表的综合视图。 在数据库设计中，表与表之间的关系大致分为三种： 一对一 描述：每个表中的一行与另一个表中的一行相关联。 实现：通常通过在一个表中包含另一个表的主键作为外键来实现。 示例：用户表和用户详细信息表，其中每个用户只有一组详细信息。 一对多 描述：一个表中的一行与另一个表中的多行相关联。 实现：在“多”的一方表中包含“一”的一方表的主键作为外键。 示例：客户表和订单表，其中一个客户可以有多个订单，但每个订单只能属于一个客户。 多对多 描述：一个表中的多行与另一个表中的多行相关联。 实现：通常通过一个单独的关联表（或桥接表）实现，该表包含两个表的主键作为外键。 示例：学生表和课程表，其中一个学生可以参加多个课程，一个课程可以有多个学生参加。 笛卡尔积 定义：当两个表进行交叉连接（CROSS JOIN）时，产生的结果集是这两个表的笛卡尔积。这意味着结果集中的每一行是由第一个表中的一行和第二个表中的一行组合而成的。 行为：如果表A有M行，表B有N行，它们的笛卡尔积将包含M×N个结果。 内连接 内连接用于从两个或多个表中选择符合连接条件的记录。内连接仅返回两个表中关联键匹配的行。如果在一个表中的行在另一个表中没有匹配的行，则这些行不会出现在查询结果中。 隐式内连接 1SELECT 字段列表 FROM 表1 , 表2 WHERE 条件 ... ; 1select emp.name , dept.name from emp , dept where emp.dept_id = dept.id ; 显式内连接 1SELECT 字段列表 FROM 表1 [ INNER ] JOIN 表2 ON 连接条件 ... ; 1select e.name, d.name from emp e join dept d on (e.dept_id = d.id); 外连接 外连接用于从两个表中检索数据，即使在另一个表中没有匹配的行也能返回结果。外连接可以是左连接（LEFT JOIN）或右连接（RIGHT JOIN）。 左连接 左连接返回左表（LEFT JOIN左边的表）的所有行，以及右表中匹配的行。如果在右表中没有匹配的行，则结果中右表的部分将为NULL。 1SELECT 字段列表 FROM 表1 LEFT JOIN 表2 ON 条件 ... ; 右连接 右连接与左连接相反，返回右表的所有行，以及左表中匹配的行。如果在左表中没有匹配的行，则结果中左表的部分将为NULL。 1SELECT 字段列表 FROM 表1 RIGHT JOIN 表2 ON 条件 ... ; 自连接 自连接（Self Join）是一种特殊类型的连接，用于将一个表与自身进行连接。这在处理具有 层级 或 递归 关系的数据时特别有用，例如，当一个表中的记录引用了同一个表中的其他记录。自连接可以是内连接、左连接或右连接。 1SELECT 字段列表 FROM 表A 别名A JOIN 表A 别名B ON 条件 ... ; on 和 where 的区别 执行顺序 ON 子句在进行表的连接操作时先执行。 WHERE 子句在 JOIN 操作之后执行。 如果是 内连接 则条件放在 ON 和 WHERE 是一样的。 如果是 外链接 ON 过滤主表条件不成立，WHERE 过滤主表条件成立。 123select *from a left join b on(a.id=b.id and a.id &gt; 10)# 此时a.id &gt; 10 条件不成立 联合查询 对于union查询，就是把多次查询的结果合组合起来，形成一个新的查询结果集，每个查询的列的数量、顺序、数据类型必须一致。 123SELECT 字段列表 FROM 表A ...UNION [ ALL ]SELECT 字段列表 FROM 表B ....; UNION ALL UNION ALL 用于合并多个 SELECT 查询的结果集，但它包括所有重复的行。 假设 student 表有6个学生，直接合并结果集： 123456select snofrom studentunion allselect snofrom student# 返回12条 sno, 每条重复 UNION UNION 用于合并多个 SELECT 查询的结果集，并且默认去除重复的行。 123456select snofrom studentunionselect snofrom student# 返回6条 sno, 不重复 子查询 在 SQL 语句中，嵌套的 SELECT 语句称为子查询。子查询可以嵌入在 SELECT、INSERT、UPDATE 或 DELETE 语句中。 例如： 1SELECT * FROM t1 WHERE column1 = ( SELECT column1 FROM t2 ); 子查询分类 标量子查询： 返回单个值的子查询。 通常用在条件表达式中，如 WHERE 子句或 SELECT 列表中。 列子查询： 返回一列数据的子查询。 常用于 IN 或 ANY/SOME 子句中。 行子查询： 返回一行数据的子查询。 可用于比较多个列，通常与行构造函数一起使用。 表子查询： 返回多行多列的子查询。 常见于 IN 子句、EXISTS 子句或者作为 FROM 子句中的一个表。 子查询位置 在 SELECT 之后： 作为计算列的一部分，或者在 SELECT 列表中为列赋值。 在 FROM 之后： 作为一个临时表使用，在主查询中用于联合、过滤或聚合。 在 WHERE 之后： 作为条件筛选，与主查询表的数据进行比较或关联。 在 INSERT/UPDATE/DELETE 语句中： 在 INSERT INTO ... SELECT 结构中用于提供要插入的数据。 在 UPDATE ... SET column = (SELECT ...) 结构中为列赋值。 在 DELETE FROM ... WHERE column1 in (SELECT ...) 结构中用于指定删除条件。 标量子查询 标量子查询在执行时返回单个值（即一个标量值）。这种子查询可以嵌入在更大的 SQL 查询中的多个位置，比如 SELECT、WHERE 或 HAVING 子句中。 特性 单值返回： 标量子查询的主要特点是它只返回一个单一的值，而不是一列或多列的数据集。 如果子查询返回多于一个值，则 SQL 会报错。 用途广泛： 可以用在几乎任何需要单一值的地方，例如在 SELECT 列表中，或者作为 WHERE 或 HAVING 子句中的条件。 与主查询的关系： 可以独立于主查询执行，也可以引用主查询中的数据。 用法 独立执行 1select * from emp where dept_id = (select id from dept where name = &#x27;销售部&#x27;); 引用主查询中的数据 123456SELECT e.name, e.salary, (SELECT AVG(salary) FROM employees WHERE department_id = e.department_id) AS avg_department_salaryFROM employees e; 列子查询 列子查询返回一个列的数据，而不是单个值或多行数据。这种子查询通常用于 IN、NOT IN、ANY、SOME 或 ALL 子句中，与主查询的条件进行比较。 操作符 描述 IN 检查某个值是否在子查询或提供的值列表中。常用于确定一个值是否属于列表中的某个值。 NOT IN 检查某个值是否不在子查询或提供的值列表中。用于排除列表中的值。 ANY 与比较运算符结合使用，检查是否至少有一个子查询返回的值满足比较条件。 SOME 功能与 ANY 相同，与比较运算符结合使用，用于检查是否有子查询返回的值中的某些值满足比较条件。 ALL 与比较运算符结合使用，确保子查询返回的所有值都满足比较条件。 根据部门ID, 查询员工信息 1select * from emp where dept_id in (select id from dept where name in (&#x27;销售部&#x27;,&#x27;市场部&#x27;)); 比 财务部 所有人工资都高的员工信息 12select * from emp where salary &gt; all ( select salary from emp where dept_id =(select id from dept where name = &#x27;财务部&#x27;) ); 比 研发部 其中任意一人工资高的员工信息 12select * from emp where salary &gt; any ( select salary from emp where dept_id =(select id from dept where name = &#x27;研发部&#x27;) ); 行子查询 行子查询比较一个值组合（即一行数据）与子查询返回的一行或多行数据。这种类型的查询通常用于处理涉及多个列的复杂条件。 常用的操作符：= 、&lt;&gt; 、IN 、NOT IN等。 查询与 “张三” 的薪资及直属领导相同的员工信息 12select * from emp where (salary,managerid) = (select salary, managerid from empwhere name = &#x27;wgf&#x27;); 表子查询 表子查询，也称为派生表（Derived Tables），是一种在SELECT语句的FROM子句中使用的子查询。这种子查询返回一个临时表，可以在外部查询中像普通表一样使用。 123456SELECT customer_id, COUNT(*) as total_orders, AVG(amount) as avg_amountFROM ( SELECT customer_id, amount FROM orders) AS order_summaryGROUP BY customer_id; in 和 exists 的区别 ININ 子句用于检查某个值是否存在于由子查询或值列表提供的集合中。当使用 IN 时，MySQL首先计算子查询，将所有结果存储在一个临时表中，外层查询数据的每行都需要判断是否包含在子查询结果中。在子查询返回的结果集较小的情况下更高效，因为整个结果集需要被加载和存储在内存中。1select * from A where id in (select id from B) EXISTSEXISTS 子句用于检查子查询是否返回至少一行数据。使用 EXISTS 时，MySQL对外层查询的每一行执行子查询。只要子查询返回第一行数据，就会停止检查，因为它只关心是否存在符合条件的行。通常在子查询与外层查询紧密相关，或当子查询可能返回大量数据时更有效，因为它不需要处理整个子查询的结果集。1select * from A where exists (select 1 from B where B.id = A.id) 选择子查询返回少量数据：IN子查询返回大量数据：EXISTS 事务 MySQL中的事务是一系列操作，这些操作作为一个整体执行，要么全部成功，要么全部失败。事务是数据库管理的重要特性，确保数据的完整性和一致性。MySQL中的事务主要通过以下四个关键属性（ACID属性）来保障： 原子性（Atomicity）：事务中的所有操作要么全部完成，要么全部不完成。如果事务的一部分操作失败，整个事务会回滚到开始状态，如同这些操作从未发生过一样。 一致性（Consistency）：事务确保数据库从一个一致的状态转换到另一个一致的状态。 隔离性（Isolation）：事务的执行不应受其他事务的干扰。多个同时运行的事务之间是相互隔离的，每个事务对数据的修改在提交之前对其他事务不可见。 持久性（Durability）：一旦事务被提交，它对数据库的修改是永久性的，即使发生系统故障也不会丢失。 事务控制 查看/设置事务提交方式 12SELECT @@autocommit;SET @@autocommit = 0; 自动提交事务： 默认 开启 自动提交事务。 每个单独的SQL语句（如 INSERT, UPDATE, DELETE 等）会立即执行并提交。这意味着，一旦语句执行，其更改就是永久性的，不能回滚。 如果你想在单个操作中执行多个语句，必须显式地开始一个新事务，并在所有操作完成后提交它。 开启事务 1START TRANSACTION 或 BEGIN ; 提交事务 1COMMIT; 回滚事务 1ROLLBACK; 并发事务问题 脏读 定义：脏读发生在一个事务读取了另一个事务未提交的数据。 举例：事务A修改了一行数据但还未提交，这时事务B读取了这个未提交的数据。如果事务A回滚，事务B读取的数据就是不正确的，因为这些数据从未被实际提交到数据库中。 不可重复读 定义：不可重复读发生在一个事务读取了相同的行两次，但在两次读取之间，另一个事务修改了该行并提交，导致两次读取的数据不一致。 举例：事务A读取了一行数据，事务B修改了这行数据并提交。当事务A再次读取同一行数据时，数据已经改变。 幻读 定义：新增或删除操作，在一个事务内执行一个范围查询时，然后插入或删除满足条件的行，事务提交发现数据冲突。 举例：事务A根据某个条件查询数据范围，而事务B在这个范围内插入了新的数据并提交。当事务A再次执行相同的范围查询时，会发现之前不存在的“幻”数据。 事务隔离级别 隔离级别（+:允许出现，-:不允许出现） 脏读 不可重复读 幻读 未提交读 + + + 提已交读 - + + 可重复读 - - + 序列化读 - - - 事务隔离级别 实现方式 读未提交（RU） 事务可以读取未提交的数据。不使用行锁来防止对读取的数据的修改，允许脏读。 读已提交（RC） 事务只能读取已提交的数据。使用行锁，读取数据时加锁，读取完毕后立即释放锁，防止脏读，但允许不可重复读。 可重复读（RR） 事务读取数据时，对读取的数据加行锁直到事务结束，防止脏读和不可重复读。在MySQL中，还使用间隙锁来防止幻读。 串行化（S） 事务读取或修改数据时，使用表锁，直到事务结束。这避免了脏读、不可重复读和幻读，但可能导致性能问题。 数据库进阶 视图 视图是一个虚拟表，它基于一个或多个实际表的查询结果。视图本身不包含数据，而是保存了一个SQL查询，这个查询动态地生成数据。 狭义理解：视图就是对sql逻辑的封装。 优点： 安全性：视图可以隐藏底层表的某些列和行，仅展示必要的数据，这有助于增强数据的安全性。 简化SQL操作：如果你经常需要执行复杂的查询（例如包含多个表的JOIN操作），可以将这些复杂的查询封装在视图中，简化后续的查询操作。 逻辑数据独立：视图提供了一种逻辑上的数据独立。应用程序可以只依赖视图，而不是实际的表结构。这意味着即使底层数据结构发生变化，只要视图保持不变，应用程序也不需要修改。 重用SQL语句：视图允许重用SQL语句。你可以创建一个视图来封装复杂的SQL逻辑，然后在多处重用这个视图。 语法 创建 12CREATE [OR REPLACE] VIEW 视图名称[(列名列表)] AS SELECT语句 [ WITH [CASCADED | LOCAL ] CHECK OPTION ] 查询 12查看创建视图语句：SHOW CREATE VIEW 视图名称;查看视图数据：SELECT * FROM 视图名称 ...... ; 修改 使用 OR REPLACE 对原视图进行覆盖 12345方式一：CREATE [OR REPLACE] VIEW 视图名称[(列名列表)] AS SELECT语句 [ WITH[ CASCADED | LOCAL ] CHECK OPTION ]方式二：ALTER VIEW 视图名称[(列名列表)] AS SELECT语句 [ WITH [ CASCADED |LOCAL ] CHECK OPTION ] 删除 1DROP VIEW [IF EXISTS] 视图名称 [,视图名称] ... 索引 视图不能创建索引，但是能使用原表上的索引 123456789# 创建视图create view score_view as (select * from score where degree &gt;= 90);# 使用视图select *from score_view;# 修改视图create or replace view score_view as (select * from score where degree &gt;= 90); 检查选项 WITH CHECK OPTION 是在创建或更改视图时使用的子句，用于确保所有通过视图进行的数据修改操作（如INSERT、UPDATE、DELETE）都符合视图定义的条件。 目的：保证视图的完整性，防止通过视图进行的数据修改操作破坏视图定义的规则。 12# 创建视图create view v1 as (select * from score where degree &gt;= 90); 这个视图中可以插入 degree 大于90 的数据，但是视图查询不出来，因为这个视图没有声明检查选项。 12# 创建视图create view v2 as (select * from score where degree &gt;= 90) WITH CASCADED CHECK OPTION; 这个视图中不可以插入 degree 大于90 的数据，在插入时会进行检查，因为声明了检查选项。 CASCADED 级联 这是默认选项。当设置为 CASCADED 时，WITH CHECK OPTION 会检查所有基础视图的约束。换句话说，不仅当前视图的定义被用于检查数据的合法性，所有依赖的视图（如果当前视图是基于其他视图创建的）也会进行检查。 LOCAL 本地 当设置为 LOCAL 时，WITH CHECK OPTION 仅检查当前视图的定义条件，不考虑任何基础视图的约束。 视图的更新 为了使视图可更新，需要视图中的每一行都与基础表中的行存在一对一的关系。这意味着对视图的更新可以无歧义地映射到基础表上。以下是影响视图可更新性的一些关键因素： 聚合函数或窗口函数：使用了如 SUM() , MIN() , MAX() , COUNT() 等函数的视图不可更新，因为这些操作改变了行与基础表的直接对应关系。 DISTINCT：使用 DISTINCT 关键字的视图不可更新，因为 DISTINCT 可能会消除重复行，从而破坏行与基础表行之间的一对一关系。 GROUP BY 和 HAVING：这些子句通常与聚合操作一起使用，它们改变数据的展示方式，使得视图中的行不再直接对应于基础表中的行，因此这样的视图不可更新。 UNION 或 UNION ALL：由于 UNION 或 UNION ALL 合并了来自多个表或多个查询的结果，这可能导致行与基础表中行之间的对应关系不明确，因此包含这些操作的视图通常不可更新。 触发器 自动化操作：触发器是一种特殊类型的存储过程，它不是由直接调用执行的，而是在特定的数据库表事件发生时自动触发。这些事件通常包括INSERT、DELETE和UPDATE操作。 数据完整性和自动化：触发器常用于保持数据的完整性和一致性，例如自动更新表中的数据，或检查插入/更新的数据是否满足特定的条件。 事件驱动：触发器是事件驱动的。当与触发器关联的表发生特定操作时，触发器中定义的SQL语句会被执行。 触发阶段 BEFORE：在数据操作之前。 AFTER：在数据操作之后。 触发条件 使用别名OLD和NEW来引用触发器中发生变化的记录内容，这与其他的数据库是相似的。现在触发器还只支持行级触发，不支持语句级触发 条件 NEW 和 OLD INSERT NEW表示将要或者已经新增的数据 UPDATE OLD 表示修改之前的数据，NEW 表示将要或已经修改后的数据 DELETE OLD表示将要或者已经删除的数据 使用 语法 123456CREATE TRIGGER trigger_nameBEFORE/AFTER INSERT/UPDATE/DELETEON tbl_name FOR EACH ROW -- 行级触发器BEGIN trigger_stmt ;END; 查看 1SHOW TRIGGERS; 删除 1DROP TRIGGER [schema_name.]trigger_name; INSERT 12345CREATE TRIGGER test_insert AFTER INSERT ON test1 FOR EACH ROWBEGIN INSERT INTO test2 ( id, test_name ) VALUES( new.id, new.test_name );END; UPDATE 1234CREATE TRIGGER test_update AFTER UPDATE ON test1 FOR EACH ROWBEGIN UPDATE test2 set test_name = new.test_name where id = old.id;END; DELETE 1234CREATE TRIGGER test_delete AFTER DELETE ON test1 FOR EACH ROWBEGIN DELETE FROM test2 WHERE id = old.id;END; 分区 分区键必须是主键的一部分 MySQL从5.1版本开始支持表的分区。分区是一种数据库架构技术，它允许将一个大型表或索引分解为多个较小、更易于管理的部分，这些部分称为分区。 特点： 物理分离：从应用程序的角度来看，仍然只操作一个逻辑表或索引。但在底层，这个逻辑表或索引实际上可能被分成许多物理分区。 独立性：每个分区可以独立于其他分区进行处理。这意味着可以单独优化、备份或维护每个分区，而不影响整个表。 分区的好处 提高查询性能：对于大型表，特别是当查询可以限制在特定的几个分区中时，分区可以显著提高查询性能。这是因为查询操作只需要扫描相关分区的数据而不是整个表。 并行处理：不同的分区可以在不同的处理器上并行进行查询处理，特别是针对分区键进行的大型扫描或聚合查询。这种并行性能够显著提高查询的处理速度，从而增加吞吐量。 提高数据管理的效率：数据管理任务，如删除和归档数据，可以通过简单地删除整个分区来高效完成，这比逐行删除数据要快得多。 分区的限制和缺点 分区键限制：如果表有主键或唯一键，则分区键必须包含主键的一部分和唯一键所有列。这意味着你不能随意选择分区键，必须考虑表的主键或唯一键设计。 跨分区操作：跨多个分区的查询和事务会比在单个分区内的操作性能差。 数据分布：不均匀的数据分布可能导致分区效果不理想。例如，某些分区可能数据量非常大，而其他分区则数据量很小，这可能会影响查询和维护操作的性能。 NULL值处理：如范围分区，NULL值的处理可能会导致数据分配到不预期的分区，需要特别注意。 分区和分表的区别库的限制：分区只能在一个数据库内进行；分表可以在同一个库分表，也可以跨库分表。水平扩展：跨库分表还提供水平扩展能力，水平扩展 CPU, I/O 资源。实现方式：分区由数据库层实现，使用简单；分表由服务层实现，增加系统复杂性。 对已存在表创建分区 添加分区 RANGE 分区 分区范围：左开右闭 RANGE 分区允许根据指定列的值范围来分配行到不同的分区。这种方法非常适合于那些数据值分布在一个可预知范围内的场景，如日期、时间、状态码等，这个范围遵循左开右闭的规则。 12345678910111213141516CREATE TABLE sales ( id INT NOT NULL AUTO_INCREMENT, sale_date DATE NOT NULL, amount DECIMAL(10,2) NOT NULL, PRIMARY KEY (id, sale_date) -- 分区键必须是主键的一部分)PARTITION BY RANGE( YEAR(sale_date) ) ( PARTITION p0 VALUES LESS THAN (1991), -- 包括1990年及之前的数据 PARTITION p1 VALUES LESS THAN (1992), -- 包括1991年的数据 PARTITION p2 VALUES LESS THAN (1993), -- 包括1992年的数据 PARTITION p3 VALUES LESS THAN MAXVALUE -- 包括1993年及之后的数据);# 查看表的分区信息select * from information_schema.partitions where table_schema=database() and table_name=&#x27;sales&#x27; 使用场景适用于数据有明确的连续范围，如 日期、时间、数值 等。常用于时间序列数据，例如日志、交易记录。便于数据归档和删除，例如，快速删除某个日期范围的数据。 LIST 分区 分区列不能插入分区不存在的值 适合分区键不会变的场景 LIST分区允许根据列值的列表将数据行分配到不同的分区中。LIST 分区非常适合于列值是离散的且 预先已知 的情况，例如状态码、地区代码、部门ID等。 工作原理：定义了一组值的列表，并为每组值指定一个分区。每行数据根据分区键的值被分配到相应的分区中。 123456789101112# 上商品信息按照店铺分区CREATE TABLE product ( id INT NOT NULL, store_id INT not null, primary key(id, store_id))PARTITION BY LIST(store_id)( PARTITION p0 VALUES IN (1,2), PARTITION p1 VALUES IN (3,4), PARTITION p2 VALUES IN (5,6)， PARTITION p3 DEFAULT # 默认分区，5.7支持。其他数据会添加到此分区) 假设上面未设置默认分区：在 LIST 分区中，如果尝试插入一个行，其分区键的值不在任何已定义的分区值列表中，该插入操作将失败，并显示错误。 123insert into product(id, store_id)values (1, 20);#[1526] 找不到分区错误 适用场景当数据的分区键列具有一组明确且离散的值，如 状态、类型、地区代码 等。用于分类数据，便于管理特定类别或组的数据。当数据行可以基于列值的列表明确分组时。 HASH 分区 支持自定义函数 PARTITION BY HASH(customer(id)) Hash分区根据用户定义的表达式将数据行分配到分区中。在 HASH 分区中，MySQL使用用户提供的表达式对每行数据的分区键值进行哈希处理，然后基于哈希值将数据行分配到特定的分区。 原理：在HASH分区中，分区键的哈希值是通过一个用户定义的表达式计算得出的。MySQL将这个哈希值对分区的数量取模，得到的结果决定了数据行应该存储在哪个分区。 12345678create table store( id int not null, area_id int not null, primary key (id, area_id)) PARTITION BY HASH (area_id) PARTITIONS 4; 优缺点 优点 均匀的数据分布：HASH分区通常能够确保数据在各个分区之间较为均匀地分布。 简化查询：对于基于分区键的查询，HASH分区可能提高查询性能。 缺点 不适合范围查询：对于涉及分区键的范围查询，HASH 分区没有性能优势，因为数据行可能分布在所有分区中。 依赖分区表达式：分区的有效性高度依赖于哈希表达式。如果表达式不合适，分区可能不会均匀。 适用场景当没有明显的分区范围或列表，且需要均匀分布数据时。对于分区键的值分布均匀，且查询模式不依赖于特定范围或列表的场景。通常用于负载均衡，尤其是在数据行没有明显的逻辑分组时。 KEY 分区 KEY分区是一种基于哈希的分区策略，它使用MySQL内置的哈希函数对一列或多列（分区键）的值进行哈希处理，然后根据这些哈希值将数据行分配到不同的分区。与 HASH分区相比，KEY 分区的特点是它使用MySQL的哈希函数而不是用户定义的表达式。 1234567create table tb_user ( id int not null, area varchar(20) not null , primary key (id, area))PARTITION BY KEY(area)PARTITIONS 5; 适用场景类似于HASH分区，但专用于MySQL环境，使用MySQL的哈希函数。适用于需要均匀数据分布但不希望自定义哈希函数的场景。常用于分布式场景中，以均衡存储和查询负载。 COLUMNS 分区 COLUMNS分区许根据一个或多个列的值直接进行分区。这种分区类型适用于无法使用普通的范围（RANGE）或列表（LIST）分区处理的数据类型，如字符串或日期时间类型。 工作原理： 基于一列或多列的值进行分区。与 RANGE 和 LIST 分区类似，COLUMNS 分区允许你为每个分区指定一个 值范围 或 值列表。 不同于 RANGE 和 LIST 分区只能用于整数类型（Hash值），COLUMNS 分区可以用于非整数类型的列，如字符串、日期等。 123456789create table user_log( id int not null, create_time datetime)PARTITION BY RANGE COLUMNS (create_time) ( PARTITION p0 VALUES LESS THAN (&#x27;2017-01-01 00:00:00&#x27;), PARTITION p1 VALUES LESS THAN (&#x27;2018-01-01 00:00:00&#x27;), PARTITION p_max values less than maxvalue); 适用场景当需要基于多个列进行分区，特别是非整数类型的列（如字符串、日期等）时。适用于复合键分区，尤其是当这些键是不同类型的数据时。当RANGE或LIST分区的列类型不是整数类型时。 查看分区数据量 123SELECT PARTITION_NAME,TABLE_ROWSFROM INFORMATION_SCHEMA.PARTITIONSWHERE TABLE_NAME = tablename; 分区性能参考 操作类型 记录数 非分区表 分区表 插入性能 500万 2693 秒 3084 秒 插入性能 1000万 5440 秒 6277 秒 插入性能 2000万 12753 秒 14175 秒 查询性能 2000万记录，分区键索引，查询100万次 126 秒 90 秒 查询性能 2000万记录，非分区键索引，查询100万次 691 秒 727 秒 DDL性能 新增索引 66 秒 56 秒 存储空间 500万(数据+索引) 255+384 MB 351+555 MB 存储空间 1000万(数据+索引) 511+900 MB 551+900 MB 存储空间 2000万(数据+索引) 1000+1900 MB 1000+2100 MB 索引 索引是用于加速数据检索的数据结构，类似于书籍的目录。以下是索引的详细描述： 数据结构：索引是一种特殊的数据结构，存储在数据库的文件系统中，用于快速查找表中的特定行。常见的数据结构包括B+Tree和哈希表。 查询加速：索引主要用于加速查询操作，尤其是对大数据量的表进行查询时。可以显著减少数据库需要扫描的数据量。 优缺点 优点 提高检索效率：能够快速定位所需数据，特别是在大型数据集或其连接查询中。 优化排序和分组：加快了ORDER BY和GROUP BY子句的处理速度，因为索引是有序的。 支持唯一约束：唯一索引可以确保表中每行数据的唯一性，保证数据的正确性。 索引覆盖：只针对加了索引的字段进行查询，能够减少回表次数，提高查询效率。 缺点 降低写操作性能：当表中的数据被修改时，索引也需要被更新。这会降低INSERT、UPDATE和DELETE语句的性能。 占用额外空间：索引需要占用物理存储空间。对于拥有大量数据的表，可能会造成磁盘压力。 设计和维护成本：设计有效的索引策略需要对数据的使用模式有深入的了解。不合理的索引反而会带来性能问题。 索引结构 MySQL的索引是在 存储引擎层 实现的，不同的存储引擎有不同的索引结构，主要包含以下几种： 索引结构 描述 B+ Tree索引 最常见的索引类型，适用于全值匹配、范围查询和排序操作，大部分存储引擎都支持 Hash索引 基于哈希表实现，适用于精确匹配索引列的查询，不支持范围查询和排序，主要由Memory引擎使用 R-tree索引（空间索引） 专门用于空间数据类型，如地理数据存储，主要由MyISAM引擎支持 Full-text索引 用于全文搜索，建立倒排索引以快速匹配文档，MySQL 5.6及更高版本的InnoDB支持 存储引擎对索引的支持： 索引类型 InnoDB支持情况 MyISAM支持情况 Memory支持情况 B+tree索引 支持 支持 支持 Hash索引 自适应哈希索引（特定情况下） 不支持 支持 R-tree索引 不支持 支持 不支持 Full-text索引 5.6版本之后支持 支持 不支持 二叉树 假如说MySQL的索引结构采用二叉树的数据结构，比较理想的结构如下： 如果主键是顺序插入的，那么二叉树就会退化成为一个链表，结构如下： 所以，如果选择二叉树作为索引结构，会存在以下缺点： 退化为链表：顺序插入时，会形成一个链表，查询性能大大降低。 树的层级较深：大数据量情况下，层级较深，检索速度慢。 防止树退化为单向链表的解决方法是 红黑树。 红黑树 一个节点存储一个元素 一个节点只能拥有两个子节点 左子树和右子树有序（左小右大），不是完全有序 红黑树是一种自平衡的二叉查找树，它通过左旋和右旋来保持树的平衡，从而防止树过度倾斜。在红黑树中，每个节点带有颜色属性，颜色为红色（Red）或黑色（Black）。 红黑树的特性： 节点颜色：每个节点要么是黑色，要么是红色。 根节点：根节点总是黑色。 叶子节点：所有叶子节点（NIL节点）是黑色。 红色节点的子节点：如果一个节点是红色的，则它的直接子节点都是黑色的。 黑色路径：从任一节点到其每个叶子的所有路径都包含相同数量的黑色节点。 特征：红黑树的关键特性是它的自平衡能力，确保树不会退化成链表，这避免了普通二叉查找树在最坏情况下可能出现的性能下降。 在MySQL中的应用及局限性： 树的高度：海量数据下树的高度非常高，影响查询性能。 范围搜索：无法支持范围搜索。 B树 一个节点存储多个元素 一个节点可以拥有2个以上的子节点 按递增次序排列，并遵循左小右大原则 节点存储键值和数据 B树是一种多路平衡查找树，用于高效的数据存取，特别是在磁盘等外部存储上。以下是B树的主要特性和应用： 多路平衡树：B树是一种多叉树，每个节点可以有多个子节点，这减少了树的高度，并优化了数据的读取速度。 排序和子节点数：节点中的键值按递增顺序排列，每个非叶节点的子节点数介于2到M之间（M是树的阶数）。 叶子节点：所有叶子节点都位于同一层。 数据存储：节点存储键值和数据，其中键值用于指导搜索和排序，数据则是与键值相关联的实际信息。 B树和红黑树最大的区别是B树是完全有序的，且B树是一颗多叉树（一个节点可以存储多个子节点）。 Mysql为什么不用B树 B树通过自平衡解决了退化链表问题。 B树是一颗多叉树，解决树高度问题，解决查询效率问题。 不选择B树是因为B树无法支持高效的范围查询。 B+树 一个节点存储多个元素 按递增次序排列，并遵循左小右大原则 非叶子节点存储键值，叶子节点存储键值和数据 叶子节点通过指针两两相连，便于实现范围查找和排序 B+树是一种自平衡的多路查找树，是B树的变体，用于数据库和文件系统中的索引和数据组织。其特点包括： 多路平衡查找树：B+树是一颗多叉树，其中每个节点有多个子节点。这种结构减少了树的高度，提高了查找效率。 存储结构：非叶子节点存储数据的键，叶子节点存储数据的键值。 叶子节点之间相连：B+树的所有叶子节点通过指针相连，形成了一个有序链表，这使得对数据的范围查询和顺序访问变得非常高效。 聚簇索引B+树结构：在nnoDB存储引擎中，聚簇索引使用B+树结构。这意味着表的数据实际上存储在B+树的叶子节点中。排序和范围查找：由于数据在叶子节点中按键值排序，聚簇索引特别适合于范围查找和排序操作。叶子节点之间的链接使得范围扫描变得高效。 MySql对B+树的优化 MySql索引数据结构对经典的B+Tree进行了优化。在原B+Tree的基础上，增加一个指向相邻叶子节点的链表指针，就形成了带有顺序指针的B+Tree，提高区间访问的性能。 B树和B+树的区别 B树节点存储键值和数据，B+树非叶子节点存键值，叶子节点存键值和数据 B+树叶子节点两两相连形成一个有序的链表，可以支持高效的范围查询；而B树每个节点的键值存储在一起，无法行程一个有序列表。 Hash 原理类似HashMap,通过列的值转成hashCode,hash冲突就用链表 哈希索引就是采用一定的hash算法，将键值换算成新的hash值，映射到对应的槽位上，然后存储在hash表中。 如果两个(或多个)键值，映射到一个相同的槽位上，他们就产生了hash冲突（也称为hash碰撞），可以通过链表来解决。 创建hash索引后，会为每个键值通过特定的算法计算出一个哈希码（hash code)，需要注意的是不同的键值计算出来的hash值可能是相同的，上图金庸和杨逍就存在hash冲突，当冲突时使用链表存储多个冲突的数据，然后再次遍历这条链表，找到需要的数据，这就解释了为啥hash冲突严重了，hash索引效率降低的原因。 优缺点 优点 快速查找：对于等值查询，哈希提供了快速的访问速度。由于哈希函数的直接计算，它能够快速定位到数据的存储位置。 空间效率：哈希通常比其他类型的（如B+树）更加紧凑，占用更少的空间。 缺点 不支持范围查询：哈希不适用于范围查询。由于哈希函数的特性，无法用哈希索引来有效地处理大于、小于或介于某两个值之间的查询。 不支持排序操作：哈希不保证顺序性，因此无法支持基于索引的数据排序。 Hash冲突：当Hash冲突严重时，使用链表存储冲突的Hash数据，会严重影响查询性能。 索引语法 创建索引 1CREATE [ UNIQUE | FULLTEXT ] INDEX index_name ON table_name (index_col_name,... ); 查看索引 1SHOW INDEX FROM table_name; 删除索引 1DROP INDEX index_name ON table_name; 索引分类 MySql索引按叶子节点存储的是否为完整表数据分为：聚簇索引、二级索引（辅助索引）。全表数据存储在聚簇索引中，聚簇索引以外的其他索引叫做二级索引，也叫辅助索引。 在MySQL数据库，将索引的具体类型主要分为以下几类： 分类 含义 特点 关键字 主键索引 针对表中的主键创建的索引 只能有一个，不可为空，性能最高 PRIMARY 唯一索引 保证某列或列组合的数据唯一性 可以有多个，列值可以为空，确保数据的唯一性 UNIQUE 普通索引 用于快速定位特定数据 可以有多个，提升查询效率，不强制唯一性 INDEX 前缀索引 在字符串列上的索引，只对值的前缀部分建立索引 节省空间，适用于长文本字段 组合索引 在多个列上建立的索引，用于涵盖多个列的查询 提高多列查询的效率，顺序敏感 全文索引 用于全文搜索，特别是在文本数据中搜索关键词 针对大文本内容的快速搜索，不适用于精确匹配 FULLTEXT 聚簇索引 InnoDB存储引擎使用聚簇索引存储数据（B+树），它有以下特点： 数据与索引的存储：在聚簇索引中，表数据直接存储在索引的叶子节点上，非叶子节点是主键索引。这意味着数据行和它的索引是紧密结合在一起的，索引结构实际上就是数据表。 主键顺序存储：聚簇索引按照主键的顺序存储数据。如果表中定义了主键，InnoDB会使用这个主键作为聚簇索引的键。 主键选择： 默认创建表会定义一个主键。 没有主键，寻找第一个非空唯一列作为聚簇索引的键。 不存在非空唯一列，InnoDB会自动创建一个隐含的主键 ROWID。 页分裂和插入顺序：由于数据是按主键顺序存储的，非顺序插入（如随机插入）可能导致页分裂，影响性能。顺序插入，如使用自增主键，会有更好的性能。 页合并和数据删除：当大量数据删除时，相邻的两个页数据达到合并的阈值时，会进行合并以提高查询性能。 主键更新：在聚簇索引中，更新主键的代价较高，因为它涉及到实际数据的物理移动。 InnoDB表设计要有自增主键的原因 聚簇索引：如果表有一个主键，InnoDB就会使用这个主键作为聚簇索引。当主键是自增的，新记录会被顺序地添加到索引的末端，这有助于减少 页分裂，提高写入效率。 插入性能：当使用自增主键时，每个新插入的行都会被顺序地添加到表的末尾。这种末尾追加与随机插入相比，减少了磁盘I/O，提高了插入速度。 数据排序：聚簇索引（B+树）是有序的，需要根据主键对数据进行有序排序。 主键选择：如果不指定索引则会选择第一个非空唯一列，否则会生成一个自增的 ROWID。 聚簇索引优缺点 优点 主键查询效率高：对于通过主键访问的查询，因为索引和数据是同时存储的，所以检索数据不需要额外的回表操作。 优化范围查询：由于数据是按索引键顺序存储的，因此对数据进行范围查询会更快。 索引覆盖查询：查询的列都是聚簇索引的键时，可以直接在索引中完成查询，无需进行回表查询。 缺点 需要顺序插入：如果插入数据的主键不是有序的，会导致数据页分裂，影响插入性能。 数据碎片化：数据的频繁插入和删除，聚簇索引可能导致数据在物理存储上的碎片化，影响数据库性能。 OPTIMIZE TABLE table_name; 重建表消除磁盘碎片。 非主键查询的性能影响：非主键查询需要额外的回表操作才能获取查询数据，性能不如主键查询。 非聚簇索引 MyISAM表中的主键索引和非主键索引的结构是一样的。在这两种类型的索引中，索引的叶子节点并不直接存储表数据本身，而是存放指向表数据的物理地址。这种设计意味着MyISAM表的索引主要用于快速定位数据，而不是直接存储数据内容。 二级索引 B+树 二级索引与聚簇索引分开存储，它自己是独立的一颗B+树，索引的键就是创建二级索引指定的字段，叶子节点不是存储完整的表数据，而是其对应的ID。 优缺点 优点 提高查询效率：对于非主键的查询，二级索引可以显著提高检索速度。 灵活的查询：可以基于表中任意列组合创建索引，增加多字段查询的灵活性。 缺点 额外的存储开销：每个二级索引都需要占用额外的存储空间。 性能损耗：数据的插入、更新、删除操作都需要同时更新聚簇索引和所有相关的二级索引，降低数据修改效率。 回表查询 回表查询也称为&quot;二次查找&quot;或&quot;索引跳跃&quot;，发生在使用二级索引检索数据时，二级索引不能完全提供所检索的数据，因此需要进一步访问聚簇索引来获取完整的数据行。 工作原理： 扫描二级索引：查询首先在二级索引中进行，以查找符合条件的索引条目。 获取主键：二级索引的叶子节点存储对应数据行的聚簇索引键（通常是主键）。 访问聚簇索引：数据库使用从二级索引中获取的聚簇索引键，去聚簇索引中检索完整的数据行。 返回数据：最后，从聚簇索引中检索到的数据行被返回给用户。 回表查询需要额外的聚簇索引搜索，必然会降低查询效率。 索引覆盖 定义：二级索引包含了所有的查询字段，查询结果直接由二级索引返回，不需要回到聚簇索获取完整的数据行。 工作原理： 查询的列在索引中：查询所涉及的所有列都包含在一个索引中。 索引满足查询需求：查询可以完全通过查看索引来得到所需的结果，而不需要查找表中的实际数据行，提高查询效率。 Example 1234567891011121314create table tb_test(id int not null primary key,user_name varchar(20) not null,age int not null,sex int,birthday datetime);# 建立组合索引create index idx_user_name_age on tb_test(user_name, age);# 进行索引覆盖查询explainselect id, user_name, age # 所有查询的字段都在二级索引中from tb_test; 索引覆盖解决%在左边的like查询索引失效 索引下推 思想：尽早地利用索引信息来过滤掉不符合条件的行，减少数据访问和处理的开销。 具体实现：查询涉及多个条件时，InnoDB 可以在索引的叶子节点上执行部分查询条件，将不符合条件的行过滤掉，只将符合条件的行返回给查询引擎。 原理： 索引遍历：查询优化器确定使用索引后进行索引遍历。 条件下推：遍历索引时，尽可能利用 Where 子句条件过滤索引数据。比如查询条件不包含组合索引中间字段，依然使用后面字段过滤索引数据。 减少数据返回Server层：经过条件下推过滤后，减少返回Server层数据。 提高查询效率：减少回表检索完整数据工作量。 特点： 索引条件下推：在索引扫描阶段，即使索引条件被中断，存储引擎仍能够在索引内部应用WHERE子句的一部分条件，从而减少返回的数据行数。 优势：在索引层面提前过滤掉不满足条件的行，减少了对聚簇索引的访问次数。 EXAMPLE 使用一张用户表tuser，表里创建联合索引（name, age） 1select * from tuser where name like &#x27;张%&#x27; and age=10; 最左匹配原则，那么就知道这个语句在搜索索引树的时候，只能用 张，找到的第一个满足条件的记录id为1。 没有使用索引条件下推由于组合索引被 like 中断，引擎层在扫描索引时只扫描 姓张 的用户，即使 WHERE 子句还有其他条件用于索引过滤。将粗略扫描的数据返回 SERVER 层。根据返回数据回表到聚簇索引取出完整数据，然后再进行条件判断。将筛选后的数据返回客户端。 使用索引条件下推条件下推：在存储引擎扫描索引时，虽然使用了 like 查询，但是 WHERE 子句中有可用于当前索引的过滤条件，会继续取出过滤条件对索引进行数据过滤。减少SERVER层数据返回：通过下推条件对索引进行过滤后，返回少量数据到SERVER层；减少回表次数：由于 SERVER 层数据减少，需要回表的次数也相应减少。取出数据后继续过滤非索引条件。将筛选后的数据返回客户端。 主键索引 主键唯一，不能为空 聚簇索引的key,不用回表，性能最佳 建立在主键上的索引被称为 主键索引，一张数据表只能有一个主键索引，索引列值不允许有空值，通常在创建表时一起创建。 特点： 唯一性： 作为聚簇索引的键，保证每一行的主键都唯一。 非空性： 主键字段不能包含NULL值。 查询优化：作为聚簇索引的键，使用主键查询不需要回表操作，查询效率最高。 1234create table test( id int not null primary key # 建立主键时自动创建主键索引); 唯一索引 能为空 建立聚簇索引Key的备选方案 唯一索引用于保证一个表中的每一行在指定列上的值是唯一的，一张表可以有多个唯一索引，索引列值允许为空，列值中出现多个空值不会发生重复冲突。 特点： 保证唯一：唯一索引在表的一列或列组合上创建，确保在这一列或列组合中的每个值都是唯一的。 约束检查：当增删改数据时，会检查新的值是否违反了唯一约束，违反则抛出异常。 允许NULL：NULL 不会引发唯一索引冲突。 1create unique index un_idx_user_name on tb_test(user_name); 普通索引 最常用 在MySQL中，建立在表的普通字段上的索引被称为 普通索引 或 非唯一索引。这种索引允许字段中包含重复的值。如果查询仅使用到了索引中的字段，这称为 索引覆盖查询，因为索引本身就足够满足查询需求，无需访问表中的实际数据。在非覆盖索引的情况下，数据库引擎需要进行 回表查询，即先通过索引找到对应的行标识（如行ID），然后再到数据表中检索完整的行数据。 特点： 优化查询。 索引覆盖查询。 1create index idx_user_name on tb_test(user_name); 前缀索引 前缀索引是通过在创建索引时指定列值的 前几个字符 来建立的，而不是在整个字段上建索引。如果前几个字符基本就能确定数据唯一性， 前缀索引可以大大减少索引占用的存储空间，也能提升索引的查询效率。 特点： 长文本字段：适合char、varchar、binary、varbinary、text的列上使用。 减少空间占用：前缀索引只索引部分字符，占用的空间比完整索引小。 提高效率：通过扫描字符前缀便能查找数据，比全字符查询效率高。 字段选择：如果字段的字符前缀重复度很高，索引效果会很差。 12# 前5个字符加索引create index idx_user_name on tb_test(user_name(5)); 组合索引 查询优化 索引覆盖 建立在多个列上的索引被称为组合索引（也称为复合索引或多列索引），它可以同时涵盖多个字段，使得基于这些字段的查询更加高效。 原理： 有序结构：组合索引中的数据是根据索引列的顺序存储的。首先是第一列的数据排序，然后是第二列的数据排序，依此类推。 索引查找：如果查询条件包含索引的首列，索引最为有效，跳过首列索引不生效。 特点： 优化多列查询：对于经常同时在多个列上进行查询的场景，组合索引可以显著提高查询效率。 排序和分组：组合索引可以加速涉及多个列的排序（ORDER BY）和分组（GROUP BY）操作。 索引覆盖查询：如果查询的选择列完全包含在索引中，则无需访问表中的行，这称为索引覆盖扫描。 1create index idx_name_age on tb_user(name, age); 组合索引的使用需要遵循 最左匹配原则。 ORDER BY 优化假设 employees 表创建了department_id, last_name, first_name 这三个字段的索引。1SELECT * FROM employees ORDER BY department_id, last_name, first_name;即使 WHERE 子句没有使用组合索引，ORDER BY 或 GROUP BY 仍有可能利用该索引进行优化。1234SELECT * FROM employees WHERE department_id = 5ORDER BY last_name, first_name; GROUP BY 优化123SELECT department_id, COUNT(*) FROM employees GROUP BY department_id, last_name, first_name;1234SELECT department_id, job_title, AVG(salary) AS average_salaryFROM employeesWHERE department_id &gt; 5GROUP BY department_id, job_title; 索引使用注意事项 有效地使用 MySQL 中的索引可以显著提高数据库查询的性能，滥用索引反而会导致数据库性能下降。 使用短索引 当长字符串字段的前几位字符区别很大时，可以指定前缀长度来创建前缀索引。这种方法意味着只对字符串数据的前部分进行索引，而不是整个字段。 优势： 提高查询性能：短索引的扫描速度更快，能提升查询效率。 减少磁盘空间占用：短索引占用的磁盘空间更少。 优化I/O操作：通过减少索引大小，MySQL可以更高效地维护和访问索引。 1234567create table test( id int not null primary key , business_no varchar(50) .... );# 使用短索引create index idx_business_no on test(business_no(5)); 频繁更新的字段不使用索引 频繁更新的字段会导致索引经常重新构建，从而增加数据库的工作负担。 额外性能开销：每次对表中的数据进行更新操作时，都需要更新索引结构，这会导致额外的性能开销。 影响优化器选择：频繁更新的字段加了索引，查询优化器可能会在选择执行计划时受到影响，导致查询性能下降。 索引列存在隐式类型转换 如果索引列的数据类型和查询条件的数据类型不一致时，MySQL会进行 隐式的类型转换。使得优化器无法准确估计索引的查询成本。，从而降低查询效率。 12345678create table tb_user( id int not null primary key, name varchar(20) null, age int null, adderss varchar(50) null);create index idx_address on tb_user (adderss); 不要在索引列上调用函数或数学运算 在索引列上直接应用函数或进行数学运算可能导致MySQL查询优化器无法有效利用索引。 在MySQL中，如果在查询中对一个已经被索引的列直接使用函数调用或进行数学运算，例如 SELECT * FROM table WHERE YEAR(date_column) = 2021，这样的操作会使得优化器无法直接利用该列的索引。这是因为索引是针对列原始值建立的，而函数或数学运算改变了列的内容，从而使得索引失效。 like使用 LIKE查询中百分号（%）位置对索引的影响： 场景一：当 % 出现在模式的左边或者两边时，索引失效。 12EXPLAIN SELECT * FROM users WHERE user_name LIKE &#x27;tom%&#x27;;EXPLAIN SELECT * FROM users WHERE user_name LIKE &#x27;%tom%&#x27;; 这两种情况下，由于MySQL无法从索引中快速定位起始匹配位置，必须对整个字段进行扫描来查找匹配项，因此索引不会被有效利用。 场景二：当%仅出现在模式的右边时，索引可以生效。 1EXPLAIN SELECT * FROM users WHERE user_name LIKE &#x27;tom%&#x27;; 在这种情况下，查询能够利用索引，因为MySQL可以利用索引快速定位到以’tom’开头的记录。 利用索引覆盖优化like的末尾匹配 当使用LIKE操作符进行模糊查询且模式以%开始时，如 '%value%' 或 '%value'，一般情况下索引会失效。这是因为搜索模式的起始部分不固定，导致MySQL无法利用索引进行高效搜索。然而，可以通过索引覆盖（Covering Index）来优化这类查询。 例如，如果在user_name, user_age, user_level上创建了组合索引，即使LIKE查询的模式以%开头，查询也可以更高效，因为MySQL会进行 全索引扫描 而非全表扫描。 1234EXPLAIN SELECT user_name FROM users WHERE user_name LIKE &#x27;%jack%&#x27;;EXPLAIN SELECT user_name FROM users WHERE user_name LIKE &#x27;%jack&#x27;;EXPLAIN SELECT user_name, user_age, user_level FROM users WHERE user-name LIKE &#x27;%jack%&#x27;;EXPLAIN SELECT user_name, user_age, user_level FROM users WHERE user-name LIKE &#x27;%jack%&#x27;; like失效原因 %号在右（如 LIKE 'value%'）： 原理：当%号在右侧时，搜索条件固定了字符串的开始部分。由于B+树索引是按字典顺序排序的，可以有效地在索引中定位到以指定前缀开头的字符串。 效果：这种模式可以高效地利用索引进行范围查找，因为它限定了字符串的开始部分。 %号在左（如 LIKE '%value'）： 原因：当%号在左侧时，搜索条件固定了字符串的结束部分。由于B+树索引是基于整个字符串的字典顺序建立的，而不是基于字符串的尾部，这使得索引在这种情况下无法有效地定位匹配。 效果：索引在这种模式下无法被有效利用，因为它没有限定字符串的开始部分。 两个%号（如 LIKE '%value%'）： 原因：当两边都有%号时，这表示在字符串的任意位置匹配给定的模式。由于B+树索引是基于整个字符串值的排序，而非其子部分，这种模式不利于在索引中定位匹配。 效果：在这种情况下，索引通常也无法被有效利用。 谨慎使用or 如果OR连接的条件中，某些列有索引而另一些列没有，那么这通常会导致整个查询无法有效利用索引。 优化建议： 建立合适的索引：确保所有在OR条件中涉及的列都有索引。 使用UNION替代：可以使用UNION来重写含有OR的查询，因为UNION可以单独优化每个查询分支，并且可能更高效地利用索引。 索引列排序 排序处理方式 执行计划extra列信息 排序性能比较 利用索引进行排序 Using index 高 临时表排序没有适合排序的索引,在内存创建临时表并在其中排序 Using temporary 低 文件排序将排序数据存储在磁盘上的临时文件中，并进行排序操作 Using filesort 低 当MySQL可以利用索引的有序性来执行 ORDER BY 操作时，这是最高效的排序方式。因为数据在索引中已经是有序的，所以MySQL可以直接按索引顺序检索数据，无需额外的排序操作。 数据分布影响 数据的分布特性会影响优化器是否选择使用索引。MySQL优化器在决定是否使用索引时，会评估使用索引的成本与全表扫描的成本，并选择更高效的方案。 使用索引的成本考虑： 查询数据量少：使用索引通常更高效，因为索引可以快速定位到符合条件的少量数据。 索引数据区分度高：扫描索引能高效检索数据。 全表扫描的成本考虑： 查询数量多：如果查询条件涵盖了表中的大部分数据，索引的效益就会降低。在这种情况下，索引的使用可能不如全表扫描高效。 索引数据区分度低：扫描索引收益不高可能选择全表扫描。 比如一张表只有10万条数据，并且id是自增的，使用以下Sql则会放弃索引走全表扫描，放弃索引的原因是因为表中大多数数据都满足条件。 123select *from testwhere id &lt; 95000 最左匹配原则 一个查询通常只使用一个索引来优化 WHERE 子句、ORDER BY 子句和 GROUP BY 子句。这意味着，如果WHERE子句已经使用了一个索引，那么在ORDER BY子句中使用另一个不同的索引来排序可能不会生效。为了优化含有多列排序的查询，可以考虑以下策略： 避免多列非索引排序：尽量避免在ORDER BY子句中包含没有被索引的多个列。 使用复合索引：复合索引可以提高那些同时使用WHERE子句和ORDER BY子句的查询的效率。 索引覆盖：查询字段和WHERE子句和ORDER BY子句中的列都属于同一个索引时，使用这个索引进行查询，排序和返回数据。 最佳左前缀底层原理： 键值排序：在复合索引中，数据首先按照索引的第一个列的值排序，对于第一个列值相同的行，再按照第二个列的值排序，以此类推。 连续匹配：当执行查询时，MySQL会在索引中查找与查询条件匹配的行。为了高效地利用索引，查询条件需要从复合索引的最左边开始连续匹配索引中的列。这意味着，如果查询条件跳过了复合索引中的某个列，索引的后续部分就无法被有效利用。 123456create table myTest(a int,b int,c int,KEY a(a,b,c)); 12select * from myTest where a=3 and b=5 and c=4; -- abc顺序abc三个索引都在where条件里面用到了，而且都发挥了作用 12select * from myTest where c=4 and b=6 and a=3;where里面的条件顺序在查询之前会被自动优化，效果跟上一句一样 12select * from myTest where a=3 and c=7;a用到索引，b没有用，所以c是没有用到索引效果的 12select * from myTest where a=3 and b&gt;7 and c=3; -- b范围值，断点，阻塞了c的索引a用到了，b也用到了，c没有用到，这个地方b是范围值，也算断点，只不过自身用到了索引 12select * from myTest where a=3 and b&gt;=7 and c=3; -- abc全部使用索引b是大于等于包含等值，因此不会阻断查询 12select * from myTest where b=3 and c=4; --组合索引必须遵循最左匹配原则因为a索引没有使用，所以这里 bc都没有用上索引效果 12select * from myTest where a=3 order by b;a用到了索引，b在结果排序中也用到了索引的效果，a下面任意一段的b是排好序的(索引列排序) 12select * from myTest where a=3 order by c;a用到了索引，但是这个地方c没有发挥排序效果，因为中间断点了，使用 explain 可以看到 filesort (索引列排序不生效，使用文件排序) 12select * from myTest where b=3 order by a;b没有用到索引，排序中a也没有发挥索引效果 (没有遵循最左匹配原则) 12select * from myTest a=3 and b like &#x27;7%&#x27; and c=3; -- 左前缀模糊匹配，索引生效abc都使用了索引 12select * from myTest a=3 and b like &#x27;%7&#x27; and c=3; -- 非左前缀模糊匹配，索引失效a使用索引，b右匹配索引阻断 Index Skip Scan 索引跳跃扫描（Index Skip Scanning）是在一个复合索引中（MySql 8.0），如果查询条件 没有包含组合索引的第一列，但包含了后续的某些列，传统的索引扫描方法可能无法直接使用这个索引。索引跳跃扫描的原理是在索引的第一列上“跳跃”查找，然后在找到的每个区间内，进一步检查是否满足查询的其他条件。 原理： 跳跃动作：在扫描索引过程中，优化器会“跳过”不符合查询条件的索引部分，定位到下一个可能匹配的索引点。 重复跳跃：这个跳跃过程可能会发生多次，直到找到满足所有查询条件的数据。 避免全索引扫描：通过跳跃不满足查询条件的条目，提高索引扫描效率。 官方例子 1234567891011CREATE TABLE t1 (f1 INT NOT NULL, f2 INT NOT NULL, PRIMARY KEY(f1, f2));INSERT INTO t1 VALUES (1,1), (1,2), (1,3), (1,4), (1,5), (2,1), (2,2), (2,3), (2,4), (2,5);INSERT INTO t1 SELECT f1, f2 + 5 FROM t1;INSERT INTO t1 SELECT f1, f2 + 10 FROM t1;INSERT INTO t1 SELECT f1, f2 + 20 FROM t1;INSERT INTO t1 SELECT f1, f2 + 40 FROM t1;ANALYZE TABLE t1;EXPLAIN SELECT f1, f2 FROM t1 WHERE f2 &gt; 40; 索引跳跃过程： 获取f1字段第一个唯一值，也就是f1=1。 构造f1=1 and f2 &gt; 40，进行范围查询。 获取f1字段第二个唯一值，也就是f1=2。 构造f1=2 and f2 &gt; 40，进行范围查询。 一直扫描完f1字段所有的唯一值，最后将结果合并返回。 索引创建原则 在 经常需要搜索 的列上创建索引，可以加快搜索的速度 针对于常作为查询条件（where）、排序（order by）、分组（group by）操作的字段建立索引 在经常用在 连接的列 上，这些列主要是一些外键，可以加快连接的速度 尽量选择 区分度高 的列作为索引，尽量建立唯一索引，区分度越高，使用索引的效率越高 在经常需要根据 范围进行搜索 的列上创建索引，因为索引已经排序，其指定的范围是连续的 在经常需要 排序的列 上创建索引，因为索引已经排序，这样查询可以利用索引的排序，加快排序查询时间 尽量使用联合索引，减少单列索引，查询时，联合索引很多时候可以 覆盖索引 ，节省存储空间，避免回表，提高查询效率 要 控制索引的数量 ，索引并不是多多益善，索引越多，维护索引结构的代价也就越大，会影响增删改的效率 如果索引列不能存储NULL值，请在创建表时使用 NOT NULL约束 它。当优化器知道每列是否包含NULL值时，它可以更好地确定哪个索引最有效地用于查询 InnoDB: 最多 64个二级索引 和 一个主键索引, 单个索引最多包含16列。 SQL提示 SQL提示，是优化数据库的一个重要手段，简单来说，就是在SQL语句中加入一些人为的提示来达到优化操作的目的。 USE INDEXUSE INDEX 提示可以在 SELECT 查询中使用，让MySQL参考使用指定的索引。1explain select * from city use index(idx_city_name) where city_name = &#x27;深圳&#x27;;需要注意的是，虽然 USE INDEX 提示可以用于指定要使用的索引，但它并不能保证 MySQL 实际会使用指定的索引。MySQL 查询优化器会在执行查询时根据统计信息和查询成本进行决策。如果查询优化器认为其他索引更适合执行查询，它可能会忽略 USE INDEX 提示。 IGNORE INDEXIGNORE INDEX 用于告诉查询优化器在执行查询时忽略特定的索引。通过使用 IGNORE INDEX，可以让查询优化器绕过指定的索引，而选择其他索引或执行全表扫描来处理查询。1explain select * from city ignore index(idx_user_pro) where city_name = &#x27;深圳&#x27;;通常情况下，MySQL 的查询优化器会根据统计信息和索引选择算法选择最佳的索引，不需要手动干预。只有在特定的情况下，当你确定优化器选择了错误的索引时，才考虑使用 IGNORE INDEX 提示进行干预。 FORCE INDEXFORCE INDEX 是 MySQL 中用于强制查询优化器选择特定索引的语句。当你使用 FORCE INDEX 语句时，查询优化器将会忽略其他可能的索引并且只使用指定的索引来执行查询。1explain select * from city force index(idx_user_pro) where city_name = &#x27;深圳&#x27;;需要注意，强制使用索引可能导致查询性能下降，因为查询优化器无法选择更有效的索引。 SQL性能分析 show status 运行状态 MySQL 客户端连接成功后，通过 show [session|global] status 命令可以提供服务器状态信息。通过如下指令，可以查看当前数据库的INSERT、UPDATE、DELETE、SELECT的访问频次。 123-- session 是查看当前会话 ;-- global 是查询全局数据 ;SHOW GLOBAL STATUS LIKE &#x27;Com%&#x27;; Com_delete: 删除次数。 Com_insert: 插入次数。 Com_select: 查询次数。 Com_update: 更新次数。 状态名 作用域 详细解释 Aborted_clients Global 由于客户端没有正确关闭连接导致客户端终止而中断的连接数 Aborted_connects Global 试图连接到MySQL服务器而失败的连接数 Binlog_cache_disk_use Global 使用临时二进制日志缓存但超过binlog_cache_size值并使用临时文件来保存事务中的语句的事务数量 Binlog_cache_use Global 使用临时二进制日志缓存的事务数量 Bytes_received Both 从所有客户端接收到的字节数。 Bytes_sent Both 发送给所有客户端的字节数。 com* 各种数据库操作的数量 Compression Session 客户端与服务器之间只否启用压缩协议 Connections Global 试图连接到(不管是否成功)MySQL服务器的连接数 Created_tmp_disk_tables Both 服务器执行语句时在硬盘上自动创建的临时表的数量 Created_tmp_files Global mysqld已经创建的临时文件的数量 Created_tmp_tables Both 服务器执行语句时自动创建的内存中的临时表的数量。如果Created_tmp_disk_tables较大，你可能要增加tmp_table_size值使临时 表基于内存而不基于硬盘 Delayed_errors Global 用INSERT DELAYED写的出现错误的行数(可能为duplicate key)。 Delayed_insert_threads Global 使用的INSERT DELAYED处理器线程数。 Delayed_writes Global 写入的INSERT DELAYED行数 Flush_commands Global 执行的FLUSH语句数。 Handler_commit Both 内部提交语句数 Handler_delete Both 行从表中删除的次数。 Handler_discover Both MySQL服务器可以问NDB CLUSTER存储引擎是否知道某一名字的表。这被称作发现。Handler_discover说明通过该方法发现的次数。 Handler_prepare Both A counter for the prepare phase of two-phase commit operations. Handler_read_first Both 索引中第一条被读的次数。如果较高，它建议服务器正执行大量全索引扫描；例如，SELECT col1 FROM foo，假定col1有索引。 Handler_read_key Both 根据键读一行的请求数。如果较高，说明查询和表的索引正确。 Handler_read_next Both 按照键顺序读下一行的请求数。如果你用范围约束或如果执行索引扫描来查询索引列，该值增加。 Handler_read_prev Both 按照键顺序读前一行的请求数。该读方法主要用于优化ORDER BY … DESC。 Handler_read_rnd Both 根据固定位置读一行的请求数。如果你正执行大量查询并需要对结果进行排序该值较高。你可能使用了大量需要MySQL扫描整个表的查询或你的连接没有正确使用键。 Handler_read_rnd_next Both 在数据文件中读下一行的请求数。如果你正进行大量的表扫描，该值较高。通常说明你的表索引不正确或写入的查询没有利用索引。 Handler_rollback Both 内部ROLLBACK语句的数量。 Handler_savepoint Both 在一个存储引擎放置一个保存点的请求数量。 Handler_savepoint_rollback Both 在一个存储引擎的要求回滚到一个保存点数目。 Handler_update Both 在表内更新一行的请求数。 Handler_write Both 在表内插入一行的请求数。 Innodb_buffer_pool_pages_data Global 包含数据的页数(脏或干净)。 Innodb_buffer_pool_pages_dirty Global 当前的脏页数。 Innodb_buffer_pool_pages_flushed Global 要求清空的缓冲池页数 Innodb_buffer_pool_pages_free Global 空页数。 Innodb_buffer_pool_pages_latched Global 在InnoDB缓冲池中锁定的页数。这是当前正读或写或由于其它原因不能清空或删除的页数。 Innodb_buffer_pool_pages_misc Global 忙的页数，因为它们已经被分配优先用作管理，例如行锁定或适用的哈希索引。该值还可以计算为Innodb_buffer_pool_pages_total - Innodb_buffer_pool_pages_free - Innodb_buffer_pool_pages_data。 Innodb_buffer_pool_pages_total Global 缓冲池总大小（页数）。 Innodb_buffer_pool_read_ahead_rnd Global InnoDB初始化的“随机”read-aheads数。当查询以随机顺序扫描表的一大部分时发生。 Innodb_buffer_pool_read_ahead_seq Global InnoDB初始化的顺序read-aheads数。当InnoDB执行顺序全表扫描时发生。 Innodb_buffer_pool_read_requests Global InnoDB已经完成的逻辑读请求数。 Innodb_buffer_pool_reads Global 不能满足InnoDB必须单页读取的缓冲池中的逻辑读数量。 Innodb_buffer_pool_wait_free Global 一般情况，通过后台向InnoDB缓冲池写。但是，如果需要读或创建页，并且没有干净的页可用，则它还需要先等待页面清空。该计数器对等待实例进行记数。如果已经适当设置缓冲池大小，该值应小。 Innodb_buffer_pool_write_requests Global 向InnoDB缓冲池的写数量。 Innodb_data_fsyncs Global fsync()操作数。 Innodb_data_pending_fsyncs Global 当前挂起的fsync()操作数。 Innodb_data_pending_reads Global 当前挂起的读数。 Innodb_data_pending_writes Global 当前挂起的写数。 Innodb_data_read Global 至此已经读取的数据数量（字节）。 Innodb_data_reads Global 数据读总数量。 Innodb_data_writes Global 数据写总数量。 Innodb_data_written Global 至此已经写入的数据量（字节）。 Innodb_dblwr_pages_written Global 已经执行的双写操作数量 Innodb_dblwr_writes Global 双写操作已经写好的页数 Innodb_log_waits Global 我们必须等待的时间，因为日志缓冲区太小，我们在继续前必须先等待对它清空 Innodb_log_write_requests Global 日志写请求数。 Innodb_log_writes Global 向日志文件的物理写数量。 Innodb_os_log_fsyncs Global 向日志文件完成的fsync()写数量。 Innodb_os_log_pending_fsyncs Global 挂起的日志文件fsync()操作数量。 Innodb_os_log_pending_writes Global 挂起的日志文件写操作 Innodb_os_log_written Global 写入日志文件的字节数。 Innodb_page_size Global 编译的InnoDB页大小(默认16KB)。许多值用页来记数；页的大小很容易转换为字节。 Innodb_pages_created Global 创建的页数。 Innodb_pages_read Global 读取的页数。 Innodb_pages_written Global 写入的页数。 Innodb_row_lock_current_waits Global 当前等待的待锁定的行数。 Innodb_row_lock_time Global 行锁定花费的总时间，单位毫秒。 Innodb_row_lock_time_avg Global 行锁定的平均时间，单位毫秒。 Innodb_row_lock_time_max Global 行锁定的最长时间，单位毫秒。 Innodb_row_lock_waits Global 一行锁定必须等待的时间数。 Innodb_rows_deleted Global 从InnoDB表删除的行数。 Innodb_rows_inserted Global 插入到InnoDB表的行数。 Innodb_rows_read Global 从InnoDB表读取的行数。 Innodb_rows_updated Global InnoDB表内更新的行数。 Key_blocks_not_flushed Global 键缓存内已经更改但还没有清空到硬盘上的键的数据块数量。 Key_blocks_unused Global 键缓存内未使用的块数量。你可以使用该值来确定使用了多少键缓存 Key_blocks_used Global 键缓存内使用的块数量。该值为高水平线标记，说明已经同时最多使用了多少块。 Key_read_requests Global 从缓存读键的数据块的请求数。 Key_reads Global 从硬盘读取键的数据块的次数。如果Key_reads较大，则Key_buffer_size值可能太小。可以用Key_reads/Key_read_requests计算缓存损失率。 Key_write_requests Global 将键的数据块写入缓存的请求数。 Key_writes Global 向硬盘写入将键的数据块的物理写操作的次数。 Last_query_cost Session 用查询优化器计算的最后编译的查询的总成本。用于对比同一查询的不同查询方案的成本。默认值0表示还没有编译查询。 默认值是0。Last_query_cost具有会话范围。 Max_used_connections Global 服务器启动后已经同时使用的连接的最大数量。 ndb* ndb集群相关 Not_flushed_delayed_rows Global 等待写入INSERT DELAY队列的行数。 Open_files Global 打开的文件的数目。 Open_streams Global 打开的流的数量(主要用于记录)。 Open_table_definitions Global 缓存的.frm文件数量 Open_tables Both 当前打开的表的数量。 Opened_files Global 文件打开的数量。不包括诸如套接字或管道其他类型的文件。 也不包括存储引擎用来做自己的内部功能的文件。 Opened_table_definitions Both 已经缓存的.frm文件数量 Opened_tables Both 已经打开的表的数量。如果Opened_tables较大，table_cache 值可能太小。 Prepared_stmt_count Global 当前的预处理语句的数量。 (最大数为系统变量: max_prepared_stmt_count) Qcache_free_blocks Global 查询缓存内自由内存块的数量。 Qcache_free_memory Global 用于查询缓存的自由内存的数量。 Qcache_hits Global 查询缓存被访问的次数。 Qcache_inserts Global 加入到缓存的查询数量。 Qcache_lowmem_prunes Global 由于内存较少从缓存删除的查询数量。 Qcache_not_cached Global 非缓存查询数(不可缓存，或由于query_cache_type设定值未缓存)。 Qcache_queries_in_cache Global 登记到缓存内的查询的数量。 Qcache_total_blocks Global 查询缓存内的总块数。 Queries Both 服务器执行的请求个数，包含存储过程中的请求。 Questions Both 已经发送给服务器的查询的个数。 Rpl_status Global 失败安全复制状态(还未使用)。 Select_full_join Both 没有使用索引的联接的数量。如果该值不为0,你应仔细检查表的索引 Select_full_range_join Both 在引用的表中使用范围搜索的联接的数量。 Select_range Both 在第一个表中使用范围的联接的数量。一般情况不是关键问题，即使该值相当大。 Select_range_check Both 在每一行数据后对键值进行检查的不带键值的联接的数量。如果不为0，你应仔细检查表的索引。 Select_scan Both 对第一个表进行完全扫描的联接的数量。 Slave_heartbeat_period Global 复制的心跳间隔 Slave_open_temp_tables Global 从服务器打开的临时表数量 Slave_received_heartbeats Global 从服务器心跳数 Slave_retried_transactions Global 本次启动以来从服务器复制线程重试次数 Slave_running Global 如果该服务器是连接到主服务器的从服务器，则该值为ON。 Slow_launch_threads Both 创建时间超过slow_launch_time秒的线程数。 Slow_queries Both 查询时间超过long_query_time秒的查询的个数。 Sort_merge_passes Both 排序算法已经执行的合并的数量。如果这个变量值较大，应考虑增加sort_buffer_size系统变量的值。 Sort_range Both 在范围内执行的排序的数量。 Sort_rows Both 已经排序的行数。 Sort_scan Both 通过扫描表完成的排序的数量。 ssl＊ ssl连接相关 Table_locks_immediate Global 立即获得的表的锁的次数。 Table_locks_waited Global 不能立即获得的表的锁的次数。如果该值较高，并且有性能问题，你应首先优化查询，然后拆分表或使用复制。 Threads_cached Global 线程缓存内的线程的数量。 Threads_connected Global 当前打开的连接的数量。 Threads_created Global 创建用来处理连接的线程数。如果Threads_created较大，你可能要增加thread_cache_size值。缓存访问率的计算方法Threads_created/Connections。 Threads_running Global 激活的（非睡眠状态）线程数。 Uptime Global 服务器已经运行的时间（以秒为单位）。 Uptime_since_flush_status Global 最近一次使用FLUSH STATUS 的时间（以秒为单位）。 慢查询日志 慢查询日志是一种记录执行时间超过指定阈值的SQL语句的日志。它可以帮助我们定位执行效率较低的查询，并进行性能优化。 12# 查看是否开启慢查询日志show variables like &#x27;slow_query_log&#x27; 作用： 性能优化：通过分析慢查询日志，可以找出执行效率较低的SQL语句，针对性地进行优化，提升数据库性能。 查询分析：慢查询日志记录了执行时间超过指定阈值的SQL语句，可以用于查询分析、性能瓶颈定位和故障排查等工作。 开启慢查询日志 如果要开启慢查询日志，需要在MySQL的配置文件（my.cnf）中配置如下信息： 12345678# 设置为1表示开启慢查询日志slow_query_log = 1# 指定慢查询日志的文件路径和文件名slow_query_log_file = /path/to/slow-query.log# 指定一个阈值，单位为秒，超过该阈值的SQL语句将被记录到慢查询日志中long_query_time = 2 配置完毕之后，通过以下指令重新启动MySQL服务器即可。 慢查询日志的格式： 执行时间：Time字段，以秒为单位。 锁等待时间：Lock_time字段，以秒为单位。 返回行数：Rows_sent字段，表示查询结果返回的行数。 扫描行数：Rows_examined字段，表示查询过程中扫描的行数。 SQL语句：Query字段，记录执行的SQL语句。 profile详情 MySQL的慢查询日志是一种记录执行时间超过指定阈值的SQL语句的日志，而Profile是另一种用于分析SQL查询性能的工具。 12345# 是否支持profileSELECT @@have_profiling;# 是否开启profileSELECT @@profiling; 开启 profile： 在执行SQL查询语句时，可以使用 SET PROFILING = 1; 命令来开启Profile功能。 开启Profile后，MySQL会记录查询的执行信息，包括每个阶段的耗时和资源消耗。 查询并获取Profile信息： 执行一系列Sql的操作，然后通过如下指令查看指令的执行耗时。 12345678-- 查看每一条SQL的耗时基本情况show profiles;-- 查看指定query_id的SQL语句各个阶段的耗时情况show profile for query query_id;-- 查看指定query_id的SQL语句CPU的使用情况show profile cpu for query query_id; explain 执行计划 EXPLAIN 是一种用于分析查询执行计划的工具。通过EXPLAIN命令，我们可以获取MySQL优化器在执行查询时选择的查询执行计划，从而了解查询的执行方式、索引使用情况以及可能存在的性能问题。 id Columns JSON Name Meaning 1 id select_id 查询的标识符，每个查询都有一个唯一的标识符 2 select_type None 查询的类型，常见的类型有SIMPLE、PRIMARY、SUBQUERY、DERIVED等 3 table table_name 查询涉及的表名 4 partitions partitions 查询涉及的分区信息 5 type access_type 访问类型，表示MySQL在访问表时使用的策略，常见的类型有ALL、INDEX、RANGE、REF等 6 possible_keys possible_keys 可能使用到的索引 7 key key 经过优化器评估最终使用的索引 8 key_len key_length 使用到的索引长度 9 ref ref 与索引比较的列或常数 10 rows rows rows_examined，要得到最终记录索要扫描经过的记录数 11 filtered filtered 通过条件过滤后的行百分比 12 Extra None 额外的信息说明 select_type ​ 表示查询中每个select子句的类型 id select_type value JSON name 含义 1 SIMPLE None 简单的SELECT语句（不包括UNION操作或子查询操作） 2 PRIMARY None PRIMARY：最外层的SELECT语句 3 UNION None UNION：UNION操作中的内层SELECT语句（内层的SELECT语句与外层的SELECT语句没有依赖关系） 4 DEPENDENT UNION dependent(true) DEPENDENT UNION：UNION操作中的内层SELECT语句（内层的SELECT语句与外层的SELECT语句有依赖关系） 5 UNION RESULT union_result UNION RESULT：UNION操作的结果，id值通常为NULL 6 SUBQUERY None SUBQUERY：子查询中的第一个SELECT语句（如果有多个子查询存在） 7 DEPENDENT SUBQUERY dependent(true) DEPENDENT SUBQUERY：子查询中的第一个SELECT语句，但依赖于外层的表（如果有多个子查询存在） 8 DERIVED None DERIVED：作为驱动表的SELECT子查询（子查询位于FROM子句） 9 MATERIALIZED materialized_form_subquery MATERIALIZED：被物化的子查询 10 UNCACHEABLE SUBQUERY cacheable(false) UNCACHEABLE SUBQUERY：对于外层的主表，子查询不可被物化，每次都需要计算（耗时操作） 11 UNCACHEABLE UNION cacheable(false) UNCACHEABLE UNION：UNION操作中的不可被物化的内层子查询（类似于UNCACHEABLE SUBQUERY） type type 字段描述了查询执行计划中MySQL选择的访问方法，也称为访问类型（access type）。它指的是MySQL在执行查询时选择的如何访问表中数据的方式。 性能由好到坏排序： id type value Meaning 1 system 表示使用系统表，通常用于内部操作，不涉及具体的数据表。 2 const 表示通过常量条件（例如主键或唯一索引的等值查询）来访问表的一行数据。这是最快的访问方法之一。 3 eq_ref 表示在多表连接中，被驱动表的连接列上有主键或唯一索引的检索，用于精确匹配。 4 ref 表示使用非唯一索引或唯一索引的非唯一部分进行的等值查询。 5 range 表示使用索引进行范围查询，通常用于 BETWEEN、&gt;、&lt; 等条件的查询。 6 index 表示索引扫描，与&quot;ALL&quot;类型类似，但只扫描索引树，而不读取实际数据行。 7 all 表示全表扫描，即访问表的所有数据。这是最慢的访问方法之一，应尽量避免。 Extra id type value Meaning 1 const row not found 所要查询的表为空 2 Distinct mysql正在查询distinct值，因此当它每查到一个distinct值之后就会停止当前组的搜索，去查询下一个值 3 Impossible WHERE where条件总为false，表里没有满足条件的记录 4 Impossible WHERE noticed after reading const tables 在优化器评估了const表之后，发现where条件均不满足 5 no matching row in const table 当前join的表为const表，不能匹配 6 Not exists 优化器发现内表记录不可能满足where条件 7 Select tables optimized away 在没有group by子句时，对于MyISAM的select count(*)操作，或者当对于min(),max()的操作可以利用索引优化，优化器发现只会返回一行。 8 Using filesort 使用filesort来进行order by操作,出现filesort就说明拍序列没使用上索引 9 Using index 覆盖索引 10 Using index for group-by 对于group by列或者distinct列，可以利用索引检索出数据，而不需要去表里查数据、分组、排序、去重等等 11 Using join buffer 之前的表连接在nested loop之后放进join buffer，再来和本表进行join。适用于本表的访问type为range，index或all 12 Using sort_union,using union,using intersect index_merge的三种情况 13 Using temporary 使用了临时表来存储中间结果集，适用于group by，distinct，或order by列为不同表的列。 14 Using where 在存储引擎层检索出记录后，在server利用where条件进行过滤，并返回给客户端 15 Using index condition 这是MySQL 5.6出来的新特性，叫做索引条件下推 SQL优化 Insert 优化 单条插入效率问题 1234insert into test values(...);insert into test values(...);insert into test values(...);insert into test values(...); 当对MySQL数据库进行 大量 的单条数据插入时，每条 INSERT 语句都被视作一个独立的操作。这意味着： 效率低下：每条 INSERT 语句都被视为一个独立的隐式事务，这意味着每次插入都涉及事务的开启和提交。 占用连接资源：大量的单条插入语句占用更多的数据库连接资源，从而影响数据库的整体性能。 网络延迟累积：单条插入语句占用一个数据库连接，单独与数据库交互。在大批量插入的情境下，会话延迟会累积，增加总的数据插入时间。 批量插入 1234insert into test values(...),(...),(...); 优势： 减少网络延迟：多条记录作为单个操作发送到数据库的，减少了与数据库的网络交互次数。 减少事务开销：批量插入允许多条记录在一个事务中被插入，减少了事务提交和日志写入的次数，提高写入效率。 优化索引更新：量插入减少了索引更新的次数，因为它在插入多条记录后一次性更新索引。 减少CPU占用：批量插入一次性处理事务和索引的维护，避免频繁提交事务和维护索引带来的额外性能开销。 注意： 批次大小：通常建议的批次大小在500至1000条记录之间，过大的批次可能导致内存消耗增加或事务超时，而过小的批次则无法充分利用批量插入的优势。 大小限制：单个INSERT语句的长度可能受到SQL语句大小限制（由max_allowed_packet参数控制）。 手动控制事务 12345begin;insert into test values(...);insert into test values(...);insert into test values(...);commit; 在进行批量插入时，还有一种优化手段就是手段控制事务，将多条 Insert 语句在一个事务中提交。 优势： 减少事务开销：批量插入允许多条记录在一个事务中被插入，减少了事务提交和日志写入的次数，提高写入效率。 灵活性：可以在单独的 INSERT 语句间进行更复杂的操作，如条件逻辑、错误处理等。 适应性：对于不能一次性装入一个INSERT语句的大量数据，可以分批进行插入。 load data infile Mybatis 使用 load data infile 1234567-- 客户端连接服务端时，加上参数 -–local-infilemysql –-local-infile -u root -p-- 设置全局参数local_infile为1，开启从本地加载文件导入数据的开关set global local_infile = 1;-- 执行load指令将准备好的数据，加载到表结构中load data local infile &#x27;/root/sql1.log&#x27; into table tb_user fields terminated by &#x27;,&#x27; lines terminated by &#x27;\\n&#x27; ;-- 意思是文件中每个字段用 `,` 分割，每行用 `\\n` 分割 LOAD DATA INFILE 是用于高效地从文件中（CSV）批量加载数据到数据库表的命令。它特别适用于快速导入大量数据，比如在数据迁移或大规模数据处理场景中。 优势： 导入高效：LOAD DATA INFILE 是导入大量数据到 MySQL 表中最快的方法之一。它比逐行使用 INSERT 语句或通过客户端脚本导入数据快得多。 支持大数据集：对于非常大的数据集，使用 LOAD DATA INFILE 可以有效地处理和导入数据，而不必担心内存溢出或其他资源限制。 事务性：整个 LOAD DATA INFILE 操作可以作为一个单独的事务进行处理，这有助于保持数据一致性。 Order By 优化 MySQL的排序，有两种方式： Using filesort : 当无法直接通过索引来满足排序要求时，会使用一种叫做 Filesort 的算法来进行排序，可以在内存中或者在磁盘上进行。 Filesort 排序会比索引排序效率低，尤其是当需要处理的数据量大而无法完全放入内存时。 Using temporary : 没有适合排序的索引,在内存创建临时表并在其中排序。 Using index : 当排序要求可以通过已有的索引顺序直接满足时，MySQL 会使用 Using index 方式来直接从索引中读取有序数据。这种情况下，MySQL 利用了索引的有序性质，无需额外进行排序操作。 排序优化： 利用索引进行排序：根据排序字段建立合适的索引，多字段排序时，也遵循 最左前缀法则，避免了额外的排序步骤。 索引覆盖排序：直接使用索引中的数据来完成查询，包括排序操作。 对索引字段指定排序方式：多字段排序, 需要按照实际需求，可为每个字段指定升降序（ASC/DESC）。 调整排序缓冲区大小：可以调整MySQL的排序缓冲区大小（sort_buffer_size参数，256KB），以适应特定的查询需求。 索引列排序 如果 ORDER BY 子句中的列完全匹配索引列，并且查询条件允许使用该索引，MySQL就会利用索引的有序性进行排序。 123456789101112create table test2 ( id int primary key auto_increment, user_name varchar(20), age int, address varchar(30));create index idx_age_user_name on test2(age, user_name);explainselect age, user_namefrom test2order by age; # 使用索引列排序 遵循最左匹配原则 1234explainselect user_namefrom test2order by address desc, user_name desc 这时候索引排序不生效，没有遵循最左匹配原则。 升降序排序 1234explainselect age, user_namefrom test2order by age asc, user_name desc 此时 age 字段使用索引排序，user_name 字段使用 FileSort，因为索引在创建的时候默认是按照 ASC 排序的，优化此SQL可以重新创建一个索引，指定不同字段的排序。 1create index idx_age_username_2 on test2(age asc, user_name desc) 再次执行SQL执行计划 Group By 优化 对 GROUP BY 子句的优化主要目的是提高数据聚合查询的效率，以下是一些针对GROUP BY查询的优化技巧： 使用索引：如果GROUP BY子句中的列已经被索引，MySQL可以直接使用索引进行分组操作，从而提高效率。 索引覆盖：如果查询中的过滤条件列和 GROUP BY 列全部包含在一个索引中，可以实现索引覆盖。 防止滥用HAVING子句：HAVING子句可以用来过滤分组后的结果，但如果能通过 WHERE 子句提前过滤数据，则更为高效。 12345678create table test( id int auto_increment primary key, user_name varchar(20) null, age int null, address varchar(30) null); 1234explainselect count(0)from testgroup by age, user_name 在额外信息中我们看到 Using temporary，意思是使用临时表进行分组或排序，其过程比较耗时。 使用索引分组 前提：满足索引覆盖 创建组合索引。 1create index idx_age_user_name on test(age, user_name); 再次执行执行计划。 说明使用了索引分组。 最左匹配原则 1234explainselect count(0)from testgroup by user_name; 12345explainselect count(0)from testwhere age = 20group by user_name; Limit 优化 在使用 LIMIT 进行分页查询时，尤其是深度分页（例如：LIMIT 100000, 20），MySQL需要先检索出前100020条记录，然后丢弃前100000条，这导致效率低下。 索引覆盖和表子查询优化 利用索引覆盖和表子查询可以有效地优化深度分页的性能： 先在子查询中使用 LIMIT 和 索引覆盖 定位到所需的数据行的索引或主键。 然后在外层查询中根据这些索引或主键检索完整的数据行。 12select a.*from sys_user a join (select id from sys_user order by id limit 9000000, 20) b on a.id = b.id; 注意：确保子查询中的ORDER BY字段是索引覆盖的，这样可以提高定位效率。子查询的 ORDER BY 确保了数据的正确排序。 主键索引优化 对于深度分页： 记录上一页的最大ID值。 在后续查询中使用这个ID作为过滤条件，避免了 LIMIT 中过大的偏移量。 1234select a.*from sys_user a joinwhere id &gt; #&#123;maxId&#125; limit 20 注意：使用这种方法时，确保查询保持了逻辑上的连续性和一致性。这种方式适用于基于递增ID的连续分页，但可能不适用于需要复杂排序的场景。 Count 优化 如果数据量很大，在执行count操作时，是非常耗时的 MyISAM存储引擎：在处理简单的 COUNT(*) 查询时效率更高，因为它可以直接读取存储的行数。然而，在处理带条件的 COUNT()查询时，MyISAM的效率也会下降，因为需要遍历符合条件的数据行。 InnoDB存储引擎：在执行任何类型的 COUNT() 操作时通常效率较低，无论是带条件还是不带条件的查询。这是因为InnoDB需要遍历数据行来确保事务的可见性，而不是直接读取表的总行数。 Count用法 COUNT用法 含义 COUNT(主键) InnoDB引擎会遍历整张表，把每一行的主键ID值都取出来，返回给服务层。服务层拿到主键后，直接按行进行累加。 COUNT(字段) 没有NOT NULL约束：InnoDB引擎会遍历整张表，把每一行的字段值都取出来，返回给服务层，服务层判断是否为NULL，不为NULL时，计数累加。有NOT NULL约束：InnoDB引擎会遍历整张表，把每一行的字段值都取出来，返回给服务层，直接按行进行累加。 COUNT(数字) InnoDB引擎遍历整张表，但不取值。服务层对于返回的每一行，放一个数字“1”进去，直接按行进行累加。 COUNT(*) InnoDB引擎并不会把全部字段取出来，而是专门做了优化，不取值，服务层直接按行进行累加。 按照效率排序的话，COUNT(字段) &lt; COUNT(主键 ID) &lt; COUNT(1) ≈ COUNT(*)，所以尽量使用 COUNT(*)。 Update 优化 1update sys_user set name = &#x27;test&#x27; where name = &#x27;test1&#x27; 锁升级问题：在执行上述SQL时，如果 name 字段没有索引，InnoDB可能无法有效地定位需要更新的行，导致MySQL对更多的行甚至整个表加锁，从而影响性能。 行锁与索引：InnoDB的行锁是基于索引的。如果WHERE子句中使用的字段没有索引，InnoDB可能无法使用行锁，而是使用更粗粒度的锁，如表锁。 注意：InnoDB的行锁是针对索引加的锁，不是针对记录加的锁 ，并且该索引不能失效，否则会从行锁升级为表锁。 因此在进行 Update Delete 操作时尽量优先使用 主键，没有主键也要使用索引字段操作，防止行锁升级表锁。 优化策略： 使用索引：为 UPDATE 操作中的WHERE子句中的字段添加索引。在本例中，为 name 字段添加索引可以帮助InnoDB更高效地定位到需要更新的行，减少锁的粒度。 优先使用主键：在可能的情况下，使用主键进行 UPDATE 和 DELETE 操作。主键索引是最高效的，可以最大程度地减少锁竞争。 避免大范围更新：避免一次性更新大量行，这可能导致锁竞争和性能下降。如果需要，可以分批次执行更新。 监控锁等待：使用性能监控工具观察锁等待情况，以识别和解决性能瓶颈。 考虑查询效率：确保UPDATE查询尽可能高效，以减少锁持有时间。 事务管理：合理管理事务，避免长事务，因为长事务会持有锁更长时间，增加锁竞争。 NOT IN 优化 查询 address 表中不存在于 user 表的地址。 原始SQL示例： 123SELECT *FROM addressWHERE id NOT IN (SELECT address_id FROM user); 优化方法： 使用 LEFT JOIN：使用 LEFT JOIN 代替 NOT IN 可以提高查询效率，特别是当子查询返回的结果集较大时。 优化后的SQL： 1234SELECT a.*FROM address aLEFT JOIN user b ON a.id = b.address_idWHERE b.id IS NULL; 使用索引：确保 address 表的 id 列和 user 表的 address_id 列都有索引。这有助于提高 JOIN 操作的效率。 OR 优化 原始SQL： 1SELECT * FROM TB1 WHERE c1 = &#x27;xxx&#x27; OR c2 = &#x27;xxx&#x27;; 优化方法： 使用UNION代替OR：当 OR 条件中的每个列都有索引时，可以使用 UNION 来代替 OR，以提高查询效率。这是因为 OR 可能导致索引失效，而 UNION 可以分别利用各个列上的索引。 优化后的SQL： 123SELECT * FROM TB1 WHERE c1 = &#x27;xxx&#x27;UNIONSELECT * FROM TB1 WHERE c2 = &#x27;xxx&#x27;; 确保 c1 和 c2 上各自有索引，这样每个分开的查询都能高效地利用索引。 主键优化 在InnoDB中，表数据按主键顺序存储。如果主键插入顺序是随机的，可能导致页频繁分裂，进而影响性能。这是因为每次插入都可能需要重新整理页中的数据以保持顺序。 页分裂 上图是一个主键乱序插入的场景，1#page 和 2#page 此时已经存满了，但是此时需要插入一条id为50的数据，由于聚簇索引（B+树）数据是根据id排序的，50应该插入到47后面的位置，页如果存储不了则会进行分裂。 页分裂是一种耗费性能的操作，因为它涉及到数据移动和页指针的更新。在主键乱序插入的场景中，页分裂尤为频繁。 页合并 当我们对已有数据进行删除时，具体的效果如下: 当删除一行记录时，实际上记录并没有被物理删除，只是记录被标记（flaged）为删除并且它的空间变得允许被其他记录声明使用（自由空间链表）。 当页中删除的记录达到 MERGE_THRESHOLD（默认为页的50%），InnoDB会开始寻找最靠近的页（前或后）看看是否可以将两个页合并以优化空间使用。 页合并通常在页中大量数据被删除后发生，这可以减少存储空间的浪费。页合并也是一个性能密集型操作，因为它可能涉及数据移动和页重新组织。 主键选择 使用自增主键：自增主键是最佳选择，特别是对于InnoDB存储引擎。它们保证新插入的数据总是添加到索引的末尾，避免页分裂并提高插入效率。 降低主键长度：在满足业务需求的情况下，应尽量减少主键长度。较短的主键可以减少索引占用的空间，提高数据处理效率。例如，选择INT而非BIGINT可以节省空间。 单调递增的主键策略：如果使用自定义生成的主键，应确保它是单调递增的。这样可以减少随机插入导致的页分裂，从而优化插入性能。 避免使用UUID作为主键：尽管UUID在全局唯一性方面表现优秀，其高度随机性可能导致频繁的页分裂，并增加索引大小。在InnoDB中，尤其要注意这一点。如果必须使用UUID，考虑优化其生成方式以减少对性能的影响。 锁 在MySQL中，锁是用来管理多个用户或进程同时访问数据库时数据一致性和完整性的机制。MySQL支持多种类型的锁，不同的存储引擎支持不同的锁机制。 MySql中的锁 全局锁（Global Locks） 描述：全局锁会锁定整个数据库实例，阻止对数据库实例的所有修改操作。它通常用于全库导出或其他维护任务，以确保数据一致性。 用途：例如，FLUSH TABLES WITH READ LOCK 是设置全局读锁的命令，用于创建数据库的全备份。 表锁（Table Locks） 描述：表锁会锁定整张表，阻止对该表的并发写操作。在MyISAM和InnoDB使用非索引修改数据中常见。 用途：适用于读多写少的场景。读操作不阻塞其他读操作，但写操作会阻塞所有其他读写操作。 行锁（Row-Level Locks） 描述：行锁会锁定单个或多个数据行。InnoDB存储引擎支持行锁，它允许高度并发的数据访问。 用途：适用于事务性操作，减少锁定资源的数量，提高并发访问性能。 间隙锁（Gap Locks） 描述：间隙锁不锁定实际的数据行，而是锁定索引之间的间隙，或者是一个索引与范围末尾之间的间隙。主要用于防止幻读。 用途：保证可重复读（Repeatable Read）隔离级别下的一致性，阻止其他事务在间隙内插入行。 数据准备 1234567891011121314151617create table test2( id int auto_increment primary key, user_name varchar(20) null, age int null, address varchar(30) null);INSERT INTO test.test2 (id, user_name, age, address) VALUES (1, &#x27;test&#x27;, 18, &#x27;shenzhen&#x27;);INSERT INTO test.test2 (id, user_name, age, address) VALUES (2, &#x27;test2&#x27;, 21, &#x27;guangzhou&#x27;);INSERT INTO test.test2 (id, user_name, age, address) VALUES (4, &#x27;test1&#x27;, 20, &#x27;shenzhen&#x27;);INSERT INTO test.test2 (id, user_name, age, address) VALUES (5, &#x27;test2&#x27;, 30, &#x27;shanghai&#x27;);INSERT INTO test.test2 (id, user_name, age, address) VALUES (13, &#x27;test3&#x27;, 22, &#x27;beijing&#x27;);INSERT INTO test.test2 (id, user_name, age, address) VALUES (19, &#x27;test4&#x27;, 24, &#x27;guangzhou&#x27;);INSERT INTO test.test2 (id, user_name, age, address) VALUES (25, &#x27;test5&#x27;, 26, &#x27;zhongshan&#x27;);INSERT INTO test.test2 (id, user_name, age, address) VALUES (37, &#x27;test&#x27;, 18, &#x27;shenzhen&#x27;); 全局锁 全局锁会对整个MySQL数据库实例加锁。在加锁期间，整个数据库实例将处于只读状态。这意味着任何数据修改语句（DML）、数据定义语句（DDL）以及试图提交更新操作的事务都将被阻塞。 作用：全局锁的典型使用场景包括进行全库逻辑备份。通过对所有表加锁，可以确保获得一致性的数据库快照，从而保证备份数据的完整性和一致性。 语法 加全局锁 1FLUSH TABLES WITH READ LOCK 数据备份 1mysqldump -uroot -pPassword [database name] &gt; [dump file] mysqldump 也可用于远程连接备份。 释放锁 1unlock tables; 数据库中加全局锁，存在以下性能问题： 主库备份的问题：在主库上进行带全局锁的备份会阻止所有更新操作，这可能会导致业务暂停或性能下降。 从库备份的问题：在从库上进行备份时，由于全局锁的存在，从库可能无法应用主库同步过来的二进制日志（binlog），导致主从复制的延迟。 InnoDB的备份策略优化 对于InnoDB引擎，可以使用 mysqldump --single-transaction 参数来进行不加锁的一致性数据备份。 此选项会启动一个新的事务，并将隔离级别设置为REPEATABLE READ，确保在备份过程中数据的一致性。由于InnoDB的多版本并发控制（MVCC）特性，这种备份方式不会锁定表。 1mysqldump --single-transaction -u root -p[Password] [database name] &gt; [dump file] 注意：在使用--single-transaction参数进行备份时，应确保备份期间没有执行DDL操作（如ALTER TABLE, CREATE TABLE, DROP TABLE, RENAME TABLE, TRUNCATE TABLE等），因为这些操作可能会影响事务的一致性读。 表级锁 表级锁，每次操作锁住整张表。锁定粒度大，发生锁冲突的概率最高，并发度最低。应用在MyISAM、InnoDB、BDB等存储引擎中 对于表级锁，主要分为以下三类 表锁 元数据锁（meta data lock，MDL） 意向锁 表锁 对于表锁，分为两类： 读锁（共享锁, Read Lock） 特点：允许多个会话对表进行读取操作，但不允许任何会话进行写操作。 适用场景：适合在数据不需要被修改的情况下进行大量的读取操作，如数据分析或报告生成。 加锁方式：可以通过LOCK TABLES table_name READ命令显示加读锁。 写锁（排他锁, Write Lock） 特点：只允许一个会话对表进行读写操作，阻止其他会话的所有读写操作。 适用场景：适合需要修改数据且要防止其他会话读取或修改同一数据的情况。 加锁方式：可以通过LOCK TABLES table_name WRITE命令显示加写锁。 释放锁：unlock tables 元数据锁 作用： 自动加锁：MDL（Meta Data Lock）的加锁过程是由MySQL系统自动控制的，无需用户显式操作。在访问或修改表的结构时，MySQL会自动加上相应的MDL。 数据一致性：MDL的主要作用是维护表的元数据（如表结构）的一致性。当一张表上有活动事务时，不允许对该表的元数据进行修改。 防止冲突：MDL用于避免DML（数据修改语句）和DDL（数据定义语句）之间的冲突，保证数据库操作的正确性。 应用： 表的结构修改：当表涉及到未提交的事务时，不允许修改该表的结构（例如，通过ALTER TABLE）。 版本引入：MDL在MySQL 5.5中被引入，用于提高数据库操作的安全性和一致性。 锁类型： 对于非结构性的表操作（如查询、插入、更新、删除），MySQL加MDL读锁（共享锁）。 对于表结构变更操作，加MDL写锁（排他锁）。 锁类型和SQL操作对应关系: 对应SQL 锁类型 说明 LOCK TABLES xxx READ / WRITE SHARED_READ_ONLY / SHARED_NO_READ_WRITE SELECT、SELECT ... LOCK IN SHARE MODE SHARED_READ 与SHARED_READ、SHARED_WRITE兼容，与EXCLUSIVE互斥 INSERT、UPDATE、DELETE、SELECT ... FOR UPDATE SHARED_WRITE 与SHARED_READ、SHARED_WRITE兼容，与EXCLUSIVE互斥 ALTER TABLE ... EXCLUSIVE 与其他MDL互斥 操作 查看元数据锁 12select object_type,object_schema,object_name,lock_type,lock_duration fromperformance_schema.metadata_locks; 元数据排它锁 123begin;alter table test2add column test int; 下面操作将阻塞 12begin;update test set user_name=&#x27;test1&#x27; where id = 1 意向锁 定义：意向锁是一种表级锁，它表明事务打算在表中的某些行上加行级锁。 两种类型： 意向共享锁（Intention Shared Lock, IS）：表明事务打算在表中的某些行上加共享锁。 意向排他锁（Intention Exclusive Lock, IX）：表明事务打算在表中的某些行上加排他锁。 作用： 兼容性检查：意向锁使得InnoDB能够快速判断是否可以在表上加表级锁。例如，如果有事务持有意向排他锁，InnoDB知道不能在该表上加共享表级锁，意向锁是一个标识，为了解决表锁和行锁冲突而设计的锁。 避免死锁：通过在表级别上快速检查锁的兼容性，意向锁有助于避免死锁的发生。 意向锁实践 意向共享锁(IS): 由语句select … lock in share mode添加 。 与表锁共享(read)兼容，与表锁排他锁(write)互斥 意向排他锁(IX): 由insert、update、delete、select…for update添加 。与表锁共享锁(read)及排他锁(write)都互斥，意向锁之间不会互斥 事务的提交或回滚意向锁都会释放 意向共享锁 123456# 上行共享锁begin;select *from test2where id = 1 lock in share mode; 12# 此时上表读锁可以兼容，因为意向共享锁兼容表共享锁lock tables test2 read; 意向排它锁 12345# 上行排它锁begin;update test2 set user_name = &#x27;test2&#x27;where id = 1; 12# 此时不能上锁，意向排它锁和表排他锁互斥lock tables test2 write; 查看意向锁和行锁加锁情况 12select object_schema,object_name,index_name,lock_type,lock_mode,lock_data fromperformance_schema.data_locks; 行级锁 行级锁，每次操作锁住对应的行数据。锁定粒度最小，发生锁冲突的概率最低，并发度最高。应用在InnoDB存储引擎中 InnoDB的数据是基于索引组织的，行锁是通过对索引上的索引项加锁来实现的，而不是对记录加的锁。对于行级锁，主要分为以下三类： 行锁（Record Lock）：锁定单个行记录的锁，防止其他事务对此行进行 update 和 delete 。在 RC、RR 隔离级别下都支持 间隙锁（Gap Lock）：锁定索引记录间隙（不含该记录），确保索引记录间隙不变，防止其他事务在这个间隙进行 insert，产生幻读。在 RR 隔离级别下支持 临键锁（Next-Key Lock）：行锁 和 间隙锁 组合，同时锁住数据，并锁住数据前面的间隙Gap，在 RR 隔离级别下支持 行锁 默认情况下，InnoDB在 REPEATABLE READ事务隔离级别运行，InnoDB使用 临键锁（next-key） 锁进行搜索和索引扫描，以防止 幻读 针对 唯一索引 进行检索时，对 已存在的记录进行等值匹配时，将会自动优化为 行锁 InnoDB实现了以下两种类型的行锁： 共享锁（S）：允许一个事务去读一行，阻止其他事务获得相同数据集的排它锁 排他锁（X）：允许获取排他锁的事务更新数据，阻止其他事务获得相同数据集的共享锁和排他锁 常见的SQL语句，在执行时，所加的行锁如下： SQL 行锁类型 说明 INSERT … 排他锁 自动加锁 UPDATE … 排他锁 自动加锁 DELETE … 排他锁 自动加锁 SELECT（正常） 不加任何锁 SELECT … LOCK IN SHARE MODE 共享锁 需要手动在SELECT之后加LOCK IN SHARE MODE SELECT … FOR UPDATE 排他锁 需要手动在SELECT之后加FOR UPDATE 共享锁 1234begin;select *from test2where id = 1 lock in share mode; # 唯一索引加共享锁，行锁 1234begin;select *from test2where id = 1 lock in share mode; # 唯一索引加共享锁，行锁 在两个会话中分别开启事务，对id为1的数据进行加锁，结果是两个会话都可以将数据查询出来 查看加锁情况，发现id=的数据被加上了两把共享锁 12select object_schema,object_name,index_name,lock_type,lock_mode,lock_data fromperformance_schema.data_locks; 排它锁 排它锁与排他锁之间互斥 当会话一执行 id=1 的 update 语句后， 如果事务没有提交，那么会话一的事务会一直持有 id=1 这条数据的锁，其他会话事务的上锁操作将被阻塞，除非会话一的事务提交或回滚 12345begin;update test2set user_name = &#x27;test99&#x27;where id = 1; 12345begin;select *from test2for update; 此时会话二被阻塞，查看行锁情况，id=1 的排他锁确实没有释放 1234begin;select *from test2; 打开会话三开启新事务进行查询，查询不会阻塞（快照读），因为 select 不加锁 升级表锁 InnoDB的行锁是针对于索引加的锁，不通过索引条件检索数据，那么InnoDB将对表中的所有记录加锁，此时就会升级为表锁 12345begin;update test2set user_name = &#x27;test99&#x27;where user_name = &#x27;test&#x27;; 此时我们先查询锁的占用情况，发现表的所有id都被加了排他锁，原因就是行锁是对索引加锁的，user_name 字段没有加锁则升级为全局锁 间隙锁 默认情况下，InnoDB在 REPEATABLE READ事务隔离级别运行，InnoDB使用 临键锁（next-key） 锁进行搜索和索引扫描，以防止 幻读 间隙锁唯一目的是防止其他事务插入间隙。间隙锁可以共存，一个事务采用的间隙锁不会阻止另一个事务在同一间隙上采用间隙锁 产生间隙锁原因： RR 隔离级别 对不存在的记录上锁 对索引列进行范围查询（in，between） 123456begin;select *from test2where id between 10 and 20for update; 这里看到间隙锁标志，此时对 (5，12]，（13，18]，（19，24] 上间隙锁 1234begin;insert into test2 (id, user_name, age)values (8, &#x27;test18&#x27;, 18); 由于间隙锁的缘故，id = 8 的插入操作将被阻塞 临键锁 RR级别解决幻读：在进行 范围查询 或 索引扫描 时，InnoDB会在涉及的索引范围内设置间隙锁和行锁，防止其他事务在这些范围内插入或删除记录。 1234567# 创建普通索引create index idx_age on test2(age);select *from test2where age between 18 and 21for update; 场景： 范围查询：当对非唯一索引执行范围查询（如WHERE column &lt; 10）时，InnoDB会使用临键锁锁定匹配范围内的所有记录以及这些记录之间的间隙。 非唯一索引上的等值查询：对于非唯一索引的等值查询（如WHERE column = 5），由于可能有多条记录具有相同的索引值，InnoDB同样会使用临键锁来锁定这些记录和间隙。 索引扫描：在执行索引扫描时（如全表扫描或使用索引进行部分表扫描），InnoDB会对扫描到的记录以及记录之间的间隙加临键锁。 插入操作：当插入一条记录时，InnoDB会对新插入的记录前的间隙加锁，以防止幻读。 更新和删除操作：在执行更新或删除操作时，如果这些操作涉及到非唯一索引上的范围查询或是非唯一值匹配，InnoDB也会使用临键锁。 锁优化 避免行锁升级为表锁 控制事务大小，减少锁定的资源量和锁定时间长度,比如分批 合理设计索引，尽量缩小锁的范围(DML一定要使用索引列) 条件是大范围，可以按照范围分批 死锁 相互等待 T1 T2 set autocommit = 0; set autocommit = 0; select * from user where id = 1 for update; select * from user where id = 2 for update; select * from user where id = 2 for update; 此时T1等待T2释放锁 update join 多表加锁 1234567891011T1set autocommit = 0;update user a join address b on a.address_id = b.idset a.user_name = &#x27;kkk&#x27;where b.id = 1;T2update addressset address_name=&#x27;HN&#x27;where id = 1此时T2会等待T1释放 address id=1 行锁 delete join 多表加锁 123456789101112T1set autocommit = 0;delete afrom user a join address b on a.address_id = b.idwhere a.id = 1;T2set autocommit = 0;update addressset address_name=&#x27;HN&#x27;where id = 1T2等待T1释放 id = 1 的行锁 解决死锁 查看是否锁表： show OPEN TABLES where In_use &gt; 0 查询进程： show processlist 杀死进程id（就是上面命令的id列）： kill id Innodb 底层原理 从MySQL 5.5版本开始，InnoDB成为了默认的存储引擎。这一变化标志着对事务安全和性能的重视。Innodb引擎具有以下特点： 事务处理：专为事务处理设计，支持ACID（原子性、一致性、隔离性、持久性）事务，确保数据库操作的可靠性和完整性。 行级锁定和MVCC：支持行级锁定和多版本并发控制（MVCC），优化了并发操作，减少了锁争用，提高了性能。 外键约束：支持外键，允许在表之间创建参照完整性约束，这是其他一些存储引擎（如MyISAM）不支持的功能。 缓冲池：拥有缓冲池（buffer pool）机制，用于缓存数据和索引，减少磁盘I/O操作，提升查询性能。 数据存储：使用聚簇索引来存储表数据，这意味着表数据实际上存储在索引的叶节点上。 崩溃恢复：具备崩溃恢复能力，通过日志（如重做日志，即redo log）来保证数据在系统崩溃后的完整性和一致性。 外存架构 表空间 table space 定义：表空间是数据库中用于存储数据的逻辑单元。它定义了数据存储的位置（物理文件）和方式。表空间可以包含一个或多个文件，这些文件位于文件系统上。 如果用户启用了参数 innodb_file_per_table (在8.0版本中默认开启) ，则每张表都会有一个表空间（xxx.ibd）一个mysql实例可以对应多个表空间，用于存储记录、索引等数据。表空间存储的对象是段，由一个或多个段组成 默认情况下，MySql的表空间目录在 /var/lib/mysql 目录下，每个数据库对应一个目录，每张表对应一个 ibd文件 分类 系统表空间 独立表空间 通用表空间 通用表空间，需要通过 CREATE TABLESPACE 语法创建通用表空间，在创建表时，可以指定该表空间 undo表空间 MySQL实例在初始化时会自动创建两个默认的undo表空间（初始大小16M），用于存储 undo log日志 临时表空间 InnoDB 使用会话临时表空间和全局临时表空间。存储用户创建的临时表等数据 作用 数据组织：表空间允许将数据库数据分割成不同的部分，每部分可以单独管理和优化。 存储管理：例如，可以将不同的表空间放置在不同的磁盘或存储设备上，以优化性能和空间使用。 备份和恢复：可以单独备份或恢复特定的表空间。 语法 12# 创建表空间CREATE TABLESPACE ts_name ADD DATAFILE &#x27;file_name&#x27; ENGINE = engine_name; 12# 创建表时指定表空间CREATE TABLE xxx ... TABLESPACE ts_name; 段 segment 段是一组具有相似特性的连续页（pages）的集合，每个段都用于特定类型的数据存储，对存储数据进行分类，段用来管理多个Extent（区）。 分类 数据段（Data Segment）：用于存储表的行数据。 索引段（Index Segment）：用于存储B+树索引结构，包括主键索引和辅助索引。 回滚段（Rollback Segment）：存储事务回滚时所需的数据。 临时段（Temporary Segment）：用于存储临时数据，如排序操作或哈希表。 作用 空间管理：每个段负责管理其内部的页，包括分配新页和回收不再使用的页。 责任分离：不同类型的段支持不同功能，如数据存储、索引维护和事务处理。 数据分类：通过将数据划分到不同的段中，InnoDB可以更有效地组织数据。 区 extent 一个区是由连续的页（Page）组成的数据块。在InnoDB中，默认情况下，一个区包含 1MB 的空间，这相当于连续的 64 个 16KB 大小的页。 作用 优化空间管理：需要分配或回收大量空间时，InnoDB可以按区而不是页来操作，从而提高效率。 提升访问效率：在处理大量数据时，区能提高数据访问效率。 页 page 页，是InnoDB 存储引擎磁盘管理的最小单元，每个页的大小默认为 16KB。为了保证页的连续性，InnoDB 存储引擎每次从磁盘申请 4-5 个区。 12查看页大小SHOW GLOBAL STATUS like &#x27;Innodb_page_size&#x27;; 数据结构 页头 PageHeader：记录页面控制信息；56个字节。包括页的左右兄弟页指针、页面空间使用情况等 虚记录：用于确定当前页的记录范围 最小虚记录：最小记录是数据页上最小的记录，比页内最小主键小 最大虚记录：最大记录是这个数据页中逻辑上最大的记录，比页内最大主键还大 记录堆 ：行记录存储区，包含已删除记录的空间（大链表） 自由空间链表: 把被删除的记录空间链起来。便于重复利用空间 未分配空间: 还未使用的记录堆 Slot区 （槽位）：槽位的作用是将页内的数据链表拆分成多个子链表（hashTable思想）， 类似跳表 页尾: 8个字节，主要存储页面的校验信息 页内记录维护 顺序保证 物理有序插入删除需要移动数据（连续的存储空间），IO操作较多，插入效率不理想。基本不会使用 逻辑有序使用链表实现，但是查询效率没有物理有序好（物理有序可以使用二分法） ​ InnoDB必然是使用逻辑有序，物理有序写操作没有优化空间，逻辑有序的查询可以通过数据结构和算法进行优化（槽位） 插入策略 优先使用自由空间链表，减少空洞，其次使用未分配空间 即使是优先使用自由空间链表也不能保证表没有空洞，因为每条数据的长度不一致 页内查询 页内槽位定位：每个槽位对应于页内某个位置的记录，通过二分查找快速定位到接近目标记录的槽位。 链表遍历：每个槽位都有一个链表，包含了所有映射到该槽位的记录。 找到目标记录：根据槽位定位到链表后，在链表中顺序查找直到找到匹配的记录。 类型 数据页：存储表的实际行数据。 索引页：存储B+树索引结构，包括主键索引和辅助索引。 Undo页：存储事务的undo信息，用于数据恢复和MVCC。 系统页：存储表空间的元数据和其他系统信息。 行 row 行（Row）是数据存储和处理的基本单位。 在行中，默认有两个隐藏字段： Trx_id：每次对某条记录进行改动时，都会把对应的事务id赋值给trx_id隐藏列。 Roll_pointer：每次对某条引记录进行改动时，都会把旧的版本写入到undo日志中，然后这个隐藏列就相当于一个指针，可以通过它来找到上个版本的信息，是 MVCC 实现的核心。 聚簇索引 ​ 数据结构：B+树 聚簇索引并不是一种单独的索引类型，而是一种 InnoDB 数据存储方式 内存架构 InnoDB会将近期使用的数据先加载到内存中，当内存满了则根据LRU算法将内存中的数据写盘或者释放。释放出内存给其他热点数据使用。 主要分为这么四大块： Buffer Pool Change Buffer AdaptiveHash Index Log Buffer Buffer Pool 缓冲池（Buffer Pool）：一个内存中的区域，用于缓存InnoDB存储引擎最频繁访问的数据，包括数据页（Data Pages）和索引页（Index Pages）。 作用： 减少I/O操作：通过在内存中缓存数据和索引，缓冲池减少了对磁盘的读写次数，从而提高了数据访问速度。 数据页缓存：存储表数据的数据页在被访问时加载到缓冲池中。 索引页缓存：用于表索引的B+树结构的索引页同样被缓存，以加快索引查找和维护速度。 管理： LRU算法：InnoDB使用优化后的 双段LRU算法 来管理缓冲池中的页面。确保最活跃的数据保持在缓冲池中。 脏页刷新：当缓冲池中的数据页被修改后，它们最终会被写回到磁盘以保持数据的持久性。这个过程称为“刷新”（Flush）。 Buffer Pool默认大小是 128M，以 Page 页为单位，Page页默认大小 16K，而控制块的大小约为数据页的5%，大概是 800字节。 12# 查看 buffer pool 信息show global variables like &#x27;innodb_buffer%&#x27;; 变量 说明 innodb_buffer_pool_chunk_size 定义InnoDB缓冲池大小调整操作的块大小 innodb_buffer_pool_size 缓冲池大小 innodb_buffer_pool_instances InnoDB 缓冲池划分为的区域数 缓冲池中页的类型 free page ： 定义：空闲页是尚未被分配用来存储数据的页。 位置：这些页位于Free List中，等待被分配用于存储新的数据或索引信息。 特点：它们是完全空白的，不包含任何有效数据。 clean page ： 定义：干净页是已被分配并包含数据的页，但自上次读入缓冲池或写回磁盘后未被修改。 位置：这些页位于LRU List中，表示它们是最近被访问过的，但并不需要写回磁盘，因为它们与磁盘上的数据一致。 特点：在需要为新的数据页或更“脏”的页腾出空间时，干净页可以被较快地从缓冲池中移除，因为它们不需要额外的磁盘写入操作。 dirty page： 定义：脏页是已被分配并包含数据的页，且自上次读入缓冲池后已被修改。 位置：这些页同时位于LRU List和Flush List中。LRU List表示它们是最近被访问过的，而Flush List用于跟踪需要被写回磁盘的脏页。 特点：脏页需要在适当时机写回磁盘以保证数据的持久性和一致性。InnoDB会定期将脏页刷新回磁盘，特别是在事务提交或检查点（Checkpoint）发生时。 Page List Free List： 定义：包含当前未使用的页，即那些没有分配任何数据的空白页。 用途：当InnoDB需要新的页来存储数据或索引时，它会从Free List中分配页。 LRU List ： 定义：LRU列表用于存储最近被访问过的页。 干净页（Clean Page）：已分配且包含数据，但与磁盘上的数据一致，未被修改过。 脏页（Dirty Page）：已分配且包含数据，且自上次读入后已被修改。 Flush List： 定义：用于跟踪那些标记为脏的页，即需要被写回磁盘的页。 用途：Flush List确保数据的持久化和一致性，特别是在事务提交或达到检查点（Checkpoint）时。 Page Hash表 映射关系维护： 描述：Page Hash维护了磁盘上数据页和Buffer Pool中页的映射关系。 快速查找：这种映射机制允许InnoDB快速确定某个特定的数据页是否已经在Buffer Pool中，以及它在Buffer Pool中的具体位置。 判断缓存：通过Page Hash，InnoDB可以快速判断某个页是否已经在Buffer Pool中，从而避免不必要的磁盘I/O操作。 数据页加载单位： 以页为单位：InnoDB从磁盘加载数据到内存的操作是以页为单位进行的，每个页通常为16KB。 不以行为单位：虽然基于行的加载可能提高内存利用率，但以页为单位可以优化I/O效率，尤其是对于排序、分页查询等操作。 页面装载流程 页面装载单位：数据库从磁盘加载数据到内存时，是以页（默认是16KB）为单位进行的。当数据库需要读取磁盘上的数据时，它会检查该数据是否已经在Buffer Pool中。如果不在，则从磁盘加载相应的页。 使用Free List：当有新的页需要加载到Buffer Pool时，InnoDB首先会尝试使用Free List中的空闲页。 LRU List淘汰：Free List中没有可用的空闲页，InnoDB会从LRU List的冷数据端开始驱逐，腾出空间加载新的数据页。 脏页刷新：当LRU List驱逐数据后还是无法加载新的数据页，则将脏页进行刷新，保存到磁盘。 Change Buffer 定义： 优化写操作：Change Buffer是一个内存中的缓冲区域，用于暂时存储对非唯一索引的修改操作。这些修改操作随后会被批量地合并到磁盘上的实际索引页中。 减少I/O需求：对于非唯一索引的修改，如果每次修改都直接写入磁盘，将会产生大量的随机I/O。Change Buffer通过合并这些操作，减少了I/O操作的次数。 工作机制： 缓存修改操作：当对一个非唯一索引进行插入、删除或更新操作时，如果相应的 索引页 不在缓冲池中，InnoDB会将这些修改操作记录到Change Buffer中，而不是立即从磁盘加载索引页。 延迟合并：随后，在后台，当这些被修改的索引页因其他查询或操作而被加载到缓冲池时，InnoDB会将Change Buffer中的修改与这些页中的数据合并。 优化I/O：这种延迟合并操作减少了对磁盘的即时I/O需求，因为它避免了为了执行每个小的索引修改而频繁地从磁盘读取和写入索引页。 为什么只针对非唯一索引优化 唯一约束校验：对于具有唯一性约束的索引，任何插入和更新操作都直接在Buffer Pool中进行，确保唯一性约束在修改时被即时验证，它们不能被缓存在Change Buffer中。 Log Buffer 定义： 缓存事务日志：Log Buffer是一个在内存中的缓存区域，用于暂时存储事务的日志信息（Redo Log），比如数据的修改操作。 减少磁盘I/O：通过首先将日志信息写入到Log Buffer，InnoDB减少了直接写入到磁盘的操作，提高I/O性能。 提高事务性能：Log Buffer允许事务快速完成，因为事务的日志信息不需要即时写入磁盘。 工作原理： 记录事务日志：当执行事务时，所有的修改操作（如INSERT、UPDATE、DELETE）会生成日志记录，并首先被写入Log Buffer。 异步刷新到磁盘：Log Buffer中的内容会定期或在特定事件（如事务提交）时被刷新到磁盘上的重做日志文件（Redo Log）中。 日志序列号（LSN）：每个日志记录都有一个唯一的日志序列号，用于跟踪和管理日志记录。 参数配置 innodb_log_buffer_size：设置Log Buffer的大小。根据事务的大小和频率进行适当调整。 12# 查看 Log Buffer 大小SHOW GLOBAL VARIABLES LIKE &#x27;innodb_log_buffer_size&#x27;; innodb_flush_log_at_trx_commit 控制重做日志写入和刷新到磁盘的时机： 0： 事务提交时不会立即将日志写入磁盘，而是将日志缓存在操作系统的页缓存中，之后由操作系统决定何时将其刷新到磁盘。 这种模式提供了最高的性能，但在崩溃的情况下，会丢失数据。 1（默认值）： 每次事务提交时，InnoDB都会将日志刷新到磁盘。 这提供了最高的数据持久性保证，因为即使发生崩溃，最近提交的事务也不会丢失。 因为每次事务都需要磁盘I/O而降低性能。 2： 日志只在每个事务提交时写入日志缓冲区，每秒钟将日志刷新到磁盘一次。 这提供了折中的性能和持久性保证，但在发生崩溃时，最近一秒的事务可能会丢失。 性能考虑：如果存在大量的更新、插入或删除操作，增加Log Buffer的大小可以减少I/O操作，特别是在高并发的OLTP系统中。 自适应Hash索引 InnoDB会自动监控对表上索引的查询模式。当它发现某些查询符合使用哈希索引的效率优势时，就会为这些查询自动创建哈希索引。这个过程是完全自动的，不需要用户干预。 使用场景： 频繁等值查询：当表上的特定数据行被频繁查询时，例如通过主键或唯一索引的等值查询。 读密集型应用：适用于读操作远多于写操作的应用场景。 创建条件：当InnoDB检测到对某些 索引 的等值查询非常频繁时，它会自动在内存中为这些索引创建哈希索引。 配置 默认情况下，自适应哈希索引功能是开启的。 可以通过SET GLOBAL innodb_adaptive_hash_index=OFF/ON;来关闭或开启这一功能。 使用SHOW VARIABLES LIKE '%adaptive_hash_index';来查看当前设置。 Hash索引的优缺点 双写缓冲区 MySQL Buffer 一页的大小是 16K，文件系统一页的大小是 4K，也就是说，MySQL将 Buffer 中一页数据刷入磁盘，要写4个文件系统里的页。 注意：Innodb Page 默认不是 4K 是因为考虑I/O效率，较大的页大小可以减少数据库在读写操作时需要的I/O次数，更大尺寸的页适合于表扫描和大范围查询。 如上图所示，MySQL里page=1的页，等于物理磁盘的四个页，刷盘的这个操作并非原子，如果执行到一半断电或宕机，就会出现 页数据损坏。 如上图所示，MySQL内page=1的页准备刷入磁盘，才刷了3个文件系统里的页，断电了。重启后，page=1的页，物理上对应磁盘上的1+2+3+4四个格，数据完整性被破坏。 解决页数据损坏 如上图所示，当有页数据要刷盘时： 预写入：当脏页被刷新到磁盘时，InnoDB首先将这些页的副本写入双写缓冲区（一个特定的磁盘区域）。 确认写入：只有双写缓冲区数据成功写入磁盘文件后，InnoDB才会将这些页写入实际的表空间文件。 恢复使用：如果发生崩溃，InnoDB在重启时会检查双写缓冲区，以确保页的完整性。 内外存数据交换 内存数据淘汰 触发条件 Buffer Pool空间不足。 需要加载新的数据页到内存。 内存淘汰算法 LRU LRU ： (Least recently used) 最近最少使用。 使用全表扫描对 普通LRU 算法的影响： 如果全表扫描一张大表，根据 普通LRU 算法，会淘汰内存中所有数据。 全表扫描的数据可能就只使用一次，造成 缓存污染。 MySql 改良的URL InnoDB实际上使用一个单一的LRU链表，但这个链表被分为两个部分：热数据区域（LRU列表的前端）和冷数据区域（LRU列表的尾部）。这种分区有助于保护频繁访问的热点数据不被轻易淘汰。 页面淘汰时机：当Buffer Pool中的Free List没有空闲页，并且需要加载新的数据页时，InnoDB会根据LRU算法从LRU List中淘汰页。 保护热数据：为了减少全表扫描等操作对Buffer Pool的影响，InnoDB引入了中段插入策略。新加载的页不是直接被放到LRU列表的最前端，而是被放置在列表的 中间位置。 淘汰流程： 淘汰LRU列表尾部的页：优先淘汰LRU列表尾部的数据，是较少访问的冷数据。 脏页的处理：如果需要淘汰的页是脏页（即已修改但未写回磁盘的页），则这些页需要先被刷新（写回）到磁盘。 锁定页的处理：如果LRU列表尾部的某些页被锁定或无法淘汰（例如，由于正在参与活跃事务），则向前移动，寻找可淘汰的页。 重新分配到Free List：一旦页被淘汰，它们的空间被释放回Free List，供新的页使用。 冷数据变为热数据 当数据页被首次读取到Buffer Pool时，它们被放入LRU链表的中间部分，即所谓的 冷数据 区域。这是为了防止大规模的非顺序读取（如全表扫描）迅速淘汰掉已经存在于LRU链表前端的热点数据。 冷数据晋升热数据条件： 初次加载：当数据页首次加载到Buffer Pool时，被放置在LRU链表的冷数据区域（冷数据区域头部）。 存活时间：数据页在冷数据区域中必须存活一定时间，这个时间由参数innodb_old_blocks_time设定（单位是毫秒）。默认值通常是1000毫秒（1秒）。 再次访问：如果在存活时间过后，这个页再次被访问，它将从冷数据区域移动到热数据区域（LRU链表的前端）。这标志着数据页从冷数据变成了热数据。 访问频率：只有那些被重新访问的页才会从冷数据区晋升到热数据区，这意味着频繁访问的页更有可能成为热数据。 12# 必须在冷数据区停留多长才有资格晋升热数据区show variables like &#x27;innodb_old_blocks_time&#x27; 总结：在冷数据区存活一定时间，并且再次访问。 好处：进行全表扫描频繁淘汰的区域是冷数据区，热数据区域的热点数据不会受影响。 热数据淘汰 Midpoint 定义：Midpoint是LRU链表中的一个指针，标识着热数据区域和冷数据区域的分界点。 指针移动： 向左移动：热数据区域将数据淘汰到冷数据区域。 向右移动：冷数据区域的数据晋升到热数据区域。 位置保持：热数据区域占整个LRU链表的大约 5/8，而冷数据区域占 3/8。数据的晋升和淘汰让指针一直保持这个比例。 数据淘汰优化： free_page_clock：是一个全局计数器，当缓冲池中的页被访问时 free_page_clock 会递增。 快照计数器的值：一个页从冷数据区域晋升到热数据区域时，会记录当前的 free_page_clock 值。 数据移动：当前的 free_page_clock 与页晋升时的快照差超过热区域长度的 1/4 时，数据页才向左移动。 降低频率：这种机制降低了页移动的频率，从而减少了由频繁的页移动引起的性能开销。 后台线程 在InnoDB的后台线程中，分为4类，分别是：Master Thread 、IO Thread、Purge Thread、Page Cleaner Thread。 Master Thread职责：Master Thread是InnoDB的核心后台线程，负责多种调度和维护任务。主要功能:异步刷新：将缓冲池中的脏页异步刷新到磁盘。合并Change Buffer：合并Change Buffer中的数据到实际的索引页。Undo页的回收：清理不再需要的Undo页。 IO ThreadAIO处理：InnoDB大量使用异步I/O（AIO）来提高性能，IO Thread主要负责处理这些异步I/O请求。职责：Read Thread：处理读取操作。Write Thread：处理写操作。Log Thread：负责将重做日志（Redo Log）刷新到磁盘。Insert Buffer Thread：处理Change Buffer的合并操作。 Purge ThreadUndo日志回收：回收事务提交后不再需要的Undo日志，释放空间。 Page Cleaner Thread辅助刷新脏页：主要职责是协助Master Thread将脏页刷新到磁盘，减轻Master Thread的负担，降低刷新操作对性能的影响。 事务原理 事务是数据库执行过程中的一个逻辑单位，由一系列的操作组成。这些操作要么全部成功，要么全部失败。事务处理保证了即使在系统故障的情况下，操作的完整性也不会被破坏。 事务的四大特性： 原子性（Atomicity）：事务中的所有操作要么全部成功，要么全部失败，不会停留在中间状态。 一致性（Consistency）：事务必须保证数据库从一个一致性状态转移到另一个一致性状态。 隔离性（Isolation）：事务的执行不会被其他事务干扰，多个并发事务之间的数据库操作是隔离的。 持久性（Durability）：一旦事务完成，其对数据库的修改应该是永久性的，即使出现系统故障。 原理： Redo Log：保证事务的 持久性，即使在系统崩溃后，已提交事务的修改不会丢失。 Undo Log：支持 原子性 和 一致性，用于在事务失败时回滚操作，并支持MVCC中的读一致性。 隔离性保证 写隔离：InnoDB通过锁定机制来维护事务的隔离性，防止数据在并发环境下被破坏。 读隔离：允许多个事务同时读取同一数据而无需等待其他事务完成（MVCC快照读取）。 redo log 重做日志是事务持久性的关键，通过记录数据页的物理修改日志，它确保了即使在数据库崩溃的情况下，所有已提交事务的修改都能被恢复。 组成： 重做日志缓冲（Log Buffer）： 功能：位于内存中，用于临时存放生成的重做日志记录。这些记录表示了事务对数据页的物理修改。 作用：使用内存缓冲区可以提高日志记录的写入效率，减少对磁盘的直接写操作。 重做日志文件（Redo Log File）： 持久存储：位于磁盘上，用于持久化存储事务的修改。这些文件确保了即使在系统崩溃的情况下，所有已提交的事务修改都可以被恢复。 循环写入：包含多个文件，以循环方式写入，当一个文件写满后，日志写入移动到下一个文件。 当前重做日志文件（例如，ib_logfile0）写满后，日志写入操作会转移到下一个文件（例如，ib_logfile1）。 当所有的重做日志文件都被写满后，系统会回到第一个文件并开始覆盖旧的日志记录，从而形成一个循环。 redo log作用 背景：InnoDB对数据的修改在缓冲池中会产生 脏页。 脏页不是实时刷新到磁盘的，而是在特定时机进行刷新，那么内存数据和磁盘数据就有一段时间不一致。 作用： 保证事务持久性：当事务执行数据修改操作时，这些更改首先被记录到 Redo Log Buffer 中，然后根据策略刷新到磁盘的Redo Log文件中。 支持数据恢复：在数据库崩溃后重启时，InnoDB使用Redo Log中的信息来恢复崩溃时尚未刷新到磁盘的脏页数据，恢复未刷盘的事务操作。 工作流程： 修改数据页：当事务执行数据修改操作时，这些更改首先在内存中的缓冲池（Buffer Pool）上进行，修改后的数据页变成了所谓的“脏页”。 记录Redo Log：与此同时，对这些脏页的修改会生成相应的重做日志记录，这些记录被写入到内存中的 Log Buffer。 刷新Redo Log：Log Buffer中的内容会根据特定的策略定期或在事务提交时刷新到磁盘上的Redo Log文件中。 脏页刷新：脏页的刷新到磁盘是一个独立的过程，它可能发生在事务提交之后的某个时间点，由InnoDB的后台进程管理。 事务提交为什么要先写redo log Write-Ahead Logging（WAL，先写日志）：是一种在数据库管理系统中广泛使用的日志技术，核心原则是在任何数据库数据被永久写入到磁盘之前，先将其修改操作写入到日志中。 提高事务执行效率：直接写入数据文件涉及随机I/O操作，这在大多数存储介质上效率较低。相比之下，写入Redo Log文件是顺序I/O操作，效率更高（WAL）。 简化崩溃恢复过程：使用WAL，数据库恢复只需重放Redo Log文件中记录的操作，而无需检查和重写每个数据文件。 undo log 回滚日志是用来记录数据变更前的状态。当事务进行修改操作时，会同时记录相应的Undo Log，以便在事务回滚时使用。 作用支持事务回滚：如果事务失败或被显式回滚，用来恢复数据到事务开始之前的状态。实现MVCC：多版本并发控制（MVCC）中，Undo Log被用来为不同事务提供历史数据的快照。即使数据被一个事务修改了，其他事务仍然可以看到修改前的数据版本。 生命周期生成：当事务执行数据修改操作（如INSERT、UPDATE、DELETE）时，InnoDB会生成相应的Undo Log。存储：Undo Log以段（segment）的形式存储在 表空间 中，每个Undo段包含多个Undo日志页。销毁：当没有任何事务需要访问Undo Log中的旧数据版本时，这些日志记录才会被清理。 MVCC 定义： 全称：Multi-Version Concurrency Control（多版本并发控制）。 作用：MVCC允许在数据库中维护数据的多个版本，这样就可以在读写操作中减少冲突，提高并发性能。 实现机制： 隐式字段：每条数据库记录包含隐式字段。 DB_TRX_ID（事务ID）：记录了最后修改该记录的事务ID。 DB_ROLL_PTR（回滚指针）：指向该记录对应的undo log记录。 Undo Log：存储了数据的旧版本信息，用于在读取时提供历史数据，以及在需要时回滚事务。 Read View：在事务开始时创建的一致性视图，用于保证事务在执行期间看到数据库的一个一致性状态。 当前读 定义：当前读（Current Read）确保读取数据的最新版本，同时在读取过程中对数据加锁，防止其他并发事务对这些数据进行修改。 加锁目的：加锁机制防止了数据在读取过程中被其他并发事务所更改。 当前读的操作类型： 加共享锁的读：SELECT ... LOCK IN SHARE MODE，这个操作在读取数据的同时对其加上共享锁，允许其他事务读取但阻止修改。 加排他锁的读： SELECT ... FOR UPDATE：对所选记录加排他锁，阻止其他事务读取或修改这些记录。 UPDATE、INSERT、DELETE：这些DML操作不仅修改数据，而且还会对所涉及的记录加排他锁。 快照读 定义：指的是在执行普通的SELECT查询（不加锁）时，读取的数据版本取决于事务开始的时间点。它不会看到在事务开始之后其他事务所做的修改。 基于MVCC：快照读的实现基于MVCC机制，MVCC通过维护数据的不同版本来允许多个读写操作并发执行，而不直接影响彼此。 特点： 一致性视图：快照读提供了一个事务开始时的数据视图，避免了读取过程中的数据变动。 非阻塞操作：由于快照读不加锁，它允许其他事务并发地修改数据。 避免非重复读：在Repeatable Read级别下，快照读避免了非重复读的问题，即在同一事务中多次读取同一数据集返回的结果始终保持一致。 事务隔离级别的影响： Read Committed：每次SELECT查询都会读取最新提交的数据。这意味着如果其他事务在两次查询之间提交了更改，后续的查询将看到这些更改。。 Repeatable Read：事务中的第一个SELECT创建一个快照，后续的SELECT查询在事务内将读取这个快照，即使其他事务提交了更改。 Serializable：最高的事务隔离级别，它要求事务之间完全串行执行，退化为当前读。 隐藏字段 在创建表时，InnoDB会自动创建两个隐藏字段用来实现 MVCC。 隐藏字段 含义 DB_TRX_ID 事务ID，记录插入这条记录或最后一次修改该记录的事务ID DB_ROLL_PTR 回滚指针，指向这条记录的上一个版本，用于配合 undo log，指向上一个版本，实现快照读 DB_ROW_ID 隐藏主键，如果表结构没有指定主键，将会生成该隐藏字段 上述的前两个字段是肯定会添加的， 是否添加最后一个字段DB_ROW_ID，得看当前表有没有主键，如果有主键，则不会添加该隐藏字段。 测试 快照读 RR 隔离级别 T1事务 T2事务 begin; 开启事务 select * from test2 where id = 100; //查询不到 begin; 开启事务 insert into test2(id, user_name, age, address) values(100, ‘test100’, 20, null); commit; select * from test2 where id = 100; //快照读不可见 select * from test2 where id = 100 for update; //当前读可见 commit; 版本链 工作机制版本链构建：InnoDB通过DB_ROLL_PTR字段在undo log中构建了一个版本链，每个版本代表了数据行在历史上的某个状态。读取操作：当进行快照读（如普通SELECT查询）时，InnoDB会根据事务的版本（由DB_TRX_ID和Read View确定）在版本链中查找相应的数据版本。版本可见性：根据当前事务的ID和Read View，判断各个历史版本是否对当前事务可见。 作用支持MVCC：版本链是MVCC实现的核心，允许事务访问数据的一致性历史版本，而不影响其他事务的操作。高效并发控制：通过维护数据的多个版本，InnoDB能够支持高并发的读写操作，减少锁的需求。 Read View 定义：Read View（读视图）是一个用于实现多版本并发控制（MVCC）的内部数据结构，它定义了在一个给定事务中哪些数据版本是可见的。 ReadView 中包含了四个核心字段： 字段 含义 m_ids 当前活跃的事务列表，包括ReadView创建时刻所有已开始但未提交的事务ID m_up_limit_id 最小活跃事务ID，m_ids集合中的最小事务ID，小于这个事务ID的数据都是可见的 m_low_limit_id 下一个将要被分配的事务ID，是ReadView可见事务的上界 m_creator_trx_id ReadView创建者的事务ID，标识创建此ReadView的事务 不同的隔离级别，生成ReadView的时机不同： READ COMMITTED ：在每个SQL语句执行前生成ReadView。 REPEATABLE READ：只在事务的第一个SELECT语句执行时生成。一旦生成，整个事务期间都会使用这个ReadView。 事务可见性判断 RR 隔离级别： 创建快照这一刻，还未提交的事务不能读。 创建快照之后创建的事务不能读。 ReadView判断过程 RR 隔离级别： 生成ReadView：当事务进行第一次SELECT查询时，InnoDB为该事务生成一个ReadView。ReadView中包含当前活跃的事务ID集合（m_ids），最小活跃事务ID（min_trx_id），以及下一个将要分配的事务ID（max_trx_id）。 读取判断： 如果数据的创建事务ID小于 最小活跃事务id，说明这个事务在当前事务开始之前已提交，因此数据行对当前事务是可见的。 如果数据的创建事务ID 在活跃事务id中，说明当前事务是活跃的，数据对当前事务是不可见。 如果数据的创建事务ID大于或等于 下一个要分配的事务ID，说明这个事务在当前事务开始之后开始，因此数据行对当前事务是不可见的。 查询流程 查询缓存（Query Cache）： 在解析之前，MySQL会检查查询缓存。如果相同的查询之前已执行并且结果存储在查询缓存中，则直接返回缓存结果。 解析器（Parser）： 当用户发出SQL查询时，MySQL首先通过解析器对SQL语句进行语法分析。解析器检查SQL语句的语法是否正确，并将其转换为一种内部格式的查询表示（解析树）。 预处理器（Preprocessor）： 在解析后，预处理器进一步检查解析树的语义正确性，比如表和列的存在性、数据类型的匹配等。 优化器（Optimizer）： 优化器负责查询的优化。它分析多种可能的执行计划，选择一种成本最低的计划来执行查询。这包括决定使用哪些索引，如何连接表等。 执行查询执行（Execution）： MySQL根据执行计划对数据库进行操作。在InnoDB存储引擎中，这涉及到如下几个步骤： a. 行锁定：如果是当前读，会对数据行进行锁定。 b. 数据检索：访问存储在磁盘上的数据页面，可能涉及到索引的使用。 c. 缓冲池（Buffer Pool）：InnoDB会首先检查数据是否已在缓冲池中。如果不在，从磁盘读取数据并放入缓冲池。 d. ReadView：在RR隔离级别下，如果进行的是一致性读操作，InnoDB会创建一个ReadView来保证数据的一致性。 结果返回： 执行完查询后，将结果返回给用户。对于SELECT查询，这包括了从数据行中检索出的数据；对于UPDATE、INSERT或DELETE查询，这包括了影响的行数等信息。 查询缓存 工作机制： 当一个SELECT查询执行时，MySQL首先检查查询缓存。 如果找到一个与当前查询完全相同（文本匹配，包括空格和注释）的缓存条目，MySQL就会直接返回缓存中的结果。 如果没有找到匹配的缓存，查询将继续进行SQL解析、优化和执行。 缓存失效： 查询缓存对数据变动非常敏感。当任何涉及缓存查询的表被修改（INSERT、UPDATE、DELETE等），所有相关的缓存条目都会立即失效。 局限性： 查询缓存在高并发和高更新场景下成为性能瓶颈，因为频繁的缓存失效和重建会消耗大量资源。 12# 查询查询缓存是否开启show variables like &#x27;%query_cache%&#x27;; 查询缓存被移除：在MySQL 8.0及以后的版本中，查询缓存功能被完全移除。这是因为查询缓存在现代高并发数据库系统中往往带来更多的性能问题而非益处。 性能优化：MySQL 8.0集中优化了查询执行器和优化器，提供更高效的查询处理机制，尤其是在高并发场景下。 语法解析和预处理 语法解析分析SQL语句：检查SQL语句的语法是否正确。这包括识别关键字、运算符、表名、列名等。生成解析树：如果语法没有错误，解析器将SQL语句转换为一种内部表示形式，是一种解析树（也称为语法树）。这个树结构表示了SQL语句的组成部分及其关系。错误处理：如果解析器发现语法错误，它会停止处理并返回错误消息。 预处理语义检查：确保SQL语句在语义上是有效的。这包括检查引用的表和列是否存在，数据类型是否匹配，解析名称等。权限验证：检查执行该查询的用户是否具有相应的权限。变量替换：处理SQL语句中的变量和参数。解析表达式：评估SQL语句中的表达式。 查询优化 优化器查询重写：优化器可能会改写查询，例如简化查询条件、去除冗余的条件等。索引选择：优化器根据可用的索引和统计信息来决定是否使用索引，以及使用哪个索引。连接顺序：在涉及多个表的查询中，确定连接这些表的最有效顺序。连接策略：选择适合的连接算法，如嵌套循环连接（Nested-Loop Join）或哈希连接（Hash Join）。 成本估算和执行计划选择基于表的统计信息和索引特性，优化器会估算不同执行计划的成本，包括I/O成本、CPU成本等。最终选择总成本最低的执行计划。 查询执行引擎 查询执行：执行引擎根据优化器提供的执行计划执行查询，包括从存储引擎检索数据、执行联接、处理排序和聚合等操作。 存储引擎接口：查询执行过程中，执行引擎会调用存储引擎（如InnoDB）提供的接口，这些接口被称为handler API。它们用于完成数据的实际读取、写入以及处理事务等任务。 返回客户端结果 缓存结果：如果查询可以被缓存，mysql会在这个阶段将结果存放到查询缓存中，8.0后移除。 流式返回：一旦查询生成了第一条结果，就开始向客户端逐步发送结果，而不需要等待整个结果集生成完毕。 写入流程 开启新的事务 数据修改：执行INSERT、UPDATE或DELETE操作时，首先在缓冲池中修改数据页。如果所需页不在缓冲池中，则先从磁盘读取该页到缓冲池。 日志记录： undo log：对每项数据进行修改前都会记录数据修改前的状态（undo log），用于事务回滚。 redo log：对每项数据进行修改都会生成重做日志条目（Redo log）并暂存到 Log Buffer 中。 索引处理： 唯一索引检查：如果修改涉及到唯一索引，必须检查新数据是否违反唯一性约束。 非唯一索引：如果索引页不在缓冲池，先将索引的修改操作暂存到 Change Buffer 中，等待索引被加载后进行索引页合并。 事务提交： Prepare（准备）：将 Log Buffer 中的 Redu Log 刷新到磁盘日志文件，用于数据异常恢复。 Commit（提交）：在所有 Redo Log 成功写入磁盘后，在重做日志中添加一个 事务提交 的记录，标志着事务已经成功提交。 bin log：事务的binlog事件首先写入系统缓冲区，并根据 sync_binlog 的配置决定何时刷新到磁盘。 sync_binlog： 值为 0：Mysql不主动刷新缓冲区，由操作系统进行刷盘，效率最高。 值为 1（默认）：事务提交后马上对 bin log 进行刷盘，增加数据安全。 值大于 1：指定在这么多次事务提交后刷新 bin log 到磁盘，可在数据安全性和性能之间做出权衡。 bin log binlog，或称为二进制日志，用于记录数据库中所有修改数据的操作，如INSERT、UPDATE、DELETE和DDL（数据定义语言）操作如CREATE TABLE、ALTER TABLE。 作用： 数据复制：binlog是MySQL主从复制的基础。主服务器上的binlog记录了所有的数据变更，这些记录会被复制到从服务器并重放，实现数据的一致性。 数据恢复：在发生数据丢失时，可以使用 binlog 来恢复操作，将数据库恢复到特定的时间点。 三种模式： Statement：基于语句的复制。这种模式下，binlog记录了执行的SQL语句。 Row（8.0默认）：基于行的复制。记录了更改后行的内容。 Mixed：混合模式，结合了语句和行模式。 语句复制：正常情况下使用。 行复制：使用了自定义函数。 SQL语句复制 Statement 把所有的修改语句都记录在bin log中。 优点：由于只记录了SQL语句本身，而非行级别的变化，减少I/O开销，从而提高性能。 缺点：特殊场景可能导致主从数据不一致，尤其是当使用一些非确定性的函数（如NOW()、RAND()）或依赖于数据库状态的查询（如LAST_INSERT_ID()）时。 行复制 ROW 记录的是每行数据变更的具体内容，包括修改前和修改后的数据 优点： 数据一致性：由于记录了具体的行变化，减少由于执行上下文差异引起的数据不一致问题。 缺点： 日志体积：较大的日志体积，特别是在进行大量数据变更的操作时（如批量插入或更新）。 复制效率：进行大量数据变更的操作会产生大量日志，复制效率低。 混合复制 根据上下文自动选择语句复制或行复制。 优点： 效率和一致性：通过自动转换模式找到复制效率和数据一致性的平衡点。 灵活性：自动选择最合适的复制方式，根据不同类型的操作动态调整。 缺点： 复杂性：自动切换复制模式可能导致复制过程的复杂性增加，需要更细致的监控和管理。 常见问题 主键使用自增还是UUID auto_increment 优点： 性能提升：自增主键新数据插入在表的末尾，减少了页分裂，提升写入性能。 空间效率：数字型主键占用空间小，比如INT或BIGINT类型，有利于提高检索性能。 易于索引和排序：数字主键对索引和排序操作更为高效，因为数字比较操作通常比字符串或其他类型的比较更快。 简化设计：自增主键由数据库自动管理，简化了数据插入过程，无需手动生成主键。 缺点： 业务量暴露：自增主键可能暴露业务量等敏感信息，存在安全隐患。 自增锁竞争：在高并发场景下，自增锁竞争可能降低数据库的吞吐能力。 扩展性限制：在数据迁移、分库分表场景中，自增主键的全局唯一性维护困难。 UUID UUID泛指自动生成单调递增的ID。 优点： 全局唯一性：在全局范围内确保唯一性，适合 分布式 系统和需要唯一标识符的场景。 提升插入性能：在应用层生成，无需数据库支持。 缺点： 实现复杂：增加系统复杂性，要保证全局唯一，单调递增。 性能问题：如果非数字类型，会占用更大的存储空间，主键检索效率降低。 数据页分裂：如果无法保证单调递增将导致数据库中的数据页分裂，影响插入操作的性能。 B树、B+树的区别 数据存储：B树在每个节点中存储键和数据。而B+树叶子节点存储键和数据，非叶子节点仅存储键。 范围查询：B+树叶子节点使用指针两两相连，便于支持范围查询和排序。 I/O效率：B+树的非叶子节点不存储数据，可以存储更多的键，从而树的高度更低，在相同数据量的情况下减少磁盘I/O次数。 MyISAM 和 Innodb的区别 使用Innodb于MyIsam特性做比较 MyISAM MyISAM 存储引擎 5.5版本之前的默认引擎。 支持全文索引。 不支持事务。 锁级别是表锁，不支持行级锁。 不支持外键约束。 索引和数据分开存储。 Innodb InnoDB存储引擎 支持事务，MVCC。 使用聚簇索引，索引和数据一起存储。 支持行级锁，间隙锁。 支持外键约束。 mysql的索引有哪些，聚簇和非聚簇索引又是什么 参考 索引 按数据结构分 B+树索引 hash索引 按特性分 主键索引 唯一索引 普通索引 组合索引 前缀索引 聚簇索引：聚簇索引是一种数据组织方式，非叶子节点存储数据的键，叶子节点存储数据的键和值，叶子节点两两相连。 非聚簇索: 也叫二级索引，非聚簇索引是表的一个单独的索引结构，包含了索引列的值和对应行的主键值，但不包含行的完整数据。 回表查询: 当通过非聚簇索引检索数据时，如果需要的数据不全部包含在索引中，就会进行回表查询。 回表查询是一个两步过程：首先，通过非聚簇索引找到对应行的主键；然后，使用这个主键去聚簇索引中检索完整的行数据。 什么是覆盖索引和回表 索引覆盖 回表查询 什么是索引下推 索引下推 在索引扫描阶段，MySQL能够在索引内部应用WHERE子句的一部分条件，从而仅检索那些可能符合所有搜索条件的行。 什么是执行计划 explain 执行计划 锁的类型有哪些 数据库进阶 锁 按粒度分 全局锁 表级锁 行级锁 按类型分 共享锁（S）：允许一个事务去读一行，阻止其他事务获得相同数据集的排他锁 排他锁（X）：写锁是排他的，它会阻塞其他的写锁和读锁 按种类分 全局锁 表级锁 表锁 元数据锁 意向锁 行级锁 行锁 间隙锁 临键锁 事务的基本特性和隔离级别 事务原理 基本特性 原子性（Atomicity）：事务中的所有操作要么全部成功，要么全部失败，不会停留在中间状态。 一致性（Consistency）：事务必须保证数据库从一个一致性状态转移到另一个一致性状态。 隔离性（Isolation）：事务的执行不会被其他事务干扰，多个并发事务之间的数据库操作是隔离的。 持久性（Durability）：一旦事务完成，其对数据库的修改应该是永久性的，即使出现系统故障。 隔离级别 隔离级别（+:允许出现，-:不允许出现） 脏读 不可重复读 幻读 未提交读 + + + 提交读 - + + 可重复读 - - + 序列化读 - - - 问题 说明 脏读 在一个事务中读取到了另一个事务未提交的数据。这可能导致读取到错误的信息。 不可重复读 在同一个事务中，对同一数据的多次读取结果不一致。这通常是由于在两次读取之间，另一个事务修改了数据。 幻读 在同一个事务中，两次执行相同的查询，返回的结果集不一致。通常是因为在两次查询之间，另一个事务插入或删除了数据行。 可重复读级别解决幻读：在可重复读隔离级别下，InnoDB通过使用临键锁（结合行锁和间隙锁）防止了幻读问题。在进行 范围查询 或 索引扫描 时，InnoDB会在涉及的索引范围内设置间隙锁和行锁，防止其他事务在这些范围内插入或删除记录。 ACID靠什么保证 事务原理 什么是幻读，什么是MVCC 幻读 MVCC 什么是间隙锁 间隙锁是可重复读级别下才会有的锁，用于解决幻读问题。 间隙锁 分库分表怎么做 首先分库分表分为垂直和水平两个方式，拆分的顺序是先垂直后水平。 垂直分库 定义：垂直分库是指按照业务模块或服务功能将数据库分成独立的部分，每个部分包含相关的表和数据。这种分库策略通常与微服务架构相结合，实现服务的数据隔离。 实现方式：在微服务架构中，每个服务通常对应一个专用的数据库。这样，服务间的数据库操作互不干扰，实现了数据和业务的解耦。 优点： 提高性能：由于数据库较小，查询效率更高。 故障隔离：故障影响范围限于单个服务，不会波及整个系统。 缺点： 数据一致性：跨服务的数据一致性维护变得更加复杂。 资源冗余：每个服务都需要单独的数据库实例，导致资源使用上的冗余。 垂直分表 定义：垂直分表是指将一个数据表按照字段进行拆分，将不同的字段划分到不同的表中。这种拆分通常基于字段的使用频率、数据大小或相关性 实现方式： 常用字段和不常用字段分离：将频繁访问的字段和较少访问的字段分离到不同的表中。 大字段分离：将数据量大的字段（如文本、BLOB等）从主表中分离出去，以优化主表的访问性能。 优点: 提高性能：减少单个查询的数据量，提高查询速度和效率。 提高可维护性：简化了表结构，使得维护和管理更加容易。 缺点： 增加复杂性：查询涉及多个表时，需要进行表间的关联查询，增加了查询的复杂性。 水平分表 定义：水平分表指的是将一个表中的行分割成多个较小的表，每个表包含相同的列，但只存储原始表中的一部分行。这种分割通常基于某些列的值，如时间戳、地理位置或其他业务相关的键。 时间分割法：是一种基于时间维度来进行水平分表的方法。比如按照月、季度、年或其他时间单位将数据分割到不同的表中。 Hash取模法：是根据分片键的hash值对数据进行分表。选取一个或多个具有业务意义的字段作为分片键，对这个字段的值取hash，然后根据分表的数量进行取模运算来决定数据存储在哪个分表中。 映射法​ 定义：分表映射法是将数据从一个大表分散到多个小表中，并使用一个额外的 映射表 来记录数据在这些分表中的位置（比如ID）。映射表根据关键查询字段来指向相应的分表和行，从而优化数据的检索过程。实现过程：数据分散：根据一定的规则，如范围、哈希值或其他业务逻辑，将原始大表中的数据分散到多个小表中。创建映射表：建立一个映射表，包含用于查询的字段和子表的主键以及数据对应表的位置。查询流程：当需要查询特定数据时，首先在映射表中查询以确定目标数据存储在哪个分表的哪个位置（ID）。然后直接访问该分表来获取详细数据。优点：高效查询：映射表通过索引覆盖快速定位子表位置。缺点：跨子表查询的性能下降：如果查询需要涉及多个子表（尤其是跨表查询），会导致性能下降，因为这相当于在多个表上进行查询和汇总。关联查询：通过子表字段实现关联困难。 业务后缀法​ 定义：业务后缀法是一种将大表按照业务逻辑分割成多个小表的方法。每个小表都具有相同的结构，但表名通常会附加一个与业务相关的后缀，以标识它包含特定业务或数据段的数据。实现过程：确定分表依据：根据业务逻辑选择适当的分表依据，如用户地区、用户类型、产品类别等。创建分表：对于每个业务维度或类别，创建一个单独的表。表名通常包含一个后缀，如orders_us、orders_uk等，表示不同地区的订单数据。数据分配：根据业务规则将数据插入到相应的分表中。优点：容易实现：根据业务快速实现水平分表，比如初始化。业务清晰：每个表对应特定的业务逻辑，使得数据组织更加清晰。缺点：数据分布不均：如果业务分布不均衡，某些表可能会比其他表数据量大很多，导致性能瓶颈。维护难度：当业务逻辑变更时，可能需要调整分表策略，增加了维护的难度 基因注入法定义：通过一个或多个业务字段生成一个key，把该key作为分片键。实现过程：选择分片键：根据业务需求选择一个或多个字段，这些字段具有将数据合理分散的特性。生成分片键：通过这些字段计算得出一个分片键，可以是哈希值、组合键或其他形式的唯一标识符。数据分配：根据分片键的值将数据分散到不同的分表中。例如，可以根据分片键的哈希值对分表数量取模来决定数据应该存储在哪个表中。优点：通过算法将数据更均匀分配到各个子表中。缺点：需要按照业务场景实现pub字段算法。 时效分表法数据具有时效性在某个时间段内双写表，某个时间后自动删除旧表使用场景：数据具有时效性。比如系统消息。定义：基于时间属性对数据进行分表，将数据插入到两张表中。在数据达到某个时间阈值后，通过删除或归档旧表来管理历史数据。要求：数据不会被频繁更新。实现过程：时间段表：比如根据创建时间按月，按年分表。双写策略：将新数据同时写入当前活跃表和即将成为活跃的新表。这确保了在切换到新表时，数据的连续性和完整性。旧表处理：在切换到新表后的某个时间点，自动删除或归档旧表优点：防止成为大表：通过时间不段创建和删除表，控制数据规模。历史数据查询：通过双写策略，既保证了历史数据的可查询性，也保持了查询高效。缺点：增加写入负载：在双写阶段，对两个表的写入会增加数据库的负载。 分表后的ID怎么保证唯一性 主键的选择要符合单调递增，有效减少页合并和分裂 号段模式 描述：使用数据库记录当前最大的全局ID，并每次分配一个固定范围的号段（如1到1000）给应用服务器。服务器在本地内存中缓存这些ID，并逐个使用。 适用场景：适合高并发环境。 注意事项：需要处理号段分配和回收，防止号段耗尽或冲突。 雪花算法（Snowflake） 描述：结合时间戳、机器标识和序列号生成唯一ID。只要服务器时间保持向前，就能保证ID的唯一性。 适用场景：分布式系统。 注意事项：需要确保服务器时钟的准确性和同步。 Redis的INCR命令 描述：使用Redis的单线程特性，通过INCR命令生成递增的唯一数字ID。 适用场景：轻量级应用或非极端高唯一性要求的场景。 注意事项：在Redis故障重启或数据丢失时可能存在风险。 美团Leaf 描述：美团的Leaf是一个分布式ID生成服务，结合号段模式和雪花算法等多种策略来生成唯一ID。 适用场景：需要高度定制化的大型分布式环境。 注意事项：提供了高可用性和可扩展性，适合服务化部署。 分表后非sharding_key的查询怎么处理 使用索引表（映射表）：建立一个额外的索引表（或映射表），记录非sharding key与所在分表的映射关系。查询时，先查询索引表确定数据所在的分表，再对这些分表执行查询。 使用搜索引擎ES：将分表数据同步给搜索引擎，使用搜索引擎对数据进行检索。 使用中间件：使用支持分表的数据库中间件，如 MyCAT、Shardingsphere，这些中间件提供跨分表查询能力。 MySql主从同步怎么做的 事务提交与二进制日志（Binlog）：在主服务器（master）上，提交的事务被写入二进制日志（binlog）。这个日志记录了改变数据库状态的所有操作。 从服务器与主服务器建立连接：从服务器（slave）上的I/O线程连接到主服务器，并请求从上一次已知的binlog位置开始的binlog内容。 主服务器的Dump线程：主服务器创建一个dump线程来处理来自从服务器的请求。该线程将binlog的内容发送到从服务器。 中继日志（Relay Log）：从服务器上的I/O线程读取从主服务器接收到的binlog，并将其记录到本地的中继日志（relay log）中。 执行同步操作：从服务器上的SQL线程读取中继日志中的事件，并在从服务器数据库上执行这些操作，从而实现数据的同步。 从服务器的二进制日志：如果启用了从服务器的binlog（在进行链式复制或备份时），则从服务器上的操作也会被记录到其自己的binlog中。 同步模式 异步复制（Asynchronous Replication 默认）工作机制：在异步复制中，主服务器在事务提交后不等待从服务器确认即完成事务。主服务器的二进制日志事件（binlog events）被发送到从服务器，但主服务器不会等待从服务器确认它们已接收和处理这些事件。特点：性能：更高的性能，因为主服务器不需要等待从服务器的响应。数据延迟：会出现数据延迟，从服务器可能落后于主服务器。适用场景：适用于对数据实时性要求不高的场景。 半同步复制（Semi-Synchronous Replication）工作机制：在半同步复制中，主服务器在事务提交后会等待至少一个从服务器确认已接收到二进制日志事件后才视为事务完成。如果在指定时间内没有从服务器确认，主服务器将退回到异步复制模式。特点：数据安全性：提高了数据一致性和可靠性，减少了数据丢失的风险。性能影响：影响写入性能，因为主服务器需要等待从服务器的确认。适用场景：适用于需要平衡数据可靠性和系统性能的场景。 全同步复制（Fully Synchronous Replication）工作机制：全同步复制意味着所有的事务在主服务器和从服务器上几乎同时提交。比如使用多主节点复制系统（如Galera Cluster）实现。特点：数据一致性：提供强数据一致性保证。性能影响：写入性能最差，需要在所有节点上确认。适用场景：适用于对数据一致性有严格要求的高可用性集群环境。 binlog同步支持哪些格式 binlog 主从的延迟怎么解决 硬件和网络优化 并行复制 调整复制格式 比如使用行格式（ROW）的二进制日志可以减少从服务器应用更改所需的时间，因为它不需要解析SQL语句。 并行复制是指从服务器（slave）上并行（多线程）应用主服务器（master）的二进制日志（binlog）中的事件，以提高复制性能和减少延迟的机制。在传统的串行复制中，从服务器上的SQL线程会按照二进制日志中的事件顺序逐个执行这些事件。并行可选粒度：数据库：适用版本：MySQL 5.6及以上。粒度：以数据库为单位进行并行复制。这意味着来自不同数据库的事务可以在从服务器上并行执行。限制：同一数据库内的事务仍然会串行执行，这限制了并行度。组提交：适用版本：MySQL 5.7及以上。粒度：通过对事务进行分组，如果组内事务能同时提交成功，那么它们就不会共享任何锁（冲突）。优势：组内事务并行复制，提供了更高的并行度。事务写集：适用版本：MySQL 8.0及以上。写集：用行级别粒度判断事务之间的依赖关系，在有主键或唯一键的表中，只要两个事务没有更新同一行，就能并行回放。粒度：基于每个事务的写集（即事务修改的数据集合）来判断事务之间是否存在冲突，从而实现更细粒度的并行复制。优势：进一步提高并行复制的效率，特别是在事务修改不同行或列时。 读写分离延迟导致数据不一致解决 读取主库 策略：对于需要立即读取最新写入数据的操作，直接从主库读取。 实施：在应用层逻辑中判断，如果是刚刚写入的数据或对实时性要求高的查询，则直接连接到主数据库执行查询。 监控主从延迟 策略：定期或在关键操作前检查主从同步的延迟。 实施：使用 SHOW SLAVE STATUS 命令获取从服务器 Seconds_Behind_Master 的值，判断主从同步是否有显著延迟后动态切换。 同步模式 策略：异步复制模式修改为半同步或全同步模式。 实施：使用组复制或Galera Cluster集群方案。 使用缓存 策略：使用缓存系统来存储最新写入的数据标记。 实施： 在写入数据后，将相关数据或标记存入缓存系统（如Redis、本地缓存、客户端缓存等）。 读取操作首先查询缓存，如果缓存中存在最新数据，则使用缓存数据；如果没有，则从数据库读取。 训练脚本 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384create table student(sno varchar(3) primary key,sname varchar(8),sex varchar(2),sbirthday datetime,class varchar(5)) default charset = utf8;insert into studentvalues (&#x27;108&#x27;, &#x27;曾华&#x27;, &#x27;男&#x27;, &#x27;1977-09-01&#x27;, &#x27;95033&#x27;), (&#x27;105&#x27;, &#x27;匡明&#x27;, &#x27;男&#x27;, &#x27;1975-10-02&#x27;, &#x27;95031&#x27;), (&#x27;107&#x27;, &#x27;王丽&#x27;, &#x27;女&#x27;, &#x27;1976-01-23&#x27;, &#x27;95033&#x27;), (&#x27;101&#x27;, &#x27;李军&#x27;, &#x27;男&#x27;, &#x27;1976-02-20&#x27;, &#x27;95033&#x27;), (&#x27;109&#x27;, &#x27;王芳&#x27;, &#x27;女&#x27;, &#x27;1975-02-10&#x27;, &#x27;95031&#x27;), (&#x27;103&#x27;, &#x27;陆君&#x27;, &#x27;男&#x27;, &#x27;1974-06-03&#x27;, &#x27;95031&#x27;);create table course(cno varchar(5) primary key,cname varchar(10),tno varchar(3)) default charset = utf8;insert into coursevalues (&#x27;3-105&#x27;, &#x27;计算机导论&#x27;, &#x27;825&#x27;), (&#x27;3-245&#x27;, &#x27;操作系统&#x27;, &#x27;804&#x27;), (&#x27;6-166&#x27;, &#x27;数字电路&#x27;, &#x27;856&#x27;), (&#x27;9-888&#x27;, &#x27;高等数学&#x27;, &#x27;831&#x27;);create table score(sno varchar(3),cno varchar(5),degree decimal(4, 1)) default charset = utf8;insert into scorevalues (&#x27;103&#x27;, &#x27;3-245&#x27;, 86), (&#x27;105&#x27;, &#x27;3-245&#x27;, 75), (&#x27;109&#x27;, &#x27;3-245&#x27;, 68), (&#x27;103&#x27;, &#x27;3-105&#x27;, 92), (&#x27;105&#x27;, &#x27;3-105&#x27;, 88), (&#x27;109&#x27;, &#x27;3-105&#x27;, 76), (&#x27;101&#x27;, &#x27;3-105&#x27;, 64), (&#x27;107&#x27;, &#x27;3-105&#x27;, 91), (&#x27;108&#x27;, &#x27;3-105&#x27;, 78), (&#x27;101&#x27;, &#x27;6-166&#x27;, 85), (&#x27;107&#x27;, &#x27;6-166&#x27;, 79), (&#x27;108&#x27;, &#x27;6-166&#x27;, 81);create table teacher(tno varchar(3),tname varchar(8),tsex varchar(2),tbirthday datetime,prof varchar(6),depart varchar(10)) default charset = utf8;insert into teachervalues (&#x27;804&#x27;, &#x27;李诚&#x27;, &#x27;男&#x27;, &#x27;1958-12-02&#x27;, &#x27;副教授&#x27;, &#x27;计算机系&#x27;), (&#x27;856&#x27;, &#x27;张旭&#x27;, &#x27;男&#x27;, &#x27;1969-03-12&#x27;, &#x27;讲师&#x27;, &#x27;电子工程系&#x27;), (&#x27;825&#x27;, &#x27;王萍&#x27;, &#x27;女&#x27;, &#x27;1972-05-05&#x27;, &#x27;助教&#x27;, &#x27;计算机系&#x27;), (&#x27;831&#x27;, &#x27;刘冰&#x27;, &#x27;女&#x27;, &#x27;1977-08-14&#x27;, &#x27;助教&#x27;, &#x27;电子工程系&#x27;);# 等级create table grade(low int(3),upp int(3),`rank` char(1));insert into gradevalues (90, 100, &#x27;A&#x27;);insert into gradevalues (80, 89, &#x27;B&#x27;);insert into gradevalues (70, 79, &#x27;C&#x27;);insert into gradevalues (60, 69, &#x27;D&#x27;);insert into gradevalues (0, 59, &#x27;E&#x27;);","categories":[],"tags":[]},{"title":"Kafka","slug":"Kafka","date":"2022-02-24T08:52:56.000Z","updated":"2023-10-27T06:11:47.632Z","comments":true,"path":"2022/02/24/Kafka/","link":"","permalink":"https://wugengfeng.cn/2022/02/24/Kafka/","excerpt":"","text":"思维脑图 项目涉及源码 消息队列概述 两种工作模式 点对点模式 一对一，消费者主动拉取数据，消息收到后消息清除 在点对点消息队列（Point-to-Point, P2P）模式中，消息生产者将消息发送至特定的队列（Queue），而消息消费者则从该队列中主动拉取并处理消息。每个消息只能被一个消费者接收和处理。一旦消费者成功处理了消息，该消息就会从队列中被移除，确保不会被其他消费者再次处理。虽然一个队列可以拥有多个消费者，每条消息只能被其中一个消费者接收。此模式确保每条消息都会被精确地处理一次，避免重复或遗漏的情况发生。 发布/订阅模式 一对多，消费者消费数据之后不会清理消息，有时间限制 在消息队列的发布-订阅（Publish-Subscribe）模式中，消息生产者（或称发布者）将消息发送到一个称作“主题”（Topic）的通道中。与点对点模式不同的是，所有订阅了该主题的消费者（或称订阅者）都有能力接收到这些消息。重要的是，一条发布到主题的消息会被所有活跃的订阅者消费，实现了一条消息能够同时被多个消费者接收的效果。此模式支持一条消息被多次处理，由不同的消费者分别进行处理和响应。发布到主题的消息通常会在所有订阅者消费完成后保留一段预定的时间或直至满足某些条件后才被清理。 kafka 是发布订阅模式，只有一个消费者组就类似点对点模式，多个消费者组就是发布/订阅模式，但是不会马上删除数据，需要配合配置文件 两种获取消息的方式 推模式 在推模式中，当 Broker 接收到 Producer 的消息后，它会主动地将这些消息推送给 Consumer。具体来说，客户端（Consumer）与服务端（Broker）之间会建立一个持久化的连接。当Broker有新的消息时，它会直接通过这个已建立的连接推送消息到客户端，从而确保客户端可以实时接收到最新的消息。 优点 实时性：由于消息是被主动推送的，客户端能够快速地接收到最新的消息，确保了消息的实时传递。 客户端实现简单：只需要监听服务端的推送消息 缺点 无法应对不同能力的消费者：每个客户端的消费能力是不同的，如果简单粗暴进行消息推送就会出现 消息堆积 而引发宕机 拉模式 在拉模式中，Consumer 会主动向 Broker 请求消息，而不是靠Broker将消息推送给它。这意味着Consumer决定何时接收消息，并控制获取消息的频率。 长轮询：客户端向服务端发起请求，如果此时有数据就直接返回，如果没有数据就保持连接，等到有数据时就直接返回。如果一直没有数据，超时后客户端再次发起请求，保持连接 优点 避免消息堆积：由于Consumer控制消息的拉取时机，它可以确保在上一批消息处理完成后再拉取新的消息，从而避免在客户端出现消息堆积。 长轮询实现的拉模式实时性也能够保证消息时效性 缺点 实现复杂性：客户端需要维护拉取消息的逻辑和处理潜在的超时/重试情况，又要考虑消息的时效性和避免忙等 kafka consumer从broker拉取消息 Kafka概念 定义 Kafka传统定义： Kafka 通常被定义为一个分布式的、基于发布/订阅模式的消息队列系统，它在大数据和实时分析领域具有广泛的应用。 发布/订阅：消息发布者（Producer）不会直接将消息发送到特定的接收者（Consumer）。相反，发布者将消息发送到一个中间层，通常是一个“主题”（Topic），而订阅者则从这个主题订阅它们感兴趣的消息。这种解耦的方式使得生产者和消费者可以独立地扩展和演变。 Kafka最新定义：如今通常被认为是一个开源的分布式事件流平台，它不仅仅能处理高吞吐量的事件流数据，也能支持高性能的数据管道、数据集成、实时分析和关键任务应用。其灵活的架构使其在全球范围内的数千家公司中被用于构建多种应用场景的事件驱动解决方案。 优缺点 优点： 高性能与高吞吐量：Kafka展现了卓越的性能表现和高吞吐量，单机事务吞吐量可达到百万条/秒级别。 高可用性：作为一个分布式系统，Kafka通过数据副本保证了高可用性和数据的持久性。即使部分节点宕机，由于多副本的存在，数据仍不会丢失，系统依然可用。 主动拉取消息：消费者采用Pull（拉取）方式获取消息，支持消息有序消费，并能通过控制策略确保消息被消费一次且仅被消费一次。 生态支持：Kafka拥有稳定的社区支持，以及丰富的第三方工具，例如Kafka Web管理界面EFAK。在日志处理和大数据实时处理领域（如Flink、CDC、ETL等）具有广泛的应用。 功能定位：虽然功能相对简洁，主要专注于消息队列（MQ）功能，但在大数据领域，这种定位使其在实时计算和日志采集等应用场景下表现卓越。 缺点： 再平衡延迟：消费者组成员的变化可能导致再平衡，这会引入额外的延迟。 实时性与轮询策略：消费者使用轮询方式获取消息，因此消息的实时性会受轮询间隔时间的影响。 不支持自动重试：在消费消息失败的场景下，Kafka本身不提供自动重试的机制。 消息顺序问题：保证单分区内消息有序，但是不保证多分区间的消息有序。 应用场景 缓存/消峰 Kafka 能够作为一个高效的缓冲层，协助控制和优化数据在系统间的流动，尤其适用于生产消息的速度和消费消息的处理速度不匹配的场景。以秒杀功能为例：在高流量场景下，如用户请求量超过服务器的处理能力，可以利用Kafka进行秒杀请求的缓存。服务器根据其处理能力，稳定地从队列中拉取数据进行处理，从而有效地实现流量的削峰。 解耦 Kafka 在系统解耦方面表现卓越，特别是在需要异构数据同步的上下文中。例如，在一个需要从多个服务系统中收集日志数据、并将数据分发到不同数据库进行各异分析的日志收集系统中。通过Kafka进行数据交流和传递，不同的服务和处理模块可以解耦合，保持独立运行和演进，只需确保数据的消费处理逻辑保持一致。 异步通信 Kafka 支持异步通信模型，允许生产者将消息发布到队列中而无需立即进行处理。消费者可以根据自身的处理能力，在适当的时候从队列中拉取消息进行处理。一个常见的应用场景是电商平台的订单通知系统：当用户下单成功时，下单信息被发送到Kafka，短信通知服务作为消费者异步从Kafka中获取这些信息，进而在可控的速度和时间内，发送订单确认短信给用户。 数据聚合 Kafka 可以用于收集来自多个源的数据，将其聚合在一处，方便进行数据分析和信息提取。比如多个服务的操作日志可以汇集至Kafka，再由专门的日志分析服务消费，实现集中式的日志管理和分析。 事件驱动架构 在事件驱动架构中，Kafka 常作为事件传递的中介，各个服务发布和订阅事件，从而实现低耦合的交互和协作。通过精细定义事件的种类和格式，服务可以轻松响应其他服务的状态变化或请求，而无需直接交互。 Kafka架构 几个重要配置参数 Broker 定义 消息处理中心：Kafka Broker（服务实例） 接收来自生产者的消息，存储这些消息，并处理消费者的读取请求。 分布式节点：在Kafka的分布式环境中，Broker是作为独立节点运行的服务器或一组服务器。 核心功能 数据存储：Broker负责将生产者发送的消息持久化到磁盘上，确保数据的安全性。 数据提取：Broker允许消费者读取存储的数据，支持并行读取。 分区管理：Kafka的数据存储在不同的Topic中，每个Topic可以被分成多个分区。Broker管理这些分区的数据，每个分区可以在不同的Broker上。 副本管理：为了提高数据的可用性和耐用性，Kafka的数据通常会在多个Broker上进行复制。Broker负责管理这些副本，并在必要时进行故障转移。 协调和同步：在多Broker环境下，Broker间会协调并同步消息和事务状态。 Topic 在Kafka中，消息被归类到不同的 topic。每条发送到Kafka集群的消息都应指定一个 topic。默认情况下，topic中的数据保留7天（168小时），但这是可以配置的。 Topic 在Kafka中是逻辑上的分区，而其下的 partition 则代表物理上的分区。 特殊Topic 特殊Topic 从Kafka 0.10.x版本开始，消费者的 offset 信息默认存储在内置的 __consumer_offsets topic中。当消费者首次尝试消费Kafka数据时，这个特殊的topic会自动被创建。需要注意的是，该topic的副本数不受集群的默认topic副本数配置的影响，而它的默认分区数为50，但这也是可以调整的。 Partition Topic -&gt; Partition -&gt; 副本 Partition（分区）是用于实现消息在处理时的并行化的基本单位（物理分区）。每个Topic可以被分成一个或多个Partition，每个Partition可以存在于多个节点上以提供数据冗余，以确保数据在某些节点失败的情况下仍然可用。这样，Partition既能提供系统的横向扩展性，也提供了数据的高可用性。 核心功能： 并行处理：通过将Topic分区，Kafka可以在多个broker（节点）上并行处理数据，也允许在多个消费者之间并行处理数据。 数据持久化：每个Partition都是一个有序的、不可变的记录集，并且可以持久地存储到磁盘上。每个在Partition中的消息都会被分配一个唯一的、递增的ID号，称为“offset”。 水平扩展：通过增加Partition数量，Kafka能够水平扩展处理更多的消息。 复制：Partition也支持复制，以防止数据丢失。每个Partition会有一个Leader和零个或多个Follower。所有的读写操作都由Leader处理，而Follower用来在Leader失败时提供冗余备份。 消费者组内的并行消费：消费者组中的每个消费者实例都会消费Topic的一个或多个Partition的数据。一个Partition在一个消费者组内只会被一个消费者实例消费，从而实现了在消费者组内的消息处理的负载均衡。 Partition数量只能增加不能减少 Replica Kafka中的每个分区（Partition）可以有一个或多个副本，这些副本分散在集群中的不同集群节点上，以实现数据的持久化和高可用。 Leader 在Kafka中，每个分区都有一个并且只有一个 Leader 副本，它负责处理该分区的所有读写操作。虽然一个Topic可能包含多个分区，但每个分区的Leader副本都是独立且唯一的。 Follower Follower 副本主要用于备份和同步Leader副本中的数据。在Leader副本出现故障时，Kafka会从Follower副本中选举一个新的Leader来保证高可用性。 ISR 表示和 Leader 保持同步的副本集合，主节点宕机可作为备选节点。 OSR 表示 Follower 与 Leader 副本同步时，延迟过多的副本集合。 Producer 生产者（Producer）是消息生产和发布的实体。它负责将应用程序的消息发布到指定的 Kafka 主题。 main线程 主线程主要负责将消息（Records）封装到一个 ProducerRecord 对象中，并发送给内部的 RecordAccumulator。 序列化： 将 Key 和 Value 对象序列化为字节数组。 分区： 如果 ProducerRecord 中没有指定 Partition，Sender 线程需要通过 Partitioner 来决定消息将发送到 Topic 的哪个 Partition 上。 RecordAccumulator（内存缓冲区） RecordAccumulator 是一个消息的内存缓冲区。当主线程调用 Producer 的 send() 方法发送消息时，消息实际上被先存储在此缓冲区中。它会尽可能地批量处理这些消息（即尽可能地将多个消息打包在一个 batch 中）以提高效率。当 batch 满了或经过一定时间（linger.ms 属性设定的时间）后，消息会被发送到目标分区（Partition）。 send线程 Sender 线程是 Kafka 生产者的核心，它从 RecordAccumulator 中取出消息并负责将消息发送到 Kafka Broker 上对应的 Partition 中。 压缩： 根据配置，Sender 线程可能会将几个消息压缩到一起发送以节省带宽。 发送： Sender 线程将消息发送到 Broker 并等待 Broker 的确认。 重试： 如果消息发送失败，根据生产者的配置（例如 retries 和 retry.backoff.ms 等参数），Sender 线程可能会进行一定次数的重试。 回调： 如果消息发送成功或者最终失败，Sender 线程将调用回调方法（如果在发送消息时提供了回调函数）。 ConsumerGroup 在Kafka中，消费者组是一种机制，使多个消费者可以协作处理同一个主题的消息。消费者组内的每个消费者负责读取该主题的不同分区，确保每条消息只被消费者组中的一个消费者消费一次。 消费者组内的再分配：当消费者组中的消费者数量发生变化（例如新的消费者加入或现有的消费者离开）时，Kafka会重新分配主题分区给消费者。这种再分配确实会对系统造成一些开销，但它允许Kafka的消费模式具有弹性和容错性。 分区和消费者的数量关系：为了确保每个消费者都有数据可以读取，一个主题的分区数量应当大于或等于消费者组中消费者的数量。 消费者和消费者组：每个消费者都属于一个消费者组，并可以为每个消费者指定一个组名。如果不指定组名，消费者会属于默认组。一条消息可以被多个消费者组中的消费者读取，但在一个消费者组内，一条消息只会被一个消费者读取。 Consumer 消费者在Kafka中负责从Broker读取消息。Kafka采用的是发布-订阅模式，但与传统的发布-订阅系统不同的是，Kafka中的订阅者是消费者组而非单一的消费者实例。在消费者组内，每条消息只会被一个消费者实例处理。但值得注意的是，不同的消费者组可以独立地并行消费同一条消息。 Kafka集群搭建 java8环境安装 12sudo apt-get updatesudo apt-get install openjdk-8-jdk 下载并解压 kafka最新版本下载 新版本Kafka不需要额外安装zookeeper，使用内置zookeeper搭建集群。目前还支持kraft方式部署，不过目前版本还未稳定故不选择 下载kafka_2.12-3.2.0.tgz（或目前最新版本） 123456789# 创建目录mkdir /data/kafka/k1mkdir /data/kafka/k2mkdir /data/kafka/k3# 解压到创建目录tar -zvxf kafka_2.12-3.2.0.tgz -C /data/kafka/k1/tar -zvxf kafka_2.12-3.2.0.tgz -C /data/kafka/k2/tar -zvxf kafka_2.12-3.2.0.tgz -C /data/kafka/k3/ zookeeper存储位置配置 1234567891011121314151617# 创建zookeeper数据存储目录mkdir /data/kafka/k1/kafka_2.12-3.2.0/zookeepermkdir /data/kafka/k1/kafka_2.12-3.2.0/zookeeper/datamkdir /data/kafka/k1/kafka_2.12-3.2.0/zookeeper/logmkdir /data/kafka/k2/kafka_2.12-3.2.0/zookeepermkdir /data/kafka/k2/kafka_2.12-3.2.0/zookeeper/datamkdir /data/kafka/k2/kafka_2.12-3.2.0/zookeeper/logmkdir /data/kafka/k3/kafka_2.12-3.2.0/zookeepermkdir /data/kafka/k3/kafka_2.12-3.2.0/zookeeper/datamkdir /data/kafka/k3/kafka_2.12-3.2.0/zookeeper/log# 创建myid文件echo 1 &gt; /data/kafka/k1/kafka_2.12-3.2.0/zookeeper/data/myidecho 2 &gt; /data/kafka/k2/kafka_2.12-3.2.0/zookeeper/data/myidecho 3 &gt; /data/kafka/k3/kafka_2.12-3.2.0/zookeeper/data/myid zookeeper配置修改 将三个目录下的zookeeper配置文件分别修改下文三个配置 /data/kafka/k1/kafka_2.12-3.2.0/config/zookeeper.properties /data/kafka/k2/kafka_2.12-3.2.0/config/zookeeper.properties /data/kafka/k3/kafka_2.12-3.2.0/config/zookeeper.properties 123456789101112131415161718#心跳时间2秒 tickTime=2000#Follower跟随者服务器与Leader领导者服务器之间初始化连接时能容忍的最多心跳数10*tickTime initLimit=10#集群中Leader与Follower之间的最大响应时间单位5*tickTime syncLimit=5#存储快照文件 snapshot 的目录。默认情况下，事务日志也会存储在这里。建议同时配置参数dataLogDir, 事务日志的写性能直接影响zk性能dataDir=/data/kafka/k1/kafka_2.12-3.2.0/zookeeper/data#事务日志输出目录。尽量给事务日志的输出配置单独的磁盘或是挂载点，这将极大的提升ZK性能dataLogDir=/data/kafka/k1/kafka_2.12-3.2.0/zookeeper/log#zookeeper端口 clientPort=2181#单个客户端与单台服务器之间的连接数的限制，是ip级别的，默认是60，如果设置为0，那么表明不作任何限制maxClientCnxns=60#server.1代表一台服务器的编号，第一个端口为集群通讯端口，第二个端口代表Leader选举的端口 server.1=127.0.0.1:2881:3881server.2=127.0.0.1:2882:3882server.3=127.0.0.1:2883:3883 123456789101112131415161718#心跳时间2秒 tickTime=2000#Follower跟随者服务器与Leader领导者服务器之间初始化连接时能容忍的最多心跳数10*tickTime initLimit=10#集群中Leader与Follower之间的最大响应时间单位5*tickTime syncLimit=5#存储快照文件 snapshot 的目录。默认情况下，事务日志也会存储在这里。建议同时配置参数dataLogDir, 事务日志的写性能直接影响zk性能dataDir=/data/kafka/k2/kafka_2.12-3.2.0/zookeeper/data#事务日志输出目录。尽量给事务日志的输出配置单独的磁盘或是挂载点，这将极大的提升ZK性能dataLogDir=/data/kafka/k2/kafka_2.12-3.2.0/zookeeper/log#zookeeper端口 clientPort=2182#单个客户端与单台服务器之间的连接数的限制，是ip级别的，默认是60，如果设置为0，那么表明不作任何限制maxClientCnxns=60#server.1代表一台服务器的编号，第一个端口为集群通讯端口，第二个端口代表Leader选举的端口 server.1=127.0.0.1:2881:3881server.2=127.0.0.1:2882:3882server.3=127.0.0.1:2883:3883 123456789101112131415161718#心跳时间2秒 tickTime=2000#Follower跟随者服务器与Leader领导者服务器之间初始化连接时能容忍的最多心跳数10*tickTime initLimit=10#集群中Leader与Follower之间的最大响应时间单位5*tickTime syncLimit=5#存储快照文件 snapshot 的目录。默认情况下，事务日志也会存储在这里。建议同时配置参数dataLogDir, 事务日志的写性能直接影响zk性能dataDir=/data/kafka/k3/kafka_2.12-3.2.0/zookeeper/data#事务日志输出目录。尽量给事务日志的输出配置单独的磁盘或是挂载点，这将极大的提升ZK性能dataLogDir=/data/kafka/k3/kafka_2.12-3.2.0/zookeeper/log#zookeeper端口 clientPort=2183#单个客户端与单台服务器之间的连接数的限制，是ip级别的，默认是60，如果设置为0，那么表明不作任何限制maxClientCnxns=60#server.1代表一台服务器的编号，第一个端口为集群通讯端口，第二个端口代表Leader选举的端口 server.1=127.0.0.1:2881:3881server.2=127.0.0.1:2882:3882server.3=127.0.0.1:2883:3883 编写启动脚本启动 /data/kafka/k1/kafka_2.12-3.2.0 /data/kafka/k1/kafka_2.12-3.2.0 /data/kafka/k1/kafka_2.12-3.2.0 zk-start.sh 12#!/bin/bashnohup bin/zookeeper-server-start.sh config/zookeeper.properties &gt; logs/zookeeper/zookeeper.log 2&gt;1 &amp; 12345678910# 创建log目录mkdir /data/kafka/k1/kafka_2.12-3.2.0/logsmkdir /data/kafka/k1/kafka_2.12-3.2.0/logs/zookeepermkdir /data/kafka/k2/kafka_2.12-3.2.0/logsmkdir /data/kafka/k2/kafka_2.12-3.2.0/logs/zookeepermkdir /data/kafka/k3/kafka_2.12-3.2.0/logsmkdir /data/kafka/k3/kafka_2.12-3.2.0/logs/zookeeper# 分别启动zk. zk-start.sh 创建kafka数据目录 123mkdir /data/kafka/k1/kafka_2.12-3.2.0/datamkdir /data/kafka/k2/kafka_2.12-3.2.0/datamkdir /data/kafka/k3/kafka_2.12-3.2.0/data kafka配置修改 将三个目录下的kafka配置文件分别修改下文三个配置 /data/kafka/k1/kafka_2.12-3.2.0/config/server.properties /data/kafka/k2/kafka_2.12-3.2.0/config/server.properties /data/kafka/k3/kafka_2.12-3.2.0/config/server.properties 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687# broker的全局唯一编号，不能重复broker.id=1# 提供给客户端响应的地址和端口listeners = PLAINTEXT://0.0.0.0:9091# 提供给客户端响应的地址和端口（允许外网访问）advertised.listeners=PLAINTEXT://127.0.0.1:9091# 处理网络请求的线程数量，也就是接收消息的线程数。# 接收线程会将接收到的消息放到内存中，然后再从内存中写入磁盘。num.network.threads=3# 消息从内存中写入磁盘是时候使用的线程数量。# 用来处理磁盘IO的线程数量num.io.threads=8# 发送套接字的缓冲区大小socket.send.buffer.bytes=102400# 接受套接字的缓冲区大小socket.receive.buffer.bytes=102400# 请求套接字的缓冲区大小socket.request.max.bytes=104857600# kafka运行日志存放的路径log.dirs=/data/kafka/k1/kafka_2.12-3.2.0/data# topic在当前broker上的分片个数num.partitions=3# 我们知道segment文件默认会被保留7天的时间，超时的话就# 会被清理，那么清理这件事情就需要有一些线程来做。这里就是# 用来设置恢复和清理data下数据的线程数量num.recovery.threads.per.data.dir=1# 副本因子# 实际使用过程中很多用户都会将offsets.topic.replication.factor设置成大于1的数以增加可靠性，这是推荐的做法# 存储的消费者客户端offsets偏移量offsets.topic.replication.factor=3# 副本因子事物状态日志数量，存储事务明细transaction.state.log.replication.factor=3# 事物状态日志最小数量transaction.state.log.min.isr=3# segment文件保留的最长时间，默认保留7天（168小时），# 超时将被删除，也就是说7天之前的数据将被清理掉。log.retention.hours=168# 日志文件中每个segment的大小，默认为1Glog.segment.bytes=1073741824# 上面的参数设置了每一个segment文件的大小是1G，那么# 就需要有一个东西去定期检查segment文件有没有达到1G，# 多长时间去检查一次，就需要设置一个周期性检查文件大小# 的时间（单位是毫秒）。log.retention.check.interval.ms=300000# 日志清理是否打开log.cleaner.enable=true# broker需要使用zookeeper保存meta数据zookeeper.connect=127.0.0.1:2181,127.0.0.1:2182,127.0.0.1:2183# zookeeper链接超时时间zookeeper.connection.timeout.ms=18000# 上面我们说过接收线程会将接收到的消息放到内存中，然后再从内存# 写到磁盘上，那么什么时候将消息从内存中写入磁盘，就有一个# 时间限制（时间阈值）和一个数量限制（数量阈值），这里设置的是# 数量阈值，下一个参数设置的则是时间阈值。# partion buffer中，消息的条数达到阈值，将触发flush到磁盘。log.flush.interval.messages=10000# 消息buffer的时间，达到阈值，将触发将消息从内存flush到磁盘，# 单位是毫秒。log.flush.interval.ms=3000# 删除topic需要server.properties中设置delete.topic.enable=true否则只是标记删除delete.topic.enable=true# 防止成员加入请求后本应当即开启的rebalancegroup.initial.rebalance.delay.ms=0 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586# broker的全局唯一编号，不能重复broker.id=2# 提供给客户端响应的地址和端口listeners = PLAINTEXT://0.0.0.0:9092# 提供给客户端响应的地址和端口（允许外网访问）advertised.listeners=PLAINTEXT://127.0.0.1:9092# 处理网络请求的线程数量，也就是接收消息的线程数。# 接收线程会将接收到的消息放到内存中，然后再从内存中写入磁盘。num.network.threads=3# 消息从内存中写入磁盘是时候使用的线程数量。# 用来处理磁盘IO的线程数量num.io.threads=8# 发送套接字的缓冲区大小socket.send.buffer.bytes=102400# 接受套接字的缓冲区大小socket.receive.buffer.bytes=102400# 请求套接字的缓冲区大小socket.request.max.bytes=104857600# kafka运行日志存放的路径log.dirs=/data/kafka/k2/kafka_2.12-3.2.0/data# topic在当前broker上的分片个数num.partitions=3# 我们知道segment文件默认会被保留7天的时间，超时的话就# 会被清理，那么清理这件事情就需要有一些线程来做。这里就是# 用来设置恢复和清理data下数据的线程数量num.recovery.threads.per.data.dir=1# 副本因子# 实际使用过程中很多用户都会将offsets.topic.replication.factor设置成大于1的数以增加可靠性，这是推荐的做法# 存储的消费者客户端offsets偏移量offsets.topic.replication.factor=3# 副本因子事物状态日志数量，存储事务明细transaction.state.log.replication.factor=3# 事物状态日志最小数量transaction.state.log.min.isr=3# segment文件保留的最长时间，默认保留7天（168小时），# 超时将被删除，也就是说7天之前的数据将被清理掉。log.retention.hours=168# 日志文件中每个segment的大小，默认为1Glog.segment.bytes=1073741824# 上面的参数设置了每一个segment文件的大小是1G，那么# 就需要有一个东西去定期检查segment文件有没有达到1G，# 多长时间去检查一次，就需要设置一个周期性检查文件大小# 的时间（单位是毫秒）。log.retention.check.interval.ms=300000# 日志清理是否打开log.cleaner.enable=true# broker需要使用zookeeper保存meta数据zookeeper.connect=127.0.0.1:2181,127.0.0.1:2182,127.0.0.1:2183# zookeeper链接超时时间zookeeper.connection.timeout.ms=18000# 上面我们说过接收线程会将接收到的消息放到内存中，然后再从内存# 写到磁盘上，那么什么时候将消息从内存中写入磁盘，就有一个# 时间限制（时间阈值）和一个数量限制（数量阈值），这里设置的是# 数量阈值，下一个参数设置的则是时间阈值。# partion buffer中，消息的条数达到阈值，将触发flush到磁盘。log.flush.interval.messages=10000# 消息buffer的时间，达到阈值，将触发将消息从内存flush到磁盘，# 单位是毫秒。log.flush.interval.ms=3000# 删除topic需要server.properties中设置delete.topic.enable=true否则只是标记删除delete.topic.enable=true# 防止成员加入请求后本应当即开启的rebalancegroup.initial.rebalance.delay.ms=0 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586# broker的全局唯一编号，不能重复broker.id=3# 提供给客户端响应的地址和端口listeners = PLAINTEXT://0.0.0.0:9093# 提供给客户端响应的地址和端口（允许外网访问）advertised.listeners=PLAINTEXT://127.0.0.1:9093# 处理网络请求的线程数量，也就是接收消息的线程数。# 接收线程会将接收到的消息放到内存中，然后再从内存中写入磁盘。num.network.threads=3# 消息从内存中写入磁盘是时候使用的线程数量。# 用来处理磁盘IO的线程数量num.io.threads=8# 发送套接字的缓冲区大小socket.send.buffer.bytes=102400# 接受套接字的缓冲区大小socket.receive.buffer.bytes=102400# 请求套接字的缓冲区大小socket.request.max.bytes=104857600# kafka运行日志存放的路径log.dirs=/data/kafka/k3/kafka_2.12-3.2.0/data# topic在当前broker上的分片个数num.partitions=3# 我们知道segment文件默认会被保留7天的时间，超时的话就# 会被清理，那么清理这件事情就需要有一些线程来做。这里就是# 用来设置恢复和清理data下数据的线程数量num.recovery.threads.per.data.dir=1# 副本因子# 实际使用过程中很多用户都会将offsets.topic.replication.factor设置成大于1的数以增加可靠性，这是推荐的做法# 存储的消费者客户端offsets偏移量offsets.topic.replication.factor=3# 副本因子事物状态日志数量，存储事务明细transaction.state.log.replication.factor=3# 事物状态日志最小数量transaction.state.log.min.isr=3# segment文件保留的最长时间，默认保留7天（168小时），# 超时将被删除，也就是说7天之前的数据将被清理掉。log.retention.hours=168# 日志文件中每个segment的大小，默认为1Glog.segment.bytes=1073741824# 上面的参数设置了每一个segment文件的大小是1G，那么# 就需要有一个东西去定期检查segment文件有没有达到1G，# 多长时间去检查一次，就需要设置一个周期性检查文件大小# 的时间（单位是毫秒）。log.retention.check.interval.ms=300000# 日志清理是否打开log.cleaner.enable=true# broker需要使用zookeeper保存meta数据zookeeper.connect=127.0.0.1:2181,127.0.0.1:2182,127.0.0.1:2183# zookeeper链接超时时间zookeeper.connection.timeout.ms=18000# 上面我们说过接收线程会将接收到的消息放到内存中，然后再从内存# 写到磁盘上，那么什么时候将消息从内存中写入磁盘，就有一个# 时间限制（时间阈值）和一个数量限制（数量阈值），这里设置的是# 数量阈值，下一个参数设置的则是时间阈值。# partion buffer中，消息的条数达到阈值，将触发flush到磁盘。log.flush.interval.messages=10000# 消息buffer的时间，达到阈值，将触发将消息从内存flush到磁盘，# 单位是毫秒。log.flush.interval.ms=3000# 删除topic需要server.properties中设置delete.topic.enable=true否则只是标记删除delete.topic.enable=true# 防止成员加入请求后本应当即开启的rebalancegroup.initial.rebalance.delay.ms=0 编写启动脚本启动 /data/kafka/k1/kafka_2.12-3.2.0 /data/kafka/k2/kafka_2.12-3.2.0 /data/kafka/k3/kafka_2.12-3.2.0 在上面三个目录分别创建 kafka-start.sh 启动脚本 12#!/bin/bashnohup ./bin/kafka-server-start.sh config/server.properties &gt; logs/kafka/kafka.log 2&gt;1 &amp; 1234# 创建日志目录mkdir /data/kafka/k1/kafka_2.12-3.2.0/logs/kafkamkdir /data/kafka/k2/kafka_2.12-3.2.0/logs/kafkamkdir /data/kafka/k3/kafka_2.12-3.2.0/logs/kafka 启动 1. kafka-start.sh 安装kafka-manager kafka-manager 安装 Kafka命令 Topic 参数 描述 --bootstrap-server &lt;String: server toconnect to&gt; 连接的 Kafka Broker 主机名称和端口号 --topic &lt;String: topic&gt; 操作的 topic 名称 --create 创建主题 --delete 删除主题 --alter 修改主题 --list 查看所有主题 --describe 查看主题详细描述 --partitions &lt;Integer: # of partitions&gt; 设置分区数 --replication-factor&lt;Integer: replication factor&gt; 设置分区副本 --config &lt;String: name=value&gt; 更新系统默认的配置 列出所有topic 1sh kafka-topics.sh --list --bootstrap-server localhost:9092 创建topic 12sh kafka-topics.sh --bootstrap-server localhost:9092 --create --topic test --partitions 3 --replication-factor 3参数 --topic 指定 Topic 名，--partitions 指定分区数，--replication-factor 指定备份数 查看topic 12sh kafka-topics.sh --describe --bootstrap-server localhost:9092 --topic test可以查看每个broker上的topic分区副本信息 增加topic的partition数量 Partition数量只能增加不能减少 1sh kafka-topics.sh --bootstrap-server localhost:9092 --alter --topic test --partitions 2 删除topic 1sh kafka-topics.sh --bootstrap-server localhost:9092 --topic test3 --delete 生产消息 参数 描述 --bootstrap-server &lt;String: server toconnect to&gt; 连接的 Kafka Broker 主机名称和端口号 --topic &lt;String: topic&gt; 操作的 topic 名称 12sh kafka-console-producer.sh --bootstrap-server localhost:9092 --topic test此时控制台会出现光标，这时候就可以发送消息 消费消息 参数 描述 --bootstrap-server &lt;String: server toconnect to&gt; 连接的 Kafka Broker 主机名称和端口号 --topic &lt;String: topic&gt; 操作的 topic 名称 --from-beginning 从头开始消费 --group &lt;String: consumer group id&gt; 指定消费者组名称 1sh kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic first 1sh kafka-console-consumer.sh --bootstrap-server localhost:9092 --from-beginning --topic first Kafka生产者 消息发送过程 创建Producer实例：当我们创建一个Kafka Producer实例，后台会启动几个线程，其中最重要的是 main 和 Sender 线程。 序列化：使用配置的序列化器对消息的 key 和 value 进行序列化。 分区：消息在被添加到 RecordAccumulator 之前需要确定目标分区。这是通过Partitioner完成的，它可以基于消息键、消息内容或自定义策略来决定消息应该进入哪个分区。 添加消息到缓冲区：在应用程序的 main 线程中，当调用Producer的 send 方法时，它实际上并不直接发送消息。消息首先会被添加到一个批次中（称为Batch），然后存储在一个名为 RecordAccumulator 的内部结构中。这个 RecordAccumulator 实际上是由多个双端队列组成的，每个队列对应一个Kafka的分区。 消息批处理：Kafka Producer将消息分组成批次，这样可以一次性发送多个消息，从而提高吞吐量。每个批次都有一个大小上限（由配置参数 batch.size 确定），当批次满了，或达到一定的延迟时（由 linger.ms 确定），Sender线程就会发送这些消息。 Sender线程：这是一个后台线程，不断地从 RecordAccumulator 中拉取待发送的批次，然后将它们发送到相应的Kafka Broker。发送完成后，它也会处理来自Broker的响应，例如确认消息是否已经成功写入。 消息确认：Kafka支持几种确认模式，这是由 acks 参数确定的。 acks=0：不进行ack确认。 acks=1：只要Leader收到消息，Producer就认为消息已成功发送。 acks=all 或 acks=-1：Leader + ISR列表里面的同步副本收到消息，Producer才会认为消息发送成功。 重试策略：如果消息发送失败，Kafka Producer会尝试重新发送。重试的次数和间隔是可以配置的。 关闭Producer：当你完成所有的消息发送并调用 close 方法时，它会确保所有待发送的消息都被发送出去，并且所有已发送的消息都得到了确认。 Send线程拉取参数 batch.size：只有数据积累到batch.size之后，sender才会发送数据。默认16k linger.ms：如果数据迟迟未达到batch.size，sender等待linger.ms设置的时间到了之后就会发送数据。单位ms，默认值是0ms，表示没有延迟 生产者重要参数 参数名称 描述 bootstrap.servers 生产者连接集群所需的 broker 地址清单，多个逗号隔开。注意这里并非需要所有的 broker 地址，因为生产者从给定的 broker 里查找到其他 broker 信息 key.serializer 和 value.serializer 指定发送消息的 key 和 value 的序列化类型。一定要写全类名 buffer.memory RecordAccumulator 缓冲区总大小，默认 32m batch.size 缓冲区一批数据最大值，默认 16k。适当增加该值，可以提高吞吐量，但是如果该值设置太大，会导致数据传输延迟增加 linger.ms 如果数据迟迟未达到 batch.size，sender 等待 linger.time 之后就会发送数据。单位 ms，默认值是 0ms，表示没有延迟。生产环境建议该值大小为 5-100ms 之间 acks 0：生产者发送过来的数据，不需要等数据落盘应答。 1：生产者发送过来的数据，Leader 收到数据后应答。 -1（all）：生产者发送过来的数据，Leader+和 isr 队列 里面的所有节点收齐数据后应答。默认值是-1，-1 和 all 是等价的。 max.in.flight.requests.per.connection 允许最多没有返回 ack 的次数，默认为 5，开启幂等性要保证该值是 1-5 的数字 retries 当消息发送出现错误的时候，系统会重发消息。retries 表示重试次数。默认是 int 最大值，2147483647。 如果设置了重试，还想保证消息的有序性，需要设置 MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION=1 否则在重试此失败消息的时候，其他的消息可能发送成功了 retry.backoff.ms 两次重试之间的时间间隔，默认是 100ms enable.idempotence 是否开启幂等性，默认 true，开启幂等性 compression.type 生产者发送的所有数据的压缩方式。默认是 none，也就是不压缩。 支持压缩类型：none、gzip、snappy、lz4 和 zstd 异步发送 异步发送概述：Kafka提供异步方式发送消息，主要特点是消息发送操作不会因等待Broker的确认而阻塞。 消息异步提交到 RecordAccumulator ：在异步模式下，Main线程调用 send 方法将消息提交消息到 RecordAccumulator 时是非阻塞的。这意味着Main线程可以迅速地继续其它任务，而不需等待消息实际被发送到Broker。 Sender线程：Sender线程在后台工作，负责从 RecordAccumulator 取出消息并发送至Kafka Broker。 非阻塞的优势：这种异步方式的主要优势是它提高了生产者的吞吐量和效率，因为Main线程不会因消息发送而被频繁阻塞。 项目添加依赖 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt; &lt;version&gt;3.2.0&lt;/version&gt;&lt;/dependency&gt; kafka-client 1234567891011121314151617181920212223242526272829303132333435@Slf4jpublic class ProductTest &#123; private KafkaProducer&lt;String, String&gt; kafkaProducer; @BeforeEach public void init() &#123; // 1. 创建 kafka 生产者的配置对象 Properties properties = new Properties(); // 2. 给 kafka 配置对象添加配置信息：bootstrap.servers properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, &quot;127.0.0.1:9092&quot;); // key,value 序列化（必须）：key.serializer，value.serializer properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;); properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;); // 3. 创建 kafka 生产者对象 kafkaProducer = new KafkaProducer&lt;&gt;(properties); &#125; /** * 异步发送 */ @Test public void send() &#123; for (int i = 0; i &lt; 5; i++) &#123; kafkaProducer.send(new ProducerRecord&lt;&gt;(&quot;first&quot;, &quot;hello &quot; + i)); &#125; kafkaProducer.close(); &#125;&#125; 带回调函数的异步发送 123456789101112131415161718/** * 带回调函数的异步发送 */@Testpublic void callBack() &#123; for (int i = 0; i &lt; 5; i++) &#123; kafkaProducer.send(new ProducerRecord&lt;&gt;(&quot;first&quot;, &quot;hello &quot; + i), new Callback() &#123; @Override public void onCompletion(RecordMetadata recordMetadata, Exception e) &#123; if (Objects.isNull(e)) &#123; log.info(&quot;主题：&#123;&#125;,分区：&#123;&#125;&quot;, recordMetadata.topic(), recordMetadata.partition()); &#125; &#125; &#125;); &#125; kafkaProducer.close();&#125; Spring Boot 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.kafka&lt;/groupId&gt; &lt;artifactId&gt;spring-kafka&lt;/artifactId&gt;&lt;/dependency&gt; 12345678spring: kafka: bootstrap-servers: localhost:9091 # 连接kafka的地址，多个地址用逗号分隔 producer: batch-size: 16384 # batch.size 批次大小，默认16k buffer-memory: 33554432 # RecordAccumulator 大小,默认32M key-serializer: org.apache.kafka.common.serialization.StringSerializer # 关键字的序列化类 value-serializer: org.apache.kafka.common.serialization.StringSerializer # 值的序列化类 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768@Slf4jpublic class ProductTest &#123; private KafkaProducer&lt;String, String&gt; kafkaProducer; @BeforeEach public void init() &#123; // 1. 创建 kafka 生产者的配置对象 Properties properties = new Properties(); // 2. 给 kafka 配置对象添加配置信息：bootstrap.servers properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, &quot;121.37.23.172:9092&quot;); // key,value 序列化（必须）：key.serializer，value.serializer properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;); properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;); // 3. 创建 kafka 生产者对象 kafkaProducer = new KafkaProducer&lt;&gt;(properties); &#125; /** * 异步发送 */ @Test public void send() &#123; for (int i = 0; i &lt; 5; i++) &#123; kafkaProducer.send(new ProducerRecord&lt;&gt;(&quot;first&quot;, i + &quot;&quot;)); &#125; kafkaProducer.close(); &#125; /** * 带回调函数的异步发送 */ @Test public void callBack() &#123; for (int i = 0; i &lt; 500; i++) &#123; kafkaProducer.send(new ProducerRecord&lt;&gt;(&quot;first&quot;, i+&quot;&quot;,&quot;hello &quot; + i), new Callback() &#123; @Override public void onCompletion(RecordMetadata recordMetadata, Exception e) &#123; if (Objects.isNull(e)) &#123; log.info(&quot;主题：&#123;&#125;,分区：&#123;&#125;&quot;, recordMetadata.topic(), recordMetadata.partition()); &#125; &#125; &#125;); &#125; kafkaProducer.close(); &#125; /** * 同步发送 */ @Test @SneakyThrows public void syncSend() &#123; for (int i = 0; i &lt; 5; i++) &#123; kafkaProducer.send(new ProducerRecord&lt;&gt;(&quot;first&quot;, &quot;hello &quot; + i)).get(); &#125; kafkaProducer.close(); &#125;&#125; 同步发送 同步发送概述：在Kafka的同步发送模式中，Main线程会在每次发送消息后阻塞，直到从Broker收到确认。 消息提交到 RecordAccumulator：Main线程首先将消息提交到 RecordAccumulator，这是一个内部缓冲区，用于存储待发送的消息。 Sender线程：Sender线程负责从 RecordAccumulator 中提取消息并发送到Kafka Broker。 阻塞等待确认：不像异步发送，同步发送会导致Main线程在消息被Sender线程发送并收到Broker的ACK之前一直阻塞。只有在收到ACK后，Main线程才会进行下一次发送或其他操作。 只需在异步发送的基础上，再调用一下 get()方法即可 123456789@Test@SneakyThrowspublic void syncSend() &#123; for (int i = 0; i &lt; 5; i++) &#123; kafkaProducer.send(new ProducerRecord&lt;&gt;(&quot;first&quot;, &quot;hello &quot; + i)).get(); &#125; kafkaProducer.close();&#125; 自定义拦截器 拦截器目的：Kafka的Producer拦截器提供了一种机制，允许用户在关键的消息发送阶段介入，以实现特定的自定义需求。 主要功能： 消息发送前处理：在消息实际被发送到Broker之前，拦截器的 onSend 方法会被调用。 ACK接收后处理：当Producer从Broker接收到一个ACK或消息发送失败时，onAcknowledgement 方法会被触发。 拦截链：Producer支持多个拦截器的配置。这些拦截器会按照配置的顺序依次处理消息，形成所谓的“拦截链”。 如何实现： 要创建自己的拦截器，用户需实现 ProducerInterceptor 接口。 此接口中有两个主要方法，分别是 onSend（在消息发送前调用）和 onAcknowledgement（在从Broker接收到响应前调用）。 如何配置：通过Producer的配置属性 interceptor.classes，用户可以指定一个或多个拦截器的完整类名。Producer在初始化时会按照配置的顺序加载并激活这些拦截器。 拦截器接口 1234567891011121314151617181920212223public interface ProducerInterceptor&lt;K, V&gt; extends Configurable &#123; /** * 获取配置信息或初始化数据时调用 */ public void configure(Map&lt;String, ?&gt; configs) /** * 该方法封装进KafkaProducer.send方法中，即它运行在用户主线程中。Producer确保在消息被序列化以及计算分区前调用该方法。用户可以在该方法中对消息做任何操作，但最好保证不要修改消息所属的topic和分区，否则会影响目标分区计算 */ public ProducerRecord&lt;K, V&gt; onSend(ProducerRecord&lt;K, V&gt; record); /** * 会在消息从RecordAccumulator成功发送到kafka broker之后，或者再发送过程中失败调用，并且通常都是在producer回调逻辑触发之前。它运行在producer的IO线程，因此不要放入很重的逻辑，否则会影响消息发送效率 */ public void onAcknowledgement(RecordMetadata metadata, Exception exception); /** * 关闭interceptor,主要用于执行一些资源释放 * interceptor 可能被运行在多个线程中，因此需要注意线程安全问题。如果指定多个interceptor,则producer将按照指定顺序调用它们，并仅仅是补货每个interceptor可能抛出的一场记录到错误日志中而非向上传递 */ public void close();&#125; 定义拦截器 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859** * 消息添加时间戳 */@Slf4jpublic class TimeInterceptor implements ProducerInterceptor&lt;String, String&gt; &#123; @Override public ProducerRecord&lt;String, String&gt; onSend(ProducerRecord&lt;String, String&gt; record) &#123; String value = record.value(); value = String.format(&quot;%s_%s&quot;, value, System.currentTimeMillis()); return new ProducerRecord&lt;String, String&gt;(record.topic(), value); &#125; @Override public void onAcknowledgement(RecordMetadata metadata, Exception exception) &#123; log.info(&quot;metadata: &#123;&#125;&quot;, JSON.toJSONString(metadata)); log.info(&quot;exception: &#123;&#125;&quot;, JSON.toJSONString(exception)); &#125; @Override public void close() &#123; &#125; @Override public void configure(Map&lt;String, ?&gt; configs) &#123; log.info(JSON.toJSONString(configs)); &#125;&#125;/** * 计数拦截器 */@Slf4jpublic class CountInterceptor implements ProducerInterceptor&lt;String, String&gt; &#123; private final AtomicInteger sendCount = new AtomicInteger(0); @Override public ProducerRecord&lt;String, String&gt; onSend(ProducerRecord&lt;String, String&gt; record) &#123; return record; &#125; @Override public void onAcknowledgement(RecordMetadata metadata, Exception exception) &#123; if (Objects.isNull(exception)) &#123; log.info(&quot;发送成功：&#123;&#125;&quot;, sendCount.incrementAndGet()); &#125; &#125; @Override public void close() &#123; &#125; @Override public void configure(Map&lt;String, ?&gt; configs) &#123; &#125;&#125; SpringBoot方式集成 123456789101112131415server: port: 9000spring: kafka: bootstrap-servers: 127.0.0.1:9092 # 连接kafka的地址，多个地址用逗号分隔 producer: retries: 0 # 若设置大于0的值，客户端会将发送失败的记录重新发送 batch-size: 16384 # 当将多个记录被发送到同一个分区时， Producer 将尝试将记录组合到更少的请求中。这有助于提升客户端和服务器端的性能。这个配置控制一个批次的默认大小（以字节为单位）。16384是缺省的配置 buffer-memory: 33554432 # #Producer 用来缓冲等待被发送到服务器的记录的总字节数，33554432是缺省配置 key-serializer: org.apache.kafka.common.serialization.StringSerializer # 关键字的序列化类 value-serializer: org.apache.kafka.common.serialization.StringSerializer # 值的序列化类 properties: interceptor.classes: com.wgf.interceptor.TimeInterceptor,com.wgf.interceptor.CountInterceptor # 支持多个，逗号隔开 Kafka-client方式集成 12345678910111213141516171819202122232425262728293031public class InterceptorTest &#123; private KafkaProducer&lt;String, String&gt; kafkaProducer; @BeforeEach public void init() &#123; // 1. 创建 kafka 生产者的配置对象 Properties properties = new Properties(); // 2. 给 kafka 配置对象添加配置信息：bootstrap.servers properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, &quot;127.0.0.1:9092&quot;); // key,value 序列化（必须）：key.serializer，value.serializer properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;); properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;); // 添加拦截器 List&lt;String&gt; interceptors = new ArrayList&lt;&gt;(); interceptors.add(&quot;com.wgf.interceptor.CountInterceptor&quot;); interceptors.add(&quot;com.wgf.interceptor.TimeInterceptor&quot;); properties.put(ProducerConfig.INTERCEPTOR_CLASSES_CONFIG, interceptors); // 3. 创建 kafka 生产者对象 kafkaProducer = new KafkaProducer&lt;&gt;(properties); &#125; @Test public void sendTest() &#123; kafkaProducer.send(new ProducerRecord&lt;&gt;(&quot;first&quot;, &quot;interceptor&quot;)); &#125;&#125; 自定义序列化 Kafka 自定义消息序列化和反序列化方式 生产者 需要实现序列化接口Serializer，序列化包含key和value的序列化，两者的序列化方式可不同，分别对应key.serializer和value.serializer 12345678910111213public interface Serializer&lt;T&gt; extends Closeable &#123; default void configure(Map&lt;String, ?&gt; configs, boolean isKey) &#123; &#125; byte[] serialize(String var1, T var2); default byte[] serialize(String topic, Headers headers, T data) &#123; return this.serialize(topic, data); &#125; default void close() &#123; &#125;&#125; 消费者 需要实现反序列化接口Deserializer，反序列化包含key和value的反序列化，两者的反序列化方式可不同，分别对应key.deserializer和value.deserializer 12345678910111213public interface Deserializer&lt;T&gt; extends Closeable &#123; default void configure(Map&lt;String, ?&gt; configs, boolean isKey) &#123; &#125; T deserialize(String var1, byte[] var2); default T deserialize(String topic, Headers headers, byte[] data) &#123; return this.deserialize(topic, data); &#125; default void close() &#123; &#125;&#125; 生产者分区 分区的目的：Kafka使用分区实现数据的水平扩展性、并行处理和容错能力。 存储和分发： 多Broker存储：分散存储和IO负载，Kafka将每个Topic的数据划分为多个分区，并将这些分区分布在多个Broker上。 负载均衡：通过合理地分配分区到各个Broker，可以有效地实现负载均衡，确保每个Broker承担相似的数据和流量。 提高并行度： 生产者并行发送：生产者可以同时向多个分区发送数据，从而提高消息的生产速率。 消费者并行消费：消费者组中的不同消费者实例可以并行地从不同分区消费数据，从而提高整体的数据消费速率。 分区策略 ProducerRecord 1234567891011public class ProducerRecord&lt;K, V&gt; &#123; // 该消息需要发往的主题 private final String topic; // 该消息需要发往的主题中的某个分区，如果该字段有值，则分区器不起作用，直接发往指定的分区 // 如果该值为null，则利用分区器进行分区的选择 private final Integer partition; private final Headers headers; // 如果partition字段不为null，则使用分区器进行分区选择时会用到该key字段，该值可为空 private final K key; private final V value; private final Long timestamp; 指定Partition 在发送消息时，如果明确指定了分区，则消息会直接发送到该指定分区。如果同时提供了 partition 和 key，则 partition 优先级高于 key。 12345678910111213 private Callback callback = (metadata, exception) -&gt; log.info(&quot;发送的分区是：&#123;&#125;&quot;, metadata.partition()); /** * 指定分区 */ @Test public void partitionTest() &#123; for (int i = 0; i &lt; 3; i++) &#123; kafkaProducer.send(new ProducerRecord&lt;&gt;(&quot;first&quot;, 0, null, &quot;hello&quot;), callback); &#125; &#125;//14:44:05.375 [kafka-producer-network-thread | producer-1] INFO com.wgf.kafka.PartitionTest - 发送的分区是：0//14:44:05.375 [kafka-producer-network-thread | producer-1] INFO com.wgf.kafka.PartitionTest - 发送的分区是：0//14:44:05.375 [kafka-producer-network-thread | producer-1] INFO com.wgf.kafka.PartitionTest - 发送的分区是：0 指定key 当发送消息时未显式指定 Partition，但提供了key，Kafka会使用 key 的MurmurHash值与Topic的Partition数量进行取模运算，以确定消息的目标Partition。 123456789101112 /** * 指定key */ @Test public void keyTest() &#123; for (int i = 0; i &lt; 3; i++) &#123; kafkaProducer.send(new ProducerRecord&lt;&gt;(&quot;first&quot;, &quot;aaa&quot;, &quot;hello&quot;), callback); &#125; &#125;//14:44:46.084 [kafka-producer-network-thread | producer-1] INFO com.wgf.kafka.PartitionTest - 发送的分区是：1//14:44:46.085 [kafka-producer-network-thread | producer-1] INFO com.wgf.kafka.PartitionTest - 发送的分区是：1//14:44:46.085 [kafka-producer-network-thread | producer-1] INFO com.wgf.kafka.PartitionTest - 发送的分区是：1 都不指定 若既未指定partition值，又未提供key值，Kafka将使用Sticky Partition策略。初始时，它随机选择一个整数作为起始点。对于随后的每次调用，此整数值会递增。为确定消息的目标Partition，该整数值会与Topic的可用Partition总数进行取模运算，这一策略类似于轮询（round-robin）算法。 1234567891011121314 /** * Partition和Key不指定则使用轮询 */ @Test @SneakyThrows public void roundRobinTest() &#123; for (int i = 0; i &lt; 3; i++) &#123; TimeUnit.SECONDS.sleep(1); kafkaProducer.send(new ProducerRecord&lt;&gt;(&quot;first&quot;, &quot;hello&quot;), callback); &#125; &#125;//14:47:26.704 [kafka-producer-network-thread | producer-1] INFO com.wgf.kafka.PartitionTest - 发送的分区是：0//14:47:27.700 [kafka-producer-network-thread | producer-1] INFO com.wgf.kafka.PartitionTest - 发送的分区是：1//14:47:28.685 [kafka-producer-network-thread | producer-1] INFO com.wgf.kafka.PartitionTest - 发送的分区是：0 自定义分区策略 在Kafka中，为满足特定的业务需求或数据路由逻辑，可以自定义分区策略。这可以通过实现 Partitioner 接口并覆写其方法来完成。虽然Kafka提供了 DefaultPartitioner 作为默认的分区策略，但有时候，为了实现更优的负载均衡或根据特定的数据属性将消息路由到不同的分区，自定义的分区策略是必要的。 12345678910111213141516171819202122232425public class MyPartitioner implements Partitioner &#123; // 返回的是第几个分区 @Override public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) &#123; // 获取topic的partitions List&lt;PartitionInfo&gt; partitionInfoList = cluster.partitionsForTopic(topic); int numPartitions = partitionInfoList.size(); // 根据key进行hash计算 int hashCode = key.hashCode(); int partition = Math.abs(hashCode % numPartitions); return partition; &#125; @Override public void close() &#123; &#125; @Override public void configure(Map&lt;String, ?&gt; configs) &#123; &#125;&#125; SpringBoot配置 1234567891011121314spring:kafka: bootstrap-servers: 127.0.0.1:9092 # 连接kafka的地址，多个地址用逗号分隔 producer: transaction-id-prefix: kafka-tx- retries: 1 # 若设置大于0的值，客户端会将发送失败的记录重新发送, 若开启事务则需要大于0 acks: -1 # 若开启事务，则必须设置为-1 batch-size: 16384 # 当将多个记录被发送到同一个分区时， Producer 将尝试将记录组合到更少的请求中。这有助于提升客户端和服务器端的性能。这个配置控制一个批次的默认大小（以字节为单位）。16384是缺省的配置 buffer-memory: 33554432 # #Producer 用来缓冲等待被发送到服务器的记录的总字节数，33554432是缺省配置 key-serializer: org.apache.kafka.common.serialization.StringSerializer # 关键字的序列化类 value-serializer: org.apache.kafka.common.serialization.StringSerializer # 值的序列化类 properties: enable.idempotence: true # 开启消息幂等 partitioner.class: com.wgf.partition.MyPartitioner # 自定义分区策略 kafka-client配置 123456789101112131415161718public void init() &#123; // 1. 创建 kafka 生产者的配置对象 Properties properties = new Properties(); // 2. 给 kafka 配置对象添加配置信息：bootstrap.servers properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, &quot;localhost:9092&quot;); // key,value 序列化（必须）：key.serializer，value.serializer properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;); properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;); // 自定义分区器 properties.put(ProducerConfig.PARTITIONER_CLASS_CONFIG, &quot;com.wgf.partition.MyPartitioner&quot;); // 3. 创建 kafka 生产者对象 kafkaProducer = new KafkaProducer&lt;&gt;(properties);&#125; 生产者提高吞吐量 由上图我们可知，batch.size，RecordAccumulator，linger.ms，配置的大小都会影响吞吐量 batch.size：控制批次大小，默认为16K。当批次大小增加时，每次发送的数据量也增加，从而提高吞吐量。 linger.ms：延迟等待时间，默认为0ms。适当增加这个值（例如，设置在5-100ms范围内）可以使批次中累积更多的消息，从而提高吞吐量。但是，这也意味着消息的延迟会增加。 RecordAccumulator：生产者用于缓存数据的总内存大小，默认为32M。在有大量的生产者分区或多个Topic的情况下，适当增大此值可以提高吞吐量。 compression.type：设置消息的压缩算法，可以提高网络吞吐量，但会增加CPU的使用，常用的压缩类型为 snappy。 SpringBoot配置 12345678910111213spring: kafka: bootstrap-servers: localhost:9092,localhost:9093,localhost:9094 # 连接kafka的地址，多个地址用逗号分隔 producer: acks: -1 # 若开启事务，则必须设置为-1 batch-size: 16384 # batch.size 批次大小，默认16k buffer-memory: 33554432 # RecordAccumulator 大小,默认32M compression-type: snappy # 开启压缩类型 key-serializer: org.apache.kafka.common.serialization.StringSerializer # 关键字的序列化类 value-serializer: org.apache.kafka.common.serialization.StringSerializer # 值的序列化类 properties: linger.ms: 1 enable.idempotence: true # 开启消息幂等 kafka-client配置 1234567891011121314151617181920212223242526272829303132public class ProducerThroughputTest &#123; private KafkaProducer&lt;String, String&gt; kafkaProducer; @BeforeEach public void init() &#123; // 1. 创建 kafka 生产者的配置对象 Properties properties = new Properties(); // 2. 给 kafka 配置对象添加配置信息：bootstrap.servers properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, &quot;localhost:9092&quot;); // key,value 序列化（必须）：key.serializer，value.serializer properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;); properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;); // batch.size：批次大小，默认 16K properties.put(ProducerConfig.BATCH_SIZE_CONFIG, 16384); // linger.ms：等待时间，默认 0 properties.put(ProducerConfig.LINGER_MS_CONFIG, 10); // RecordAccumulator：缓冲区大小，默认 32M：buffer.memory properties.put(ProducerConfig.BUFFER_MEMORY_CONFIG, 33554432); // compression.type：压缩，默认 none，可配置值 gzip、snappy、lz4 和 zstd properties.put(ProducerConfig.COMPRESSION_TYPE_CONFIG,&quot;snappy&quot;); // 3. 创建 kafka 生产者对象 kafkaProducer = new KafkaProducer&lt;&gt;(properties); &#125;&#125; 生产者消息可靠性保证 为了确保Producer发送的数据被可靠投递指定的Topic，Kafka引入了一个 acks 机制。当Topic的某个partition收到Producer发送的数据后，会根据 acks 的配置决定是否向Producer发送ACK响应。如果Producer收到 ACK，则认为数据已经被成功发送，否则会尝试重新发送。 生产者的ack机制 acks 描述 优点 缺点 0 不进行ack确认, 一旦Broker接收到消息就立即返回确认 数据吞吐量最大 当broker发生故障时，消息有可能会丢失 1 仅leader ack，当leader落盘成功后返回ack（默认配置） 平衡了性能和数据安全性 如果follower未完成同步时leader发生故障，消息可能丢失 -1(all) 所有的ISR (In-Sync Replicas) 成员都必须ack 数据安全性最高，只有当所有ISR成员都确认后才认为消息已写入 如果leader发生故障或网络问题，可能造成数据重复，原因是消息已写入，acks无法及时应答，Producer进行重试 SpringBoot 生产者ACK配置 123456spring: kafka: bootstrap-servers: 127.0.0.1:9092 # 连接kafka的地址，多个地址用逗号分隔 producer: acks: -1 retries: 3 # 开启ack一般配合重试次数使用,默认Integer最大值 kafka-client配置 1234567891011121314151617181920212223242526public class AckTest &#123; private KafkaProducer&lt;String, String&gt; kafkaProducer; @BeforeEach public void init() &#123; // 1. 创建 kafka 生产者的配置对象 Properties properties = new Properties(); // 2. 给 kafka 配置对象添加配置信息：bootstrap.servers properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, &quot;localhost:9092&quot;); // key,value 序列化（必须）：key.serializer，value.serializer properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;); properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;); // 设置ack properties.put(ProducerConfig.ACKS_CONFIG, &quot;-1&quot;); // 设置重试次数，默认是 int 最大值，2147483647 properties.put(ProducerConfig.RETRIES_CONFIG, 3); // 3. 创建 kafka 生产者对象 kafkaProducer = new KafkaProducer&lt;&gt;(properties); &#125;&#125; 数据去重 由上文的ACK应答机制可知，当消息发送失败会进行重试，那么重试就有可能导致数据重复 数据传递语义 At most once（最多一次）: 消息可能会丢失，但绝不会被传递多次。 生产者设置： acks=0: 不等待任何确认。 retries=0: 不进行重新发送。 At least once（最少一次）: 消息不会丢失，但可能会被传递多次。 要实现这种语义，可以考虑以下配置： 生产者设置： acks=1 或 acks=all: 确保至少leader副本或所有in-sync副本都已确认。 retries 设置为一个较大的值（如 Integer.MAX_VALUE）：确保消息在失败时会被重新发送。 消费者： 设置 enable.auto.commit=false 并手动提交offset，这样只有在消息被成功处理之后才提交offset。 Exactly once（仅一次）: 消息既不会丢失，也不会被重复传递。这是最强的传输保证，但也是最复杂的。 要实现这种语义，可以考虑以下配置： 生产者设置： acks=all: 确保所有in-sync副本都已确认。 enable.idempotence=true: 确保消息不会在网络故障等情况下被重复发送。 使用事务发送消息 消费者： 使用事务消费消息，并确保消费和生产是在同一事务中。 设置 enable.auto.commit=false 并手动提交offset。 消息幂等 幂等性在 Kafka 中确保了，无论生产者向 broker 发送多少次重复的消息，broker 端都只会持久化一条相同的消息，从而避免了数据的重复。 重复数据的判断基于以下标准：消息具有相同的 &lt;Producer ID, Partition, SeqNumber&gt; 组合将被视为重复的。其中： Producer ID：是由 Kafka 分配的唯一标识符。每次 Kafka 重启时都会为生产者分配一个新的 Producer ID。 Partition：表示该消息所在的分区号。 Sequence Number：是在每个分区中单调自增的数字，由生产者为每条消息分配。 只有当上述三个属性完全相同时，消息才被视为重复，Broker 在这种情况下只会持久化一条数据。 局限性 生产者重启时，ProducerID 会改变，而不同的Partition对应不同的 SequenceNumber。因此，Kafka的幂等性保证仅限于单个生产者会话。一旦生产者重启，之前的幂等性保证将不再有效，即Kafka只能保证会话内的幂等，而无法保证跨会话的幂等。 123456789101112131415server: port: 9000spring: kafka: bootstrap-servers: 127.0.0.1:9092 # 连接kafka的地址，多个地址用逗号分隔 producer: retries: 1 # 若设置大于0的值，客户端会将发送失败的记录重新发送，如果开启事务或者幂等，则必须大于0 acks: -1 # 若开启事务，则必须设置为-1 batch-size: 16384 # 当将多个记录被发送到同一个分区时， Producer 将尝试将记录组合到更少的请求中。这有助于提升客户端和服务器端的性能。这个配置控制一个批次的默认大小（以字节为单位）。16384是缺省的配置 buffer-memory: 33554432 # #Producer 用来缓冲等待被发送到服务器的记录的总字节数，33554432是缺省配置 key-serializer: org.apache.kafka.common.serialization.StringSerializer # 关键字的序列化类 value-serializer: org.apache.kafka.common.serialization.StringSerializer # 值的序列化类 properties: enable.idempotence: true # 开启消息幂等 生产者事务 当进行批量消息发送时，所有消息要么全部成功发布，要么全部不发布，确保数据的一致性。 开启事务，必须开启幂等性 在事务启用的情况下，Kafka 将 TransactionID 与 ProducerID 关联，从而确保在生产者重启的情况下也不会丢失 ProducerID，进一步强化了幂等性的持久保障。 跨分区跨会话事务 为了支持跨分区和跨会话的事务，Kafka 引入了一个全局唯一的 TransactionID。每当一个 Producer 启动事务时，它会获得一个特定的 ProducerID ，并将这个 ProducerID 与 TransactionID 进行绑定。这种设计确保了，即使 Producer 重启，也能够通过 TransactionID 重新获取原始的 ProducerID ，从而保持事务的连续性。 Kafka 为了管理和维护事务状态，引入了一个专门的组件：Transaction Coordinator（事务协调器）。Producer 与 Transaction Coordinator 交互以获得 TransactionID 和管理事务的生命周期。为了持久化和跟踪事务的状态，Transaction Coordinator 将事务的元数据写入 Kafka 的一个专用的内部 Topic。这样，即使 Kafka 集群中的节点重启，由于事务的状态被持久化，任何进行中的事务都可以从上次的状态恢复，保证事务的完整性。 事务API 123456789101112// 1 初始化事务void initTransactions();// 2 开启事务void beginTransaction() throws ProducerFencedException;// 3 在事务内提交已经消费的偏移量（主要用于消费者）void sendOffsetsToTransaction(Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets, String consumerGroupId) throws ProducerFencedException;// 4 提交事务void commitTransaction() throws ProducerFencedException;// 5 放弃事务（类似于回滚事务的操作）void abortTransaction() throws ProducerFencedException; 前置 transaction.state.log.replication.factor 的值必须大于等于 transaction.state.log.min.isr 的值：事务状态日志的副本数量必须大于等于最小副本同步因子。这是为了确保事务状态日志的可靠性和一致性。 transaction.state.log.replication.factor 的值必须小于等于 Kafka 集群中的可用 broker 数量：事务状态日志的副本数量不能超过 Kafka 集群中可用的 broker 数量。这是为了确保事务状态日志的复制和分布在 Kafka 集群中的可行性。 SpringBoot使用事务 springboot 事务配置 producer配置 12345678910111213spring: kafka: bootstrap-servers: 127.0.0.1:9092 # 连接kafka的地址，多个地址用逗号分隔 producer: transaction-id: my-transaction-id # 开启事务，设置事务id retries: 1 # 若设置大于0的值，客户端会将发送失败的记录重新发送, 若开启事务则需要大于0 acks: -1 # 若开启事务，则必须设置为-1 batch-size: 16384 # 当将多个记录被发送到同一个分区时， Producer 将尝试将记录组合到更少的请求中。这有助于提升客户端和服务器端的性能。这个配置控制一个批次的默认大小（以字节为单位）。16384是缺省的配置 buffer-memory: 33554432 # #Producer 用来缓冲等待被发送到服务器的记录的总字节数，33554432是缺省配置 key-serializer: org.apache.kafka.common.serialization.StringSerializer # 关键字的序列化类 value-serializer: org.apache.kafka.common.serialization.StringSerializer # 值的序列化类 properties: enable.idempotence: true # 开启消息幂等 consumer配置 1234567891011121314spring: kafka: bootstrap-servers: 127.0.0.1:9092 # 连接kafka的地址，多个地址用逗号分隔 consumer: enable-auto-commit: false # 参数设置成true。那么offset交给kafka来管理，offset进行默认的提交模式,置成false。那么就是Spring来替为我们做人工提交，从而简化了人工提交的方式.需要手动提交还需要指定ack-model properties: session.timeout.ms: 15000 key-deserializer: org.apache.kafka.common.serialization.StringDeserializer value-deserializer: org.apache.kafka.common.serialization.StringDeserializer auto-offset-reset: earliest listener: ack-mode: manual # 设置手动ack properties: isolation.level: read_committed # 置为 read_committed ，Consumer 仅读取已提交的消息， 否则不生效 使用事务 1234567891011121314151617181920212223242526// 第一种用法，注解@Transactionalpublic void send(boolean flag) &#123; this.kafkaTemplate.send(&quot;testTopic&quot;, &quot;before&quot;); if (flag) &#123; throw new RuntimeException(); &#125; this.kafkaTemplate.send(&quot;testTopic&quot;, &quot;after&quot;);&#125;// 第二种用法，手动操作public void send(boolean flag) &#123; kafkaTemplate.executeInTransaction(operations -&gt; &#123; this.kafkaTemplate.send(&quot;testTopic&quot;, &quot;before&quot;); if (flag) &#123; throw new RuntimeException(); &#125; this.kafkaTemplate.send(&quot;testTopic&quot;, &quot;after&quot;); return true; &#125;); &#125; kafka-client使用事务 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849public class TransactionTest &#123; private KafkaProducer&lt;String, String&gt; kafkaProducer; @BeforeEach public void init() &#123; // 1. 创建 kafka 生产者的配置对象 Properties properties = new Properties(); // 2. 给 kafka 配置对象添加配置信息：bootstrap.servers properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, &quot;localhost:9092&quot;); // key,value 序列化（必须）：key.serializer，value.serializer properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;); properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;); // 设置事务 id（必须），事务 id 任意起名 properties.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, &quot;test&quot;); // 3. 创建 kafka 生产者对象 kafkaProducer = new KafkaProducer&lt;&gt;(properties); &#125; @Test public void test() &#123; // 初始化事务 kafkaProducer.initTransactions(); // 开启事务 kafkaProducer.beginTransaction(); try &#123; for (int i = 0; i &lt; 6; i++) &#123; // 发送消息 kafkaProducer.send(new ProducerRecord&lt;&gt;(&quot;first&quot;, &quot;test &quot; + i)); &#125; // int i = 1 / 0; // 提交事务 kafkaProducer.commitTransaction(); &#125; catch (Exception e) &#123; // 终止事务 kafkaProducer.abortTransaction(); &#125; finally &#123; // 5. 关闭资源 kafkaProducer.close(); &#125; &#125;&#125; 数据有序 由于 Topic 可以分多个 Partition，Kafka为了保证其高吞吐量，只能保证单个 Partition 数据有序，不能保证多个 Partition 之间数据的有序性，如果要保证 Topic 数据的有序性有以下几种做法： 单分区顺序保证：当一个主题只有一个分区时，可以保证消息的全局有序性。因为只有一个分区，所有消息都按照发送顺序写入该分区，消费者可以按照相同的顺序读取消息，实现全局有序性。 有序分区键：当一个主题有多个分区时，可以使用有序分区键来实现相对有序性。有序分区键是根据业务需求生成的键，不同的业务数据类型（如订单类型）或业务相关键将路由到不同的分区。这样，同一类型的消息将被发送到同一分区，消费者可以按照分区顺序消费消息，实现相对有序性。 数据乱序 Kafka 数据分区有序性： Kafka 通过分区内的偏移量来保证单个分区内的消息有序。每个分区的消息都具有唯一的偏移量，它们按照偏移量的顺序被消费者读取。这确保了分区内的消息是有序的，不论是 Kafka 1.x 之前还是之后的版本。 max.in.flight.requests.per.connection 配置： 它控制了生产者在与 Kafka 服务器的连接上可以同时发送的未确认请求的最大数量。这个参数影响了生产者的性能、吞吐量以及消息的顺序性。 Kafka 1.x 之前版本： 在 Kafka 1.x 之前的版本中，为了保证数据的有序性，生产者通常需要将 max.in.flight.requests.per.connection 设置为 1，以确保每次只发送一个请求，避免乱序。 Kafka 1.x 及以后版本： 在 Kafka 1.x 以及之后的版本，有两种情况： 当未启用幂等性时，依然建议将 max.in.flight.requests.per.connection 设置为 1，以确保数据的有序性。 当启用幂等性时，可以将 max.in.flight.requests.per.connection 设置为小于或等于 5。这是因为 Kafka 服务端会缓存生产者发送的最近 5 个请求的元数据，无论如何，都可以保证这最近的 5 个请求的数据是有序的。 生产者可以根据需要对 max.in.flight.requests.per.connection 进行配置，以在吞吐量和有序性之间找到适当的平衡。 Kafka Broker ZK存储的Kafka信息 controller：负责管理集群Broker的上下线，所有Topic的分区副本分配和Leader的选举工作等 broker：服务节点信息，包括上线的节点id和Topic信息 Broker重要参数 参数名称 描述 replica.lag.time.max.ms ISR 中，如果 Follower 长时间未向 Leader 发送通 信请求或同步数据，则该 Follower 将被踢出 ISR。 该时间阈值，默认 30s auto.leader.rebalance.enable 默认是 true。 自动 Leader Partition 平衡 leader.imbalance.per.broker.percentage 默认是 10%。每个 broker 允许的不平衡的 leader 的比率。如果每个 broker 超过了这个值，控制器会触发 leader 的平衡 leader.imbalance.check.interval.seconds 默认值 300 秒。检查 leader 负载是否平衡的间隔时间 log.segment.bytes Kafka 中 log 日志是分成一块块存储的，此配置是 指 log 日志划分成块的大小，默认值 1G log.index.interval.bytes 默认 4kb，kafka 里面每当写入了 4kb 大小的日志 （.log），然后就往 index 文件里面记录一个索引 log.retention.hours Kafka 中数据保存的时间，默认 7 天 log.retention.minutes Kafka 中数据保存的时间，分钟级别，默认关闭 log.retention.ms Kafka 中数据保存的时间，毫秒级别，默认关闭 log.retention.check.interval.ms 检查数据是否保存超时的间隔，默认是 5 分钟 log.retention.bytes 默认等于-1，表示无穷大。超过设置的所有日志总大小，删除最早的 segment log.cleanup.policy 默认是 delete，表示所有数据启用删除策略； 如果设置值为 compact，表示所有数据启用压缩策略 num.io.threads 默认是 8。负责写磁盘的线程数。整个参数值要占总核数的 50% num.replica.fetchers 副本拉取线程数，这个参数占总核数的 50%的 1/3 num.network.threads 默认是 3。数据传输线程数，这个参数占总核数的 50%的 2/3 log.flush.interval.messages 强制页缓存刷写到磁盘的条数，默认是 long 的最大值，9223372036854775807。一般不建议修改， 交给系统自己管理 log.flush.interval.ms 每隔多久，刷数据到磁盘，默认是 null。一般不建议修改，交给系统自己管理 服役新节点 假设firstTopic有三个分区和三个副本分别分布在三台Broker上（0，1，2），这时候新服役了一台id为3的节点，如何把现有的分区和副本重新负载到这四台节点上 编写平衡主题脚本 1vim topics-to-move.json 12345678&#123; &quot;topics&quot;:[ &#123; &quot;topic&quot;:&quot;first&quot; &#125; ], &quot;version&quot;:1&#125; 生成一个负载均衡计划 1bin/kafka-reassign-partitions.sh --bootstrap-server localhost:9092 --topics-to-move-json-file topics-to-move.json --broker-list &quot;0,1,2,3&quot; --generate 这是控制台会输出均衡计划 12345Current partition replica assignment&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[0,2,1],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[2,1,0],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[1,0,2],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;]&#125;Proposed partition reassignment configuration&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[2,3,0],&quot;log_dirs&quot;[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[3,0,1],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[0,1,2],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;]&#125; 创建副本存储计划 将合适的计划复制到一个文件中 1vim increase-replication-factor.json 1&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[2,3,0],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[3,0,1],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[0,1,2],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;]&#125; 执行副本存储计划 1bin/kafka-reassign-partitions.sh --bootstrap-server localhost:9092 --reassignment-json-file increase-replication-factor.json --execute 验证副本存储计划 1bin/kafka-reassign-partitions.sh --bootstrap-server localhost:9092 --reassignment-json-file increase-replication-factor.json --verify 退役旧节点 上文服役了四台节点，现在假设要退役BrokerId为3的节点，操作和服役新节点流程差不多，只不过生成负载均衡计划的 --broker-list变成 &quot;0,1,2&quot;即可 Kafka副本 数据可靠性和高可用性：Kafka 副本确保当 Leader 副本宕机时，Follower 可以被选举为新的 Leader，保证服务可用性。 副本数量考量： 默认设置：Kafka 的默认副本数为 1。 生产推荐：在生产环境中，建议将副本数配置为 2 或更多，以确保数据冗余和可靠性。 权衡：尽管增加的副本数会导致更多的磁盘使用和网络传输，但选择合适的副本数量是在可靠性和资源使用之间的一个权衡。 副本角色： Leader：对于每个分区，Leader 负责处理所有的读写请求。 Follower：Follower 从 Leader 同步数据，并作为备份和故障转移角色。 AR Kafka 分区中的所有副本被统称为 AR（All Replicas）。 AR = ISR + OSR ISR ISR 是 Kafka 分区中与 Leader 副本保持同步的 Follower 副本集合。 如果 Follower 副本长时间未向 Leader 发送同步请求或同步数据，则它会从 ISR 中被移除。这个“长时间”是由 replica.lag.time.max.ms 参数控制的，默认值为 30 秒。 当 Leader 副本发生故障时，一个新的 Leader 将从 ISR 中被选举。 被踢出ISR列表的副本一般有如下原因： 慢副本： 描述：Follower 副本在一定的时间段内无法追赶 Leader 的速度。 原因：常见原因是 I/O 瓶颈，导致 Follower 向 Leader 追加复制的消息速度低于从 Leader 拉取的速度。 卡住的副本： 描述：Follower 副本在一定的时间段内停止从 Leader 拉取请求。 原因：可能是由于 GC 暂停、Follower 故障或者 Follower 进程死亡。 新启动副本： 描述：当为主题增加副本时，新的 Follower 副本最初不会在 ISR 中。 原因：一旦它们完全追赶了 Leader 的日志，将加入ISR OSR OSR 代表与 Leader 副本同步时存在较大延迟的 Follower 副本。 从 ISR 到 OSR： 触发条件：在 replica.lag.time.max.ms 时间范围内，若 Follower 的 LEO (Log End Offset) 没有追赶上该 partition 的 HW (High Watermark)，则该 Follower 被移至 OSR 列表。 从 OSR 到 ISR： 触发条件：当 OSR 列表中的 Follower 的 LEO 同步并追赶上该 partition 的 HW 时，该 Follower 重新被加入到 ISR 列表。 Leader选举流程 Broker 注册: 当 Kafka Broker 启动后，它向 ZooKeeper 的 /brokers/ids 路径注册其当前节点 ID。 Controller 节点选举： 在所有的Broker中选举一个Controller节点。Controller节点负责管理所有Partition的Leader选举。 第一个在 Zookerper 成功创建 /controller node的 Broker 将成为 Controller。 Controller 监听节点上下线： 选举出来的 Controller 会监听 /brokers 路径的变化。 Controller 选举 Leader： Controller 根据特定规则来决定 Partition 的 Leader。 选举规则是：在 ISR 列表中处于活跃状态，且按照 AR 列表的顺序排列。例如：如果 AR 是 [1,0,2] 且 ISR 是 [1,0,2]，那么 Leader 就会按照 1, 0, 2 的顺序来轮询选举。 选举结果上报 ZooKeeper： Controller 将选举的结果更新到 ZooKeeper 的 /brokers/topics/xxx/partitions/xxx 路径。 Broker 同步： 其他 Broker 会从 ZooKeeper 的相应路径同步选举结果。 Leader 宕机处理： 假设 Broker1 中的 Leader 宕机了。 Controller 会监听到这个节点的变化。 Controller 获取该 Partition 的 ISR 列表。 根据上述选举规则，选举新的 Leader。例如，如果 0 在 AR 列表中的位置较前，且在 ISR 中处于活跃状态，它就会被选为新的 Leader。 Controller 更新新的 Leader 和 ISR 列表。 验证 此时如果停止Broker为3的节点，那么Partition2的Leader将替换为1 重新启动Broker为3的节点，经过一段时间的 自动平衡 后，Leader将重新替换为3 故障处理 HW: （High Watermark） 俗称高水位，代表消费者可以安全读取的最后一条消息的位置。 因为HW之前的消息已被所有ISR节点确认过。 作用：确保消费者读取的是持久化的消息，保证数据一致性。 移动：当 ISR 副本都确认某个消息，HW往前移动。 LEO: （Log End Offset） 代表下条消息写入的位置。 作用：这个标识用于从 Leader 拉取要同步的数据。 如图所示，它代表一个日志文件，这个日志文件中有 8 条消息，第一条消息的offset（LogStartOffset）为0，最后一条消息的offset为7，offset为8代表下一条待写入的消息。分区的HW为5，表示消费者只能拉取到offset在0至4之间的消息，而offset为5的消息对消费者而言是不可见的，是为了保证数据存储的一致性。 Follower 故障 当 Follower 发生故障，它会被临时移出 ISR 列表。 在此期间，Leader 继续处理新的数据写入请求，而正常的 Follower 也继续从 Leader 同步数据。 一旦故障的 Follower 恢复，它首先会读取本地磁盘上记录的最近的 HW，并将 log 文件中高于此 HW 的记录截断。 该 Follower 从 HW 开始向 Leader 请求同步，此时它位于 OSR 列表中。 一旦 Follower 的 LEO 达到或超过该 Partition 的 HW，意味着 Follower 已经追赶上 Leader，它将重新被加入 ISR 列表。 Leader 故障 当 Leader 发生故障，新的 Leader 会从 ISR 中选举出来。选举基于 AR 列表的排序，并确保选举的是 ISR 中的成员。 为确保数据一致性，所有 Follower（包括新选举的 Leader）会截断其 log 文件中超出 HW 的部分。 随后，这些 Follower 会从新的 Leader 开始同步数据。 只能保证副本之间数据的一致性，消费数据一致性，不能保证数据是否丢失或不重复 分区副本分配 如果 Kafka 服务器只有 4 个节点，那么设置 Kafka 的分区数大于服务器台数，在 kafka底层如何分配存储副本呢 创建 16 分区，3 个副本 12345678910111213141516171819hadoop102Topic: second4 Partition: 0 Leader: 0 Replicas: 0,1,2 Isr: 0,1,2Topic: second4 Partition: 1 Leader: 1 Replicas: 1,2,3 Isr: 1,2,3Topic: second4 Partition: 2 Leader: 2 Replicas: 2,3,0 Isr: 2,3,0Topic: second4 Partition: 3 Leader: 3 Replicas: 3,0,1 Isr: 3,0,1Topic: second4 Partition: 4 Leader: 0 Replicas: 0,2,3 Isr: 0,2,3Topic: second4 Partition: 5 Leader: 1 Replicas: 1,3,0 Isr: 1,3,0Topic: second4 Partition: 6 Leader: 2 Replicas: 2,0,1 Isr: 2,0,1Topic: second4 Partition: 7 Leader: 3 Replicas: 3,1,2 Isr: 3,1,2Topic: second4 Partition: 8 Leader: 0 Replicas: 0,3,1 Isr: 0,3,1Topic: second4 Partition: 9 Leader: 1 Replicas: 1,0,2 Isr: 1,0,2Topic: second4 Partition: 10 Leader: 2 Replicas: 2,1,3 Isr: 2,1,3Topic: second4 Partition: 11 Leader: 3 Replicas: 3,2,0 Isr: 3,2,0Topic: second4 Partition: 12 Leader: 0 Replicas: 0,1,2 Isr: 0,1,2Topic: second4 Partition: 13 Leader: 1 Replicas: 1,2,3 Isr: 1,2,3Topic: second4 Partition: 14 Leader: 2 Replicas: 2,3,0 Isr: 2,3,0Topic: second4 Partition: 15 Leader: 3 Replicas: 3,0,1 Isr: 3,0,1 均匀分布 Leader: Kafka 会尽量确保每个 broker 上的 leader partition 数量均匀分布，以便平衡 broker 之间的读写负载。 错开 Leader: Kafka 在分配 leader 时，会尽量错开 broker，确保不同 partition 的 leader 分布在不同的 broker 上，以提升集群的容错性。 分散副本: Kafka 会尽量确保一个 partition 的所有副本分散在不同的 broker 上，避免因单个 broker 宕机导致的数据不可用。 手动调整分区副本分配 在生产环境中，每台服务器的配置和性能不一致，但是Kafka只会根据自己的代码规则创建对应的分区副本，就会导致个别服务器存储压力较大。所有需要手动调整分区副本的存储 创建一个新的topic，4个分区，两个副本，名称为three。将该topic的所有副本都存储到broker0和broker1两台服务器上 创建一个新的Topic 1bin/kafka-topics.sh --bootstrap-server localhost:9092 --create --partitions 4 --replication-factor 2 --topic three 查看分区副本存储情况 1bin/kafka-topics.sh --bootstrap-server localhost:9092 --describe --topic three 创建副本存储计划 所有副本都指定存储在 broker0、broker1 中 1vim increase-replication-factor.json 12345678910111213141516171819202122232425262728293031323334353637&#123; &quot;version&quot;:1, &quot;partitions&quot;:[ &#123; &quot;topic&quot;:&quot;three&quot;, &quot;partition&quot;:0, &quot;replicas&quot;:[ 0, 1 ] &#125;, &#123; &quot;topic&quot;:&quot;three&quot;, &quot;partition&quot;:1, &quot;replicas&quot;:[ 0, 1 ] &#125;, &#123; &quot;topic&quot;:&quot;three&quot;, &quot;partition&quot;:2, &quot;replicas&quot;:[ 1, 0 ] &#125;, &#123; &quot;topic&quot;:&quot;three&quot;, &quot;partition&quot;:3, &quot;replicas&quot;:[ 1, 0 ] &#125; ]&#125; 执行副本存储计划 1bin/kafka-reassign-partitions.sh --bootstrap-server localhost:9092 --reassignment-json-file increase-replication-factor.json --execute 验证副本存储计划 1bin/kafka-reassign-partitions.sh --bootstrap-server localhost:9092 --reassignment-json-file increase-replication-factor.json --verify Leader Partition自动平衡 在正常情况下，Kafka 会自动将 leader partition 均匀分散在各个 broker 上，以保证每台机器的读写吞吐量均衡。然而，当一些 broker 宕机时，leader partition 可能会过度集中在少数活跃的 broker 上，导致这些 broker 承受较高的读写请求压力。与此同时，重启后的 broker 将作为 follower partition，它们主要负责与 leader 同步数据，处理的读写请求相对较少，从而导致集群的负载不均衡。 自动平衡参数 参数名称 描述 auto.leader.rebalance.enable 控制是否自动进行 Leader Partition 平衡。默认值为 true。尽管平衡可以增强集群的健壮性，但在生产环境中，频繁的 leader 重选举可能会带来性能影响。因此，建议根据实际情况考虑将其设置为 false 或调整其他相关参数来减少影响。 leader.imbalance.per.broker.percentage 定义每个 broker 上允许的不平衡 leader 比例。默认值为 10%。当任意 broker 的不平衡 leader 比例超过此值时，控制器会触发 leader 重新分配过程。 leader.imbalance.check.interval.seconds 控制检查 leader 分布是否平衡的时间间隔。默认值为 300 秒。 平衡算法 分区2的AR优先副本是0节点，但是0节点却不是Leader节点，所以不平衡数加1，AR副本总数是4，所以broker0节点不平衡率为1/4&gt;10%，需要再平衡 增加副本因子 先创建一个三个分区一个副本的Topic 1sh kafka-topics.sh --bootstrap-server localhost:9092 --create --topic test --partitions 3 --replication-factor 1 查看Topic 创建存储计划 1vim increase-replication-factor.json 1234567891011121314151617181920212223242526272829303132&#123; &quot;version&quot;:1, &quot;partitions&quot;:[ &#123; &quot;topic&quot;:&quot;test&quot;, &quot;partition&quot;:0, &quot;replicas&quot;:[ 1, 2, 3 ] &#125;, &#123; &quot;topic&quot;:&quot;test&quot;, &quot;partition&quot;:1, &quot;replicas&quot;:[ 1, 2, 3 ] &#125;, &#123; &quot;topic&quot;:&quot;test&quot;, &quot;partition&quot;:2, &quot;replicas&quot;:[ 1, 2, 3 ] &#125; ]&#125; 执行副本存储计划 1sh kafka-reassign-partitions.sh --bootstrap-server localhost:9092 --reassignment-json-file increase-replication-factor.json --execute Kafka文件存储机制 每个Topic包含一个或多个Partition，每个Partition在物理存储层面对应一个文件夹。在这个文件夹下存储了Partition的 数据 和 索引 文件。Partition内部的消息是有序的，但不保证多个Partition之间消息的顺序。 Topic是一类消息的集合，每条消息都需要指定一个Topic。在物理层面上，一个Topic会被划分为一个或多个Partition，每个Partition会有多个副本分布在不同的Broker中。 Partition在存储层面是由一系列 append log 文件组成，发布到此Partition的消息会被追加到log文件的尾部。这种 顺序写入 磁盘的方式较随机写入效率更高。每条消息在log文件中的位置由一个长整数型的offset（偏移量）表示，该offset唯一标识了一条消息。 每个Partition的log文件会被分割成多个 Segment，每个Segment大小为1GB，并由 log 文件和 index 文件组成。 消费者保存的唯一元数据是 offset 值，该值由消费者完全控制，保存在一个特殊的Topic或Zookeeper中。维度是consumer-group.topic.partition。 不同于传统的消息队列，Kafka集群会保留所有消息，而不是在消费后立即删除，以优化IO操作。由于磁盘空间的限制，Kafka不可能永久保留所有消息，消息的保存期限可以通过配置来指定。 消息索引过程 log 文件和 index 文件的命名规则都是基于该文件中第一条消息的 offset。具体地，文件名是这个起始 offset 的长整数形式。这种命名方式便于在多个 log 文件中快速定位到包含特定 offset 的消息的文件。 稀疏索引：每当 log 文件增加了约4KB的数据，index 文件就会为这部分数据生成一条索引记录，这是稀疏的方式，而不是为 log 文件中的每一条消息都生成索引。 索引过程： 通过偏移量定位到Partition目录下对应的index文件（文件名也是索引） 通过index文件定位到指定offset消息的log文件偏移量（index是相对偏移量，可降低文件大小） 通过index文件提供的偏移量读取log文件进行遍历（最坏情况下至多遍历4KB） 通过工具查看文件内容 1sh kafka-run-class.sh kafka.tools.DumpLogSegments --files ./00000000000000000000.log 文件清理策略 Kafka 中默认的日志保存时间为 7 天，可以通过调整如下参数修改保存时间 参数 说明 log.retention.hours 最低优先级小时，默认 7 天(168) log.retention.minutes 分钟级别 log.retention.ms 最高优先级毫秒 log.retention.check.interval.ms 负责设置检查周期，默认 5 分钟 log.retention.bytes 所有日志段的总大小上限。一旦达到这个大小，最旧的日志段将被删除 Kafka 中提供的日志清理策略有 delete(删除) 和 compact（压缩） 两种 delete log.cleanup.policy = delete 所有数据启用删除策略 基于时间：默认打开。使用每个 Segment 中最大时间戳作为该日志段的时间戳。当消息在Kafka中存在的时间超过一定阈值时，Kafka将删除旧的日志段以释放磁盘空间。 基于大小：当所有日志段的总大小超过设置的限制时，Kafka删除最早的日志段，以控制磁盘使用。 如果一个 segment 中有一部分数据过期，一部分没有过期，怎么处理? 等待 timeindex 文件的所有数据过期直接删除整个文件 compact compact日志压缩：对于相同key的不同value值，只保留最后一个版本 log.cleanup.policy = compact 所有数据启用压缩策略 压缩后的offset可能是不连续的，比如上图中没有6，当从这些offset消费消息时，将会拿到比这个offset大的offset对应的消息，实际上会拿到offset为7的消息，并从这个位置开始消费。 这种策略适合一些特殊的场景，比如数据同步，同一条数据只保留最新版本。 数据高效读写 集群，分片技术 Kafka本身就是一个支持分布式的集群系统。 Topic采用分区技术，将多个分区分布在不同的Broker上。 顺序写盘 Kafka的producer负责生产数据，并将其顺序地追加到log文件的尾部。这种顺序写的方式在性能上有明显优势。据官方数据显示，相同的磁盘在顺序写的情况下能达到600M/s的速度，而随机写的速度仅为100K/s。这种性能差异主要是因为顺序写减少了磁头寻址的时间，而这与磁盘的机械结构有直接关系。 稀疏索引 在读取数据时利用稀疏索引，使得在大量的log文件中能迅速定位到数据对应的文件和该数据在文件中的偏移量位置。 页缓存和零拷贝 PageCache（页缓存）：Kafka高度依赖操作系统提供的PageCache机制。当发生写操作时，数据首先被写入到PageCache中，而不是直接写入磁盘。读操作先从PageCache中搜索数据；如果PageCache中没有所需的数据，系统才会从磁盘读取。简言之，PageCache将大部分的空闲内存利用作为磁盘缓存。 零拷贝：数据不会从内核空间拷贝到用户空间。实际上，在数据传输时，数据直接从内核空间复制到网卡，避免了不必要的数据复制操作。 Producer的零拷贝 消息存储：当你使用Kafka Producer发送消息时，消息首先被放入 RecordAccumulator。这是一个内部缓冲区，它负责批处理和组织消息，以便于高效发送。 准备发送：Kafka的 Sender 线程会从 RecordAccumulator 中提取批量消息。这些消息通常被组织为按目标分区和Broker的顺序。 零拷贝技术的实现：在传统的数据发送方法中，数据需要从应用程序的用户空间被拷贝到操作系统的内核空间，然后再从内核空间被拷贝到 Socket。这涉及至少两次数据拷贝。但是，使用零拷贝技术，Kafka可以直接从 RecordAccumulator（用户空间）发送数据到 Socket（内核空间），避免中间的拷贝步骤。 这主要是通过Java的 FileChannel.transferTo 方法实现的。这允许数据从文件或缓冲区直接传输到网络套接字，绕过了额外的用户空间和内核空间之间的数据拷贝。 Broker的零拷贝 PageCache与磁盘交互：当数据被写入Kafka Broker或从Broker读取时，它们首先写入操作系统的 PageCache。这是操作系统为文件系统提供的一个高速缓存。 实现：Kafka使用Java的NIO（非阻塞I/O）来实现这种零拷贝技术。特别是，它使用 FileChannel.transferTo 方法来从 PageCache 中直接将数据拷贝到 Socket。 Kafka消费者 消费者组重要参数 参数名称 描述 bootstrap.servers 向 Kafka 集群建立初始连接用到的 host/port 列表 key.deserializer和value.deserializer 指定接收消息的 key 和 value 的反序列化类型。一定要写全类名 group.id 标记消费者所属的消费者组 enable.auto.commit 默认值为 true，消费者会自动周期性地向服务器提交偏移量 auto.commit.interval.ms 如果设置了 enable.auto.commit 的值为 true， 则该值定义了消费者偏移量向 Kafka 提交的频率，默认 5s auto.offset.reset 当 Kafka 中没有初始偏移量或当前偏移量在服务器中不存在 （如，数据被删除了），该如何处理？ earliest：当各分区下有已提交的offset时，从提交的offset开始消费；无提交的offset时，从头开始消费latest：默认，当各分区下有已提交的offset时，从提交的offset开始消费；无提交的offset时，消费新产生的该分区下的数据none：如果消费组原来的（previous）偏移量不存在，则向消费者抛异常anything：向消费者抛异常 offsets.topic.num.partitions __consumer_offsets 的分区数，默认是 50 个分区 heartbeat.interval.ms Kafka 消费者和 coordinator 之间的心跳时间，默认 3s。 该条目的值必须小于 session.timeout.ms ，也不应该高于 session.timeout.ms 的 1/3 session.timeout.ms Kafka 消费者和 coordinator 之间连接超时时间，默认 45s。 超过该值，该消费者被移除，消费者组执行再平衡 max.poll.interval.ms 消费者处理消息的最大时长，默认是 5 分钟。超过该值，该 消费者被移除，消费者组执行再平衡 fetch.min.bytes 默认 1 个字节。消费者获取服务器端一批消息最小的字节数 fetch.max.wait.ms 默认 500ms。如果没有从服务器端获取到一批数据的最小字节数。该时间到，仍然会返回数据 fetch.max.bytes 默认 Default: 52428800（50 m）。消费者获取服务器端一批 消息最大的字节数。如果服务器端一批次的数据大于该值 （50m）仍然可以拉取回来这批数据，因此，这不是一个绝 对最大值。一批次的大小受 message.max.bytes （broker config）or max.message.bytes （topic config）影响 max.poll.records 一次 poll 拉取数据返回消息的最大条数，默认是 500 条。 消费者组原理 Consumer Group (CG): 定义为一组具有相同 groupId 的消费者集合。 在消费者组内，每个消费者专门负责消费某个或某些分区的数据。这确保了同一消息只会被组内的一个消费者处理。 不同的消费者组间是独立的，它们的消费活动不会相互干扰。 每个消费者必须关联到一个消费者组，可以将消费者组视作逻辑上的订阅者。 为了确保有效的消费，消费者组中的消费者数量不应该超过其所消费 Topic 的分区数量。超过时，部分消费者将会处于空闲状态。 具有相同 groupId 的消费者自动归入同一消费者组。 消费者组初始化流程 coordinator 协调器 作用 消费者组管理: 再平衡: 如果消费者组内的成员关系发生变化（例如，新消费者加入、现有消费者离开或失败），coordinator 负责触发再均衡，重新分配分区给组内的消费者。 offset管理: coordinator 负责记录和存储消费者组内消费者所消费的消息的偏移量，这样消费者在失败后重启时可以从上次的位置继续消费。 心跳检测: 通过消费者发送的心跳，coordinator 能够监控消费者的健康状态。如果在指定时间内没有收到某消费者的心跳，coordinator 会认为该消费者已经死亡并触发再均衡。 事务管理 事务状态维护: coordinator 负责维护与生产者事务相关的状态和元数据。 幂等性: 为了确保生产者重试不会导致数据的重复写入，coordinator 与生产者协作确保消息的幂等性。 跨分区的事务原子性: 在涉及多个分区的事务中，coordinator 确保这些分区中的所有写操作要么全部成功，要么全部失败。 协调器选择: 每个消费者组都有一个与之关联的 coordinator，这个 coordinator 是 Kafka 集群中的某个 broker。Kafka 的设计确保了协调器的负载均衡，每个 broker 都可以成为 coordinator。 Coordinator选择：coordinator节点选择 = groupId的hashCode值 % __consumer_offsets的分区数。例如，若__consumer_offsets的分区数为50，且groupId的hashCode为1，则1 % 50 = 1。假设第1分区的Leader位于Broker1上，则Broker1将被选为该消费者组的coordinator节点。此消费者组内的所有消费者在提交offset时都会向此协调分区提交。 初始化流程 寻找协调器 (Coordinator): 当一个消费者启动并尝试加入消费者组时，它首先需要找到负责其消费者组的协调器。消费者会向任意一个 Broker 发送 FindCoordinator 请求来找到负责其 groupId 的协调器。 发送 JoinGroup 请求: 消费者找到协调器后，会发送一个 JoinGroup 请求来加入消费者组。 如果这是一个全新的 groupId（即组内没有任何消费者），则协调器会立即接受此消费者作为组的首个成员。如果消费者组已经存在，消费者需要等待协调器触发重新平衡来加入组。 消费者组 Leader 选举: 对于消费者组中的所有成员，协调器会选择一个作为 leader。leader 的职责是根据消费者的数量和主题的分区来确定 分区的分配策略。 由Coordinator自动选举，第一个发出 JoinGroup 的 Broker 当选。 分区分配: 协调器会向消费者组 Leader 提供组内所有消费者的信息。leader 根据指定的分区分配策略将分区分配给组内的消费者，并将分配的结果返回给协调器。 同步消费者: 协调器将分区的分配结果发送给组内的所有消费者。这是通过 SyncGroup 请求完成的。 开始消费: 一旦消费者收到其分配的分区，它会开始从指定的 offset 开始消费消息。 维持心跳: 为了告诉协调器它仍然是活跃的，并且正在消费消息，消费者会定期发送心跳。如果在指定的会话超时时间内，协调器没有收到来自消费者的心跳，它会认为该消费者已经失败，并可能触发再平衡。 再平衡 心跳与会话超时：消费者确实会向其coordinator发送心跳以表示它仍然活跃。heartbeat.interval.ms 定义了发送心跳的频率，而 session.timeout.ms 定义了协调器等待心跳的时间。如果在 session.timeout.ms （45s）时间内，coordinator没有收到心跳，消费者将被认为已经死亡。 最大轮询间隔：max.poll.interval.ms (5分钟)定义了消费者可以在两次 poll() 调用之间的最长时间。如果超过此时间，消费者会被认为是不活跃的，并从组中移除，从而可能触发再平衡。 消费者的加入和离开：消费者的加入和离开确实都可能触发再平衡。频繁的再平衡不仅可能影响Kafka集群的性能，而且可能导致消息处理的延迟。 消费者拉取流程 初始化： 当消费者启动时，它会创建一个NetworkClient实例，消费者与Kafka建立网络连接。 查找Coordinator： 在发送拉取请求之前，消费者首先需要找到Consumer Group的Coordinator，以便正确跟踪偏移量和管理组成员关系。 Partition分配： 如果是新的消费者或者消费者组重新平衡后，消费者会通过Coordinator确定自己要消费哪些Topic的哪些分区。 发送拉取请求： 一旦知道要从哪些分区拉取，消费者会使用sendFetches方法发送请求。请求中指定了Topic、分区和开始的偏移量。 Broker处理： 当Broker接收到拉取请求后，它会开始检查自己是否有满足请求条件的数据。Broker会等待，直至数据量达到1KB至50MB（默认500条数据），或达到了预设的超时时间。 数据接收与缓存： 消费者接收到数据后，会将其暂存到内部的队列中，此队列主要用于暂存数据，以便逐条处理。 数据处理： 从内部队列中拉取数据，并开始处理。如果数据处理出现异常，消费者可以配置重试策略或直接跳过。 偏移量提交： 为了避免重复消费或消息遗失，消费者在处理完消息后，会将当前的偏移量提交回Coordinator。这样，在下一次启动或者重新平衡后，消费者知道从哪里开始拉取数据。 消费者API kafka-client 监听Topic 1234567891011121314151617181920212223242526272829303132333435363738394041424344@Slf4j// 订阅主题测试public class TopicTest&#123; protected KafkaConsumer kafkaConsumer; @BeforeEach public void init() &#123; // 1. 创建 kafka 生产者的配置对象 Properties properties = new Properties(); // 2. 给 kafka 配置对象添加配置信息：bootstrap.servers properties.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, &quot;localhost:9092&quot;); // key,value 反序列化（必须） properties.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName()); properties.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName()); // 配置消费者组id properties.put(ConsumerConfig.GROUP_ID_CONFIG, &quot;test&quot;); // 创建消费者对象 kafkaConsumer = new KafkaConsumer&lt;String, String&gt;(properties); &#125; @Test public void listenerTest() &#123; // 注册要消费的主题（可以消费多个主题） ArrayList&lt;String&gt; topics = new ArrayList&lt;&gt;(); topics.add(&quot;first&quot;); kafkaConsumer.subscribe(topics); // 拉取数据打印 while (true) &#123; // 设置 1s 中消费一批数据 ConsumerRecords&lt;String, String&gt; consumerRecords = kafkaConsumer.poll(Duration.ofSeconds(1)); // 打印数据 for (ConsumerRecord&lt;String, String&gt; record : consumerRecords) &#123; log.info(record.toString()); &#125; &#125; &#125;&#125; 监听指定分区 123456789101112131415161718192021222324252627282930313233343536373839@Slf4jpublic class PartitionTest &#123; protected KafkaConsumer kafkaConsumer; @BeforeEach public void init() &#123; // 1. 创建 kafka 生产者的配置对象 Properties properties = new Properties(); // 2. 给 kafka 配置对象添加配置信息：bootstrap.servers properties.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, &quot;localhost:9092&quot;); // key,value 反序列化（必须） properties.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName()); properties.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName()); // 配置消费者组id properties.put(ConsumerConfig.GROUP_ID_CONFIG, &quot;test&quot;); // 创建消费者对象 kafkaConsumer = new KafkaConsumer&lt;String, String&gt;(properties); &#125; @Test public void listenerPartitionTest() &#123; // 消费某个主题的某个分区数据 ArrayList&lt;TopicPartition&gt; topicPartitions = new ArrayList&lt;&gt;(); topicPartitions.add(new TopicPartition(&quot;first&quot;, 0)); kafkaConsumer.assign(topicPartitions); while (true)&#123; ConsumerRecords&lt;String, String&gt; consumerRecords = kafkaConsumer.poll(Duration.ofSeconds(1)); for (ConsumerRecord&lt;String, String&gt; consumerRecord : consumerRecords) &#123; log.info(consumerRecord.toString()); &#125; &#125; &#125;&#125; Spring Boot 添加依赖 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.kafka&lt;/groupId&gt; &lt;artifactId&gt;spring-kafka&lt;/artifactId&gt;&lt;/dependency&gt; 1234567891011server: port: 9001spring: kafka: bootstrap-servers: localhost:9092 # 连接kafka的地址，多个地址用逗号分隔 consumer: properties: session.timeout.ms: 15000 key-deserializer: org.apache.kafka.common.serialization.StringDeserializer value-deserializer: org.apache.kafka.common.serialization.StringDeserializer 监听Topic 12345678@Slf4j@Componentpublic class ListenerTopic &#123; @KafkaListener(groupId = &quot;test&quot;, topics = &quot;first&quot;) public void listener(ConsumerRecord&lt;String, String&gt; record) &#123; log.info(&quot;监听消息：&#123;&#125;&quot;, record.topic(), record.value()); &#125;&#125; 监听指定指定分区 123456789@Slf4j@Componentpublic class ListenerPartition &#123; @KafkaListener(groupId = &quot;test&quot;, topicPartitions = @TopicPartition(topic = &quot;first&quot;, partitions = &quot;0&quot;)) public void listener(String msg) &#123; log.info(&quot;监听消息：&#123;&#125;&quot;, msg); &#125;&#125; 分区分配策略 Topic 可以被划分为多个分区，一个消费组内可以有多个消费者，因此分区和消费者之间是多对多的关系。为了实现分区的合理分配，必然需要一种策略将 Topic 的分区分配给对应的消费者进行消费。 需要注意的是，消费者的上下线会触发分区分配的再平衡过程，该过程会根据所选的分区分配策略重新分配分区，以保证分区数据的正常消费。 Range（默认，范围） 12345678910111213141516171819assign(topic, consumers) &#123; // 对分区和Consumer进行排序 List&lt;Partition&gt; partitions = topic.getPartitions(); sort(partitions); sort(consumers); // 计算每个Consumer分配的分区数 int numPartitionsPerConsumer = partition.size() / consumers.size(); // 额外有一些Consumer会多分配到分区 int consumersWithExtraPartition = partition.size() % consumers.size(); // 计算分配结果 for (int i = 0, n = consumers.size(); i &lt; n; i++) &#123; // 第i个Consumer分配到的分区的index int start = numPartitionsPerConsumer * i + Math.min(i, consumersWithExtraPartition); // 第i个Consumer分配到的分区数 int length = numPartitionsPerConsumer + (i + 1 &gt; consumersWithExtraPartition ? 0 : 1); // 分装分配结果 assignment.get(consumersForTopic.get(i)).addAll(partitions.subList(start, start + length)); &#125;&#125; Kafka的 Range 分区分配策略是消费者 默认 的分区分配策略。它的工作原理是为每个消费者分配连续的分区范围。 工作原理： 排序：首先，所有的Topic的分区和消费者都会被排序。 分区是基于Topic名称和分区号进行排序的，通常是字典序。 消费者是基于它们的消费者ID进行排序的。 分区分配：每个Topic下的分区都会按照消费者数量进行划分。每个消费者首先会获得 总分区数 ÷ 总消费者数 的分区。然后，如果存在余数，会按照排序的顺序，将额外的分区分配给前面的消费者。 例子： 假设我们有一个Topic A，它有8个分区：A-0, A-1, ... A-7。现在有3个消费者 C1, C2, C3。 按照 Range 策略，首先将分区和消费者进行排序。在这种情况下，分区和消费者已经是有序的。 接下来，8个分区被3个消费者平均分配。这意味着每个消费者应该处理2到3个分区。 8 ÷ 3 = 2 与余数 2。因此，每个消费者最初获得2个分区，但由于有2个额外的分区，它们会被分配给C1和C2。 结果是：C1 负责处理 A-0, A-1, A-2；C2 负责处理 A-3, A-4, A-5；而 C3 只处理 A-6, A-7。 数据倾斜 按照Topic维度分配 在 Range 分配策略下，当处理单一Topic时，如果分区数不能被消费者数整除，额外的分区分配给前面的消费者，但这种差异相对较小。然而，当消费者组订阅了多个这样的Topic时，这种不均衡的情况会被放大。具体地说，对于每一个这样的Topic，排序靠前的消费者会多被分配一个分区。随着Topic数量的增加，这种不均衡会累积，导致排序靠前的消费者，被分配到的分区数远多于其他消费者，从而产生明显的数据倾斜。 适用场景 单 Topic 场景：当只有一个 Topic 时，Range 分区策略可以保证分区间的负载均衡，每个消费者处理的分区数量相近。 Topic 分区数量都能整除消费者数量：在多个 Topic 的情况下，如果 Topic 的数量和分区数量能整除消费者数量，Range 分区策略也能够保证负载均衡。 RoundRobin（轮询） RoundRobin 轮询分区策略，主要特点是以轮询的方式平均分配分区，确保每个消费者处理的分区数量大致相同，从而实现负载均衡。 工作原理: 首先，将所有的Topic分区和消费者分别进行排序。 然后，依次遍历每个Topic的分区，并按照轮询的方式分配给消费者。 例子 假设有两个Topic：TopicA有3个分区(A0, A1, A2)，TopicB有2个分区(B0, B1)，同时有两个消费者C1和C2。根据RoundRobin策略，分区分配如下： C1分配到分区：A0, A2, B1 C2分配到分区：A1, B0 轮询分区，为每个消费者分配 数据倾斜 假设有三个Topic：T1（拥有三个分区）、T2（一个分区）和T3（一个分区）。一个消费者组内包含三个消费者：C0、C1和C2。其中，C0订阅了T1、T2和T3；C1和C2只订阅了T1。在这种情况下，由于RoundRobin分配策略的机制，C0会承担更多的分区消费任务，从而可能导致数据倾斜问题。 适用场景 多Topic：适用于多个Topic的场景，可以更均匀地分配各个Topic的分区，减少数据倾斜的可能性。 负载均衡：通过轮询的方式，确保每个消费者分配到的分区数量大致相同，从而实现负载均衡。 RoundRobin分配策略适合消费组内消费者订阅的Topic列表是相同的，在这种情况下，通过轮询分配可以确保各个消费者尽可能平均地分配到分区。 Sticky（粘性） Sticky 分区分配策略是为了解决消费者上下线时引起的分区再平衡。尽可能保持之前的分配关系避免再平衡带来的性能损耗。 首次分配：识别每个消费者可以接受的分区数量的上下限，然后尽量均衡分配 维持当前的分配：当需要再平衡时，策略首先考虑维持当前的分区-消费者关系。 最小化重新分配：如果消费者组发生变化，该策略会尽量只重新分配受影响的分区，而不是所有分区。 适用场景 消费者频繁上下线场景，可以通过参考上次分配的结果减少调整分配的变动 分区策略配置 Kafka提供的分区策略 org.apache.kafka.clients.consumer.RangeAssignor org.apache.kafka.clients.consumer.RoundRobinAssignor org.apache.kafka.clients.consumer.StickyAssignor kafka-client 12345678910111213141516171819202122232425262728@Slf4jpublic class PartitionAssignorTest &#123; protected KafkaConsumer kafkaConsumer; @BeforeEach public void init() &#123; // 1. 创建 kafka 生产者的配置对象 Properties properties = new Properties(); // 2. 给 kafka 配置对象添加配置信息：bootstrap.servers properties.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, &quot;localhost:9091,localhost:9092,localhost:9093&quot;); // key,value 反序列化（必须） properties.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName()); properties.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName()); // 配置消费者组id properties.put(ConsumerConfig.GROUP_ID_CONFIG, &quot;test&quot;); // 分区分配策略 //properties.put(ConsumerConfig.PARTITION_ASSIGNMENT_STRATEGY_CONFIG, RangeAssignor.class.getName()); properties.put(ConsumerConfig.PARTITION_ASSIGNMENT_STRATEGY_CONFIG, RoundRobinAssignor.class.getName()); //properties.put(ConsumerConfig.PARTITION_ASSIGNMENT_STRATEGY_CONFIG, StickyAssignor.class.getName()); // 创建消费者对象 kafkaConsumer = new KafkaConsumer&lt;String, String&gt;(properties); &#125;&#125; Spring Boot 123456789spring: kafka: bootstrap-servers: localhost:9092 # 连接kafka的地址，多个地址用逗号分隔 consumer: properties: partition.assignment.strategy: org.apache.kafka.clients.consumer.RoundRobinAssignor # 分区策略 session.timeout.ms: 15000 key-deserializer: org.apache.kafka.common.serialization.StringDeserializer value-deserializer: org.apache.kafka.common.serialization.StringDeserializer 自动提交offset 为了简化offset管理，从而让开发者更加专注于业务逻辑，Kafka提供了自动提交offset的功能。 关于自动提交offset的相关参数： enable.auto.commit：决定是否启用自动提交offset功能，默认值为true。 auto.commit.interval.ms：定义了两次自动提交之间的时间间隔，默认是5000毫秒（即5秒）。 自动提交存在的问题 数据丢失：当消息刚被消费，但还未在业务逻辑中完全处理结束时，如果到达自动提交周期，offset就会被自动提交。此时，如果消费者宕机，那么在重启并重新消费处理时，可能会导致数据丢失。 数据重复：如果消息已经被成功消费并处理，但在自动提交offset的周期到达前发生宕机，那么offset将不会被自动提交。因此，在重启服务后，系统将根据上一次提交的offset重新消费消息，这就导致了消息的重复消费。 kafka-client 12345678910111213141516171819202122232425262728@Slf4jpublic class AutoCommitTest &#123; protected KafkaConsumer kafkaConsumer; @BeforeEach public void init() &#123; // 1. 创建 kafka 生产者的配置对象 Properties properties = new Properties(); // 2. 给 kafka 配置对象添加配置信息：bootstrap.servers properties.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, &quot;localhost:9091,localhost:9092,localhost:9093&quot;); // key,value 反序列化（必须） properties.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName()); properties.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName()); // 是否自动提交 offset properties.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, true); // 提交 offset 的时间周期 1000ms，默认 5s properties.put(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG, 1000); // 配置消费者组id properties.put(ConsumerConfig.GROUP_ID_CONFIG, &quot;test&quot;); // 创建消费者对象 kafkaConsumer = new KafkaConsumer&lt;String, String&gt;(properties); &#125;&#125; Spring Boot 12345678spring: kafka: bootstrap-servers: localhost:9092 # 连接kafka的地址，多个地址用逗号分隔 consumer: enable-auto-commit: true # 开启自动提交（默认） auto-commit-interval: 1000 # 自动提交间隔时间（默认5秒） key-deserializer: org.apache.kafka.common.serialization.StringDeserializer value-deserializer: org.apache.kafka.common.serialization.StringDeserializer 手动提交offset 虽然自动提交offset十分简单便利，但由于其是基于时间周期性提交，开发人员难以精准把握offset提交的时机。因此，Kafka还提供了手动提交offset的API。 手动提交offset的方法主要有两种：分别是commitSync（同步提交）和commitAsync（异步提交）。两者的共同点在于，都会提交消费者成功处理的最后一条消息的偏移量 + 1。 commitSync（同步提交）：这种方式会阻塞当前线程，直到offset提交成功或失败。如果提交失败，可以选择重试。因为该方法会返回提交成功或失败的结果，可以在需要确保offset准确性的场景下使用。 commitAsync（异步提交）：这种方式在提交offset时不会阻塞当前线程，同时可以提供一个回调函数来处理提交成功或失败的情况。由于是异步的，所以性能较好，但需要注意处理可能出现的提交失败的情况。 kafka-client 同步提交 123456789101112131415161718192021222324252627282930313233343536373839404142434445@Slf4jpublic class CommitSyncTest &#123; protected KafkaConsumer kafkaConsumer; @BeforeEach public void init() &#123; // 1. 创建 kafka 生产者的配置对象 Properties properties = new Properties(); // 2. 给 kafka 配置对象添加配置信息：bootstrap.servers properties.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, &quot;localhost:9091,localhost:9092,localhost:9093&quot;); // key,value 反序列化（必须） properties.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName()); properties.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName()); // 取消自动提交 properties.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, false); // 配置消费者组id properties.put(ConsumerConfig.GROUP_ID_CONFIG, &quot;test&quot;); // 创建消费者对象 kafkaConsumer = new KafkaConsumer&lt;String, String&gt;(properties); &#125; @Test public void listenerPartitionTest() &#123; // 消费某个主题的某个分区数据 ArrayList&lt;TopicPartition&gt; topicPartitions = new ArrayList&lt;&gt;(); topicPartitions.add(new TopicPartition(&quot;first&quot;, 0)); kafkaConsumer.assign(topicPartitions); while (true)&#123; ConsumerRecords&lt;String, String&gt; consumerRecords = kafkaConsumer.poll(Duration.ofSeconds(1)); for (ConsumerRecord&lt;String, String&gt; consumerRecord : consumerRecords) &#123; log.info(consumerRecord.toString()); &#125; // 手动同步提交 kafkaConsumer.commitSync(); &#125; &#125;&#125; 手动异步提交 123456789101112131415161718192021222324252627282930313233343536373839404142434445@Slf4jpublic class CommitASyncTest &#123; protected KafkaConsumer kafkaConsumer; @BeforeEach public void init() &#123; // 1. 创建 kafka 生产者的配置对象 Properties properties = new Properties(); // 2. 给 kafka 配置对象添加配置信息：bootstrap.servers properties.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, &quot;localhost:9091,localhost:9092,localhost:9093&quot;); // key,value 反序列化（必须） properties.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName()); properties.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName()); // 取消自动提交 properties.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, false); // 配置消费者组id properties.put(ConsumerConfig.GROUP_ID_CONFIG, &quot;test&quot;); // 创建消费者对象 kafkaConsumer = new KafkaConsumer&lt;String, String&gt;(properties); &#125; @Test public void listenerPartitionTest() &#123; // 消费某个主题的某个分区数据 ArrayList&lt;TopicPartition&gt; topicPartitions = new ArrayList&lt;&gt;(); topicPartitions.add(new TopicPartition(&quot;first&quot;, 0)); kafkaConsumer.assign(topicPartitions); while (true)&#123; ConsumerRecords&lt;String, String&gt; consumerRecords = kafkaConsumer.poll(Duration.ofSeconds(1)); for (ConsumerRecord&lt;String, String&gt; consumerRecord : consumerRecords) &#123; log.info(consumerRecord.toString()); &#125; // 手动异步提交 kafkaConsumer.commitAsync(); &#125; &#125;&#125; Spring Boot 关闭手动提交：spring.kafka.consumer.enable-auto-commit: false 配置监听应答模式：spring.kafka.listener.ack-mode: manual 123456789spring: kafka: bootstrap-servers: localhost:9092 # 连接kafka的地址，多个地址用逗号分隔 consumer: enable-auto-commit: false # 关闭自动提交 key-deserializer: org.apache.kafka.common.serialization.StringDeserializer value-deserializer: org.apache.kafka.common.serialization.StringDeserializer listener: ack-mode: manual 123456789101112131415161718192021@Slf4j@Componentpublic class TestListener &#123; @KafkaListener(topics = &quot;first&quot;, groupId = &quot;test&quot;) public void testTopic(String message, Acknowledgment acknowledgment, Consumer&lt;String, String&gt; consumer) &#123; log.info(&quot;接收到消息：&#123;&#125;&quot;, message); // 手动同步提交 // consumer.commitSync(); // 手动异步提交 // consumer.commitAsync(); // ACK手动提交offset, 如果整合了spring boot 建议使用这种 // acknowledgment.acknowledge(); // 不进行ack确认,让监听器线程休眠指定毫秒后重新开始消费此条消息 // acknowledgment.nack(100); &#125;&#125; offset消费策略 auto.offset.reset 可选值： earliest、 latest、none。默认设置为 latest。 策略 说明 earliest 当指定分区有已提交的offset时，从提交的offset开始消费；如果没有提交的offset，则从分区起始位置开始消费。 latest 当指定分区有已提交的offset时，从提交的offset开始消费；如果没有提交的offset，则只消费该分区新产生的数据。 none 如果指定分区没有已提交的offset，则会抛出异常，不会从任何位置开始消费。 kafka-client 123456789101112131415161718192021222324252627282930313233343536373839404142434445@Slf4jpublic class AutoOffsetResetTest &#123; protected KafkaConsumer kafkaConsumer; @BeforeEach public void init() &#123; // 1. 创建 kafka 生产者的配置对象 Properties properties = new Properties(); // 2. 给 kafka 配置对象添加配置信息：bootstrap.servers properties.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, &quot;localhost:9091,localhost:9092,localhost:9093&quot;); // key,value 反序列化（必须） properties.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName()); properties.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName()); // 指定offset策略 properties.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, &quot;earliest&quot;); // 配置消费者组id properties.put(ConsumerConfig.GROUP_ID_CONFIG, &quot;test&quot;); // 创建消费者对象 kafkaConsumer = new KafkaConsumer&lt;String, String&gt;(properties); &#125; @Test public void listenerTest() &#123; // 注册要消费的主题（可以消费多个主题） ArrayList&lt;String&gt; topics = new ArrayList&lt;&gt;(); topics.add(&quot;first&quot;); kafkaConsumer.subscribe(topics); // 拉取数据打印 while (true) &#123; // 设置 1s 中消费一批数据 ConsumerRecords&lt;String, String&gt; consumerRecords = kafkaConsumer.poll(Duration.ofSeconds(1)); // 打印数据 for (ConsumerRecord&lt;String, String&gt; record : consumerRecords) &#123; log.info(record.toString()); &#125; &#125; &#125;&#125; Spring Boot 1234567891011spring: kafka: bootstrap-servers: localhost:9092 # 连接kafka的地址，多个地址用逗号分隔 consumer: auto-offset-reset: earliest # offset 消费策略 enable-auto-commit: false # 开启自动提交（默认） auto-commit-interval: 1000 # 自动提交间隔时间（默认5秒） key-deserializer: org.apache.kafka.common.serialization.StringDeserializer value-deserializer: org.apache.kafka.common.serialization.StringDeserializer listener: ack-mode: manual 如何重新消费队列数据 更换新的消费者组 设置 auto.offset.reset 为 earliest；重新从头消费 指定offset消费 异常恢复 任意指定 offset 位移开始消费，需要对每个分区进行设置 kafka-client 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061@Slf4jpublic class SpecifyOffsetTest &#123; protected KafkaConsumer kafkaConsumer; @BeforeEach public void init() &#123; // 1. 创建 kafka 生产者的配置对象 Properties properties = new Properties(); // 2. 给 kafka 配置对象添加配置信息：bootstrap.servers properties.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, &quot;localhost:9091,localhost:9092,localhost:9093&quot;); // key,value 反序列化（必须） properties.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName()); properties.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName()); // 配置消费者组id properties.put(ConsumerConfig.GROUP_ID_CONFIG, &quot;test2&quot;); // 创建消费者对象 kafkaConsumer = new KafkaConsumer&lt;String, String&gt;(properties); // 2 订阅一个主题 ArrayList&lt;String&gt; topics = new ArrayList&lt;&gt;(); topics.add(&quot;first&quot;); kafkaConsumer.subscribe(topics); // 获取分区信息 Set&lt;TopicPartition&gt; assignment = new HashSet&lt;&gt;(); while (assignment.size() == 0) &#123; kafkaConsumer.poll(Duration.ofSeconds(1)); // 获取消费者分区分配信息（有了分区分配信息才能开始消费） assignment = kafkaConsumer.assignment(); &#125; // 遍历所有分区，并指定 offset 从 100 的位置开始消费 for (TopicPartition tp : assignment) &#123; kafkaConsumer.seek(tp, 100); &#125; &#125; @Test public void listenerTest() &#123; // 注册要消费的主题（可以消费多个主题） ArrayList&lt;String&gt; topics = new ArrayList&lt;&gt;(); topics.add(&quot;first&quot;); kafkaConsumer.subscribe(topics); // 拉取数据打印 while (true) &#123; // 设置 1s 中消费一批数据 ConsumerRecords&lt;String, String&gt; consumerRecords = kafkaConsumer.poll(Duration.ofSeconds(1)); // 打印数据 for (ConsumerRecord&lt;String, String&gt; record : consumerRecords) &#123; log.info(&quot;topic：&#123;&#125;，offset：&#123;&#125;，value：&#123;&#125;&quot;, record.topic(), record.offset(), record.value()); &#125; &#125; &#125;&#125; Spring Boot 123456789101112131415@Slf4j@Componentpublic class SpecifyOffsetListener &#123; @KafkaListener(groupId = &quot;test3&quot;, topics = &quot;first&quot;, topicPartitions = &#123; @TopicPartition(topic = &quot;first&quot;, partitionOffsets = &#123; @PartitionOffset(partition = &quot;0&quot;, initialOffset = &quot;500&quot;), @PartitionOffset(partition = &quot;1&quot;, initialOffset = &quot;500&quot;), @PartitionOffset(partition = &quot;2&quot;, initialOffset = &quot;500&quot;) &#125;) &#125;) public void listener(ConsumerRecord&lt;String, String&gt; record) &#123; log.info(&quot;topic：&#123;&#125;，offset：&#123;&#125;，value：&#123;&#125;&quot;, record.topic(), record.offset(), record.value()); &#125;&#125; 指定时间消费offset 异常恢复 在生产环境中，会遇到最近消费的几个小时数据异常，想重新按照时间消费。例如要求按照时间消费前一天的数据，怎么处理？ 从Kafka 0.10.0版本开始，每条消息（即每个record）都有一个关联的时间戳。时间戳可以是消息创建时间（默认）或消息追加到log时的时间。 CreateTime：这是默认值。时间戳代表消息创建的时间。通常，它是在生产者客户端设置的，代表消息被发送之前的时间。 LogAppendTime：时间戳代表消息被追加到log的时间。这是在broker上设置的，代表消息被写入日志的时间。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162@Slf4jpublic class TimeOffsetListener &#123; protected KafkaConsumer kafkaConsumer; @BeforeEach public void init() &#123; // 1. 创建 kafka 生产者的配置对象 Properties properties = new Properties(); // 2. 给 kafka 配置对象添加配置信息：bootstrap.servers properties.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, &quot;localhost:9091,localhost:9092,localhost:9093&quot;); // key,value 反序列化（必须） properties.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName()); properties.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName()); // 指定offset策略 properties.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, &quot;earliest&quot;); // 配置消费者组id properties.put(ConsumerConfig.GROUP_ID_CONFIG, &quot;test&quot;); // 创建消费者对象 kafkaConsumer = new KafkaConsumer&lt;String, String&gt;(properties); // 注册要消费的主题（可以消费多个主题） ArrayList&lt;String&gt; topics = new ArrayList&lt;&gt;(); topics.add(&quot;first&quot;); kafkaConsumer.subscribe(topics); &#125; /** * kafka 提供基于时间获取offset的API */ @Test public void offsetForTimesTest() &#123; // 获取分区信息 Set&lt;TopicPartition&gt; assignment = new HashSet&lt;&gt;(); while (assignment.size() == 0) &#123; kafkaConsumer.poll(Duration.ofSeconds(1)); // 获取消费者分区分配信息（有了分区分配信息才能开始消费） assignment = kafkaConsumer.assignment(); &#125; HashMap&lt;TopicPartition, Long&gt; timestampToSearch = new HashMap&lt;&gt;(); // 封装集合存储，每个分区对应一天前的数据 for (TopicPartition topicPartition : assignment) &#123; timestampToSearch.put(topicPartition, System.currentTimeMillis() - 1 * 24 * 3600 * 1000); &#125; // 获取从 1 天前开始消费的每个分区的 offset Map&lt;TopicPartition, OffsetAndTimestamp&gt; offsets = kafkaConsumer.offsetsForTimes(timestampToSearch); offsets.forEach((k ,v) -&gt; &#123; log.info(&quot;partition：&#123;&#125;&quot;, k.partition()); log.info(&quot;offset：&#123;&#125;&quot;, v.offset()); &#125;); &#125; // 根据时间获取到每个分区对应的offset，再调用kafkaConsumer.seek(...)，设置每个分区的offset即可&#125; SpringBoot 123456789101112131415161718192021222324252627282930313233343536@Servicepublic class KafkaTimeBasedConsumerService &#123; @Autowired private ConsumerFactory&lt;String, String&gt; consumerFactory; // 这里使用了Java 8的Instant类作为时间的表示 public void consumeFromTimestamp(String topic, Instant timestampToStartFrom) &#123; try (Consumer&lt;String, String&gt; consumer = consumerFactory.createConsumer()) &#123; List&lt;PartitionInfo&gt; partitionInfos = consumer.partitionsFor(topic); List&lt;TopicPartition&gt; topicPartitions = partitionInfos.stream() .map(partitionInfo -&gt; new TopicPartition(partitionInfo.topic(), partitionInfo.partition())) .collect(Collectors.toList()); Map&lt;TopicPartition, Long&gt; timestampToSearch = topicPartitions.stream() .collect(Collectors.toMap(Function.identity(), tp -&gt; timestampToStartFrom.toEpochMilli())); Map&lt;TopicPartition, OffsetAndTimestamp&gt; offsets = consumer.offsetsForTimes(timestampToSearch); if (offsets != null) &#123; consumer.assign(offsets.keySet()); offsets.forEach((tp, ot) -&gt; &#123; if (ot != null) &#123; consumer.seek(tp, ot.offset()); &#125; &#125;); ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofMillis(1000)); for (ConsumerRecord&lt;String, String&gt; record : records) &#123; // Process your record here System.out.println(&quot;Received Message with timestamp: &quot; + record.timestamp() + &quot; Value: &quot; + record.value()); &#125; &#125; &#125; &#125;&#125; 消费者事务 实现Kafka的精确一次（Exactly Once Semantics, EOS）消费需要综合考虑生产、消费、处理以及偏移量存储的方面。 生产者： 开启幂等性，enable.idempotence=true。 使用事务，transactional.id=&lt;some-unique-id&gt;。 消费者： 为了消费那些已提交的事务消息，消费者需要将 isolation.level 配置设置为 read_committed。这样，消费者只会读取已提交事务的消息，并忽略未提交的消息。 默认情况下，isolation.level 是 read_uncommitted，这意味着消费者会读取所有消息，无论它们是否在事务中。 手动提交 offset 消费者与Offset管理： 当消费者处理事务消息时，为了确保EOS，它们需要在处理消息的同时，也在同一个事务中提交offset。 这确保了处理消息和提交offset是原子操作，从而实现了end-to-end的EOS。 同步处理和外部系统： 如果消费者的处理逻辑涉及与外部系统的交互，那么需要确保这种交互是幂等的。 例如，如果消费者需要将消息数据写入一个数据库，那么这个写操作应该是可以安全地重复的，以确保在面对失败和重试时不会有不一致的情况。 数据积压 消费能力不足 当Kafka遭遇消费能力不足导致数据积压时，可以通过以下策略来解决： 增加Topic的分区数以提高并行处理的能力。 增加消费组内的消费者数量。为了实现最佳的负载均衡，消费者的数量应该等于或小于分区数。 注意，仅增加分区数而不增加消费者数量（或反之）可能不会显著提高消费速度。两者应协同工作以实现最佳效果。 数据处理不及时 参数 描述 fetch.max.bytes fetch.max.bytes 是 Kafka 消费者的一个配置参数，其默认值为 52428800（约为 50MB）。这个参数定义了消费者在单次拉取操作中可以从Kafka服务器获取的消息的最大字节数。 max.poll.records 一次 poll 拉取数据返回消息的最大条数，默认是 500 条 当Kafka的数据积压是由于下游处理不及时引起的，可以考虑以下策略来解决： 增加每批次从Kafka拉取的消息数量，从而提高处理的吞吐量。 分析并优化数据处理的效率，确保处理速度至少与生产速度相匹配。 EFAK 安装 java 环境变量配置 EFAK 安装 Kafka调优 调优篇 Kraft模式 Kraft模式搭建 左图为 Kafka 现有架构，元数据在 zookeeper 中，运行时动态选举 controller，由 controller 进行 Kafka 集群管理。右图为 kraft 模式架构（实验性），不再依赖 zookeeper 集群， 而是用三台 controller 节点代替 zookeeper，元数据保存在 controller 中，由 controller 直接进 行 Kafka 集群管理 这样做的好处有以下几个 Kafka 不再依赖外部框架，而是能够独立运行 controller 管理集群时，不再需要从 zookeeper 中先读取数据，集群性能上升 由于不依赖 zookeeper，集群扩展时不再受到 zookeeper 读写能力限制 controller 不再动态选举，而是由配置文件规定。这样我们可以有针对性的加强controller 节点的配置，而不是像以前一样对随机 controller 节点的高负载束手无策 接入外部系统 Flume Flink Spark","categories":[],"tags":[]},{"title":"Redis","slug":"Redis","date":"2022-02-04T09:33:58.000Z","updated":"2023-12-06T04:01:06.190Z","comments":true,"path":"2022/02/04/Redis/","link":"","permalink":"https://wugengfeng.cn/2022/02/04/Redis/","excerpt":"","text":"思维导图 前言 文章源码 NoSql NoSQL（Not Only SQL）数据库主要指在设计上不遵循传统关系型数据库的模式。它包括了一系列不同类型的数据存储方式，不仅仅局限于简单的 key-value 存储模型。NoSQL 数据库的类型包括文档型、列存储、图形数据库等。 不遵循 SQL 标准。 大多不支持 ACID事务。 特定领域远超于 SQL 的性能。 适用场景 高并发读写处理能力 海量数据处理 数据的高可扩展性 不适用场景 需要强事务支持的场景： 对于那些需要严格的事务一致性和复杂事务管理的应用，NoSQL数据库可能不是最佳选择。虽然一些NoSQL数据库提供了一定程度的事务支持，但它们无法与传统的关系型数据库（如PostgreSQL或MySQL）相比，后者提供更全面、更复杂的事务管理能力，如ACID事务。 复杂SQL查询和结构化数据的场景： NoSQL数据库通常不支持复杂的SQL查询和那些需要高度结构化和关联数据的应用场景。对于需要执行复杂的连接操作、子查询、存储过程和视图等SQL特性的场景，传统的关系型数据库是更合适的选择，因为它们原生支持这些功能，能多更好地处理复杂的数据关系和结构化查询。 Redis是什么 Redis（Remote Dictionary Server，远程字典服务器）是一个完全开源且免费的、用C语言编写的基于内存的高性能键值（Key/Value）存储系统。遵循BSD协议，Redis不仅作为分布式内存数据库获得广泛应用，同时也是支持持久化的NoSQL数据库。由于其灵活的数据结构和高效的性能，Redis被广泛认为是当前最受欢迎的NoSQL数据库之一，并常被称作数据结构服务器。 Redis与其他KV系统相比具有以下独特特点： 数据持久化：Redis支持将内存中的数据持久化到磁盘中，通过RDB（快照）或AOF（追加文件）两种方式实现。这使得Redis在重启后能够重新加载并使用之前的数据。 丰富的数据类型：不同于普通的键值存储，Redis提供了多种数据结构，如列表（list）、集合（set）、有序集合（zset）、散列（hash）等，增强了其存储和处理数据的灵活性。 数据备份和主从复制：Redis支持数据备份，以及通过主从（master-slave）模式实现数据的复制。这增强了数据的可用性和容错能力，使Redis在分布式系统中更为可靠。 Redis能做什么 缓存系统：使用Redis作为数据缓存，减少数据库的读取压力，提高系统响应速度。这是Redis最常见的用途之一。 序列生成器：用于构建分布式系统下全局唯一的序列号。 分布式锁：在多个进程或服务间同步资源访问时，Redis可以用作分布式锁的实现工具。 分布式任务队列：在多个服务间分配任务，当没任务时线程阻塞。 排行榜/计分板：使用Redis的有序集合（Sorted Sets），可以方便地实现排行榜或计分板等功能。 实时计数器：Redis的原子操作特性使其适合于实现如网站访问量、在线用户数等实时计数功能。 地理空间数据处理：Redis的Geo类型支持地理空间数据的存储和查询，适用于地理位置服务。 Redis为什么这么快 高性能服务器并非必须依赖多线程：经常存在一个误区，认为高性能服务器一定需要通过多线程来实现。其实并不一定，首先需要明确，对于CPU、内存和硬盘的速度和工作机制有基本的了解是非常重要的，服务器的性能不仅仅取决于是否采用多线程。 多线程不一定比单线程更高效：另一个误解是多线程在所有情况下都比单线程更有效率。实际上，多线程编程涉及复杂性和额外的性能开销，如线程管理和上下文切换。而单线程模型，通过避免这些问题，在特定情况下可以提供更优的性能。 内存存储：Redis将所有数据保存在内存中，内存访问速度远快于磁盘。这种内存中数据处理机制使得Redis能够提供极快的数据读写速度。 单线程架构：虽然Redis是单线程的，但这恰恰使其避免了常见的多线程编程问题，如线程间的上下文切换和锁竞争。单线程也意味着在处理请求时几乎没有任何内存锁定的开销。 I/O多路复用：Redis使用非阻塞I/O多路复用技术（线程模型）。这意味着Redis可以在单个线程中同时处理多个网络连接，提高I/O操作的效率。 Redis单实例理论QPS为 8W (来源：阿里云redis 参考值) 五大基础数据类型 文档来源 key 序号 命令及描述 1 DEL key 该命令用于在 key 存在时删除 key。 UNLINK key 非阻塞删除Key，异步删除 2 DUMP key 序列化给定 key ，并返回被序列化的值。 3 EXISTS key 检查给定 key 是否存在。 4 EXPIRE key seconds 为给定 key 设置过期时间。 5 EXPIREAT key timestamp EXPIREAT 的作用和 EXPIRE 类似，都用于为 key 设置过期时间。 不同在于 EXPIREAT 命令接受的时间参数是 UNIX 时间戳(unix timestamp)。 6 PEXPIRE key milliseconds 设置 key 的过期时间以毫秒计。 7 PEXPIREAT key milliseconds-timestamp 设置 key 过期时间的时间戳(unix timestamp) 以毫秒计 8 KEYS pattern 查找所有符合给定模式( pattern)的 key 。 9 MOVE key db 将当前数据库的 key 移动到给定的数据库 db 当中。 10 PERSIST key 移除 key 的过期时间，key 将持久保持。 11 PTTL key 以毫秒为单位返回 key 的剩余的过期时间。 12 TTL key 以秒为单位，返回给定 key 的剩余生存时间(TTL, time to live)。 13 RANDOMKEY 从当前数据库中随机返回一个 key 。 14 RENAME key newkey 修改 key 的名称 15 RENAMENX key newkey 仅当 newkey 不存在时，将 key 改名为 newkey 。 16 TYPE key 返回 key 所储存的值的类型。 数据库 序号 命令 SELECT index 切换数据库 DBSIZE 返回当前数据库的 key 的数量 FLUSHDB 清空当前数据库中的所有 key FLUSHALL 清空整个 Redis 服务器的数据(删除所有数据库的所有 key ) SORT 返回或保存给定列表、集合、有序集合 key 中经过排序的元素 String Redis中的字符串（String）类型是最基本且广泛使用的数据结构。在内部，这种类型的数据实际上是以 字节数组（byte array）的形式存储的。这意味着字符串在Redis中不仅可以存储文本，还可以存储任何形式的二进制数据，比如图片或序列化的对象。 序号 命令及描述 1 SET key value [EX seconds] [PX milliseconds] [NX] 设置指定 key 的值EX: 过期时间：秒PX: 过期时间：毫秒NX: 不存在才设置成功 2 GET key 获取指定 key 的值。 3 GETRANGE key start end 返回 key 中字符串值的子字符 4 GETSET key value 将给定 key 的值设为 value ，并返回 key 的旧值(old value)。 5 GETBIT key offset 对 key 所储存的字符串值，获取指定偏移量上的位(bit)。 6 MGET key1 [key2…] 获取所有(一个或多个)给定 key 的值。 7 SETBIT key offset value 对 key 所储存的字符串值，设置或清除指定偏移量上的位(bit)。 8 SETEX key seconds value 将值 value 关联到 key ，并将 key 的过期时间设为 seconds (以秒为单位)。 9 SETNX key value 只有在 key 不存在时设置 key 的值。 10 SETRANGE key offset value 用 value 参数覆写给定 key 所储存的字符串值，从偏移量 offset 开始。 11 STRLEN key 返回 key 所储存的字符串值的长度。 12 MSET key value [key value …] 同时设置一个或多个 key-value 对。 13 MSETNX key value [key value …] 同时设置一个或多个 key-value 对，当且仅当所有给定 key 都不存在。 14 PSETEX key milliseconds value 这个命令和 SETEX 命令相似，但它以毫秒为单位设置 key 的生存时间，而不是像 SETEX 命令那样，以秒为单位。 15 INCR key 将 key 中储存的数字值增一。 16 INCRBY key increment 将 key 所储存的值加上给定的增量值（increment） 。 17 INCRBYFLOAT key increment 将 key 所储存的值加上给定的浮点增量值（increment） 。 18 DECR key 将 key 中储存的数字值减一。 19 DECRBY key decrement key 所储存的值减去给定的减量值（decrement） 。 20 APPEND key value 如果 key 已经存在并且是一个字符串， APPEND 命令将 value 追加到 key 原来的值的末尾。 数据结构123456struct SDS&lt;T&gt; &#123; T capacity; // 数组容量 T len; // 数组长度 byte flags; // 特殊标识位 byte[] content; // 数组内容&#125;String 的数据结构为简单动态字符串(Simple Dynamic String,缩写 SDS)。是可以修改的字符串，内部结构实现上类似于 Java的ArrayList。创建字符串时 len 和 capacity 一样长，不会多分配冗余空间。这是因为绝大多数场景下我们不会使用 append 操作来修改字符串。当字符串较短时，len 和 capacity 字段可能使用较小的数据类型（如8位或16位整数）来存储，以节省内存，对于更长的字符串，这些字段会使用更大的数据类型（如32位或64位整数），因此SDS多种结构体。 字符串扩容策略小于1MB的字符串扩容每次是翻倍。超过1MB的字符串，每次扩容将增加1MB空间。字符串最大长度限制为512MB。 存储方式Redis对象，是Redis内部用于表示所有键值数据的基础数据结构。这个对象不仅用于字符串，还用于列表、集合、哈希表等所有Redis支持的数据类型。1234567struct RedisObject &#123; int4 type; // 4bits int4 encoding; // 4bits int24 lru; // 24bits int32 refcount; // 4bytes void *ptr; // 8bytes，64-bit system&#125; robj;Emb（嵌入式）String小于 44 字节。字符串数据直接存储在Redis对象的内部，避免了额外的内存分配。Raw（原始式）Redis对象包含元数据。字符串数据（SDS）存储在对象之外的单独内存区域，需要额外的内存分配。内存分配最多可以给 RedisObject 分配到64字节，去掉RedisObject本身元数据占用剩下45个字节，因为字符串是以 ‘\\0’ 结尾，所以剩下 44 字节。 简单分布式锁 List Redis 列表是简单的字符串列表，按照插入顺序排序。可以添加一个元素到列表的头部(左边) 或者尾部(右边)。 主要特点 自然顺序：List中的元素按照插入的顺序排列。 双端：可以在列表的头部或尾部添加或删除元素。 元素可重复：List中的元素可以重复，即同一个值可以出现多次。 自动删除：当列表没有元素自动删除。 可用于实现队列和栈 序号 命令及描述 1 BLPOP key1 [key2 ] timeout 移出并获取列表的第一个元素， 如果列表没有元素会阻塞列表直到等待超时或发现可弹出元素为止。 2 BRPOP key1 [key2 ] timeout 移出并获取列表的最后一个元素， 如果列表没有元素会阻塞列表直到等待超时或发现可弹出元素为止。 3 BRPOPLPUSH source destination timeout 从列表中弹出一个值，将弹出的元素插入到另外一个列表中并返回它； 如果列表没有元素会阻塞列表直到等待超时或发现可弹出元素为止。 4 LINDEX key index 通过索引获取列表中的元素 5 LINSERT key BEFORE|AFTER pivot value 在列表的元素前或者后插入元素 6 LLEN key 获取列表长度 7 LPOP key 移出并获取列表的第一个元素 8 LPUSH key value1 [value2] 将一个或多个值插入到列表头部 9 LPUSHX key value 将一个或多个值插入到已存在的列表头部 10 LRANGE key start stop 获取列表指定范围内的元素 11 LREM key count value 移除列表元素 12 LSET key index value 通过索引设置列表元素的值 13 LTRIM key start stop 对一个列表进行修剪(trim)，就是说，让列表只保留指定区间内的元素，不在指定区间之内的元素都将被删除。 14 RPOP key 移除并获取列表最后一个元素 15 RPOPLPUSH source destination 移除列表的最后一个元素，并将该元素添加到另一个列表并返回 16 RPUSH key value1 [value2] 在列表中添加一个或多个值 17 RPUSHX key value 为已存在的列表添加值 数据结构在列表元素较少的情况下会使用一块连续的内存存储，这个结构是 zipList，也即是压缩列表，它将所有的元素紧挨着一起存储，分配的是一块连续的内存。当数据量比较多的时候才会改成 quickList (快速链表)。zipList 转 quickList注意：这两个条件是可以修改的，在 redis.conf 中12list-max-ziplist-value 64 list-max-ziplist-entries 512 试图往列表新添加一个字符串值，且这个字符串的长度超过 server.list_max_ziplist_value （默认值为 64 ）ziplist 包含的节点超过 server.list_max_ziplist_entries （默认值为 512 ） zipList压缩列表是一块连续的内存空间，元素之间紧挨着存储，没有任何冗余空隙。1234567struct ziplist&lt;T&gt; &#123; int32 zlbytes; // 整个压缩列表占用字节数 int32 zltail_offset; // 最后一个元素距离压缩列表起始位置的偏移量，用于快速定位到最后一个节点 int16 zllength; // 元素个数 T[] entries; // 元素内容列表，挨个挨个紧凑存储 int8 zlend; // 标志压缩列表的结束，值恒为 0xFF&#125;12345struct entry &#123; int&lt;var&gt; prevlen; // 前一个 entry 的字节长度 int&lt;var&gt; encoding; // 元素类型编码 optional byte[] content; // 元素内容&#125;压缩列表为了支持双向遍历，所以才会有 ztail_offset 这个字段，用来快速定位到最后一个元素，然后倒着遍历。Redis的压缩链表是一种内存高效的数据结构，主要用于存储较小的元素集合。它的特殊之处在于它不使用传统的链表结构，即没有为每个元素维护前驱和后继指针（prev和next）。相反，它通过存储每个元素（entry）的长度来定位元素，这样可以减少所需的存储空间。原理访问下一个元素，程序简单地将当前元素的长度加到当前元素的指针上。访问前一个元素，程序则将前一个元素的长度从当前元素的指针上减去。在集合元素较少时，通过元素长度定位元素，避免存储前驱和后继指针带来的内存开销，是一种 时间换空间 的做法。 quickList快速链表是由多个压缩列表（ziplists）组成的双向链表。每个节点（ziplist）包含了列表的一部分元素。为什么从压缩链表转向快速链表提高性能：大的压缩链表在扩容时效率更低。灵活性：拆分小的压缩列表在数据插入和删除时更具备灵活性。减少了对大量连续内存的需求。123456789101112131415161718192021222324struct ziplist &#123; ...&#125;struct ziplist_compressed &#123; int32 size; byte[] compressed_data;&#125;struct quicklistNode &#123; quicklistNode* prev; quicklistNode* next; ziplist* zl; // 指向压缩列表 int32 size; // ziplist 的字节总数 int16 count; // ziplist 中的元素数量 int2 encoding; // 存储形式 2bit，原生字节数组还是 LZF 压缩存储 ...&#125;struct quicklist &#123; quicklistNode* head; quicklistNode* tail; long count; // 元素总数 int nodes; // ziplist 节点的个数 int compressDepth; // LZF 算法压缩深度 ...&#125;快速链表插入、查找操作为了进一步节约空间，Redis 还会对 ziplist 进行压缩存储，使用 LZF 算法压缩，可以选择压缩深度。 任务队列实现 Set Set 对外提供的功能与 List 类似列表的功能，特殊之处在于 Set 是可以 自动排重 的，当需要存储一个列表数据，又不希望出现重复数据时，Set 是一个很好的选择，并且 Set 提供了判断某个成员是否在一个 Set 集合内的重要接口，这个也是 List 所不能提供的。 随机获取 去重 无序 交集 并集 差集 序号 命令及描述 1 SADD key member1 [member2] 向集合添加一个或多个成员 2 SCARD key 获取集合的成员数 3 SDIFF key1 [key2] 返回给定所有集合的差集 4 SDIFFSTORE destination key1 [key2] 返回给定所有集合的差集并存储在 destination 中 5 SINTER key1 [key2] 返回给定所有集合的交集 6 SINTERSTORE destination key1 [key2] 返回给定所有集合的交集并存储在 destination 中 7 SISMEMBER key member 判断 member 元素是否是集合 key 的成员 8 SMEMBERS key 返回集合中的所有成员 9 SMOVE source destination member 将 member 元素从 source 集合移动到 destination 集合 10 SPOP key 移除并返回集合中的一个随机元素 11 SRANDMEMBER key [count] 返回集合中一个或多个随机数 12 SREM key member1 [member2] 移除集合中一个或多个成员 13 SUNION key1 [key2] 返回所有给定集合的并集 14 SUNIONSTORE destination key1 [key2] 所有给定集合的并集存储在 destination 集合中 15 SSCAN key cursor [MATCH pattern] [COUNT count] 迭代集合中的元素 数据结构Redis的 set 底层使用了 intset 和 hashtable 两种数据结构存储。Intset（整数集合）使用条件：当所有元素都是 整数 且集合元素少于 512，Redis使用intset。intset本质上就是一个数组，用于高效地存储整数值。特点：内存效率：对于小整数集合，intset非常节省内存。性能：操作intset通常比操作哈希表快，尤其是在集合较小的情况下。插入元素时，通过二分查找法确保元素唯一。 intsetintset是一个由 整数 组成的 有序 集合，从而便于在上面进行 二分查找，用于快速地判断一个元素是否属于这个集合。主要就是针对整数型的小集合进行性能优化。123456789typedef struct intset &#123; uint32_t encoding; // 编码类型 int_16t、int_32t、int_64t uint32_t length; // 元素数量 int8_t contents[]; // 元素数组&#125;intset;#define INTSET_ENC_INT16 (sizeof(int16_t)) //16位，2个字节，表示范围-32,768~32,767#define INTSET_ENC_INT32 (sizeof(int32_t)) //32位，4个字节，表示范围-2,147,483,648~2,147,483,647#define INTSET_ENC_INT64 (sizeof(int64_t)) //64位，8个字节，表示范围-9,223,372,036,854,775,808~9,223,372,036,854,775,807intset 升级与降级比如一开始set存储的是int16_t类型的数据, 但是当我们添加了一个int32_t类项的数据时，就需要操作升级。根据新元素的类型, 扩展底层元素的空间, 并为新元素分配空间将现有的元素都转为新的元素类型, 并存储在正确的空间上面将新元素添加进数组内，不支持降级。为什么不使用 zipListSet需要对自身元素进行去重，HashTable和数组（二分查找）能提供更高效的排重性能。zipList的优势是少量集合内存使用效率高。 Dict 字典123456789101112typedef struct dict &#123; // 指向 dictType 结构的指针（dictType 结构保存的是操作特定类型键值对的函数） dictType *type; // 保存上述所说函数的参数 void *privdata; // 哈希表，2个hash table，一个新的与一个旧的 dictht ht[2]; // rehash 索引，rehash 不进行时值为 -1 long rehashidx; /* rehashing not in progress if rehashidx == -1 */ // 正在运行的迭代器数量 int iterators; /* number of iterators currently running */&#125; dict;类似 Java 的HashMap，存储使用到Key，Value为空。特点：灵活性：可以存储任意类型的元素，包括字符串和复杂对象。扩展性：适用于较大的集合，哈希表的性能优于 intset。 文章关注共同好友实现(取交集) 12345678## 文章关注人sadd article zhangsan lisi wangwu luliu chenqi## 我的好友sadd friend zhangsan wangwu## 文章共同关注好友sinter article friend 抽奖系统 12345## 初始化奖品 1 一等奖，2 二等奖，3 三等奖，&lt;3 谢谢惠顾sadd lottery 1 2 3 4 5 6 7 8 9 10## 开始抽奖srandmember lottery Hash 特点：Hash 是一个string类型的field和value的映射表，hash特别适合用于存储对象。 序号 命令及描述 1 HDEL key field1 [field2] 删除一个或多个哈希表字段 2 HEXISTS key field 查看哈希表 key 中，指定的字段是否存在。 3 HGET key field 获取存储在哈希表中指定字段的值 4 HGETALL key 获取在哈希表中指定 key 的所有字段和值 5 HINCRBY key field increment 为哈希表 key 中的指定字段的整数值加上增量 increment 。 6 HINCRBYFLOAT key field increment 为哈希表 key 中的指定字段的浮点数值加上增量 increment 。 7 HKEYS key 获取所有哈希表中的字段 8 HLEN key 获取哈希表中字段的数量 9 HMGET key field1 [field2] 获取所有给定字段的值 10 HMSET key field1 value1 [field2 value2 ] 同时将多个 field-value (域-值)对设置到哈希表 key 中。 11 HSET key field value 将哈希表 key 中的字段 field 的值设为 value 。 12 HSETNX key field value 只有在字段 field 不存在时，设置哈希表字段的值。 13 HVALS key 获取哈希表中所有值 14 HSCAN key cursor [MATCH pattern] [COUNT count] 迭代哈希表中的键值对。 数据结构zpiList元素个数小于hash-max-ziplist-entries 配置（默认 512 个）所有值都小于hash-max-ziplist-value 配置（默认 64 字节）hashTable元素过多或者value太长影响zipList性能时切换数据结构 zipList存储格式：在 ziplist 中，Hash 被存储为字段和值的连续序列。首先是字段名，紧接着是对应的值，然后是下一个字段名，以此类推。优化：通过这种方式，ziplist 能够在保持良好读写性能的同时，显著减少内存使用。自动转换：当 ziplist 中的数据超过配置的阈值时，Redis 会自动将其转换为一个更标准的哈希表结构，以维持性能。 Dict 字典12345678910111213typedef struct dict &#123; // 指向 dictType 结构的指针（dictType 结构保存的是操作特定类型键值对的函数） dictType *type; // 保存上述所说函数的参数 void *privdata; // 哈希表，2个hash表，一个新的与一个旧的 dictht ht[2]; // rehash 索引，rehash 不进行时值为 -1 long rehashidx; /* rehashing not in progress if rehashidx == -1 */ // 正在运行的迭代器数量 int iterators; /* number of iterators currently running */&#125; dict;123456789101112131415/* * 哈希表结构 * 每个字典都使用两个哈希表，从而实现渐进式 rehash 。 */typedef struct dictht &#123; // 哈希表数组 dictEntry **table; // 哈希表的大小 unsigned long size; // 哈希表的大小掩码（总是 size-1） unsigned long sizemask; // 哈希表当前的已有结点数量 unsigned long used;&#125; dictht;1234567891011121314typedef struct dictEntry &#123; // 键 void *key; // 值 union &#123; void *val; uint64_t u64; int64_t s64; double d; &#125; v; // 指向下一个哈希表结点的指针 struct dictEntry *next;&#125; dictEntry;当 哈希类型 无法满足 ziplist 的条件时，Redis 会使用 hashtable 作为 哈希 的 内部实现，因为此时 ziplist 的 读写效率 会下降。 维护用户信息 12345678910hmset wgf name wgf age 18 address shenzhen# 获取用户信息hget wgf# 维护年龄hincrby wgf age 1# 维护地址hset wgf address guangzhou Sorted Set（Z set） 成员唯一：有序集合也是由字符串类型的元素组成的集合，其中每个成员都是唯一的，不允许重复。 关联双精度分数：有序集合中的每个元素都会关联一个 double 类型的分数，作为排序的依据。 按分数排序：Redis 通过这个分数来对集合中的成员进行从小到大的排序。 分数可重复，成员不重复：尽管有序集合的成员是唯一的，但是不同的成员可以有相同的分数。 序号 命令及描述 1 ZADD key score1 member1 [score2 member2] 向有序集合添加一个或多个成员，或者更新已存在成员的分数 2 ZCARD key 获取有序集合的成员数 3 ZCOUNT key min max 计算在有序集合中指定区间分数的成员数 4 ZINCRBY key increment member 有序集合中对指定成员的分数加上增量 increment 5 ZINTERSTORE destination numkeys key [key ...] 计算给定的一个或多个有序集的交集并将结果集存储在新的有序集合 key 中 6 ZLEXCOUNT key min max 在有序集合中计算指定字典区间内成员数量 7 ZRANGE key start stop [WITHSCORES] 通过索引区间返回有序集合成指定区间内的成员 8 ZRANGEBYLEX key min max [LIMIT offset count] 通过字典区间返回有序集合的成员 9 ZRANGEBYSCORE key min max [WITHSCORES] [LIMIT] 通过分数返回有序集合指定区间内的成员 10 ZRANK key member 返回有序集合中指定成员的索引 11 ZREM key member [member ...] 移除有序集合中的一个或多个成员 12 ZREMRANGEBYLEX key min max 移除有序集合中给定的字典区间的所有成员 13 ZREMRANGEBYRANK key start stop 移除有序集合中给定的排名区间的所有成员 14 ZREMRANGEBYSCORE key min max 移除有序集合中给定的分数区间的所有成员 15 ZREVRANGE key start stop [WITHSCORES] 返回有序集中指定区间内的成员，通过索引，分数从高到底 16 ZREVRANGEBYSCORE key max min [WITHSCORES] 返回有序集中指定分数区间内的成员，分数从高到低排序 17 ZREVRANK key member 返回有序集合中指定成员的排名，有序集成员按分数值递减(从大到小)排序 18 ZSCORE key member 返回有序集中，成员的分数值 19 ZUNIONSTORE destination numkeys key [key ...] 计算给定的一个或多个有序集的并集，并存储在新的 key 中 20 ZSCAN key cursor [MATCH pattern] [COUNT count] 迭代有序集合中的元素（包括元素成员和元素分值） 数据结构ziplistzset底层的存储结构包括ziplist或skiplist，在同时满足以下两个条件的时候使用ziplist，其他时候使用skiplist，两个条件如下：有序集合保存的元素数量小于128个有序集合保存的所有元素的长度小于64字节当ziplist作为zset的底层存储结构时候，每个集合元素使用两个紧挨在一起的压缩列表节点来保存，第一个节点保存元素的成员，第二个元素保存元素的分值。 skiplist + dict双重数据结构的优势：跳跃表：用于维护元素的有序性。跳跃表优势在于它提供了快速的顺序访问，特别适用于执行范围查询和迭代操作。字典：用于高效的元素查找。字典以成员作为键，分数作为值，使得对特定元素的访问、更新或删除操作变得极为迅速。空间效率和内存优化：成员（Member）：成员由 redisObject 结构表示的字符串，dict 和 skipList 通过指针引用同一个成员。分数（Score）：在跳跃表中，每个元素的分数是直接存储在每个节点中的；在字典中，元素的成员作为键，其对应的分数作为值。 skiplist详解 skiplist 排行榜实现 1234567891011## 初始化日销售排行榜zadd rank 98789 wgf 23476 zs 48739 ls 28374 ww## 更新日销量zadd rank 99746 wgf## 获取日销量第一名zrevrange rank 0 0## 获取销量榜前三名及销售额 zrevrange rank 0 2 withscores Redis 内部数据结构 redis 内部数据结构详解 三大特殊类型 参考文档 GEO 特点：GEO 是 Redis 3.2 版本中新增的功能，专门用于存储和操作地理位置信息。这一功能允许用户高效地处理和查询地理空间数据，适用于多种地理位置相关的应用场景。 序号 描述及命令 1 GEOADD key longitude latitude member [longitude latitude member ...] geoadd 用于存储指定的地理空间位置，可以将一个或多个经度(longitude)、纬度(latitude)、位置名称(member)添加到指定的 key 中 2 GEOPOS key member [member ...] 用于从给定的 key 里返回所有指定名称(member)的位置（经度和纬度），不存在的返回 nil 3 GEODIST key member1 member2 [m|km|ft|mi] 用于返回两个给定位置之间的距离m: 米km:千米mi:英里ft:英尺 4 GEORADIUS key longitude latitude radius m|km|ft|mi [WITHCOORD] [WITHDIST] [WITHHASH] [COUNT count] [ASC|DESC] [STORE key] [STOREDIST key] 以给定的经纬度为中心， 返回键包含的位置元素当中， 与中心的距离不超过给定最大距离的所有位置元素 5 GEORADIUSBYMEMBER key member radius m|km|ft|mi [WITHCOORD] [WITHDIST] [WITHHASH] [COUNT count] [ASC|DESC] [STORE key] [STOREDIST key] 和 GEORADIUS 命令一样， 都可以找出位于指定范围内的元素， 但是 georadiusbymember 的中心点是由给定的位置元素决定的， 而不是使用经度和纬度来决定中心点 6 GEOHASH key member [member ...] GEO 使用 geohash 来保存地理位置的坐标 数据结构 底层采用 Zset 数据结构，Menber存储值，Score存储经纬度 使用 1234567891011121314151617## 初始化城市geoadd city 114.100 22.200 xiangganggeoadd city 113.233 23.166 guangzhougeoadd city 113.516 22.300 zhuhaigeoadd city 114.066 22.616 shenzhen## 获取深圳的经纬度geopos city shenzhen## 获取深圳和广州的直线距离geodist city shenzhen guangzhou km## 以给定的经纬度为中心，获取指定范围内的城市georadius city 114.100 22.200 150 km## 获取深圳100KM内的城市GEORADIUSBYMEMBER city shenzhen 100 km SpringBoot 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253@Slf4j@SpringBootTestpublic class GeoTest &#123; @Autowired private RedisTemplate redisTemplate; private static final String KEY = &quot;city&quot;; @Test public void geoAddTest() &#123; redisTemplate.opsForGeo().add(KEY, new Point(114.100d, 22.200d), &quot;xianggang&quot;); redisTemplate.opsForGeo().add(KEY, new Point(113.233, 23.166d), &quot;guangzhou&quot;); redisTemplate.opsForGeo().add(KEY, new Point(113.516d, 22.300d), &quot;zhuhai&quot;); redisTemplate.opsForGeo().add(KEY, new Point(114.066d, 22.616d), &quot;shenzhen&quot;); &#125; @Test public void geoPosTest() &#123; List&lt;Point&gt; shenzhen = redisTemplate.opsForGeo().position(KEY, &quot;shenzhen&quot;); log.info(&quot;x: &#123;&#125;, y: &#123;&#125;&quot;, shenzhen.get(0).getX(), shenzhen.get(0).getY()); &#125; @Test public void geoDistTest() &#123; Distance distance = redisTemplate.opsForGeo().distance(KEY, &quot;shenzhen&quot;, &quot;guangzhou&quot;, Metrics.KILOMETERS); log.info(&quot;value: &#123;&#125;&quot;, distance.getValue()); &#125; @Test public void geoRadiusTest() &#123; Point point = new Point(114.100d, 22.200d); Metrics kilometers = Metrics.KILOMETERS; Distance distance = new Distance(150, kilometers); Circle circle = new Circle(point, distance); GeoResults&lt;RedisGeoCommands.GeoLocation&gt; radius = redisTemplate.opsForGeo().radius(KEY, circle); radius.forEach(geo -&gt; &#123; log.info(&quot;name: &#123;&#125;&quot;, geo.getContent().getName()); &#125;); &#125; @Test public void geoRadiusByMemberTest() &#123; GeoResults&lt;RedisGeoCommands.GeoLocation&gt; results = redisTemplate.opsForGeo().radius(KEY, &quot;shenzhen&quot;, new Distance(100, Metrics.KILOMETERS)); results.forEach(geo -&gt; &#123; log.info(&quot;name: &#123;&#125;&quot;, geo.getContent().getName()); &#125;); &#125;&#125; 使用场景 位置服务 如地点搜索、附近的人/商家等功能 地理围栏 判断是否在某个经纬度的半径范围内 距离计算 计算两个地点之间的直线距离 HyperLogLog HyperLogLog 是一种用于高效进行基数统计的算法，特别适合处理大量数据元素。它的主要特点和应用场景如下： 处理重复元素 ​ HyperLogLog 算法在统计基数时，同一元素的多次出现只被计算一次。这意味着重复的元素对基数的贡献只会被累加一次，确保统计的是独一无二的元素数量。 基数估计而非值存储 ​ HyperLogLog 专注于统计不同元素的数量（即基数），而不存储这些元素本身。因此，它无法像集合那样返回输入的各个元素。 空间效率 ​ HyperLogLog 的一个显著优点是其内存效率。即使输入元素的数量或体积非常大，HyperLogLog 所需的存储空间总是固定且相对较小。每个 HyperLogLog 键仅需大约 12 KB 的内存，就可以计算接近 2^64 个不同元素的基数。 误差率 ​ 在进行基数估计时，HyperLogLog 存在一定的误差。标准误差率约为 0.81%，这对于大多数应用场景来说是一个可接受的误差范围。 应用场景 ​ HyperLogLog 的主要应用场景包括统计大量数据的独特元素数。例如，在网站分析中，它被用于估算页面浏览量（PV）或独立访客数（UV）。 序号 命令及描述 1 PFADD key element [element ...] 添加指定元素到 HyperLogLog 中。 2 PFCOUNT key [key ...] 返回给定 HyperLogLog 的基数估算值。 3 PFMERGE destkey sourcekey [sourcekey ...] 将多个 HyperLogLog 合并为一个 HyperLogLog 数据结构 TyperLogLog 详解 存储结构 HyperLogLog 在 Redis 中确实采用 String 类型进行存储。它通过一种特殊的编码方式，高效地维护统计信息。 桶（Bucket）的使用 HyperLogLog 内部维护了 16384（即 2^14）个桶。这些桶并不直接记录各自桶的元素数量，而是记录与元素散列值相关的特定统计信息。 元素的处理 当一个元素被添加到 HyperLogLog 中时，首先计算其散列值。这个散列值决定了元素被分配到哪个桶，并影响该桶的统计值。具体来说，散列值的一部分用于确定桶的编号，另一部分则用于计算并更新桶的统计信息。 概率算法 HyperLogLog 利用概率算法来估计基数。由于是基于概率的方法，每个桶的统计值并不代表精确计数，而是一个概率估计值。 基数的估算 通过将所有桶的计数值进行数学处理（如调和均值计算），HyperLogLog 能够估算出接近真实的总基数。虽然单个桶的统计值可能不够精确，但综合所有桶的信息后，得到的基数估算通常非常接近真实值。 计算页面UV 123456789101112## 模拟用户访问pfadd pv 192.168.1.1pfadd pv 192.168.1.2pfadd pv 192.168.1.1pfadd pv 192.168.1.3pfadd pv 192.168.1.3pfadd pv 192.168.1.4## 获取当前UVpfcount pv返回 4 SpringBoot 1234567891011121314151617181920@Slf4j@SpringBootTestpublic class HyperLogLogTest &#123; @Autowired RedisTemplate redisTemplate; private static String KEY = &quot;pv&quot;; @Test public void pfAddTest() &#123; for (int i = 0; i &lt; 1000; i++) &#123; redisTemplate.opsForHyperLogLog().add(KEY, i); &#125; &#125; @Test public void pfCountTest() &#123; log.info(&quot;count: &#123;&#125;&quot;, redisTemplate.opsForHyperLogLog().size(KEY)); &#125;&#125; 使用场景 独立访客计数 计算网站的独立访客量，比如通过ip维度 流量统计 估算一定时间内网页或API接口的独立请求次数 去重统计 处理大量数据时，移除或统计重复项 BitMap BitMap 是以二进制位（bit）为单位的数组，其中每个位可以是 0 或 1。在 Redis 中，BitMap 实际上是存储在字符串值中的位序列。 序号 命令及描述 1 SETBIT key offset value 添加元素到BitMap 2 GETBIT key offset 根据偏移量获取BitMap上的元素 3 BITCOUNT key [start] [end] 计算给定字符串中，被设置为 1 的比特位的数量 4 BITPOS key bit [start] [end] 返回位图中第一个值为 bit 的二进制位的位置 5 BITOP operation destkey key [key …] 对一个或多个保存二进制位的字符串 key 进行位元操作，并将结果保存到 destkey 上 6 BITFIELD key [GET type offset] [SET type offset value] [INCRBY type offset increment] [OVERFLOW WRAP|SAT|FAIL] 数据结构 底层使用 String 数据类型，采用 offset 和 bit 存储数据，bit映射被限制在512MB之内，所以最大是2^32。 注意：假设初始的offset非常大，BitMap的初始化会非常慢，假设offset值为 2^32-1=4294967295，由于 redis 没有采用压缩实现，就会直接申请到 512MB 内存空间来存储 2^32-1 bit 位置的值 1，中间的 bit 也会全填上 0（内存暴涨），在偏移量较大的场景可以参考 Kafka 相对偏移量的做法。 网站记录一周用户签到情况 1234567891011121314## 用户一周签到情况, 1代表签到setbit wgf 0 1setbit wgf 3 1setbit wgf 5 1setbit wgf 6 1## 获取一周用户签到次数bitcount wgf## 获取周二用户是否签到getbit wgf 1## 获取用户最早没有签到的日期bitpos wgf 0 SpringBoot 1234567891011121314151617181920212223242526272829303132333435363738394041@Slf4j@SpringBootTestpublic class BitMapTest &#123; @Autowired private RedisTemplate redisTemplate; @Test public void setBitTest() &#123; redisTemplate.opsForValue().setBit(&quot;wgf&quot;, 0 ,true); redisTemplate.opsForValue().setBit(&quot;wgf&quot;, 3 ,true); redisTemplate.opsForValue().setBit(&quot;wgf&quot;, 5 ,true); redisTemplate.opsForValue().setBit(&quot;wgf&quot;,6 ,true); &#125; @Test public void getBitTest() &#123; log.info(&quot;value: &#123;&#125;&quot;, redisTemplate.opsForValue().getBit(&quot;wgf&quot;, 0)); &#125; @Test public void bitCountTest() &#123; Long num = (Long) redisTemplate.execute(new RedisCallback&lt;Long&gt;() &#123; @Override public Long doInRedis(RedisConnection connection) throws DataAccessException &#123; return connection.bitCount(&quot;wgf&quot;.getBytes()); &#125; &#125;); log.info(&quot;num: &#123;&#125;&quot;, num); &#125; @Test public void bitPosTest() &#123; Long value = (Long) redisTemplate.execute((RedisCallback&lt;Long&gt;) connection -&gt; connection.bitPos(&quot;wgf&quot;.getBytes(), false)); log.info(&quot;value: &#123;&#125;&quot;, value); &#125;&#125; 使用场景 用户签到功能 状态跟踪 通过唯一id跟踪用户上下线状态 数据去重 MQ消息队列幂等 分布式布隆过滤器 事务 本质：Redis事务通过将一系列命令放入一个队列中，并在EXEC命令发出时一次性、按顺序执行这些命令。这个过程确保了事务内的命令执行时不会被其他命令打断。 Redis事务的原子性：在Redis中，事务提供了一种有限的原子性。如果事务中的命令无法执行（如因为语法错误），则整个事务将失败。然而，如果事务中的命令在运行时出错（例如，因数据类型不匹配），之前的命令仍然会被执行。 隔离级别：Redis事务没有传统意义上的隔离级别概念。Redis的事务是通过单线程的特性来实现隔离的，即在执行事务中的命令序列时，不会有其他命令插队执行。 Redis 事务可以一次执行多个命令， 并且带有以下三个重要的保证： 事务操作被放入一个队列中，并在事务提交时（使用EXEC命令）一起执行。 如果某个命令执行失败（语法性错误），其他命令仍然会继续执行。 在事务执行过程，其他客户端提交的命令请求不会插入到事务执行命令序列中。 事务的三个阶段 一个事务从开始到执行会经历以下三个阶段： 开始事务 命令入队 执行 / 取消事务 命令 序号 命令及描述 1 DISCARD 取消事务，放弃执行事务块内的所有命令。 2 EXEC 执行所有事务块内的命令。 3 MULTI 标记一个事务块的开始。 4 UNWATCH 取消 WATCH 命令对所有 key 的监视。 5 WATCH key [key ...] 监视一个(或多个) key ，如果在事务执行之前这个(或这些) key 被其他命令所改动，那么事务将被打断。 事务使用 正常提交事务 123456789101112131415## 设置库存SET stock 100## 设置订单数量SET order 0## 开启事务模拟下单MULTIDECRBY stock 10INCRBY order 10EXEC## 提交事务返回结果9010 取消事务 1234567891011121314151617181920## 设置库存SET stock 100## 设置订单数量SET order 0## 开启事务模拟下单MULTIDECRBY stock 10INCRBY order 10## 取消事务DISCARD## 查看数据GET stock100GET order0 命令性错误 不存在命令 命令参数不正确 若在事务队列中存在命令性错误（类似于java编译性错误），则执行EXEC命令时，所有命令都不会执行 12345678910111213## 设置库存SET stock 100## 开启事务MULTIDECRBY stock 10GETSET stock // 命令性错误（缺少参数，或执行非Redis命令）ERR wrong number of arguments for &#x27;getset&#x27; commandEXEC## 再次获取库存GET stock100 语法性错误 命令使用正确，但是语法使用不正确 若在事务队列中存在语法性错误（类似于java的1/0的运行时异常），则执行EXEC命令时，其他正确命令会被执行，错误命令抛出异常 12345678910111213141516## 设置库存SET stock 100SET flag test## 开启事务MULTIDECRBY stock 10INCRBY flag 10 // 语法性错误，字符串不能参与数字运算（命令没有错）DECRBY stock 20EXECGET stock70 // 事务中语法正常的命令被执行GET flag test // 事务中正确 命令会被执行，错误命令抛出异常 WATCH 监控key 乐观锁定： WATCH命令用于监视一个或多个键，以检测这些键在事务执行之前是否被其他命令修改。 如果在执行事务的EXEC命令之前，任何被WATCH监视的键被修改（包括更新、删除或过期），那么事务不会执行。 使用场景： WATCH命令通常用于复杂的事务场景，其中需要根据被监视键的值来决定是否执行事务。 它特别适用于需要避免竞态条件的场景，例如，在多个客户端同时修改同一个键的值时。 事务流程： 一个典型的使用WATCH命令的事务流程包括：首先用WATCH命令监视一个或多个键，然后读取这些键的值，接着开始一个事务（使用MULTI命令），执行一系列命令，最后提交事务（使用EXEC命令）。 如果监视的键在执行EXEC之前被修改，事务将被取消，EXEC命令返回一个空回复以指示事务未执行。 取消监视： 使用UNWATCH命令可以取消所有被WATCH命令设置的监视。 另外，执行EXEC命令后，无论事务是否成功执行，所有的监视都会自动被取消。 使用 1234567891011121314151617181920212223242526272829303132333435363738394041############# 窗口1 ################ 设置库存SET stock 100## 开启库存监控WATCH stock## 开启事务MULTI## 下单DECRBY stock 1############# 窗口2 ################ 获取库存GET stock&quot;100&quot;## 下单DECRBY stock 10############# 窗口1 ################ 提交事务EXECnull ## 放弃事务执行GET stock90## 重试WATCH stockMULTIDECRBY stock 1EXEC89 ## 事务执行成功，库存扣减 秒杀实现 StockController 这里的秒杀只是用于实践 事务 和 监控 的用法，实际上秒杀使用 Lua 为最优解，因为简单使用 decrby 会存在库存遗留问题 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768@Slf4j@RestControllerpublic class StockController &#123; @Autowired private RedisTemplate redisTemplate; private ExecutorService executorService = Executors.newFixedThreadPool(100); @GetMapping(&quot;decr2&quot;) public void decr2() &#123; Runnable task = () -&gt; &#123; Boolean flag = decrStock(&quot;stock&quot;, getNum(), 100); if (flag) &#123; log.info(&quot;下单成功&quot;); &#125; else &#123; log.info(&quot;下单失败&quot;); &#125; &#125;; for (int i = 0; i &lt; 3000; i++) &#123; executorService.execute(task); &#125; &#125; /** * 秒杀扣减库存实现 * @param key 库存key * @param num 购买数量 * @param tryCount 重试扣减次数 * @return */ private Boolean decrStock(String key, int num, int tryCount) &#123; Boolean result = (Boolean) this.redisTemplate.execute(new SessionCallback&lt;Boolean&gt;() &#123; @Override public Boolean execute(RedisOperations operations) throws DataAccessException &#123; for (int i = 0; i &lt; tryCount; i++) &#123; operations.watch(key); Integer stockNum = (Integer) operations.opsForValue().get(key); if (stockNum &lt; num) &#123; // log.warn(&quot;库存不足&quot;); operations.unwatch(); try &#123; TimeUnit.MILLISECONDS.sleep(30); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; continue; &#125; operations.multi(); operations.opsForValue().decrement(key, num); List&lt;Object&gt; result = null; result = operations.exec(); if (Objects.nonNull(result) &amp;&amp; result.size() &gt; 0) &#123; //log.info(&quot;剩余库存：&#123;&#125;&quot;, result.get(0)); return true; &#125; &#125; return false; &#125; &#125;); return result; &#125;&#125; 发布订阅 Redis的发布/订阅（Pub/Sub）是一种消息通信模式，允许客户端之间通过频道进行消息传递。这个模式包含发布者（发送消息的客户端）和订阅者（接收消息的客户端）。在Redis中，发布/订阅功能被广泛应用于构建消息队列、聊天系统、实时通知等场景。以下是其主要特点和工作方式： 频道（Channels）： 在Redis的Pub/Sub系统中，消息是通过频道发送的。频道可以被视为消息传递的媒介。 客户端可以订阅任何频道。当消息发送到频道时，这些消息会被转发给所有订阅了该频道的客户端。 发布（Publish）： 发布者使用PUBLISH命令发送消息到指定的频道。 当一个消息被发布到频道，所有订阅该频道的客户端都会收到这个消息。 订阅（Subscribe）： 客户端使用SUBSCRIBE命令订阅一个或多个频道。 一旦订阅了频道，客户端会进入一个订阅状态，等待接收任何发送到这些频道的消息。 模式订阅（Pattern Subscribe）： Redis还提供了模式订阅功能，允许客户端订阅符合特定模式的频道。这通过PSUBSCRIBE命令实现。 比如，使用PSUBSCRIBE news.*可以订阅所有以news.开头的频道。 取消订阅： 客户端可以使用UNSUBSCRIBE命令来取消订阅特定的频道。 类似地，PUNSUBSCRIBE命令用于取消模式订阅。 应用场景： Redis的Pub/Sub系统非常适合构建实时消息系统，例如实时聊天应用、实时广播更新、实时在线监控系统等。 限制： Redis的发布/订阅模型不保证消息的持久化或可靠性。如果在消息发布时没有订阅者在线，这些消息将会丢失。 它也不提供复杂的消息队列功能，如消息确认或持久订阅。 Redis的发布/订阅功能由于其简单性和高效性，在需要快速通信和实时更新的应用中非常有用。但是，在需要消息持久化和高可靠性的场景中，可能需要考虑其他消息队列解决方案。 订阅/发布消息图： 序号 命令及描述 1 PSUBSCRIBE pattern [pattern ...] 订阅一个或多个符合给定模式的频道。 2 PUBSUB subcommand [argument [argument ...]] 查看订阅与发布系统状态。 3 PUBLISH channel message 将信息发送到指定的频道。 4 PUNSUBSCRIBE [pattern [pattern ...]] 退订所有给定模式的频道。 5 SUBSCRIBE channel [channel ...] 订阅给定的一个或多个频道的信息。 6 UNSUBSCRIBE [channel [channel ...]] 指退订给定的频道。 命令 123456789101112131415############ 窗口1 ################ 订阅频道SUBSCRIBE mychannel1) &quot;message&quot; # 类型2) &quot;mychannel&quot; # 频道3) &quot;hello! # 窗口2发送消息这里会实时监控到############ 窗口2 ################ 发送消息PUBLISH mychannel &quot;hello!&quot;1 # 返回当前接收消息客户端数量############ 窗口1 ################ 取消订阅频道UNSUBSCRIBE mychannel Lua脚本 Lua 详解 Lua是一种轻量级的编程语言，被设计为嵌入到应用程序中，Redis从2.6版本开始内嵌了Lua解释器，允许执行Lua脚本。 优点 原子性执行：Lua脚本在Redis中以原子方式执行。这意味着在脚本执行期间，不会有其他Redis命令被执行，保证了操作的一致性和完整性。 减少网络开销：通过在服务器端执行脚本，可以减少在客户端和Redis服务器之间的往返通信，降低网络延迟。 操作封装：Lua脚本可以封装一系列复杂的Redis命令，简化客户端代码，使其更加整洁和易于维护。 缺点 性能影响：虽然Lua脚本在Redis中执行效率很高，但是过于复杂的脚本可能会长时间占用CPU，影响Redis服务器的性能。 安全性考虑：需要确保编写的Lua脚本安全可靠，特别是在处理外部输入数据时，避免执行恶意代码或操作。 EVAL 命令 1EVAL script numkeys [key [key ...]] [arg [arg ...]] EVAL：这是Redis执行Lua脚本的命令。 script：这是要执行的Lua脚本的文本。它是一段Lua代码，可以包含任何有效的Lua命令和Redis命令。 numkeys：这个参数指定了随后在命令中列出的键(key)的数量。这个数字告诉Redis，接下来的多少参数应该被视为键名。 [key [key ...]]：这部分是可选的，包含了将要被脚本处理的键名。这些键名放在Lua脚本中的KEYS数组里。numkeys参数指定了这里有多少个键。 [arg [arg ...]]：这部分也是可选的，包含了传递给脚本的其他参数，这些参数放在Lua脚本中的ARGV数组里。 12EVAL &quot;return redis.call(&#x27;set&#x27;, KEYS[1], ARGV[1])&quot; 1 mykey myvalueOK 这段脚本说明有一个KEYS参数 mykey，有一个ARGV参与 myvalue。然后Lua脚本使用 redis.call 调用 redis 命令，最终将执行结果返回。 EVALSHA 脚本缓存语义 EVALSHA 是 Redis 中用于执行 Lua 脚本的命令，它与 EVAL 命令类似，但有一个关键的区别：它使用 Lua 脚本的 SHA1 校验码来引用被Redis加载的脚本，而不是直接提供脚本代码。这样做的优点是，一旦脚本被加载到 Redis 中，就可以通过其 SHA1 校验码高效地多次调用，而无需重新发送整个脚本代码。 1SCRIPT LOAD script 用于将 Lua 脚本加载到 Redis 内存缓存并返回一个 sha1 码，Redis重启缓存脚本失效。 1EVALSHA sha1 numkeys key [key ...] arg [arg ...] sha1：这是预先加载到 Redis 中的 Lua 脚本的 SHA1 校验码。这个校验码是使用 SCRIPT LOAD 命令加载脚本时得到的。 numkeys：这个参数指定了随后在命令中列出的 KEY 的数量。这个数字告诉 Redis，接下来的多少参数应该被视为键名。 [key [key ...]]：这部分是可选的，包含了将要被脚本处理的键名。这些键名放在 Lua 脚本中的 KEYS 数组里。numkeys 参数指定了这里有多少个键。 [arg [arg ...]]：这部分也是可选的，包含了传递给脚本的其他参数，这些参数放在 Lua 脚本中的 ARGV 数组里。这些参数可以是任何值，包括字符串、数字等，由脚本的逻辑来决定它们的用途。 使用 EVALSHA 命令执行 Lua 脚本 1234&gt; SCRIPT LOAD &quot;return &#x27;Hello World&#x27;&quot; # 让Redis缓存一段脚本，但不执行，无限期缓存470877a599ac74fbfda41caa908de682c5fc7d4b # Redis返回的SHA1码&gt; EVALSHA &quot;470877a599ac74fbfda41caa908de682c5fc7d4b&quot; 0 # 执行缓存脚本 好处：相对于EVAL，它的第一个参数是一段脚本。EVALSHA 在执行Lua脚本可以使用缓存脚本，减少额外的网络开销。 Lua调用Redis命令 redis.call() 如果执行的命令出现错误，redis.call() 会停止脚本的执行，并将错误返回给脚本的调用者，也就是说错误要在脚本之外处理。 redis.pcall() 与 redis.call() 不同的是，如果命令执行失败，redis.pcall() 不会停止脚本执行或抛出错误，而是返回一个包含错误信息的Lua表，可以在脚本内处理错误。 123456789101112131415set k veval &quot;return redis.call(&#x27;incr&#x27;, KEYS[1])&quot; 1 k(error) ERR Error running script (call to f_2bab3b661081db58bd2341920e0ba7cf5dc77b25): @user_script:1: ERR value is not an integer or out of range-- 实际脚本要合并为一行才能执行，脚本内部错误处理eval &quot; local result = redis.pcall(&#x27;incr&#x27;, KEYS[1]) if result.err then return 0 else return result end &quot; 1 k(integer) 0 Redis 和 Lua 的数据关系 在Lua 5.2及之前的版本中，Lua只有一种数字类型，即双精度浮点数。当需要将数字存储到Redis并期望它们作为整数处理时（例如用于INCR、DECR或其他需要整数的Redis命令），需要确保这些数字在转换前是整数，可以使用math.floor保证。 在Lua 5.3及以后的版本中，Lua支持整数和浮点数。在这些版本中，Lua会根据数字的使用方式自动选择合适的类型。 Redis Lua 版本默认为5.1 Lua 数据类型 描述 转换为 Redis 数据类型 描述 示例（Lua -&gt; Redis） string 字符串 String 字符串，可以包含任何数据类型 redis.call('set', 'key', stringValue) number 数字 String 数字存储为字符串 redis.call('set', 'key', tostring(numberValue)) table 数组型表 List/Set/Sorted Set 根据使用的命令转换为列表、集合或有序集合 redis.call('rpush', 'key', unpack(arrayTable)) 键值对表 Hash 字符串字段和字符串值的映射 redis.call('hmset', 'key', unpack(hashTable)) boolean 布尔值 String 布尔值通常转换为&quot;0&quot;或&quot;1&quot;的字符串 redis.call('set', 'key', booleanValue and '1' or '0') nil 空值 删除键 在Redis中删除对应的键 redis.call('del', 'key') Redis 数据类型 描述 转换为 Lua 数据类型 描述 示例（Redis -&gt; Lua） String 字符串，可以包含任何数据类型 string/number 字符串或数字 local value = redis.call('get', 'key') List 字符串列表，按插入顺序排序 table 数组型表 local list = redis.call('lrange', 'key', 0, -1) Set 无序集合，不重复的字符串集合 table 数组型表 local set = redis.call('smembers', 'key') Sorted Set 有序集合，字符串和分数的映射 table 键值对表 local zset = redis.call('zrange', 'key', 0, -1, 'WITHSCORES') Hash 字符串字段和字符串值的映射 table 键值对表 local hash = redis.call('hgetall', 'key') Bitmap 由位组成的数组 string 字符串 local bitmap = redis.call('get', 'key') HyperLogLog 用于基数统计的概率数据结构 nil 不直接转换 N/A Stream 消息流 table 键值对表 local stream = redis.call('xrange', 'key', start, end) Geo 地理位置信息 table 键值对表 local geopos = redis.call('geopos', 'key', 'member') Lua脚本实现秒杀 12345678910111213141516171819202122232425262728293031323334353637383940414243444546 @Autowired private RedisTemplate redisTemplate; @Autowired private ExecutorService executorService;// 定义Lua脚本，库存足够执行INCRBY命令后返回0，库存不够返回0 private static RedisScript&lt;Long&gt; STOCK_SCRIPT = new DefaultRedisScript&lt;Long&gt;( &quot;local stock = redis.call(&#x27;get&#x27;, KEYS[1])\\n&quot; + &quot;if (stock &gt;= ARGV[1]) then\\n&quot; + &quot; redis.call(&#x27;decrby&#x27;, KEYS[1], ARGV[1])\\n&quot; + &quot; return 1\\n&quot; + &quot;else\\n&quot; + &quot; return 0\\n&quot; + &quot;end&quot;, Long.class); @ApiOperation(value = &quot;使用Lua实现秒杀&quot;) @GetMapping(&quot;decr3&quot;) public void decr3() &#123; Runnable task = () -&gt; &#123; int num = getNum(); Long result = (Long) this.redisTemplate.execute(STOCK_SCRIPT, Arrays.asList(&quot;stock&quot;), num); if (result &gt; 0) &#123; log.info(&quot;下单成功&quot;); &#125; else &#123; log.error(&quot;下单失败&quot;); &#125; &#125;; for (int i = 0; i &lt; 3000; i++) &#123; executorService.execute(task); &#125; // 测试最终结果： 执行效率高，解决库存遗留问题 &#125; public static int getNum() &#123; int min = 1; int max = 20; Random random = new Random(); int value = random.nextInt(max + min) + min; return value; &#125; Lua库存扣减脚本 1234567891011121314151617181920212223242526272829303132-- Lua 脚本实现多个产品库存的扣减-- 检查所有产品库存是否满足扣减条件-- 如果所有产品库存满足，执行 DECRBY-- 如果任何一个产品库存不满足，返回该产品的 KEY-- 获取键的数量local numKeys = #KEYS-- 确保 KEYS 和 ARGV 长度一致assert(#ARGV == numKeys, &quot;每个键必须有对应的扣减值&quot;)-- 首先检查所有库存是否满足扣减条件for i = 1, numKeys do local key = KEYS[i] local decrement = tonumber(ARGV[i]) local currentStock = tonumber(redis.call(&#x27;get&#x27;, key) or 0) if currentStock &lt; decrement then -- 如果任何一个产品的库存不满足条件，返回该产品的 KEY return key endend-- 执行库存扣减for i = 1, numKeys do local key = KEYS[i] local decrement = tonumber(ARGV[i]) redis.call(&#x27;decrby&#x27;, key, decrement)end-- 所有操作成功，返回 &quot;OK&quot;return &quot;OK&quot; Docker 安装 Redis 12345678910docker pull redis:latestdocker run -p 6379:6379 --name redis1 -v /data/redis/config/redis_1.conf:/etc/redis/redis.conf -v /data/redis/data/redis1:/data -d redis redis-server /etc/redis/redis.conf --appendonly yes说明-v /data/redis/config/redis_1.conf:/etc/redis/redis.conf：把宿主机配置好的redis.conf放到容器内的这个位置中-v /data/redis/data/redis1:/data： 把redis持久化的数据在宿主机内显示，做数据备份redis-server /etc/redis/redis.conf：这个是关键配置，让redis不是无配置启动，而是按照这个redis.conf的配置启动–appendonly yes：redis启动后数据持久化 配置文件 redis.conf 12345# 允许外部IP访问bind 0.0.0.0daemonize norequirepass wgf123 # 密码appendonly yes # 持久化 Redis 配置 参考 掘金文档 Redis 配置 Redis的持久化 Redis是一个高性能的内存键值存储数据库，它主要使用内存作为数据操作的主要媒介，这意味着数据主要在内存中进行读取和写入操作，确保了极高的处理速度。然而，内存的易失性意味着在服务器进程崩溃或由于其他原因意外停止时，存储在内存中的数据可能会丢失。为了克服这个挑战，Redis提供了两种主要的持久化机制：RDB 和 AOF。RDB持久化会在指定的时间间隔内创建数据集的时间点快照，而AOF持久化则记录每个写操作命令。 RDB (Redis DataBase 默认) 性能比AOF优秀 数据丢失风险大 适合海量数据恢复 适合对数据完整性要求不高的场景 什么是RDB RDB（Redis Database）是 Redis 的一种持久化机制，它通过创建内存数据集的快照来实现持久化。在配置的时间间隔或满足特定条件时，Redis 会自动执行快照操作，将当前内存中的所有数据保存到一个 RDB 文件中。 优点和缺点 优点 数据恢复速度快：在需要从磁盘恢复数据时，RDB 可以快速地加载快照文件，尤其适合大数据集的恢复。 节省磁盘空间：RDB 文件会通过压缩来减少所需的磁盘空间。 较小的性能开销：快照操作在子进程中执行，主进程可以继续处理客户端请求，对数据库性能的影响相对较小。 缺点 数据丢失的风险：RDB 通过定时创建数据的快照。如果 Redis 服务器在两次快照之间发生故障，那么自上次快照以来的所有数据更改都将丢失。 不适合实时持久化：由于 RDB 是周期性创建的，它不适合需要实时持久化的应用。 资源占用：在执行快照时，Redis 需要 fork 一个子进程，这个过程中会消耗额外系统资源。 备份如何执行 使用子进程进行持久化：Redis 通过 fork 创建一个子进程来执行 RDB 持久化操作。这样做的目的确实是为了减少主进程的 I/O 阻塞，允许主进程在持久化过程中继续处理客户端请求。子进程利用操作系统的写时复制（Copy-On-Write, COW）机制来访问数据集的快照，这意味着当主进程修改内存中的数据时（修改数据的副本），子进程所访问的数据仍然保持不变。 使用临时文件写入：子进程不是直接写入 dump.rdb 文件，而是先将快照数据写入一个临时文件中。这个过程中，子进程会创建整个数据集的一个完整、一致性的副本。 安全地同步文件：一旦子进程完成数据的写入，它会将临时文件重命名为 dump.rdb。这种方式确保了即使在持久化过程中出现故障，原有的 dump.rdb 文件也不会受到影响，从而提供了一定程度的数据安全性。 恢复效率的考虑：对于需要进行大规模数据恢复的场景，RDB 模式通常比 AOF 模式更高效，因为 RDB 文件是数据集的压缩表示，可以更快地被载入。同时，如果数据的完整性不是最重要的考虑因素，RDB 可能是更好的选择，因为 AOF 在恢复大量数据时会比较慢。 Fork工作原理 初始共享：Redis 通过 fork 创建一个子进程，他们实际上共享相同的物理内存页。操作系统只保留一份数据的副本，而多个进程可以同时访问这份副本。 写时复制：Redis 主线程在持久化期间接收客户端请求，修改这些共享的数据时，操作系统首先会为该进程创建这些数据的一个私有副本。 数据快照：子进程快照的是原始的共享数据，不会快照主线程在持久化期间创建的副本数据，这些副本数据会在下次快照时被持久化。 dump文件 配置位置及SNAPSHOTTING解析 默认配置文件 参数 说明 dir 设置快照文件的存放路径，这个配置项一定是个目录 stop-writes-on-bgsave-error 默认值为yes。当启用了RDB磁盘满了，Redis是否停止接收数据 rdbcompression 默认值是yes。对于存储到磁盘中的快照，可以设置是否进行压缩存储 rdbchecksum 默认值是yes。在存储快照后，我们还可以让redis使用CRC64算法来进行数据校验 dbfilename 设置快照的文件名，默认是 dump.rdb save 这里是用来配置触发 Redis的持久化条件，也就是什么时候将内存中的数据保存到硬盘 15分钟有一次修改触发保存 5分钟有十次修改触发保存 1分钟有一万次修改触发保存 如果想禁用RDB持久化的策略，只要不设置任何save指令，或者给save传入一个空字符串参数也可以。若要修改完毕需要立马生效，可以手动使用 save 命令，立马生效。 123456命令修改RDB频率save 120 10 # 120秒内修改10次则触发RDB如果想禁用RDB持久化的策略，只要不设置任何save指令，或者给save传入一个空字符串参数也可以。若要修改完毕需要立马生效，可以手动使用 save 命令！立马生效，需要删除dump.rdb文件 如何触发RDB快照 配置文件配置了 save 策略，服务定期备份 命令save或者是bgsave save 时只管保存，其他不管，全部阻塞 bgsave，Redis 会在后台异步进行快照操作，快照同时还可以响应客户端请求。可以通过lastsave 命令获取最后一次成功执行快照的时间。 执行 flushall 命令，也会产生 dump.rdb 文件，但里面是空的，无意义 ! 退出 的时候也会产生 dump.rdb 文件！ RDB恢复 命令获取Redis目录 123&gt; config get dirdir/data 将备份文件（dump.rdb）移动到redis安装目录并启动服务即可。 AOF（Append Only File） 数据完整性要求高 内存数据量不大 影响写入性能 什么是AOF Redis的AOF（Append-Only File）持久化机制是一种用于确保数据持久性的方法，它通过记录并保存Redis服务器执行的所有写操作到磁盘上的一个文件中。这些记录包括了所有写命令及其参数，并以追加的方式保存在日志文件中。AOF机制的关键在于它记录的是实际执行的命令，而不仅仅是数据变更，这使得通过回放这些命令能够准确重建数据集。 AOF保存的是 appendonly.aof 文件 优点和缺点 优点 安全性：AOF 持久化可以配置为每次写操作后都同步到磁盘，或者每秒同步一次，这降低了数据丢失的风险。 可读的日志格式：AOF 文件是一个只追加的日志文件，其内容是 Redis 命令的纯文本，这使得文件可以用于故障排查和数据恢复。 灵活的恢复策略：可以通过编辑 AOF 文件来手动修复数据或删除错误的写操作。 缺点 恢复速度慢：AOF 比 RDB 持久化有更高的磁盘 I/O 开销，尤其是在高写入负载的情况下。 对性能的影响： AOF 需要记录每个写操作，因此在高写入负载下会对服务性能产生影响。 数据冗余：随着时间的推移，AOF 文件可能包含一些过时或重复的命令，尽管 Redis 提供了重写机制来减少这种情况。 AOF配置 参数 说明 appendfilename appendfilename AOF 文件名称 appendfsync appendfsync aof持久化策略的配置no 写入系统内核缓冲区，由系统自动刷盘，速度最快always 每次执行写命令后都进行磁盘同步everysec 每秒进行一次磁盘同步 no-appendfsync-on-rewrite 重写时是否可以运用Appendfsync，用默认no即可，保证数据安全性 auto-aof-rewrite-min-size 64mb。设置允许重写的最小aof文件大小 auto-aof-rewrite-percentage 默认值为100。aof自动重写配置，当目前aof文件大小超过上一次重写的aof文件大小的百分之多少进行重写 1234567appendonly no # 是否以append only模式作为持久化方式，默认使用的是rdb方式持久化，这种方式在许多应用中已经足够用了appendfsync everysec # appendfsync aof持久化策略的配置# no表示不执行fsync，由操作系统保证数据同步到磁盘，速度最快。# always表示每次写入都执行fsync，以保证数据同步到磁盘。# everysec表示每秒执行一次fsync，可能会导致丢失这1s数据。 正常恢复 启动：修改默认的appendonly no，改为yes 将有数据的aof文件复制一份保存到对应目录（config get dir） 恢复：重启redis然后重新加载 异常恢复 启动：修改默认的appendonly no，改为yes 故意破坏 appendonly.aof 文件 修复： redis-check-aof --fix appendonly.aof 进行修复 恢复：重启 redis 然后重新加载 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354## 开启aof,设置数据&gt; set k1 v1OK&gt; set k2 v2OK&gt; set k3 v3OK 停止Redis 破坏aof文件结构*2$6SELECT$10*3$3set$2k1$2v1*2$6SELECT$10*3$3set$2 // 删除k2 // 删除$2v2*3$3set$2k3$2v3 执行命令/data/redis/data/redis1# redis-check-aof --fix appendonly.aof 0x 49: Expected prefix &#x27;$&#x27;, got: &#x27;*&#x27;AOF analyzed: size=102, ok_up_to=52, diff=50This will shrink the AOF from 102 bytes, with 50 bytes, to 52 bytesContinue? [y/N]: ySuccessfully truncated AOF 启动Redis&gt; keys *k1 AOF重写 压缩AOF文件 12auto-aof-rewrite-min-size 64MB // 当文件小于64M时不进行重写auto-aof-rewrite-min-percenrage 100 // 当文件比上次重写后的文件大100%时进行重写 AOF采用文件追加方式，记录了所有写入命令，因此AOF文件会随着时间的推移变得越来越大。为了避免出现过大的AOF文件，Redis引入了AOF重写机制。当AOF文件的大小超过预设的阈值（默认为64MB），或者比上次重写后的文件大小增长了100%时，Redis会触发AOF重写。 AOF重写是一种内容压缩的过程，它会根据内存中的数据重新创建一个新的AOF文件，其中只包含能够完全恢复数据的最小指令集。重写过程是在后台进行的，不会阻塞主线程的正常操作。可以使用命令 bgrewriteaof 手动触发AOF重写过程。 通过AOF重写，可以将过大的AOF文件压缩为较小的文件，从而减少磁盘空间的占用和提高AOF文件的读写效率。 重写原理 当Redis的AOF文件持续增长过大时，Redis会进行AOF重写操作以优化文件大小。AOF重写是通过fork出一个新的进程来实现的，首先该进程会将AOF文件的数据以二进制形式重新生成一个新的AOF文件，这个过程是在一个临时文件中进行的。当新的AOF文件生成完毕后，Redis会将旧的AOF文件进行覆盖，即先将临时文件重命名为新的AOF文件的名字，从而完成AOF重写操作。 参考 AOF在重写时如何保证继续服务 触发机制 当AOF文件超过64MB（可配置）或AOF文件超过上次重写大小的100%时。 调用 BGREWRITEAOF 手动触发 AOF和RDB同时开启 RDB 和 AOF 不是互斥的，可以同时开启，如果AOF和RDB同时存在的时候，Redis会优先使用从AOF文件来还原数据库状态，如果AOF关闭状态时，则从RDB中恢复。 集群数据恢复 在Redis 集群中，如果整个集群的数据量非常庞大，主节点可以开启 AOF 保证数据完整性，从节点开启 RDB 策略。 当节点宕机恢复时，先关闭 AOF 功能，并且先从从节点中同步 dump.rdb 文件使用 RDB 方式进行数据恢复，待 RDB 恢复完毕后再使用命令动态启动 AOF 恢复，保证数据完整性 12config get *config set config set appendonly yes AOF在重写时如何保证继续服务 AOF重写过程依旧提供服务，由子进程重写 触发重写：AOF 重写可以手动触发，也可以配置 Redis 自动在 AOF 文件达到特定大小后触发。手动触发是通过 BGREWRITEAOF 命令。 创建子进程：Redis 使用子进程来执行重写操作。这样做的好处是避免阻塞主 Redis 进程，保证AOF重写期间主进程继续提供服务。 重建 AOF 文件：子进程通过读取当前数据库的状态，重新生成一份写指令。 记录新命令：在重写过程中，主进程继续处理新的写命令，将它们同时追加到旧的 AOF 文件和一个缓冲区中。这确保了在重写期间对数据库所做的更改不会丢失。 切换 AOF 文件：一旦子进程完成了重建过程，它会通知主进程。然后，主进程将缓冲区中的所有新命令追加到新的 AOF 文件中，以确保数据的完整性。 文件替换：通过 rename 修改文件名完成新旧文件替换。 如何选择 结合使用 为了平衡数据安全性和性能，可以同时启用 RDB 和 AOF。例如，可以使用 RDB 进行定时备份（例如每天一次），同时使用 AOF 来保证更高级别的数据安全性。 集群上 ​ 如果做Redis集群，那么主节点选择 AOF 持久化方式保证数据完整性，从节点选择 RDB 保证速度。 业务场景 数据集大且读写频繁不太关注数据完整性场景如DB缓存层，可以使用 RDB 提高Redis性能。 数据集不大但对数据完整性要求高场景如商品库存数据，可以使用 AOF 保证数据完整性。 高可用 主从复制 AP模型 异步同步 最终一致性保证 主从同步延迟，数据可能不一致 CAP理论 主从复制是一种将一个Redis服务器的数据复制到其他Redis服务器的机制。在主从复制中，源服务器被称为主节点（master/leader），而目标服务器被称为从节点（slave/follower）。数据复制是单向的，只能从主节点复制到从节点。主节点负责写操作，而从节点主要用于读操作。 在默认情况下，每台Redis服务器都是主节点，也就是既能读取又能写入数据。每个主节点可以拥有多个从节点或者没有从节点。然而，每个从节点只能有一个主节点，它会从该主节点复制数据。 主节点：负责读写操作。 从节点：只能执行读操作，不能写入数据。 主从复制的作用 数据冗余：主节点将数据复制到多个从服务器，实现数据冗余保证数据安全。 服务冗余：主节点宕机，可以从从节点中重新指定一个主节点实现故障转移。 负载均衡：从节点分担主节点读取压力，实现读取能力的水平扩展。 主从架构的缺点 数据同步延迟：在主从架构中，主节点负责接收写操作并将数据同步到从节点，存在一定时间的数据延迟。 写入性能未提升：主从复制主要用于提高读取性能，因为所有写入操作仍然只能在主节点上进行。 单点故障：尽管主从架构可以提供高可用性，但主节点本身仍然是一个单点故障，故障转移需要时间恢复。 全量同步 slave初始化阶段 当一个从节点刚刚加入主从复制架构或由手动执行 SLAVEOF 命令，它需要从主节点获取所有数据的副本，以确保与主节点的数据一致性。这个过程称为全量同步。 全量同步过程如下： 从节点发送 SYNC 命令给主节点，请求进行全量同步。 主节点在接收到 SYNC 命令后，会开始执行 BGSAVE 命令，将当前内存中的数据快照保存到磁盘上的 RDB 文件中。 主节点在完成 BGSAVE 后，将 RDB 文件发送给从节点，并将这段时间内的所有写命令发送给从节点。 从节点接收到 RDB 文件后，会将其加载到内存中，恢复主节点的数据。 RDB 文件数据恢复后，从节点继续执行主节点发送的写命令，使自己的数据与主节点保持一致。 增量同步 增量同步是指在主从复制过程中，主节点将自己的写操作日志（也称为命令传播）发送给从节点，使得从节点可以按照相同的顺序来执行这些写操作，从而保持与主节点的数据一致性。 增量同步过程如下： 前提是完成全量同步。 从节点发送 SYNC 命令：附带一个偏移量参数，表示从节点最后一次成功接收到的写操作日志的位置。 主节点处理： 有相应日志： 将偏移量起始位置的所有操作日志发送给从节点，从节点安顺序执行。 没有相应日志：重新全量同步。 部分同步 当从节点断线后重新连接时，可以使用部分同步来只同步断线期间的增量数据，而不是重新进行全量同步。 Redis 6.2 版本引入了复制积压缓冲区（Replication backlog）的功能。通过配置主节点的复制积压缓冲区大小，可以使主节点将写操作日志保存在缓冲区中，并在从节点重新连接时将缓冲区中的日志发送给从节点，从而实现部分同步（无磁盘复制）。 无磁盘复制 在传统的主从复制中，主节点会将数据写入磁盘，并将写操作的命令发送给从节点进行同步。而在无磁盘复制中，主节点在执行写操作后，将写操作的命令直接发送给从节点进行同步，而无需将数据写入磁盘。这种方式能够提高同步的速度和效率，减少了磁盘的IO开销。 无磁盘复制的实现主要依赖于Redis的复制功能。在Redis的复制模式下，主节点将写操作的命令发送给从节点进行同步。从节点会接收到命令并执行，从而达到数据同步的目的。主节点和从节点之间通过网络进行通信，数据的传输速度取决于网络的带宽和延迟。 一主二从配置 Master配置 123456789101112bind 0.0.0.0daemonize norequirepass wgf123# 关闭rdb# save &quot;&quot;save 900 1save 300 10save 60 10000# 关闭aofappendonly no Slave1配置 123456789101112131415161718192021bind 0.0.0.0daemonize norequirepass wgf123# 关闭rdbsave 900 1save 300 10save 60 10000# 关闭aofappendonly no#主从复制# 修改端口port 6380# 配置master的ip和端口，如：192.168.1.2 6379slaveof 192.168.0.181 6379# 如果当前服务器为slave，那么这里配置的就是master的访问密码masterauth wgf123 Slave2配置 12345678910111213141516171819202122bind 0.0.0.0daemonize norequirepass wgf123# 关闭rdbsave 900 1save 300 10save 60 10000# 关闭aofappendonly no#主从复制# 修改端口port 6381# 配置master的ip和端口，如：192.168.1.2 6379slaveof 192.168.0.181 6379# 如果当前服务器为slave，那么这里配置的就是master的访问密码masterauth wgf123 docker 部署 主节点参考docker-安装redis docker 添加2个slave实例 1234567891011需要保证 /data/redis/config/redis_slave_1.conf /data/redis/config/redis_slave_2.conf这两个配置文件存在创建两个目录 /data/redis/data/redis_slave_1 /data/redis/data/redis_slave_2docker run -p 6380:6380 --name redis_slave_1 -v /data/redis/config/redis_slave_1.conf:/etc/redis/redis.conf -v /data/redis/data/redis_slave_1:/data -d redis redis-server /etc/redis/redis.confdocker run -p 6381:6381 --name redis_slave_2 -v /data/redis/config/redis_slave_2.conf:/etc/redis/redis.conf -v /data/redis/data/redis_slave_2:/data -d redis redis-server /etc/redis/redis.conf 动态指定 还有一种方式可以不通过配置文件实现一主多从，使用slaveof 命令指定 master 1slaveof 主机ip 主机端口 测试 查看节点信息 1info replication 12345#### master ####set k1 v1set k2 v2 12345#### Slave1 ####keys *k1k2 12345#### Slave2 ####keys *k1k2 故障模拟 Docker 停止 Slave2, 模拟Slave 故障 1docker stop redis_slave_2 123#### master ####set k3 v3 123456#### Slave1 ####keys *k1k3k2 Docker 启动 Slave2 1docker start redis_slave_2 123456#### Slave2 ####keys *k1k3k2 故障回复，数据重新同步 薪火相传 在Redis的主从架构中，一个从节点也可以充当其他从节点的主节点，实现多级主从复制，也被称为链式复制或级联复制。 在这种情况下，原本是主节点的节点称为顶层主节点或根节点。其他节点既是上一级节点的从节点，同时也是下一级节点的主节点。这样的架构可以形成一个主从节点的链条，实现数据的级联传递和复制。 当一个从节点成为其他从节点的主节点时，它会接收并复制来自上一级节点的数据和操作，并将这些数据和操作传递给下一级节点，以实现层层传递。 需要注意的是，在级联复制中，数据同步的延迟可能会增加，因为每个从节点都需要等待上一级节点同步数据后才能进行同步，链式复制的好处是降低根节点的数据同步压力。 1slaveof 主机ip 主机端口 使用场景 减轻主节点同步压力：多个从节点场景，主节点将数据同步给一个从节点X，其他从节点从从节点X处同步数据。 带宽优化：不同地理位置的数据中心仅需要一个节点进行异地复制，其他节点可以是内网复制。 反客为主 当主节点发生故障时，可以将一个从节点提升为新的主节点，接管写操作的处理。这个过程称为故障转移。一旦新的主节点选举完成并开始接收写操作，其他从节点会继续向新的主节点复制数据，以保持数据的一致性。 从节点切换为主节点的命令： 1slaveof no one 哨兵模式 Redis哨兵是独立于Redis服务外的独立进程 Master故障转移 Redis哨兵模式通过使用独立的哨兵进程来提供一种高可用性的解决方案。每个哨兵进程都是独立运行的，它们通过发送命令给Redis服务器并等待其响应来监控一个或多个Redis实例的健康状况。当主Redis宕机后，哨兵自行将从Redis提升为主节点，实现故障转移。 哨兵的作用 监控：通过心跳机制检测Redis的健康状态。 故障转移：如果主服务器宕机。哨兵会自动将其中一个从服务器提升为新的主服务器，并让其余的从服务器更新配置，指向新的主服务器。 服务发现：在主服务器发生故障转移后，客户端可以重新连接到新的主服务器，而无需人工修改配置。 多哨兵Redis 哨兵实例间的相互发现是通过Redis的发布-订阅功能实现的。每个Sentinel定期在特定频道发布自己的信息，并同时订阅这个频道以接收来自其他哨兵的信息。这样，当一个新的哨兵加入网络时，它通过发布信息到这个频道，被其他哨兵发现并加入到监控网络中。因此，多个哨兵监控同一个Redis主节点，它们就能够发现彼此。在一个哨兵系统中，多个哨兵实例不仅监控 Redis 主服务器和从服务器，而且互相监控以确保哨兵系统本身的健康和可靠性。哨兵Leader：哨兵们会选举出一个领导者来负责执行故障转移过程。共识决策：避免单个哨兵因网络问题或其他原因错误地判定主服务器故障，多哨兵间会达成共识。123456哨兵配置#主节点信息，格式：sentinel &lt;master-name&gt; &lt;ip&gt; &lt;redis-port&gt; &lt;quorum&gt;#&lt;master-name&gt; 为主节点设定的一个自定义名称#&lt;ip&gt; &lt;redis-port&gt; 主节点的ip和端口#&lt;quorum&gt; 多少个 Sentinel 同意主节点不可达时，才会发起故障转移（共识决策）主观下线：主观下线是单个哨兵实例认为主节点线下。客观下线：quorum 个哨兵认为主节点下线，此时需要进行故障转移。 哨兵模式的缺点 数据丢失：主服务器在故障前未能将所有数据同步到从服务器，故障转移可能导致数据丢失。 数据不一致：出现网络分区时，Sentinel 实例不能互相通信，可能导致多个从服务器被提升为主服务器，产生 脑裂 现象，导致数据不一致。 避免脑裂：quorum（故障转移达成共识的最少哨兵数）要大于等于哨兵总数的半数 + 1，满足过半原则。 哨兵配置说明 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970# Example sentinel.conf# 哨兵sentinel实例运行的端口 默认26379port 26379# 哨兵sentinel的工作目录dir /tmp# 哨兵sentinel监控的redis主节点的 ip port # master-name 可以自己命名的主节点名字 只能由字母A-z、数字0-9 、这三个字符&quot;.-_&quot;组成。# quorum 当这些quorum个数sentinel哨兵认为master主节点失联 那么这时 客观上认为主节点失联了# sentinel monitor &lt;master-name&gt; &lt;ip&gt; &lt;redis-port&gt; &lt;quorum&gt;sentinel monitor mymaster 127.0.0.1 6379 2# 当在Redis实例中开启了requirepass foobared 授权密码 这样所有连接Redis实例的客户端都要提供密码# 设置哨兵sentinel 连接主从的密码 注意必须为主从设置一样的验证密码# sentinel auth-pass &lt;master-name&gt; &lt;password&gt;sentinel auth-pass mymaster MySUPER--secret-0123passw0rd# 指定多少毫秒之后 主节点没有应答哨兵sentinel 此时 哨兵主观上认为主节点下线 默认30秒# sentinel down-after-milliseconds &lt;master-name&gt; &lt;milliseconds&gt;sentinel down-after-milliseconds mymaster 30000# 这个配置项指定了在发生failover主备切换时最多可以有多少个slave同时对新的master进行 同步，#这个数字越小，完成failover所需的时间就越长，#但是如果这个数字越大，就意味着越 多的slave因为replication而不可用。#可以通过将这个值设为 1 来保证每次只有一个slave 处于不能处理命令请求的状态。# sentinel parallel-syncs &lt;master-name&gt; &lt;numslaves&gt;sentinel parallel-syncs mymaster 1# 故障转移的超时时间 failover-timeout 可以用在以下这些方面： #1. 同一个sentinel对同一个master两次failover之间的间隔时间。#2. 当一个slave从一个错误的master那里同步数据开始计算时间。直到slave被纠正为向正确的master那里同步数据时。#3.当想要取消一个正在进行的failover所需要的时间。 #4.当进行failover时，配置所有slaves指向新的master所需的最大时间。不过，即使过了这个超时，slaves依然会被正确配置为指向master，但是就不按parallel-syncs所配置的规则来了# 默认三分钟# sentinel failover-timeout &lt;master-name&gt; &lt;milliseconds&gt;sentinel failover-timeout mymaster 180000# SCRIPTS EXECUTION#配置当某一事件发生时所需要执行的脚本，可以通过脚本来通知管理员，例如当系统运行不正常时发邮件通知相关人员。#对于脚本的运行结果有以下规则：#若脚本执行后返回1，那么该脚本稍后将会被再次执行，重复次数目前默认为10#若脚本执行后返回2，或者比2更高的一个返回值，脚本将不会重复执行。#如果脚本在执行过程中由于收到系统中断信号被终止了，则同返回值为1时的行为相同。#一个脚本的最大执行时间为60s，如果超过这个时间，脚本将会被一个SIGKILL信号终止，之后重新执行。#通知型脚本:当sentinel有任何警告级别的事件发生时（比如说redis实例的主观失效和客观失效等等），将会去调用这个脚本，#这时这个脚本应该通过邮件，SMS等方式去通知系统管理员关于系统不正常运行的信息。调用该脚本时，将传给脚本两个参数，#一个是事件的类型，#一个是事件的描述。#如果sentinel.conf配置文件中配置了这个脚本路径，那么必须保证这个脚本存在于这个路径，并且是可执行的，否则sentinel无法正常启动成功。#通知脚本# sentinel notification-script &lt;master-name&gt; &lt;script-path&gt;sentinel notification-script mymaster /var/redis/notify.sh# 客户端重新配置主节点参数脚本# 当一个master由于failover而发生改变时，这个脚本将会被调用，通知相关的客户端关于master地址已经发生改变的信息。# 以下参数将会在调用脚本时传给脚本:# &lt;master-name&gt; &lt;role&gt; &lt;state&gt; &lt;from-ip&gt; &lt;from-port&gt; &lt;to-ip&gt; &lt;to-port&gt;# 目前&lt;state&gt;总是“failover”,# &lt;role&gt;是“leader”或者“observer”中的一个。 # 参数 from-ip, from-port, to-ip, to-port是用来和旧的master和新的master(即旧的slave)通信的# 这个脚本应该是通用的，能被多次调用，不是针对性的。# sentinel client-reconfig-script &lt;master-name&gt; &lt;script-path&gt;sentinel client-reconfig-script mymaster /var/redis/reconfig.sh 配置哨兵 data/redis/config 目录下添加配置 sentinel.conf 12345678910port 26379logfile &quot;26379.log&quot;dir /datasentinel monitor mymaster 192.168.0.181 6379 1sentinel auth-pass mymaster wgf123sentinel down-after-milliseconds mymaster 30000sentinel parallel-syncs mymaster 2# 故障转移sentinel failover-timeout mymaster 60000 docker启动哨兵服务 123docker run -p 26379:26379 --name redis_sentinel \\-v /data/redis/config/sentinel.conf:/etc/redis/redis-sentinel.conf \\-d redis redis-sentinel /etc/redis/redis-sentinel.conf 查看节点信息 1234567891011121314151617181920212223&gt; info replication# Replicationrole:slave # 当前节点身份master_host:192.168.0.181 # 主节点ipmaster_port:6379 # 主节点portmaster_link_status:upmaster_last_io_seconds_ago:1master_sync_in_progress:0slave_read_repl_offset:2568slave_repl_offset:2568slave_priority:100slave_read_only:1replica_announced:1connected_slaves:0master_failover_state:no-failovermaster_replid:c5b383caf344b7bfda01b3891a7cf149983341b7master_replid2:0000000000000000000000000000000000000000master_repl_offset:2568second_repl_offset:-1repl_backlog_active:1repl_backlog_size:1048576repl_backlog_first_byte_offset:1repl_backlog_histlen:2568 暂停主节点 12docker stop redis1等待1分钟 查看节点信息 1234567891011121314151617181920212223&gt; info replication# Replicationrole:slavemaster_host:172.17.0.1master_port:6380 # 6380 节点被哨兵选择为主节点master_link_status:upmaster_last_io_seconds_ago:1master_sync_in_progress:0slave_read_repl_offset:37895slave_repl_offset:37895slave_priority:100slave_read_only:1replica_announced:1connected_slaves:0master_failover_state:no-failovermaster_replid:556e8cc700a7e93dc23b8e3bd5556302c15157cfmaster_replid2:c5b383caf344b7bfda01b3891a7cf149983341b7master_repl_offset:37895second_repl_offset:35671repl_backlog_active:1repl_backlog_size:1048576repl_backlog_first_byte_offset:173repl_backlog_histlen:37723 启动原主节点docker 1docker start redis1 查看节点信息 123456789101112131415161718192021222324info replication# Replicationrole:slavemaster_host:172.17.0.1master_port:6380 # 6380 还是主节点，原主节点下线再上线就变成从节点master_link_status:downmaster_last_io_seconds_ago:-1master_sync_in_progress:0slave_read_repl_offset:1slave_repl_offset:1master_link_down_since_seconds:-1slave_priority:100slave_read_only:1replica_announced:1connected_slaves:0master_failover_state:no-failovermaster_replid:bcbe300ea51d12f5a158f4967582598328e87864master_replid2:0000000000000000000000000000000000000000master_repl_offset:0second_repl_offset:-1repl_backlog_active:0repl_backlog_size:1048576repl_backlog_first_byte_offset:0repl_backlog_histlen:0 SpringBoot 集成哨兵模式 项目连接哨兵服务 添加依赖 1234567891011121314&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-pool2&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;version&gt;1.18.22&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt; 配置 1234567891011121314spring:redis: database: 0 password: wgf123 connect-timeout: 3000 # 连接超时时间（毫秒） sentinel: master: mymaster nodes: 127.0.0.1:26379 # 多个哨兵逗号隔开 lettuce: # Redis的Java驱动包,使用lettuce连接池 pool: max-active: 200 # 连接池最大连接数（使用负值表示没有限制） max-wait: -1 # 连接池最大阻塞等待时间（使用负值表示没有限制） max-idle: 10 # 连接池中的最大空闲连接 (默认为8) min-idle: 0 # 连接池中的最小空闲连接 测试 12345678910111213141516@Slf4j@SpringBootTestpublic class SentinelTest &#123; @Autowired RedisTemplate redisTemplate; @Test public void setTest() &#123; redisTemplate.opsForValue().set(&quot;test&quot;, &quot;test&quot;); &#125; @Test public void getTest() &#123; log.info(&quot;test = &#123;&#125;&quot;, redisTemplate.opsForValue().get(&quot;test&quot;)); &#125;&#125; Redis 集群 多主节点 哈希槽数据分片 部分参考：https://www.cnblogs.com/yufeng218/p/13688582.html Redis集群是一个由 多个主从节点群 组成的分布式服务集群，Redis集群不需要哨兵也能完成节点移除和 故障转移 的功能。需要将每个节点设置成集群模式，这种集群模式没有中心节点，可水平扩展。以下是 Redis 集群的一些关键特点和功能： 数据分片：Redis 集群通过自动将数据分布在不同的节点（称为分片）上来提供数据的水平扩展能力。这种分片是基于哈希槽（Hash Slot）的，总共有 16384 个哈希槽。 高可用性：集群模式支持主从复制（Master-Slave Replication），每个主节点都可以有一个或多个从节点。如果主节点宕机，集群会自动执行故障转移，将一个从节点提升为新的主节点。 无中心架构：Redis 集群没有中心节点，每个节点都保存着部分哈希槽的数据。这样的设计提高了系统的可靠性和可伸缩性。 故障转移：在节点出现故障时，集群能够自动进行故障转移，确保数据的持续可用性。 读写分离：可以通过从节点来分担读取负载，从而实现读写分离，提高性能。 请求重定向：客户端与集群中的任何一个节点通信，如果请求的数据不在该节点，客户端将被重定向到正确的节点。 容错性：虽然 Redis 是基于内存的，但它也支持持久化。在集群模式下，即使部分节点宕机，其他节点仍然可以继续提供服务。 水平扩展：Redis 集群支持通过增加更多节点来线性扩展系统的处理能力和存储容量。 集群搭建 部署规划 ​ 三组Redis主从复制，三个master，三个slave共6个节点 第一组 6400(主) 6401(从) 第二组 7400(主) 7401(从) 第三组 8400(主) 8401(从) 创建文件夹 在/data/redis/config/cluster目录下分别按照端口创建文件夹，用于存放配置文件 在/data/redis/data/cluster目录下分别按照端口创建文件夹，用于存放持久化数据 配置文件 12345678910111213141516171819202122232425262728293031# 记得修改端口号port 6400daemonize no#指定数据文件存放位置，必须要指定不同的目录位置，不然会丢失数据(记得修改路径)dir /data#启动集群模式cluster-enabled yes#集群节点信息文件，这里640x最好和port对应上cluster-config-file nodes-6400.conf# 节点离线的超时时间cluster-node-timeout 5000#去掉bind绑定访问ip信息bind 0.0.0.0#关闭保护模式protected-mode no #启动AOF文件appendonly yes#如果要设置密码需要增加如下配置：#设置redis访问密码requirepass wgf123#设置集群节点间访问密码，跟上面一致masterauth wgf123 分别复制6份到/data/redis/config/cluster目录对应的端口文件夹下，记得修改端口以及路径 运行docker 12345678910111213docker run -p 6400:6400 -p 16400:16400 --name redis_6400 -v /data/redis/config/cluster/6400/redis.conf:/etc/redis/redis.conf -v /data/redis/data/cluster/6400:/data -d redis redis-server /etc/redis/redis.confdocker run -p 6401:6401 -p 16401:16401 --name redis_6401 -v /data/redis/config/cluster/6401/redis.conf:/etc/redis/redis.conf -v /data/redis/data/cluster/6401:/data -d redis redis-server /etc/redis/redis.confdocker run -p 7400:7400 -p 17400:17400 --name redis_7400 -v /data/redis/config/cluster/7400/redis.conf:/etc/redis/redis.conf -v /data/redis/data/cluster/7400:/data -d redis redis-server /etc/redis/redis.confdocker run -p 7401:7401 -p 17401:17401 --name redis_7401 -v /data/redis/config/cluster/7401/redis.conf:/etc/redis/redis.conf -v /data/redis/data/cluster/7401:/data -d redis redis-server /etc/redis/redis.confdocker run -p 8400:8400 -p 18400:18400 --name redis_8400 -v /data/redis/config/cluster/8400/redis.conf:/etc/redis/redis.conf -v /data/redis/data/cluster/8400:/data -d redis redis-server /etc/redis/redis.confdocker run -p 8401:8401 -p 18401:18401 --name redis_8401 -v /data/redis/config/cluster/8401/redis.conf:/etc/redis/redis.conf -v /data/redis/data/cluster/8401:/data -d redis redis-server /etc/redis/redis.conf参数说明：-p 16400:16400 我除了要开放6400端口，还要在服务器控制台开放16400端口，才能建立起集群(原端口 +10000) 创建集群 123456789101112131415161718192021222324252627282930313233# IP排在前面的是master节点# 登录任意一台redisredis-cli -a wgf123 --cluster create 192.168.0.181:6400 192.168.0.181:7400 192.168.0.181:8400 192.168.0.181:6401 192.168.0.181:7401 192.168.0.181:8401 --cluster-replicas 1&gt;&gt;&gt; Performing Cluster Check (using node 192.168.0.181:6400)M: 8974118e3a7f1a599c744b0b7b1c7ec12db541a3 192.168.0.181:6400 slots:[0-5460] (5461 slots) master # 第一台master节点的hash槽分配 1 additional replica(s)S: 9bc6fc61a175c167c02b9c9817a17547098feb14 172.17.0.1:6401 slots: (0 slots) slave replicates c9c36b72db1a20a2436d640ba5668caf611736f9S: bafbf2b199305aacfe60ccdb5e049ba881a3d281 172.17.0.1:8401 slots: (0 slots) slave replicates 8974118e3a7f1a599c744b0b7b1c7ec12db541a3M: b4fae0480f54069e46b1ac3fd56b57426e37dc2d 172.17.0.1:8400 slots:[10923-16383] (5461 slots) master # 第三台master节点的hash槽分配 1 additional replica(s)M: c9c36b72db1a20a2436d640ba5668caf611736f9 172.17.0.1:7400 slots:[5461-10922] (5462 slots) master # 第二台master节点的hash槽分配 1 additional replica(s)S: 52a7982c543f1131f7bea9a5886d3ef6fbf7b40f 172.17.0.1:7401 slots: (0 slots) slave replicates b4fae0480f54069e46b1ac3fd56b57426e37dc2d[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots... &gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered.# 参数说明-a ：密码；--cluster-replicas 1：表示1个master下挂1个slave； --cluster-replicas 2：表示1个master下挂2个slave 集群命令 12345678redis-cli --cluster help create：创建一个集群环境host1:port1 ... hostN:portNcall：可以执行redis命令add-node：将一个节点添加到集群里，第一个参数为新节点的ip:port，第二个参数为集群中任意一个已经存在的节点的ip:portdel-node：移除一个节点reshard：重新分片check：检查集群状态 查看集群的信息cluster info，登录任意一台node 1234567891011121314151617&gt; cluster infocluster_state:okcluster_slots_assigned:16384cluster_slots_ok:16384cluster_slots_pfail:0cluster_slots_fail:0cluster_known_nodes:6cluster_size:3cluster_current_epoch:6cluster_my_epoch:5cluster_stats_messages_ping_sent:4634cluster_stats_messages_pong_sent:4667cluster_stats_messages_meet_sent:1cluster_stats_messages_sent:9302cluster_stats_messages_ping_received:4667cluster_stats_messages_pong_received:4635cluster_stats_messages_received:9302 查看节点列表cluster nodes 1234567&gt; cluster nodesb4fae0480f54069e46b1ac3fd56b57426e37dc2d 172.17.0.6:8400@18400 myself,master - 0 1644832798000 5 connected 10923-1638352a7982c543f1131f7bea9a5886d3ef6fbf7b40f 172.17.0.1:7401@17401 slave b4fae0480f54069e46b1ac3fd56b57426e37dc2d 0 1644832800077 5 connectedbafbf2b199305aacfe60ccdb5e049ba881a3d281 172.17.0.1:8401@18401 slave 8974118e3a7f1a599c744b0b7b1c7ec12db541a3 0 1644832799575 1 connected8974118e3a7f1a599c744b0b7b1c7ec12db541a3 172.17.0.1:6400@16400 master - 0 1644832799073 1 connected 0-54609bc6fc61a175c167c02b9c9817a17547098feb14 172.17.0.1:6401@16401 slave c9c36b72db1a20a2436d640ba5668caf611736f9 0 1644832799575 3 connectedc9c36b72db1a20a2436d640ba5668caf611736f9 172.17.0.1:7400@17400 master - 0 1644832799000 3 connected 5461-10922 查询集群中的值 CLUSTER KEYSLOT &lt;key&gt;: 此命令用于查询某个特定键（key）被映射到的哈希槽的编号。 CLUSTER COUNTKEYSINSLOT &lt;slot&gt;: 用于查询指定的哈希槽中包含的键的数量。 CLUSTER GETKEYSINSLOT &lt;slot&gt; &lt;count&gt;: 此命令返回指定哈希槽中的一定数量的键。 Hash Tag Redis 集群中的哈希标签（Hash Tag）是一种特殊的机制，用于确保特定的键被分配到相同的哈希槽（Hash Slot），从而允许在集群模式下完成对多个键的操作。 哈希标签通过在键名中加入 &#123;&#125; 来使用。在 &#123;&#125; 中的内容被用于计算哈希槽，而 &#123;&#125; 外的部分被忽略。例如，在键 user&#123;12345&#125;:followers 中，12345 是哈希标签。 Hash Tag 使用在 Redis 集群模式下，MSET 命令默认情况下不可用，因为它涉及到一次性对多个键进行设置，而这些键可能属于不同的哈希槽。使用哈希标签解决 MSET 问题:例如，使用 MSET key1{tag} value1 key2{tag} value2 可以确保 key1{tag} 和 key2{tag} 被分配到同一个哈希槽，从而能使用 MSET 命令。 集群原理分析 槽位分配：Redis Cluster 将所有数据划分为 16384 个槽位（slots）。每个槽位负责存储一部分数据，从而实现数据的分布式存储。 主从节点与槽位：在集群中，槽位只直接分配给主节点（master nodes）。每个主节点负责一部分槽位。虽然从节点（slave nodes）不直接分配槽位，但它们通过复制各自的主节点来存储相应槽位的数据。 客户端的槽位缓存：当 Redis 集群的客户端连接到集群时，它会接收到集群的槽位配置信息，并在本地缓存这些信息。客户端可以直接定位到包含特定键的目标节点，提高查询效率。 槽位定位算法 Cluster 默认会对 key 值使用 crc16 算法进行 hash 得到一个整数值，然后用这个整数值对 16384 进行取模来得到具体槽位。 HASH_SLOT = CRC16(key) % 16384 跳转重定位 类似ES的协调节点 纠正槽位映射 客户端指令处理：当客户端向 Redis 集群的某个节点发送指令时，这个节点首先根据指令中的 key 计算出对应的槽位。 槽位归属检查：接着，该节点检查该槽位是否属于自己管理。如果是，它会直接执行该指令；如果不是，进入下一步。 重定向操作：当槽位不属于当前节点时，该节点会向客户端发送一个重定向响应，其中包含负责该槽位的目标节点地址。 响应客户端：客户端收到重定向响应后，会根据提供的信息连接到正确的节点，并在那里重新执行原指令。 更新槽位映射表：客户端也会更新其本地槽位映射表，以反映最新的槽位和节点对应关系。这样，后续对相同或相关 key 的操作可以直接定位到正确的节点，减少重定向的发生。 Redis集群节点发现 gossip协议： Redis 集群使用 Gossip 协议进行节点间的通信和信息交换。这个协议允许节点定期随机交换信息，如其他节点的状态、已知的地址等。 Gossip 通信是增量的和不频繁的，帮助减少网络带宽消耗，同时确保了集群状态信息的最新性和一致性。 节点发现： 新节点加入集群时，它会通过已知节点的信息来发现其他节点，并通过 Gossip 协议与它们建立通信。 节点使用 TCP 连接在集群内部彼此通信，默认端口为 Redis 服务端口的加10000，如 Redis 端口为6379，则集群通信端口为16379。 Redis集群选举原理 过半机制 由其他Master节点选取新的Master 客观下线：某个主节点宕机，够多的主节点（根据集群配置的 quorum）同意某个主节点失效时，该节点被标记为客观下线（ODOWN）。 发起投票：从节点开始发起投投票，只有主节点能够投票。 投票限制：每个主节点在一个选举周期内只能投票一次。 赢得选举：某个节点获得超过半数主节点的投票，当选新的主节点。 通知和同步：通过 Gossip 协议通知集群中的所有节点当选信息，从节点开始同步数据。 故障恢复 如果主节点下线？从节点能否自动升为主节点？注意：15 秒超时。 当 6400 挂掉后，8401 成为新的主机。 主节点恢复后，主从关系会如何？主节点回来变成从机。 当 6400 重启后，6400 成为 8401 的从机。 如果所有某一段插槽的主从节点都宕掉，redis 服务是否还能继续? 如果某一段插槽的主从都挂掉，而 cluster-require-full-coverage=yes，那么 ，整个集群都挂掉。 如果某一段插槽的主从都挂掉，而 cluster-require-full-coverage=no，那么，该插槽数据全都不能使用，也无法存储。 redis.conf 中的参数 cluster-require-full-coverage Redis集群为什么至少需要三个（奇数）master节点 故障转移：在一个主节点故障时，至少需要另外两个主节点来达成共识（基于过半原则），以进行故障转移和选举新的主节点。如果只有两个主节点，当一个节点失效时，另一个节点无法单独做出决策。 避免脑裂：在网络分区的情况下，集群可能被分成两部分。根据选举过半原则，三个节点被分成两部分，其中一部分满足选举过半机制，另一部分不满足。 集群的优缺点 优点 水平扩展：通过HashSlot对数据分片，实现水平扩展。 高可用性：提供主从复制和故障转移能力。 负载均衡：数据请求分摊到多个节点，避免单个节点过载。 去中心化架构：减少单点故障的风险，提升了系统的稳定性。 缺点 多键操作限制：不允许跨节点的 MSET，LUA，事务 操作，这些操作只能在同一个节点。 数据倾斜：数据可能在节点间分布不均，导致部分节点压力较大。 SpringBoot 使用Redis集群 添加maven依赖 123456789&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring‐boot‐starter‐data‐redis&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons‐pool2&lt;/artifactId&gt;&lt;/dependency&gt; 配置文件 1234567891011121314spring:redis: database: 0 password: wgf123 connect-timeout: 3000 # 连接超时时间（毫秒） cluster: nodes: 127.0.0.1:6400,127.0.0.1:6401,127.0.0.1:7400,127.0.0.1:7401,127.0.0.1:8400,127.0.0.1:8401 lettuce: # Redis的Java驱动包,使用lettuce连接池 pool: max-active: 10 # 连接池最大连接数（使用负值表示没有限制） max-wait: -1 # 连接池最大阻塞等待时间（使用负值表示没有限制） max-idle: 8 # 连接池中的最大空闲连接 (默认为8) min-idle: 0 # 连接池中的最小空闲连接 配置类 12345678910111213141516171819202122@Configurationpublic class RedisConfig &#123; @Bean public RedisTemplate redisTemplate(RedisConnectionFactory redisConnectionFactory) &#123; RedisTemplate redisTemplate = new RedisTemplate&lt;&gt;(); redisTemplate.setConnectionFactory(redisConnectionFactory); // 使用Jackson2JsonRedisSerialize替换默认序列化 Jackson2JsonRedisSerializer jackson2JsonRedisSerializer = new Jackson2JsonRedisSerializer(Object.class); ObjectMapper objectMapper = new ObjectMapper(); objectMapper.setVisibility(PropertyAccessor.ALL, JsonAutoDetect.Visibility.ANY); objectMapper.activateDefaultTyping(new LaissezFaireSubTypeValidator(), ObjectMapper.DefaultTyping.EVERYTHING); jackson2JsonRedisSerializer.setObjectMapper(objectMapper); // 设置value的序列化规则和key的序列化规则 redisTemplate.setKeySerializer(new StringRedisSerializer()); redisTemplate.setHashKeySerializer(new StringRedisSerializer()); redisTemplate.setValueSerializer(jackson2JsonRedisSerializer); redisTemplate.setHashValueSerializer(jackson2JsonRedisSerializer); redisTemplate.afterPropertiesSet(); return redisTemplate; &#125;&#125; 单元测试 12345678910@Testpublic void test() &#123; for (int i = 1; i &lt;= 100; i++) &#123; String key = &quot;k_&quot; + i; String value = &quot;v_&quot; + i; redisTemplate.opsForValue().set(key, value); &#125;&#125;最终这100个键值对会hash散落到3个master实例中 分布式锁实现 跨JVM的互斥机制来控制共享资源的访问 spring-integration-redis 申请锁LUA脚本 123456789local lockClientId = redis.call(&#x27;GET&#x27;, KEYS[1]) // 根据锁的key获取clientIdif lockClientId == ARGV[1] thenredis.call(&#x27;PEXPIRE&#x27;, KEYS[1], ARGV[2]) // 如果当前key的value和clientId一致，则重新设置锁时间（可重入）return trueelseif not lockClientId then redis.call(&#x27;SET&#x27;, KEYS[1], ARGV[1], &#x27;PX&#x27;, ARGV[2]) // 如果clientId不存在，则尝试获取锁return trueendreturn false 释放锁LUA脚本(非阻塞) Reids unlink 12345if (redis.call(&#x27;unlink&#x27;, KEYS[1]) == 1) then // 将键与键空间断开连接，异步删除redis.call(&#x27;publish&#x27;, ARGV[1], KEYS[1])return true endreturn false 释放锁LUA脚本(阻塞) 12345if (redis.call(&#x27;del&#x27;, KEYS[1]) == 1) then redis.call(&#x27;publish&#x27;, ARGV[1], KEYS[1]) return true endreturn false 防止锁被误删 当线程A获取分布式锁后，如果不加控制，线程B调用释放锁方法可能会把线程A获得的锁进行释放，为了防止这种情况发生，Spring在实现 RedisLockRegistry 子类 RedisLock 的加锁方式时，RedisLock类内部会有一个获得锁的私有变量，当获得分布式锁时设置标志，释放锁时先判断 RedisLock 是否有获得锁的标志，有才执行锁释放方法防止误删 还有另一种做法是使用UUID，在 Redis set时，value拼接上UUID，删除时判断锁对象的UUID和Redis value的 UUID是否相等 整合 添加maven依赖 12345&lt;!-- 分布式锁支持 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.integration&lt;/groupId&gt; &lt;artifactId&gt;spring-integration-redis&lt;/artifactId&gt;&lt;/dependency&gt; 添加分布式锁配置 123456789101112131415161718@Configurationpublic class RedisLockConfiguration &#123; /** * 锁前缀 */ private final static String LOCK_NAME = &quot;redis-lock&quot;; /** * 锁过期时间 */ private final static Long EXPIRE = 20000L; @Bean public RedisLockRegistry redisLockRegistry(RedisConnectionFactory redisConnectionFactory) &#123; return new RedisLockRegistry(redisConnectionFactory, LOCK_NAME, EXPIRE); &#125;&#125; 使用 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162@Api(tags = &quot;分布式锁API&quot;)@RestControllerpublic class LockController &#123; public static int NUM = 100000; @Autowired private ExecutorService executorService; @Autowired private RedisLockRegistry redisLockRegistry; @ApiOperation(value = &quot;分布式锁&quot;) @GetMapping(&quot;lock&quot;) public int lock() &#123; CountDownLatch countDownLatch = new CountDownLatch(1000); Runnable task = new Task(countDownLatch); for (int i = 0; i &lt; 1000; i++) &#123; executorService.execute(task); &#125; try &#123; countDownLatch.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; return NUM; &#125; class Task implements Runnable &#123; private CountDownLatch latch; public Task(CountDownLatch latch) &#123; this.latch = latch; &#125; @Override public void run() &#123; // 获取锁对象 Lock lock = redisLockRegistry.obtain(&quot;lock&quot;); boolean lockRes = false; try &#123; // 尝试获取锁，10秒超时 //lockRes = lock.tryLock(10, TimeUnit.SECONDS); // 死等 lock.lock(); log.info(&quot;获取锁成功&quot;); NUM--; latch.countDown(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; // 释放锁 lock.unlock(); &#125; &#125; &#125;&#125; 总结 ​ spring-integration-redis 设计优秀之处在于引入本地锁概念，通过本地锁防止高并发场景频繁向Redis发起访问，降低Redis压力。其次是实现了可重入锁的功能，在锁里重新加锁则更新锁的持有时间（TTL） 自定义实现极简版 分布式锁 SET key value [PX milliseconds] [NX] 命令是实现分布式锁的核心 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374@Componentpublic class MyRedisLock &#123; private final String PREFIX = &quot;redis-lock:&quot;; @Autowired private RedisTemplate redisTemplate; /** * 获取锁 * * @param lockName 锁名称 * @param lockTimeOut 锁超时时间 毫秒 * @return */ public void lock(String lockName, long lockTimeOut) &#123; lockName = getLockName(lockName); while (true) &#123; boolean result = redisTemplate.opsForValue().setIfAbsent(lockName, null, lockTimeOut, TimeUnit.MILLISECONDS); if (result) &#123; return; &#125; try &#123; TimeUnit.MILLISECONDS.sleep(100); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; /** * 尝试获取锁 * * @param lockName 锁名称 * @param lockTimeOut 锁超时时间 毫秒 * @param waitTimeOut 获取锁等待超时时间 毫秒 * @return */ public boolean tryLock(String lockName, long lockTimeOut, long waitTimeOut) &#123; lockName = getLockName(lockName); long startTime = System.currentTimeMillis(); while ((System.currentTimeMillis() - startTime) &lt; waitTimeOut) &#123; boolean result = redisTemplate.opsForValue().setIfAbsent(lockName, null, lockTimeOut, TimeUnit.MILLISECONDS); if (result) &#123; return result; &#125; try &#123; TimeUnit.MILLISECONDS.sleep(100); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; return false; &#125; /** * 释放锁 * * @param lockName 锁名称 * @return */ public boolean unlock(String lockName) &#123; lockName = getLockName(lockName); return redisTemplate.delete(lockName); &#125; private String getLockName(String lockName) &#123; return String.format(&quot;%s%s&quot;, PREFIX, lockName); &#125;&#125; 使用 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152@RestControllerpublic class LockController &#123; public static int NUM = 100000; @Autowired private ExecutorService executorService; @Autowired private MyRedisLock myRedisLock; @ApiOperation(value = &quot;自定义分布式锁&quot;) @GetMapping(&quot;my_lock&quot;) public int myLock() &#123; CountDownLatch countDownLatch = new CountDownLatch(1000); Runnable task = new MyLockTask(countDownLatch); for (int i = 0; i &lt; 1000; i++) &#123; executorService.execute(task); &#125; try &#123; countDownLatch.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; return NUM; &#125; class MyLockTask implements Runnable &#123; private CountDownLatch latch; public MyLockTask(CountDownLatch latch) &#123; this.latch = latch; &#125; @Override public void run() &#123; try &#123; myRedisLock.lock(&quot;test&quot;, 10 * 1000); log.info(&quot;获取锁成功&quot;); NUM--; latch.countDown(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; myRedisLock.unlock(&quot;test&quot;); &#125; &#125; &#125;&#125; 总结 不推荐这个例子分布式锁的实现方式，死循环获取锁实现过于粗暴，当有强烈的锁竞争场景对Redis会造成较大压力 高可用问题 缓存穿透 访问了根本不存在的数据 请求全部落在DB上 概念 缓存穿透 是指当请求查询一个在缓存和数据库中都不存在的数据时，请求会穿过缓存层直接查询数据库。这种情况如果被恶意利用或频繁发生，会给数据库带来很大压力，甚至导致数据库宕机。 解决方案 布隆过滤器会有误判率 元素不能删除 布谷鸟过滤器基本原理：数据结构：布隆过滤器本质上是一个很大的位数组（bit array）和几个哈希函数。插入操作：当插入一个元素时，使用多个哈希函数对元素进行哈希，并将得到的哈希值所对应的位数组中的位置置为 1。查询操作：当查询一个元素时，同样使用这些哈希函数进行哈希，并检查位数组中相应的位置是否都是 1。如果都是 1，那么元素可能存在于集合中；如果任何一个位置是 0，则元素绝对不在集合中。特点：不确定性：布隆过滤器可能会有误判。不支持删除：标准的布隆过滤器不支持从集合中删除元素，因为将位数组的位从 1 改回 0 会影响其他元素。应用场景：网络应用：常被用于网络应用中，例如用于网页爬虫的 URL 检查。数据库：数据库系统使用布隆过滤器来快速判断数据是否存，从而避免不必要的访问。 缓存空对象基本原理：当缓存和数据库都未命中某个查询请求时，在缓存层添加一个 空对象 缓存，并设置 过期时间。后续对同一不存在数据的查询可以直接从缓存中获取这个空对象，避免请求落库。存在的问题：存储大量空对象会占用缓存空间，被恶意攻击可能导致缓存空间迅速被耗尽。即使设置了过期时间，也可能导致短暂时间的数据不一致（数据库后续添加了空对象对应的业务数据）。 缓存击穿 承载大并发的热点Key过期失效 海量请求落库查询 缓存击穿是指当某个热点数据（即高频访问数据）在缓存中失效（过期）的那一刻，同时有大量并发请求这个数据时，这些请求会直接击穿缓存，全部落到数据库上。在重新缓存数据这一期间，大量并发请求直接访问数据库，可能导致数据库崩溃。 解决方案 设置热点数据永不过期不设置过期时间：对于热点数据，缓存层不设置过期时间。逻辑过期：在缓存的 value 设置过期时间，使用单独线程定期扫描，发现逻辑过期后重新构建缓存。 使用互斥锁当缓存失效时，不是每个请求都去数据库加载数据，而是使用互斥锁机制确保只有一个请求去数据库查询数据并重新加载到缓存中，其他请求等待缓存恢复后再访问缓存。 缓存雪崩 缓存集中失效 缓存服务节点宕机、断网 缓存雪崩是指在一个缓存系统中，由于缓存系统宕机或者缓存数据大面积失效，导致后续的请求都落到了数据库上，从而引发数据库压力剧增，甚至可能导致数据库服务崩溃。场景如缓存服务宕机恢复后或大量缓存数据集中在同一时间过期。 解决方案 分散Key的过期时间随机过期时间：为缓存数据设置随机的过期时间，而不是统一的过期时间，以避免同时大量缓存失效。预设不同过期时间：根据数据的不同特性和重要性，设置不同的过期时间。 部署高可用缓存系统集群部署：Redis Cluster 缓存集群，提高缓存系统的高可用性。哨兵模式：部署一主多从模式，故障自动转移。 限流降级当缓存大面积失效后触发熔断器，对缓存服务进行降级处理，返回降级内容。在系统入口处实施限流措施，合理控制访问频率。在缓存失效或不可用时，可以启用服务降级策略，如返回默认数据或简化的服务内容。 使用布隆过滤器 布隆过滤器介绍 布隆过滤器 动画演示 单机 guava 版 添加maven依赖 12345&lt;dependency&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;version&gt;31.0.1-jre&lt;/version&gt;&lt;/dependency&gt; 使用源码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192@Api(tags = &quot;单机版本布隆过滤器&quot;)@Slf4j@RestControllerpublic class GuavaBloomFilterController &#123; /** * 插入多少数据 */ private static final int insertions = 1000000; /** * 期望的误判率 */ private static double fpp = 0.02; /** * 单机版本布隆过滤器 */ private BloomFilter&lt;String&gt; bf; /** * 用于校验bloomFilter 准确性 */ private Set&lt;String&gt; keySet = new HashSet&lt;&gt;(); @Autowired private RedisTemplate redisTemplate; /** * 数据预热 * 初始化布隆过滤器 */ @PostConstruct public void init() &#123; bf = BloomFilter.create(Funnels.stringFunnel(Charsets.UTF_8), insertions, fpp); // 添加缓存数据 for (int i = 0; i &lt; 5000; i++) &#123; String key = generate(&quot;k&quot;, i); String value = generate(&quot;v&quot;, i); // 数据添加布隆过滤器 bf.put(key); keySet.add(key); redisTemplate.opsForValue().set(key, value); &#125; &#125; @ApiImplicitParams(&#123; @ApiImplicitParam(name = &quot;start&quot;, value = &quot;起始检索点 取值[0, 5000]&quot;, dataType = &quot;int&quot;), @ApiImplicitParam(name = &quot;end&quot;, value = &quot;截止检索点 取值[0, 5000] 需要比start大&quot;, dataType = &quot;int&quot;) &#125;) @ApiOperation(value = &quot;测试单机版布隆过滤器&quot;) @GetMapping(&quot;test&quot;) public Map&lt;String, Object&gt; test(@RequestParam(&quot;start&quot;) Integer start, @RequestParam(&quot;end&quot;) Integer end) &#123; // 根据参数生成key List&lt;String&gt; keyList = IntStream.range(start, end) .mapToObj(line -&gt; generate(&quot;k&quot;, line)) .collect(Collectors.toList()); // 布隆过滤器拦截 int count = 0; List&lt;String&gt; keys = new ArrayList&lt;&gt;(); for (String key : keyList) &#123; // 布隆过滤器过滤 if (bf.mightContain(key)) &#123; keys.add(key); &#125; // 判断布隆过滤器准确性 if (keySet.contains(key)) &#123; count++; &#125; &#125; double accuracy = keys.size() / count * 100.0d; List result = redisTemplate.opsForValue().multiGet(keys); Map&lt;String, Object&gt; map = new HashMap&lt;&gt;(); map.put(&quot;bloomFilter 准确率&quot;, accuracy); map.put(&quot;缓存数据命中个数&quot;, result.size()); return map; &#125; private String generate(Object obj1, Object obj2) &#123; return String.format(&quot;%s_%s&quot;, obj1, obj2); &#125;&#125; 测试 启动项目访问 swagger Redis BitMap 分布式版 核心是使用redis的BitMap 位图代替本地内存存储缓存标识，实现多实例分布式可用的BloomFilter； 实现上依赖 com.google.guava 源码实现 BitMapBloomFilter 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164public class BitMapBloomFilter&lt;T&gt; &#123; /** * 布隆过滤器Key */ private String key; /** * 需要执行hash函数次数 */ private int numHashFunctions; /** * 过滤器大小 */ private int bitSize; private Funnel&lt;T&gt; funnel; private RedisTemplate redisTemplate; /** * @param redisTemplate * @param expectedInsertions 预计插入的元素数量 */ public BitMapBloomFilter(RedisTemplate redisTemplate, String key, int expectedInsertions) &#123; this.key = key; this.redisTemplate = redisTemplate; this.funnel = (Funnel&lt;T&gt;) Funnels.stringFunnel(Charset.defaultCharset()); bitSize = optimalNumOfBits(expectedInsertions, 0.02); numHashFunctions = optimalNumOfHashFunctions(expectedInsertions, bitSize); &#125; /** * @param redisTemplate * @param expectedInsertions 预计插入的数量 * @param fpp 期望的误判率 */ public BitMapBloomFilter(RedisTemplate redisTemplate, String key, int expectedInsertions, double fpp) &#123; this.key = key; this.redisTemplate = redisTemplate; this.funnel = (Funnel&lt;T&gt;) Funnels.stringFunnel(Charset.defaultCharset()); bitSize = optimalNumOfBits(expectedInsertions, fpp); numHashFunctions = optimalNumOfHashFunctions(expectedInsertions, bitSize); &#125; /** * @param redisTemplate * @param funnel * @param expectedInsertions 预计插入的数量 * @param fpp 期望的误判率 */ public BitMapBloomFilter(RedisTemplate redisTemplate, String key, Funnel&lt;T&gt; funnel, int expectedInsertions, double fpp) &#123; this.key = key; this.redisTemplate = redisTemplate; this.funnel = funnel; bitSize = optimalNumOfBits(expectedInsertions, fpp); numHashFunctions = optimalNumOfHashFunctions(expectedInsertions, bitSize); &#125; /** * 使用murmur hash算法计算值的下标 * * @param value * @return */ public int[] murmurHashOffset(T value) &#123; int[] offset = new int[numHashFunctions]; long hash64 = Hashing.murmur3_128().hashObject(value, funnel).asLong(); int hash1 = (int) hash64; int hash2 = (int) (hash64 &gt;&gt;&gt; 32); for (int i = 1; i &lt;= numHashFunctions; i++) &#123; int nextHash = hash1 + i * hash2; if (nextHash &lt; 0) &#123; nextHash = ~nextHash; &#125; offset[i - 1] = nextHash % bitSize; &#125; return offset; &#125; /** * 计算bit数组长度 */ private int optimalNumOfBits(long n, double p) &#123; if (p == 0) &#123; p = Double.MIN_VALUE; &#125; return (int) (-n * Math.log(p) / (Math.log(2) * Math.log(2))); &#125; /** * 计算hash方法执行次数 */ private int optimalNumOfHashFunctions(long n, long m) &#123; return Math.max(1, (int) Math.round((double) m / n * Math.log(2))); &#125; /** * 删除布隆过滤器 * * @param key KEY */ public void delete(String key) &#123; redisTemplate.delete(key); &#125; /** * 根据给定的布隆过滤器添加值，在添加一个元素的时候使用，批量添加的性能差 * * @param bloomFilterHelper 布隆过滤器对象 * @param key KEY * @param value 值 * @param &lt;T&gt; 泛型，可以传入任何类型的value */ public void add(T value) &#123; int[] offset = this.murmurHashOffset(value); for (int i : offset) &#123; redisTemplate.opsForValue().setBit(key, i, true); &#125; &#125; /** * 根据给定的布隆过滤器添加值，在添加一批元素的时候使用，批量添加的性能好，使用pipeline方式(如果是集群下，请使用优化后RedisPipeline的操作) * * @param bloomFilterHelper 布隆过滤器对象 * @param key KEY * @param valueList 值，列表 * @param &lt;T&gt; 泛型，可以传入任何类型的value */ public void addList(List&lt;? extends T&gt; valueList) &#123; redisTemplate.executePipelined(new RedisCallback&lt;Long&gt;() &#123; @Override public Long doInRedis(RedisConnection connection) throws DataAccessException &#123; connection.openPipeline(); for (T value : valueList) &#123; int[] offset = murmurHashOffset(value); for (int i : offset) &#123; connection.setBit(key.getBytes(), i, true); &#125; &#125; return null; &#125; &#125;); &#125; /** * 根据给定的布隆过滤器判断值是否存在 * * @param bloomFilterHelper 布隆过滤器对象 * @param key KEY * @param value 值 * @return 是否存在 */ public boolean contains(T value) &#123; int[] offset = this.murmurHashOffset(value); for (int i : offset) &#123; if (!redisTemplate.opsForValue().getBit(key, i)) &#123; return false; &#125; &#125; return true; &#125;&#125; BitMapBloomFilterRegistry 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859public class BitMapBloomFilterRegistry &#123; public BitMapBloomFilterRegistry(RedisTemplate redisTemplate) &#123; this.redisTemplate = redisTemplate; &#125; /** * 默认需要插入的元素 */ private static final int DEFAULT_EXPECTED_INSERTIONS = 2000; private RedisTemplate redisTemplate; /** * 过滤器前缀 */ private final String prefix = &quot;redis:bloomfilter:&quot;; /** * 布隆过滤器容器 */ private final Map&lt;String, BitMapBloomFilter&lt;CharSequence&gt;&gt; filters = new ConcurrentHashMap&lt;&gt;(16); /** * 获取布隆过滤器 * 如果获取不到则根据默认配置创建布隆过滤器 * * @param filterName * @return */ public BitMapBloomFilter&lt;CharSequence&gt; obtain(String filterName) &#123; filterName = this.getFilterName(filterName); BitMapBloomFilter&lt;CharSequence&gt; bitMapBloomFilter = filters.computeIfAbsent(filterName, key -&gt; new BitMapBloomFilter&lt;CharSequence&gt;(redisTemplate, key, DEFAULT_EXPECTED_INSERTIONS)); return bitMapBloomFilter; &#125; /** * 获取布隆过滤器，获取不到则根据配置创建 * * @param filterName 布隆过滤器名称 * @param expectedInsertions 预计插入的数量 * @param fpp 期望的误判率 * @return */ public BitMapBloomFilter&lt;CharSequence&gt; obtain(String filterName, int expectedInsertions, double fpp) &#123; filterName = this.getFilterName(filterName); Funnel&lt;CharSequence&gt; funnel = (Funnel&lt;CharSequence&gt;) Funnels.stringFunnel(Charset.defaultCharset()); BitMapBloomFilter&lt;CharSequence&gt; bitMapBloomFilter = filters.computeIfAbsent(filterName, key -&gt; new BitMapBloomFilter&lt;CharSequence&gt;(redisTemplate, key, funnel, expectedInsertions, fpp)); return bitMapBloomFilter; &#125; private String getFilterName(String name) &#123; return String.format(&quot;%s%s&quot;, prefix, name); &#125;&#125; 添加注册器配置 123456789101112131415161718192021222324252627@Configurationpublic class RedisConfig &#123; @Bean public BitMapBloomFilterRegistry bitMapBloomFilterRegistry(RedisTemplate redisTemplate) &#123; return new BitMapBloomFilterRegistry(redisTemplate); &#125; @Bean public RedisTemplate redisTemplate(RedisConnectionFactory redisConnectionFactory) &#123; RedisTemplate redisTemplate = new RedisTemplate&lt;&gt;(); redisTemplate.setConnectionFactory(redisConnectionFactory); // 使用Jackson2JsonRedisSerialize替换默认序列化 Jackson2JsonRedisSerializer jackson2JsonRedisSerializer = new Jackson2JsonRedisSerializer(Object.class); ObjectMapper objectMapper = new ObjectMapper(); objectMapper.setVisibility(PropertyAccessor.ALL, JsonAutoDetect.Visibility.ANY); objectMapper.activateDefaultTyping(new LaissezFaireSubTypeValidator(), ObjectMapper.DefaultTyping.EVERYTHING); jackson2JsonRedisSerializer.setObjectMapper(objectMapper); // 设置value的序列化规则和key的序列化规则 redisTemplate.setKeySerializer(new StringRedisSerializer()); redisTemplate.setHashKeySerializer(new StringRedisSerializer()); redisTemplate.setValueSerializer(jackson2JsonRedisSerializer); redisTemplate.setHashValueSerializer(jackson2JsonRedisSerializer); redisTemplate.afterPropertiesSet(); return redisTemplate; &#125;&#125; 使用 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869@Api(tags = &quot;Redis BitMap 版布隆过滤器&quot;)@Slf4j@RestController@RequestMapping(&quot;bitmap&quot;)public class BitMapBloomFilterController &#123; @Autowired private BitMapBloomFilterRegistry bitMapBloomFilterRegistry; @Autowired private RedisTemplate redisTemplate; /** * 数据预热 */ @PostConstruct public void init() &#123; BitMapBloomFilter&lt;CharSequence&gt; bloomFilter = bitMapBloomFilterRegistry.obtain(&quot;test&quot;); List&lt;String&gt; valueList = IntStream.range(1, 2000) .mapToObj(line -&gt; String.format(&quot;%s_%s&quot;, &quot;k&quot;, line)) .collect(Collectors.toList()); // 初始化布隆过滤器 bloomFilter.addList(valueList); // 缓存数据初始化 for (String s : valueList) &#123; redisTemplate.opsForValue().set(s, s); &#125; &#125; @ApiImplicitParams(&#123; @ApiImplicitParam(name = &quot;start&quot;, value = &quot;起始检索点 取值[1, 2000]&quot;, dataType = &quot;int&quot;), @ApiImplicitParam(name = &quot;end&quot;, value = &quot;截止检索点 取值[1, 2000] 需要比start大&quot;, dataType = &quot;int&quot;) &#125;) @ApiOperation(value = &quot;Redis BitMap版布隆过滤器&quot;) @GetMapping(&quot;test&quot;) public Map&lt;String, Object&gt; test(@RequestParam(&quot;start&quot;) Integer start, @RequestParam(&quot;end&quot;) Integer end) &#123; // 获取布隆过滤器 BitMapBloomFilter&lt;CharSequence&gt; bloomFilter = bitMapBloomFilterRegistry.obtain(&quot;test&quot;); // 根据参数生成数据 List&lt;String&gt; dataList = IntStream.range(start, end) .mapToObj(line -&gt; String.format(&quot;%s_%s&quot;, &quot;k&quot;, line)) .collect(Collectors.toList()); int errNum = 0; List&lt;Object&gt; cacheDataList = new ArrayList&lt;&gt;(); for (String data : dataList) &#123; boolean result = bloomFilter.contains(data); // 根据BloomFilter 过滤器判断，如果数据存在，则取缓存 if (result) &#123; Object cache = redisTemplate.opsForValue().get(data); cacheDataList.add(cache); &#125; else &#123; // 错误统计 errNum++; &#125; &#125; Map&lt;String, Object&gt; map = new HashMap(8); map.put(&quot;命中缓存数据个数&quot;, cacheDataList.size()); map.put(&quot;误判个数&quot;, errNum); return map; &#125;&#125; Redis 插件版本 Redis官方提供module(插件)redisbloom，这个插件集成了Bloom Filter、Cuckoo Filter、Count-Min-Sketch、Top-K 命令 说明 BF.ADD 将 item 添加到布隆过滤器 BF.EXISTS 检查过滤器中是否存在 item BF.INFO 返回有关布隆过滤器的信息 BF.INSERT 将多个 item 添加到过滤器。如果过滤器尚不存在，则可以选择设置容量 BF.LOADCHUNK 恢复以前使用 BF.SCANDUMP 保存的布隆过滤器 BF.MADD 将多个 item 添加到过滤器 BF.MEXISTS 对于多个 item，检查每个项目是否存在于过滤器中 BF.RESERVE 创建一个布隆过滤器。设置误判率和容量 BF.SCANDUMP 对Bloom进行增量持久化操作 布隆过滤器 git源码下载 Jedis 提供客户端支持 官方docker整合镜像 1docker run -p 6379:6379 --name redis-redisbloom redislabs/rebloom:latest docker 集成 源码下载 RedisBloomV2.2.12.tar 123456789101112mkidr /data/redis/modulecd /data/redis/modulewget https://codeload.github.com/RedisBloom/RedisBloom/tar.gz/refs/tags/v2.2.12#解压tar -zvxf RedisBloom-2.2.12.tar.gz#执行安装cd RedisBloom-2.2.12make# 安装成功在目录下会生成 redisbloom.so 文件 修改配置 12345678910111213141516bind 0.0.0.0daemonize norequirepass wgf123# 关闭rdb# save &quot;&quot;save 900 1save 300 10save 60 10000# 关闭aofappendonly no# module 配置# 布隆过滤器 muduleloadmodule /RedisBloom/redisbloom.so 启动容器 12345docker run -p 6379:6379 --name redis1\\-v /data/redis/config/redis_1.conf:/etc/redis/redis.conf\\-v/data/redis/data/redis1:/data\\-v/data/redis/module/RedisBloom-2.2.12/:/RedisBloom\\-d redis redis-server /etc/redis/redis.conf --appendonly yes 集成SpringBoot LUA脚本 LUA脚本对象 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687public class BloomCommand &#123; /** * 创建布隆过滤器脚本，支持指定过期时间 */ private final static RedisScript&lt;Boolean&gt; RESERVE = new DefaultRedisScript&lt;Boolean&gt;( &quot;local exists = redis.call(&#x27;EXISTS&#x27;, KEYS[1])\\n&quot; + &quot;if (exists == 0) then\\n&quot; + &quot; redis.call(&#x27;BF.RESERVE&#x27;, KEYS[1], ARGV[1], ARGV[2])\\n&quot; + &quot; if (tonumber(ARGV[3]) &gt; 0) then\\n&quot; + &quot; redis.call(&#x27;PEXPIRE&#x27;, KEYS[1], ARGV[3])\\n&quot; + &quot; end\\n&quot; + &quot; return true\\n&quot; + &quot; else\\n&quot; + &quot; return false\\n&quot; + &quot;end&quot;, Boolean.class); /** * 将一个item添加到布隆过滤器，如果过滤器不存在则创建过滤器 */ private final static RedisScript&lt;Boolean&gt; ADD = new DefaultRedisScript&lt;Boolean&gt;(&quot;return redis.call(&#x27;BF.ADD&#x27;, KEYS[1], ARGV[1])&quot;, Boolean.class); /** * 将多个item添加到布隆过滤器 */ private final static String M_ADD = &quot;return redis.call(&#x27;BF.MADD&#x27;, KEYS[1], %s)&quot;; /** * 确定一个item是否可能存在于布隆过滤器中 */ private final static RedisScript&lt;Boolean&gt; EXISTS = new DefaultRedisScript&lt;&gt;(&quot;return redis.call(&#x27;BF.EXISTS&#x27;, KEYS[1], ARGV[1])&quot;, Boolean.class); /** * 确定过滤器中是否存在一个或多个item */ private final static String M_EXISTS = &quot;return redis.call(&#x27;BF.MEXISTS&#x27;, KEYS[1], %s)&quot;; /** * 获取创建布隆过滤器脚本 * * @return */ public static RedisScript&lt;Boolean&gt; getReserveScript() &#123; return RESERVE; &#125; /** * 获取添加脚本 * * @return */ public static RedisScript&lt;Boolean&gt; getAddScript() &#123; return ADD; &#125; /** * 获取批量添加脚本 * * @param values * @return */ public static RedisScript&lt;List&gt; getMultiAdd(Collection&lt;Object&gt; values) &#123; return CommandHelper.generateScript(M_ADD, values); &#125; /** * 获取判断item是否存在脚本 * * @return */ public static RedisScript&lt;Boolean&gt; getExistsScript() &#123; return EXISTS; &#125; /** * 获取批量判断存在脚本 * * @return */ public static RedisScript&lt;List&gt; getMultiExists(Collection&lt;?&gt; values) &#123; return CommandHelper.generateScript(M_EXISTS, values); &#125;&#125; 脚本助手 12345678910111213141516171819202122232425262728293031public abstract class CommandHelper &#123; /** * 生成脚本 * * @param script * @param values * @return */ public static RedisScript&lt;List&gt; generateScript(String script, Collection&lt;?&gt; values) &#123; StringBuilder sb = new StringBuilder(); for (int i = 1; i &lt;= values.size(); i++) &#123; if (i != 1) &#123; sb.append(&quot;,&quot;); &#125; sb.append(&quot;ARGV[&quot;).append(i).append(&quot;]&quot;); &#125; return new DefaultRedisScript&lt;List&gt;(String.format(script, sb.toString()), List.class); &#125; public static List&lt;Object&gt; extract(List&lt;Long&gt; result, List&lt;Object&gt; values) &#123; List&lt;Object&gt; list = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; result.size(); i++) &#123; if (1 == result.get(i)) &#123; list.add(values.get(i)); &#125; &#125; return list; &#125;&#125; 布隆过滤器 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263public class RedisBloomFilter &#123; private final RedisTemplate redisTemplate; private final List&lt;String&gt; key; public RedisBloomFilter(RedisTemplate redisTemplate, String key) &#123; this.redisTemplate = redisTemplate; this.key = Collections.singletonList(key); &#125; /** * 创建布隆过滤器 * * @param errorRate 误报的期望概率 比如 &quot;0.0001&quot; * @param capacity 容量 * @param milliseconds 过期时间，单位毫秒。-1为用不过期 * @return */ public boolean reserve(double errorRate, int capacity, int milliseconds) &#123; return (boolean) redisTemplate.execute(BloomCommand.getReserveScript(), key, errorRate, capacity, milliseconds); &#125; /** * 添加item到布隆过滤器，不存在则创建布隆过滤器 * * @param item * @return */ public boolean add(Object item) &#123; return (boolean) redisTemplate.execute(BloomCommand.getAddScript(), key, item); &#125; /** * 批量添加item到布隆过滤器，不存在则创建布隆过滤器 * * @param items * @return 返回添加成功的item */ public List&lt;Object&gt; multiAdd(List&lt;Object&gt; items) &#123; List&lt;Long&gt; result = (List&lt;Long&gt;) redisTemplate.execute(BloomCommand.getMultiAdd(items), key, items.toArray()); return CommandHelper.extract(result, items); &#125; /** * 判断item是否存在 * @param item * @return */ public boolean exists(Object item) &#123; return (boolean) redisTemplate.execute(BloomCommand.getExistsScript(), key, item); &#125; /** * 批量判断items是否存在 * @param items * @return 返回存在的item */ public List&lt;Object&gt; multiExists(List&lt;Object&gt; items) &#123; List&lt;Long&gt; result = (List&lt;Long&gt;) redisTemplate.execute(BloomCommand.getMultiExists(items), key, items.toArray()); return CommandHelper.extract(result, items); &#125;&#125; 布隆过滤器注册器 1234567891011121314151617181920@Configurationpublic class BloomFilterRegistry &#123; private final static String PREFIX = &quot;bloom:&quot;; private final Map&lt;String, RedisBloomFilter&gt; container = new ConcurrentHashMap&lt;&gt;(8); private RedisTemplate redisTemplate; public BloomFilterRegistry(RedisTemplate redisTemplate) &#123; this.redisTemplate = redisTemplate; &#125; public RedisBloomFilter getFilter(String key) &#123; String name = getName(key); return container.computeIfAbsent(name, line -&gt; new RedisBloomFilter(redisTemplate, line)); &#125; private String getName(String key) &#123; return String.format(&quot;%s%s&quot;, PREFIX, key); &#125;&#125; 配置注册器 123456789101112131415161718192021222324252627public class RedisConfig &#123; @Bean public RedisTemplate redisTemplate(RedisConnectionFactory redisConnectionFactory) &#123; RedisTemplate redisTemplate = new RedisTemplate&lt;&gt;(); redisTemplate.setConnectionFactory(redisConnectionFactory); // 使用Jackson2JsonRedisSerialize替换默认序列化 Jackson2JsonRedisSerializer jackson2JsonRedisSerializer = new Jackson2JsonRedisSerializer(Object.class); ObjectMapper objectMapper = new ObjectMapper(); objectMapper.setVisibility(PropertyAccessor.ALL, JsonAutoDetect.Visibility.ANY); objectMapper.activateDefaultTyping(new LaissezFaireSubTypeValidator(), ObjectMapper.DefaultTyping.EVERYTHING); jackson2JsonRedisSerializer.setObjectMapper(objectMapper); // 设置value的序列化规则和key的序列化规则 redisTemplate.setKeySerializer(new StringRedisSerializer()); redisTemplate.setHashKeySerializer(new StringRedisSerializer()); redisTemplate.setValueSerializer(jackson2JsonRedisSerializer); redisTemplate.setHashValueSerializer(jackson2JsonRedisSerializer); redisTemplate.afterPropertiesSet(); return redisTemplate; &#125; // 布隆注册器 @Bean public BloomFilterRegistry bloomFilterRegistry(RedisTemplate redisTemplate) &#123; return new BloomFilterRegistry(redisTemplate); &#125;&#125; 使用 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253@Api(tags = &quot;Redis 插件 布隆过滤器&quot;)@RestController@RequestMapping(&quot;bloom&quot;)public class RedisBloomFilterController &#123; @Autowired private BloomFilterRegistry bloomFilterRegistry; private static final String filterName = &quot;bloom-filter&quot;; @ApiOperation(value = &quot;创建布隆过滤器&quot;) @GetMapping(&quot;reserve&quot;) public boolean reserve() &#123; RedisBloomFilter redisBloomFilter = bloomFilterRegistry.getFilter(filterName); return redisBloomFilter.reserve(0.001, 10000, -1); &#125; @ApiOperation(value = &quot;添加数据到布隆过滤器&quot;) @ApiImplicitParam(name = &quot;item&quot;, value = &quot;添加数据&quot;, dataType = &quot;obj&quot;) @GetMapping(&quot;add&quot;) public boolean add(@RequestParam(&quot;item&quot;) Object item) &#123; RedisBloomFilter redisBloomFilter = bloomFilterRegistry.getFilter(filterName); return redisBloomFilter.add(item); &#125; @ApiOperation(value = &quot;批量添加数据到布隆过滤器&quot;) @ApiImplicitParam(name = &quot;values&quot;, value = &quot;添加数据，json数组 [1,2,3]&quot;) @PostMapping(&quot;multi_add&quot;) public List&lt;Object&gt; multiAdd(@RequestBody List&lt;Object&gt; values) &#123; RedisBloomFilter redisBloomFilter = bloomFilterRegistry.getFilter(filterName); return redisBloomFilter.multiAdd(values); &#125; @ApiOperation(value = &quot;判断数据是否存在&quot;) @ApiImplicitParam(name = &quot;item&quot;, value = &quot;判断数据&quot;, dataType = &quot;obj&quot;) @GetMapping(&quot;exists&quot;) public boolean exists(@RequestParam(&quot;item&quot;) Object item) &#123; RedisBloomFilter redisBloomFilter = bloomFilterRegistry.getFilter(filterName); return redisBloomFilter.exists(&quot;test&quot;); &#125; @ApiOperation(value = &quot;批量判断数据是否存在&quot;) @ApiImplicitParam(name = &quot;values&quot;, value = &quot;判断数据,json数组 [1,2,3]&quot;) @PostMapping(&quot;multi_exists&quot;) public List&lt;Object&gt; multiExists(@RequestBody List&lt;Object&gt; values) &#123; RedisBloomFilter redisBloomFilter = bloomFilterRegistry.getFilter(filterName); return redisBloomFilter.multiExists(values); &#125;&#125; 测试：项目启动访问swagger 布谷鸟过滤器 可删除元素 命令 描述 CF.ADD 将 item 添加到过滤器 CF.ADDNX 仅当 item 不存在时才将项目添加到过滤器 CF.COUNT 返回 item 在过滤器中出现的可能次数 CF.DEL 从过滤器中删除 item CF.EXISTS 检查过滤器中是否存在item CF.INFO 返回过滤器信息 CF.INSERT 将多个 item 添加到过滤器。如果过滤器尚不存在，则可以选择设置容量 CF.INSERTNX 如果多个 item 尚不存在，则将它们添加到过滤器。如果过滤器尚不存在，则可以选择设置容量 CF.LOADCHUNK 恢复以前用CF.SCANDUMP 保存的布谷鸟过滤器 CF.MEXISTS 对于多个 item，检查每个 item 是否存在于过滤器中 CF.RESERVE 创建布谷鸟过滤器并设置其容量 CF.SCANDUMP 启动布谷鸟过滤器的增量保存 布谷鸟过滤器LUA脚本 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889public class CuckooCommand &#123; /** * 创建布谷鸟过滤器脚本，支持指定过期时间 */ private final static RedisScript&lt;Boolean&gt; RESERVE = new DefaultRedisScript&lt;Boolean&gt;( &quot;local exists = redis.call(&#x27;EXISTS&#x27;, KEYS[1])\\n&quot; + &quot;if (exists == 0) then\\n&quot; + &quot; redis.call(&#x27;CF.RESERVE&#x27;, KEYS[1], ARGV[1])\\n&quot; + &quot; if (tonumber(ARGV[2]) &gt; 0) then\\n&quot; + &quot; redis.call(&#x27;PEXPIRE&#x27;, KEYS[1], ARGV[2])\\n&quot; + &quot; end\\n&quot; + &quot; return true\\n&quot; + &quot; else\\n&quot; + &quot; return false\\n&quot; + &quot;end&quot;, Boolean.class); /** * 将一个item添加到布谷鸟过滤器，如果过滤器不存在则创建过滤器 */ private final static RedisScript&lt;Boolean&gt; ADD = new DefaultRedisScript&lt;Boolean&gt;(&quot;return redis.call(&#x27;CF.ADD&#x27;, KEYS[1], ARGV[1])&quot;, Boolean.class); /** * 如果item不存在，将一个item添加到布谷鸟过滤器 */ private final static RedisScript&lt;Boolean&gt; ADD_NX = new DefaultRedisScript&lt;Boolean&gt;(&quot;return redis.call(&#x27;CF.ADDNX&#x27;, KEYS[1], ARGV[1])&quot;, Boolean.class); /** * 确定一个item是否可能存在于布谷鸟过滤器中 */ private final static RedisScript&lt;Boolean&gt; EXISTS = new DefaultRedisScript&lt;&gt;(&quot;return redis.call(&#x27;CF.EXISTS&#x27;, KEYS[1], ARGV[1])&quot;, Boolean.class); /** * 删除一个item */ private final static RedisScript&lt;Boolean&gt; DEL = new DefaultRedisScript&lt;&gt;(&quot;return redis.call(&#x27;CF.DEL&#x27;, KEYS[1], ARGV[1])&quot;, Boolean.class); /** * 获取创建布谷鸟过滤器脚本 * * @return */ public static RedisScript&lt;Boolean&gt; getReserveScript() &#123; return RESERVE; &#125; /** * 获取添加脚本 * * @return */ public static RedisScript&lt;Boolean&gt; getAddScript() &#123; return ADD; &#125; /** * 获取添加脚本 * * @return */ public static RedisScript&lt;Boolean&gt; getAddNxScript() &#123; return ADD_NX; &#125; /** * 获取判断item是否存在脚本 * * @return */ public static RedisScript&lt;Boolean&gt; getExistsScript() &#123; return EXISTS; &#125; /** * 获取删除item脚本 * * @return */ public static RedisScript&lt;Boolean&gt; getDelScript() &#123; return DEL; &#125;&#125; 布谷鸟过滤器 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960public class RedisCuckooFilter &#123; private final RedisTemplate redisTemplate; private final List&lt;String&gt; key; public RedisCuckooFilter(RedisTemplate redisTemplate, String key) &#123; this.redisTemplate = redisTemplate; this.key = Collections.singletonList(key); &#125; /** * 创建布谷鸟过滤器 * * @param capacity 容量 * @param milliseconds 过期时间，单位毫秒。-1为用不过期 * @return */ public boolean reserve(int capacity, int milliseconds) &#123; return (boolean) redisTemplate.execute(CuckooCommand.getReserveScript(), key, capacity, milliseconds); &#125; /** * 添加item到布谷鸟过滤器，不存在则创建布隆过滤器 * * @param item * @return */ public boolean add(Object item) &#123; return (boolean) redisTemplate.execute(CuckooCommand.getAddScript(), key, item); &#125; /** * 如果item不存在，添加item到布谷鸟过滤器 * * @param item * @return */ public boolean addNx(Object item) &#123; return (boolean) redisTemplate.execute(CuckooCommand.getAddNxScript(), key, item); &#125; /** * 判断item是否存在 * * @param item * @return */ public boolean exists(Object item) &#123; return (boolean) redisTemplate.execute(CuckooCommand.getExistsScript(), key, item); &#125; /** * 删除item * * @param item * @return */ public boolean del(Object item) &#123; return (boolean) redisTemplate.execute(CuckooCommand.getDelScript(), key, item); &#125;&#125; 布谷鸟过滤器注册器 123456789101112131415161718public class CuckooFilterRegistry &#123; private final static String PREFIX = &quot;cuckoo:&quot;; private final Map&lt;String, RedisCuckooFilter&gt; container = new ConcurrentHashMap&lt;&gt;(8); private RedisTemplate redisTemplate; public CuckooFilterRegistry(RedisTemplate redisTemplate) &#123; this.redisTemplate = redisTemplate; &#125; public RedisCuckooFilter getFilter(String key) &#123; String name = getName(key); return container.computeIfAbsent(name, line -&gt; new RedisCuckooFilter(redisTemplate, line)); &#125; private String getName(String key) &#123; return String.format(&quot;%s%s&quot;, PREFIX, key); &#125;&#125; 配置注册器 12345678910111213141516171819202122232425262728@Configurationpublic class RedisConfig &#123; @Bean public RedisTemplate redisTemplate(RedisConnectionFactory redisConnectionFactory) &#123; RedisTemplate redisTemplate = new RedisTemplate&lt;&gt;(); redisTemplate.setConnectionFactory(redisConnectionFactory); // 使用Jackson2JsonRedisSerialize替换默认序列化 Jackson2JsonRedisSerializer jackson2JsonRedisSerializer = new Jackson2JsonRedisSerializer(Object.class); ObjectMapper objectMapper = new ObjectMapper(); objectMapper.setVisibility(PropertyAccessor.ALL, JsonAutoDetect.Visibility.ANY); objectMapper.activateDefaultTyping(new LaissezFaireSubTypeValidator(), ObjectMapper.DefaultTyping.EVERYTHING); jackson2JsonRedisSerializer.setObjectMapper(objectMapper); // 设置value的序列化规则和key的序列化规则 redisTemplate.setKeySerializer(new StringRedisSerializer()); redisTemplate.setHashKeySerializer(new StringRedisSerializer()); redisTemplate.setValueSerializer(jackson2JsonRedisSerializer); redisTemplate.setHashValueSerializer(jackson2JsonRedisSerializer); redisTemplate.afterPropertiesSet(); return redisTemplate; &#125; // 布谷鸟注册器 @Bean public CuckooFilterRegistry cuckooFilterRegistry(RedisTemplate redisTemplate) &#123; return new CuckooFilterRegistry(redisTemplate); &#125;&#125; 使用 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253@Api(tags = &quot;Redis 插件 布谷鸟过滤器&quot;)@RestController@RequestMapping(&quot;cuckoo&quot;)public class RedisCuckooFilterController &#123; @Autowired private CuckooFilterRegistry cuckooFilterRegistry; private static final String filterName = &quot;cuckoo-filter&quot;; @ApiOperation(value = &quot;创建布谷鸟过滤器&quot;) @GetMapping(&quot;reserve&quot;) public boolean reserve() &#123; RedisCuckooFilter cuckooFilter = cuckooFilterRegistry.getFilter(filterName); return cuckooFilter.reserve(10000, -1); &#125; @ApiOperation(value = &quot;添加数据到布谷鸟过滤器&quot;) @ApiImplicitParam(name = &quot;item&quot;, value = &quot;添加数据&quot;, dataType = &quot;obj&quot;) @GetMapping(&quot;add&quot;) public boolean add(@RequestParam(&quot;item&quot;) Object item) &#123; RedisCuckooFilter cuckooFilter = cuckooFilterRegistry.getFilter(filterName); return cuckooFilter.add(item); &#125; @ApiOperation(value = &quot;数据不存在，添加数据到布谷鸟过滤器&quot;) @ApiImplicitParam(name = &quot;item&quot;, value = &quot;添加数据&quot;, dataType = &quot;obj&quot;) @GetMapping(&quot;add_nx&quot;) public boolean addNx(@RequestParam(&quot;item&quot;) Object item) &#123; RedisCuckooFilter cuckooFilter = cuckooFilterRegistry.getFilter(filterName); return cuckooFilter.addNx(item); &#125; @ApiOperation(value = &quot;判断数据是否存在&quot;) @ApiImplicitParam(name = &quot;item&quot;, value = &quot;判断数据&quot;, dataType = &quot;obj&quot;) @GetMapping(&quot;exists&quot;) public boolean exists(@RequestParam(&quot;item&quot;) Object item) &#123; RedisCuckooFilter cuckooFilter = cuckooFilterRegistry.getFilter(filterName); return cuckooFilter.exists(&quot;test&quot;); &#125; @ApiOperation(value = &quot;删除数据&quot;) @ApiImplicitParam(name = &quot;item&quot;, value = &quot;判断数据&quot;, dataType = &quot;obj&quot;) @GetMapping(&quot;del&quot;) public boolean del(@RequestParam(&quot;item&quot;) Object item) &#123; RedisCuckooFilter cuckooFilter = cuckooFilterRegistry.getFilter(filterName); return cuckooFilter.del(&quot;test&quot;); &#125;&#125; 测试：项目启动访问swagger 总结 ​ 如果是非分布式系统，不存在多服务使用同一布隆过滤器场景，推荐使用 单机-guava-版 性能最佳 ​ 如果是分布式系统，推荐使用 redis-插件版本的布谷鸟过滤器, 性能和功能上比自定义的 redis-bitmap-分布式版 强， redis-bitmap-分布式版 更多的意义是用于了解布隆过滤器的实现方式 内存淘汰 键的过期删除策 Redis的数据已经设置了TTL，不是过期就已经删除了吗？为什么还存在所谓的淘汰策略呢？这个原因我们需要从redis的过期策略聊起 主动删除 过期字典 随机扫描 过期字典：Redis 维护一个专门的过期字典，其中存放了所有设置了过期时间的键。这个字典用于跟踪各个键的过期时间。 定期扫描： 默认每秒会进行十次过期扫描，大约每隔 100 毫秒一次。这个频率是为了平衡内存占用和性能开销。 扫描过程： 每次扫描采用一种随机和贪心的策略（近视LRU）， 并不遍历整个过期字典。 从过期字典中随机选择一定数量的键（默认为 20 个）。 检查并删除其中已过期的键。 如果在这批键中，超过一定比例（如 1/4）的键已过期，则进行另一轮随机扫描。 被动删除 和定期删除互补 惰性删除：与定期删除策略相辅相成。懒惰删除的主要特点是，它并不主动去查找和删除过期的键，而是在键被访问时才检查其是否已过期。 删除过程： 当一个键被访问时（例如通过 GET 命令），Redis 首先检查该键是否设置了过期时间，如果设置了，Redis 接下来会判断该键是否已经过期。 如果键已经过期，Redis 会在返回结果之前从数据库中删除该键。因此，过期的键不会被返回给客户端。 如果一个过期的键长时间没有被访问，那么它将一直留在数据库中。这种键只有在被访问或者通过定期删除策略被检查到时才会被删除。 内存淘汰策略 内存不足时触发 有了以上过期策略的说明后，就很容易理解为什么需要淘汰策略了，因为不管是定期采样删除还是惰性删除都不是一种完全精准的删除，就还是会存在key没有被删除掉的场景，当内存不足时，所以就需要内存淘汰策略进行补充。 策略 说明 适用场景 noeviction 当内存使用超过配置时会返回错误，不会驱逐任何键（·默认策略）。 适用于内存资源充足，或不允许丢失任何数据的场景。 allkeys-lru 如果内存限制被超出，首先通过 LRU 算法驱逐最久未使用的键。 适用于普通缓存场景，特别是当存储空间有限但希望保留最活跃数据时。 volatile-lru 如果内存限制被超出，首先从设置了过期时间的键中驱逐最久未使用的键。 适用于优先驱逐那些已设置过期时间且最少使用的键的场景。 allkeys-random 如果内存限制被超出，从所有键中随机删除。 当其他方法的开销过大，或没有明确的访问模式时使用。 volatile-random 如果内存限制被超出，从设置了过期时间的键中随机删除。 适用于随机驱逐已设置过期时间的键的场景。 volatile-ttl 从配置了过期时间的键中驱逐即将过期的键。 适用于当内存紧张时优先删除即将过期的键。 volatile-lfu 从所有配置了过期时间的键中驱逐使用频率最低的键。 适用于频繁访问的数据变化不大，但希望清理不经常使用的过期数据的场景。 allkeys-lfu 从所有键中驱逐使用频率最低的键。 当数据的访问频率是驱逐决策的主要因素时使用。 这八种大体上可以分为4种，lru、lfu、random、ttl 内存淘汰流程 Redis 的内存淘汰流程是在内存达到限制时被触发的一系列操作，用于释放内存空间以适应新的写入操作。以下是这个流程的基本步骤： 检测内存使用：Redis 持续监测内存使用情况。当内存使用接近配置的 maxmemory 限制时（默认没有设置），触发内存淘汰机制。 淘汰样本数量： maxmemory-samples 配置决定了在每次淘汰检查中考虑的键的数量，其默认值通常为 5。 增加 maxmemory-samples 的值可以提高接近真实 LRU 或 LFU 行为的准确性，但相应地会增加 CPU 使用率。 选择淘汰候选键：在淘汰过程中，Redis 从数据集中随机抽取 maxmemory-samples 个键，并根据配置的淘汰策略（LRU，LFU等）评估这些键。 填充 eviction pool：在每次淘汰检查中，根据评估结果，Redis 会更新 eviction pool。eviction pool 是一个固定大小的结构（通常为 16 条目），用于跟踪当前最有可能被淘汰的键。 执行淘汰：当需要释放内存时，Redis 会从 eviction pool 中选择并淘汰键。通常，从该池中淘汰的是评估为“最应该被淘汰”的键。 LRU算法 (最近最少使用算法) LRU（Least Recently Used）算法，即最近最少使用算法，是一种常用的缓存淘汰策略。这种算法的核心思想是：如果数据在最近一段时间内没有被访问，那么将来被访问的可能性也不大。因此，当缓存空间不足时，LRU算法会优先淘汰那些最长时间没有被访问的数据。 工作原理： 跟踪数据的访问顺序：每当缓存中的一个数据项被访问时，这个数据项就被移到一个记录了访问顺序的列表的前端。 淘汰最久未使用的数据：当需要空间来存储新的数据项时，位于这个列表末端的数据项（即最久未被访问的数据项）首先被淘汰。 标准LRU算法 LRU算法优缺点 上面两张图片可见，URL算法虽然可以淘汰最近一段时间访问频率比较少的数据。但是有些数据可能就某一段时间没有访问，其他时间段高频访问。上图数据如果要淘汰的话，数据B会被淘汰（淘汰了真正热点数据），URL不能准确判断淘汰的数据是否为热点数据。 优点: 相对简单，容易理解和实现。 在很多常见的场景下，能有效地预测数据项的访问模式。 缺点: 实现和维护代价相对较高，特别是在数据量大时。 并非在所有场景下都是最优的淘汰策略，特别是在访问模式频繁变化的情况下。 Redis LRU实现 全局时钟 每个key内部维护时钟 Redis维护了一个24位时钟，可以简单理解为当前系统的时间戳，每隔一定时间会更新这个时钟。每个key对象内部同样维护了一个24位的时钟，当新增key对象的时候会把系统的时钟赋值到这个内部对象时钟。比如我现在要进行LRU，那么首先拿到当前的全局时钟，然后再找到内部时钟与全局时钟距离时间最久的（差最大）进行淘汰，这里值得注意的是全局时钟只有24位，按秒为单位来表示才能存储194天，所以可能会出现key的时钟大于全局时钟的情况，如果这种情况出现那么就两个相加而不是相减来求最久的key 123456789101112131415struct redisServer &#123; pid_t pid; char *configfile; //全局时钟 unsigned lruclock:LRU_BITS; ...&#125;;typedef struct redisObject &#123; unsigned type:4; unsigned encoding:4; /* key对象内部时钟 */ unsigned lru:LRU_BITS; int refcount; void *ptr;&#125; robj; Redis中的LRU与常规的LRU实现并不相同，常规LRU会准确的淘汰掉队头的元素，但是Redis的LRU并不维护队列，只是根据配置的策略要么从所有的key中随机选择N个（N可以配置）要么从所有的设置了过期时间的key中选出N个键，然后再从这N个键中选出最久没有使用的一个key进行淘汰 为什么要使用近似LRU 1、性能问题，由于近似LRU算法只是最多随机采样N个key并对其进行排序，如果精准需要对所有key进行排序，这样近似LRU性能更高 2、内存占用问题，redis对内存要求很高，会尽量降低内存使用率，如果是抽样排序可以有效降低内存的占用 3、实际效果基本相等，如果请求符合长尾法则，那么真实LRU与Redis LRU之间表现基本无差异 4、在近似情况下提供可自配置的取样率来提升精准度，例如通过 CONFIG SET maxmemory-samples 指令可以设置取样数，取样数越高越精准，如果你的CPU和内存有足够，可以提高取样数看命中率来探测最佳的采样比例 LFU算法 (最不经常使用) LFU（Least Frequently Used）算法，即最不经常使用算法，是一种用于管理缓存空间的策略。与LRU（最近最少使用）算法不同，LFU算法的核心思想是根据数据项的访问频率来进行淘汰。简而言之，LFU算法会优先移除访问频率最低的数据项。 工作原理： 统计访问频率：每个数据项都有一个计数器，记录该数据项被访问的次数。 淘汰频率最低的数据：当缓存空间不足时，LFU算法会淘汰那些访问频率最低的数据项。如果多个数据项的访问频率相同，则可能根据其他标准（如时间）来决定哪个数据项被淘汰。 实现方式： 计数器和优先队列：实现LFU算法常用的一种方式是为每个数据项维护一个计数器，并使用优先队列来根据访问频率进行排序。 逐渐减少计数器的值：为了应对长时间运行的情况，系统可能会定期减少所有或一部分数据项的计数器值，以便新的或最近变得活跃的数据项不会被过早地淘汰。 LFU算法优缺点 优点: 更好地识别和保留那些频繁访问的数据项。 对于长期运行的应用，可能比LRU表现得更好，因为它考虑了整个运行期间的访问历史。 缺点: 实现较为复杂，特别是需要准确且高效地处理访问计数和排序。 如果访问模式发生变化，旧的数据项可能由于历史高访问频率而不被淘汰。 LFU算法非常适用于访问模式相对稳定的情况，其中一些数据项显著地比其他数据项被更频繁地访问。通过淘汰那些很少被访问的数据项，LFU算法能够有效地管理有限的缓存空间。 Redis 常见问题汇总 Redis 的优缺点 优点 读写性能优异， Redis能读的速度是110000次/s，写的速度是81000次/s 支持数据持久化，支持AOF和RDB两种持久化方式 单线程执行，Redis的所有操作都是原子性的 数据结构丰富，除了支持string类型的value外还支持hash、set、zset、list等数据结构 支持主从复制，主机会自动将数据同步到从机，可以进行读写分离 优点主要集中在redis的工作模式上，单线程、内存存储、IO复用、主从复制 缺点 内存限制，不能作用于海量数据的高性能读写 主机宕机，会丢失部分数据 使用redis有哪些好处 数据读写速度快，基于内存操作 支持丰富的数据类型，基本类型五种，特殊类型三种 所有命令都是原子性操作 key过期自动删除（定期删除和惰性删除） 为什么要用Redis做缓存 主要从“高性能”和“高并发”这两点来看待这个问题 高性能 ​ 相比从DB中获取数据，由DB再到磁盘加载数据的整个过程耗费时间长，用户体验差。使用Redis做缓存直接在内存中获取数据速度快。如果数据库中的对应数据改变的之后，同步改变缓存中相应的数据即可 高并发 ​ 当遇到高并发场景时，DB数据库容易达到性能瓶颈。如果将部分数据放在内存缓存中则可以有效降低数据库压力，防止数据库宕机 为什么要用 Redis 而不用 map/guava 做缓存 主要从“本地缓存”、“分布式缓存”、”缓存一致性“作为思考点 本地缓存与分布式缓存： 如果缓存数据只是被一个服务独享，那么使用map是最好的选择。但大多数情况下，系统中的一份缓存数据通常被多个服务共同使用，使用Redis的好处就是能让缓存转变为分布式缓存并提供给多个系统使用 保证缓存一致性：本地缓存由各服务独立维护，无法统一管理。Redis作为分布是缓存底层就只有一份缓存数据，不存在缓存不一致场景 Redis和Mysql如何保持数据一致性 一致性 强一致性： 这种一致性级别是最符合用户直觉的，它要求系统写入什么，读出来的也会是什么，用户体验性好，但实现起来往往对系统的性能影响大 弱一致性： 这种一致性级别约束了系统在写入成功后，不承诺立即可以读到写入的值，也不承诺多久之后数据能够达到一致，但会尽可能地保证到某个时间级别(比如秒级别)后，数据能够达到一致状态 最终一致性： 最终一致性是弱一致性的一个特例，系统会保证在一定时间内，能够达到一个数据一致的状态。这里之所以将最终一致性单独提出来，是因为它是弱一致性中非常推崇的一种一致性模型，也是业界在大型分布式系统的数据一致性上比较推崇的模型 实际上，没办法做到数据库和缓存的绝对的一致性，只能保证最终一致性 延迟双删 优点：数据不一致的时间很短暂，只有休眠那段时间会不一致 缺点：需要考虑删除失败造成数据不一致问题 流程 1.先删除缓存 2.再写数据库 3.短暂休眠 4.再次删除缓存 为什么需要休眠n毫秒？如何计算？ n毫秒 = 读数据业务逻辑的耗时 + 时间冗余（几百毫秒）+ Redis主从同步延迟时间 休眠延迟删除是为了防止缓存脏读（并发场景下） 在高并发的场景下，即使是在更新数据库之后立即删除缓存，也可能存在其他线程在这两个操作之间查询缓存并将旧数据加载回缓存的情况。 binlog订阅，异步更新 也可以使用消息队列替代binlog，实现异步更新，比如RabbitMq的延迟队列 优点：不需要删除多次 缺点：实现成本高，延迟受mysql压力影响 流程 A服务 发起修改请求 修改数据库数据 返回响应 缓存服务 订阅binlog 解析binlog 更新缓存 缓存失效 写入时使缓存失效：每当数据在 MySQL 中被更新时，而不是更新缓存，而是简单地从 Redis 中删除相应的缓存数据。下次需要这些数据时，从 MySQL 中重新加载并更新缓存。 定时使缓存失效：为缓存数据设置过期时间，强制定期从数据库中重新加载数据。 缓存穿透、缓存击穿、缓存雪崩 高可用问题 掘金文章 缓存穿透 缓存没有，数据库也没有数据。高频缓存穿透 恶意攻击：访问数据库不存在的数据 缓存空对象 布隆过滤器 缓存击穿 某个承载高并发的热点key过期 热点数据不过期 互斥锁 缓存雪崩 上面说的缓存击穿是一个热点key的失效，而缓存雪崩是多个热点key同时失效或缓存服务崩溃 数据预热，缓存时间随机 限流降级 redis集群 RedLock RedLock 是 Redis 官方推出的一种分布式锁的实现算法。这种算法的目的是在分布式环境中提供一种相对安全和可靠的方式来实现锁机制。RedLock 主要用于那些需要跨多个进程或系统同步资源访问的场景。 RedLock 算法的工作原理: 多个 Redis 实例: RedLock 算法要求部署多个（通常是五个）独立的 Redis 实例。这些实例互不相同，不是主从关系也不是集群模式。 获取锁: 当客户端尝试获取锁时，它会向所有的 Redis 实例发送获取锁的请求。请求包含一个唯一的锁标识符和一个时间戳。 客户端尝试在每个 Redis 实例上使用 SETNX 命令（或 SET 命令的 NX 选项）来设置一个具有过期时间的锁。 锁的获取规则: 客户端需要在大多数（至少三个）Redis 实例上成功设置锁，才被认为是成功获取了锁。 如果客户端在多数实例上未能成功获取锁，它会立即释放在所有实例上的锁。 锁的释放: 当客户端完成其操作时，它会向所有 Redis 实例发送释放锁的命令。 RedLock 的特点: 容错性: 由于 RedLock 需要在大多数实例上获取锁，因此即使其中某些实例不可用，仍然可以保证锁的有效性。 安全性: RedLock 提供了比单个 Redis 实例更高的安全保障，因为它减少了单点故障的风险。 公平性: 通过在多个实例上获取锁，RedLock 尝试确保锁的公平分配，防止单个客户端长时间占用锁。 RedLock 算法在学术和工业界有一些争议。有些专家认为 RedLock 不能提供严格的安全保障，特别是在网络分区和其他极端情况下。 为什么是删除缓存，不是更新缓存 参考文章 避免脏数据：在高并发的环境下，直接更新缓存可能导致数据不一致的情况，尤其是当有多个线程同时更新数据时。 按需缓存：不是所有数据都需要被频繁访问。通过删除缓存，可以确保只有真正需要的数据被缓存，从而有效管理缓存空间。 性能考虑：删除操作通常比更新操作要快，并且更新的数据不一定需要被缓存。 Redis 和 Memcached 有啥区别，为什么选择Redis 数据类型 与Memcached仅支持简单的key-value结构的数据记录不同，Redis支持的数据类型要丰富得多。Memcached基本只支持简单的key-value存储，不支持枚举，不支持持久化和复制等功能 持久性 redis支持数据落地持久化存储,可以将内存中的数据保持在磁盘中，重启的时候可以再次加载进行使用。 memcache不支持数据持久存储 分布式存储 redis支持master-slave复制模式 memcache可以使用一致性hash做分布式 cpu利用 redis单线程执行 memcache支持多核，海量数据性能高于Redis 为什么是Redis 首先Redis支付丰富的数据类型，满足日常开发场景需求，可以做分布式锁，可以使用List做任务队列等。其次提供AOF和RDB持久化数据，数据安全性有保证 Redis 的线程模型 文件事件处理模型 单线程的文件事件处理器: Redis 的核心是基于单线程的文件事件处理模型。这意味着它的网络请求处理、命令执行等，都在一个单线程中顺序执行。 IO 多路复用: Redis 使用 IO 多路复用机制来同时监控多个 Socket。这允许单线程高效地处理多个并发网络连接。 文件事件处理器的组成: 文件事件处理器由以下四个主要部分组成： 多 Socket：负责与客户端的网络连接。 IO 多路复用程序：监控多个客户端请求，并将请求转为事件放入事件队列。 文件事件分派器：从事件队列获取事件，并根据事件类型将它们分派给对应的事件处理器。 事件处理器：包括连接应答处理器、命令请求处理器、命令回复处理器等，根据不同事件执行相应操作。 事件处理流程: 当多个 Socket 并发产生不同的操作时，这些操作产生的文件事件被 IO 多路复用程序捕获并放入队列。事件分派器然后逐一从队列中取出事件，并将其交给相应的事件处理器进行处理。 Redis 6.0 及以上版本的多线程 I/O: 从 Redis 6.0 开始，引入了多线程来处理网络 I/O 的读写操作，但数据的实际读取、处理和回复操作仍在单个主线程中完成。 Redis分布式锁过期了但业务还没有执行完，怎么办 看门狗机制自动延长 参考 redisson 的实现，就是会有watchdog定时判断key是否过期，否则就延长key redis分布式锁需要实现一个可延期的策略，相同的客户端可以实现锁延期 锁延期的条件，锁的key存在，并且锁的value值和当前的Client要一致 每个客户端对应一个定时任务，入参为锁的key和ClientId，定期轮询，比如10秒。当redis中的key相同value和ClientId相同时，自动延长锁时间 客户端宕机 ​ 这种时候锁不会自动延期，那就等key的过期时间到期后自动删除其他客户端再申请锁 客户端宕机立即释放分布式锁 思路：客户端宕机说明不能在客户端上有任何期望，可以参考Redis哨兵的实现方式。我们额外开发一个分布式锁哨兵，它独立于Redis和客户端之外 方案： 研发一个独立的服务分布式锁哨兵 客户端启动的时候将ClientId和Redis连接信息上报给哨兵 哨兵定期心跳访问客户端，如果发现客户端宕机则遍历所有分布式锁Key, 判断其value是否为宕机的ClientId，是则删除该key 多个系统同时操作（并发）Redis带来的数据问题 参考 多个系统需要修改同一个key,可能有网络波动，如何保持顺序性 加分布式锁，保证一段时间内只有某个服务可读可写 使用事务Watch，乐观锁方案 Redis的应用场景 计数器 可以对 String 进行自增自减运算，从而实现计数器功能。Redis 这种内存型数据库的读写性能非常高，很适合存储频繁读写的计数量 缓存 将热点数据放到内存中，减少数据加载时间损耗 会话缓存 可以使用 Redis 来统一存储多台应用服务器的会话信息，实现会话共享 分布式锁实现 在分布式场景下，无法使用单机环境下的锁来对多个节点上的进程进行同步。可以使用 Redis 自带的 SETNX 命令实现分布式锁，除此之外，还可以使用官方提供的 RedLock 分布式锁实现 Redis持久化 持久化就是把内存的数据写到磁盘中去，防止服务宕机了内存数据丢失 RDB 默认 AOF AOF在重写时如何保证继续服务 如何选择合适的持久化方式 如果对数据有安全性要求，同时使用两种持久化功能。在这种情况下，当 Redis 重启的时候会优先载入AOF文件来恢复原始的数据，如果数据量大可以使用RDB做全量数据恢复，AOF做增量恢复 如果Redis数据量不大并对数据完整性有严格要求则使用AOF方式,它会记录每个写操作 如果Redis数据量很大并且对数据完整性没有强制要求则使用RDB,RDB数据恢复速读比AOF快 如只希望数据在服务器运行的时候存在，也可以不使用任何持久化方式 Redis持久化数据和缓存怎么做扩容 如果Redis被当做缓存使用，使用一致性哈希（hash槽）实现动态扩容缩容 hash环实现 一致性hash redis 扩容 Redis的过期键的删除策略 （对过期的数据怎么处理） 键的过期删除策 定期删除 惰性删除 内存淘汰策略 内存满了报错，默认 LRU LRF TTL 随机 Redis key的过期时间和永久有效分别怎么设置 expire和persist命令 如何保证redis中的数据都是热点数据 这里主要考虑到使用redis的内存淘汰策略实现 内存淘汰策略 Redis如何做内存优化 合理使用数据结构 使用更优秀的序列化技术 可以好好利用hash,list,sorted set,set等集合类型数据，因为通常情况下很多小的Key-Value可以用更紧凑的方式存放到一起。尽可能使用散列表（hashset），散列表（是说散列表里面存储的数少）使用的内存非常小，所以你应该尽可能的将你的数据模型抽象到一个散列表里面。比如你的web系统中有一个用户对象，不要为这个用户的名称，姓氏，邮箱，密码设置单独的key，而是应该把这个用户的所有信息存储到一张散列表里面 什么是Redis事务 Redis 事务的本质是通过MULTI、EXEC、WATCH等一组命令的集合。事务支持一次执行多个命令，一个事务中所有命令都会被序列化。在事务执行过程，会按照顺序串行化执行队列中的命令，其他客户端提交的命令请求不会插入到事务执行命令序列中,不保证原子性 事务 Redis分布式寻址算法 hash槽 在集群模式下，Redis 的 key 是如何寻址的？分布式寻址都有哪些算法？了解一致性 hash 算法吗？如何动态增加和删除一个节点？ hash 算法（大量缓存重建） 一致性 hash 算法（自动缓存迁移）+ 虚拟节点（自动负载均衡） Redis cluster 的 hash slot 算法 hash 算法 来了一个 key，首先计算 hash 值，然后对节点数取模，接着打在不同的 master 节点上。缺点也很明显：某一个 master 节点宕机，所有请求过来，都会基于最新的剩余 master 节点数去取模，尝试去库中取数据进行缓存。这会导致大部分的请求过来，全部无法拿到有效的缓存，导致大量的流量涌入数据库 一致性 hash 算法 将整个 hash 值空间组织成一个虚拟的圆环，整个空间按顺时针方向组织，下一步将各个 master 节点（使用服务器的 ip 或主机名）进行 hash。来了一个 key，首先计算 hash 值，并确定此数据在环上的位置，从此位置沿环顺时针“行走”，遇到的第一个 master 节点就是 key 所在位置，这样就能确定每个节点在其哈希环上的位置。在一致性哈希算法中，如果一个节点挂了，受影响的数据仅仅是此节点到环空间前一个节点（沿着逆时针方向行走遇到的第一个节点）之间的数据，其它不受影响。增加一个节点也同理。 虚拟节点：一致性哈希算法在节点太少时，容易因为节点分布不均匀而造成缓存热点的问题。为了解决这种热点问题，一致性 hash 算法引入了虚拟节点机制，即对每一个节点计算多个 hash，每个计算结果位置都放置一个虚拟节点。这样就实现了数据的均匀分布，负载均衡 hash slot 算法 Redis cluster 有固定的 16384 个 hash slot，slot是槽的概念（理解为数据管理和迁移的基本单位），所有的键根据哈希函数映射到 0~16383 整数槽内，每个节点负责维护一部分槽以及槽所映射的键值数据。 公式：slot = CRC16（key）&amp; 16384。解释：对每个 key 计算 CRC16 值，然后对 16384 取模，可以获取 key 对应的 hash slot。hash slot可以像磁盘分区一样自由分配槽位，在配置文件里可以指定，也可以让redis自己选择分配，结果均匀，这种结构很容易添加或者删除节点。如果增加一个节点，就需要从节点已有的节点 获得部分槽分配到新的节点 上。如果想移除已有的一个节点，需要将节点中的槽移到其他节点上，然后将没有任何槽的节点从集群中移除就可以了。由于缓存的key hash结果是和slot绑定的，而不是和服务器节点绑定，所以节点的更替只需要迁移slot即可平滑过渡。从一个节点将哈希槽移动到另一个节点并不会停止服务，所以无论添加删除或者改变某个节点的哈希槽的数量都不会造成集群不可用的状态","categories":[],"tags":[]},{"title":"RabbitMQ","slug":"RabbitMQ","date":"2022-01-25T15:09:28.000Z","updated":"2023-10-10T08:13:21.422Z","comments":true,"path":"2022/01/25/RabbitMQ/","link":"","permalink":"https://wugengfeng.cn/2022/01/25/RabbitMQ/","excerpt":"","text":"思维导图 MQ的相关概念 MQ介绍 MQ（Message Queue）消息队列是一种基本的数据结构，遵循先进先出 （FIFO） 原则。它通过将待传递的数据（消息）存储在队列中，利用队列机制来实现消息的传递。在这个模型中，生产者生成消息并将其放入队列，而消费者则负责处理这些消息。消费者可以主动从指定队列中拉取消息，或者通过订阅特定队列来接收MQ服务端推送的消息。这种方式实现了可靠的消息传递和异步通信。 MQ的作用 消息队列中间件是分布式系统中重要的组件，主要解决应用解耦，异步消息，流量削锋等问题，实现高性能，高可用，可伸缩和最终一致性架构。 削峰 在高并发场景下，通过消息队列实现异步业务处理，可以有效提升系统的高峰期业务处理能力，避免系统瘫痪。 如果订单系统最多能处理一万次订单，这个处理能力应付正常时段的下单时绰绰有余，正常时段我们下单一秒后就能返回结果。但是在高峰期，如果有两万次下单操作系统是处理不了的，只能限制订单超过一万后不允许用户下单。使用消息队列做缓冲，我们可以取消这个限制，把一秒内下的订单分散成一段时间来处理，这时有些用户可能在下单十几秒后才能收到下单成功的操作，但是比不能下单的体验要好 解耦 当一个业务需要多个模块协同工作，或者一条消息需要被多个系统处理时，可以通过发送一条消息队列（MQ）来解耦这些模块。主业务完成后，只需发送MQ消息，其他模块可以独立地消费这些消息，从而实现业务功能，降低了模块之间的耦合度。 异步 主要业务执行完毕后，附属业务通过消息队列（MQ）以异步方式处理，以缩短业务响应时间，从而改善用户体验。 MQ 的分类 ActiveMQ 低吞吐 优点：单机吞吐量高达万级，消息时效性在毫秒级别，具备高可用性，采用主从架构提供稳健的高可用性支持，具有较低的消息可靠性风险，降低了数据丢失的概率。 缺点：当前官方社区对ActiveMQ 5.x的维护逐渐减少，因此在高吞吐量场景下的使用相对较少。 Kafka 高吞吐 队列数量和性能成反比 优点： Kafka以其高吞吐量著称，单机写入每秒可达百万条消息。它具有出色的性能表现，时效性达毫秒级，以及高可用性。作为分布式系统，Kafka能够在多个节点上保留数据的多个副本，即使个别节点宕机也不会丢失数据或导致不可用情况。消费者可以通过拉取（Pull）方式获取消息，保证消息有序性，以及确保每条消息被消费且仅被消费一次。Kafka还受益于优秀的第三方管理界面Kafka-Manager。在日志领域，Kafka应用成熟广泛，得到多家公司和开源项目的采用。它主要用于大数据领域的实时计算（例如Flink、CDC、ETL）以及日志采集，提供简单MQ功能和可靠的消息传递。 缺点：Kafka在单机支持队列/分区超过64个时，可能导致负载明显升高。增多的队列数量会降低发送消息的响应时间，因为使用短轮询方式，实时性受轮询间隔时间影响。此外，Kafka不直接支持消费失败的消息重试，这需要由消费者自行处理。尽管它支持消息顺序，但当代理宕机时可能会导致消息乱序。需要注意的是，Kafka社区更新相对缓慢，这可能影响到新功能的发布和问题修复的速度。 RocketMQ 中吞吐 功能比kafka丰富 支持语言不多 优点：RocketMQ以中等吞吐量的性能表现著称，单机吞吐量可达十万级。它拥有出色的可用性，并采用分布式架构，因此能够实现消息的高可靠性，几乎可以做到零消息丢失。RocketMQ的MQ功能相对丰富，且它是一个分布式系统，因此具备出色的扩展性，能够支持高达10亿条消息的堆积，而不会因此导致性能下降。需要指出的是，RocketMQ是使用Java实现的，尽管它在某些设计思想上可能受到了Kafka的影响。 缺点：对于支持的客户端语言，RocketMQ主要支持Java和C++，但需要指出的是C++的客户端支持相对不够成熟。此外，RocketMQ的社区活跃度一般，相对于某些其他消息队列系统，社区支持有限。此外，RocketMQ并没有在其核心中实现JMS等接口，因此在将某些系统迁移到RocketMQ时可能需要修改大量的代码。 RabbitMQ 低吞吐 功能齐全 多语言 Kafka是一个于2007年发布的消息中间件系统，它在高级消息队列协议（AMQP）的基础上开发而成，被广泛认为是当前最主流的消息中间件之一。 优点：RabbitMQ得益于Erlang语言的高并发特性，具备出色的性能。它支持每秒万级别的吞吐量，拥有完善的消息队列（MQ）功能，以及强大的健壮性和稳定性。RabbitMQ易于使用，跨平台支持多种编程语言，包括Python、Ruby、.NET、Java、JMS、C、PHP、ActionScript、XMPP、STOMP等。该系统提供了全面的文档支持，并且具备出色的开源管理界面，为用户提供极佳的使用体验。RabbitMQ拥有活跃的社区支持，更新频率相当高。 缺点：商业版需要收费，学习成本较高。 MQ的选择 Kafka: 特点：Kafka采用基于Pull的消息消费模式，追求高吞吐量。最初设计用于日志收集和传输，非常适合处理产生大量数据的互联网服务的数据收集业务。对于大型公司，尤其是需要日志采集功能、CDC、ETL的场景，Kafka是首选。 RocketMQ: 特点：RocketMQ天生为金融互联网领域而生，特别适用于对可靠性要求极高的场景，例如电商中的订单扣款以及业务削峰。在大量交易涌入时，RocketMQ可以保证后端能够及时处理。其稳定性在阿里双11等大型活动中得到了多次验证。如果您的业务涉及到这些高并发场景，强烈建议选择RocketMQ。 RabbitMQ: 特点：RabbitMQ结合Erlang语言本身的并发优势，具有出色的性能和微秒级的时效性。社区活跃度高，管理界面也非常便捷。如果您的数据量规模不是特别大，尤其对于中小型公司而言，建议优先选择功能比较完备的RabbitMQ。 MQ的缺点 系统可用性降低。依赖服务也多，服务越容易挂掉。需要考虑MQ瘫痪的情况 系统复杂性提高。需要考虑消息丢失、消息重复消费、消息传递的顺序性 业务一致性。主业务和从属业务一致性的处理 前置 项目源码 docker快速安装 123docker pull rabbitmq:managementdocker run -di --name=rabbitmq -p 5671:5671 -p 5672:5672 -p 4369:4369 -p 15671:15671 -p 15672:15672 -p 25672:25672 rabbitmq:management RabbitMQ 的概念 RabbitMQ 是一个由 erlang 开发的 AMQP (Advanced Message Queuing Protocol) 的开源实现 生产者 产生数据发送消息的程序是生产者，产生数据后投递到交换机中 交换机 交换机（Exchange）是RabbitMQ消息传递的核心组件之一。它的主要作用是接收生产者发送的消息，并根据特定的路由规则，将这些消息路由到一个或多个队列中。 Direct exchange 直连交换机 定义与用途： 直连交换机（Direct Exchange）是RabbitMQ中的基础交换机类型。 它的主要功能是根据消息中的路由键（Routing Key）将消息准确地路由到对应的队列。 路由键匹配： 在直连交换机下，路由键必须进行精确匹配。 只有当消息的Routing Key与队列绑定的Routing Key完全一致时，消息才会被投递到该队列。 队列与路由键的关系： 一个队列可以绑定到多个Routing Key。 这为消息的灵活路由提供了可能性。 操作步骤： 将队列绑定到直连交换机，并为该绑定指定一个路由键。 当消息携带的路由键与某个队列的绑定路由键匹配时，交换机将消息路由到该队列。 demo 添加队列和交换机配置 12345678910111213141516171819202122232425262728293031323334353637@Configurationpublic class DirectConfig &#123; public static final String DIRECT_EXCHANGE = &quot;direct_exchange&quot;; public static final String DIRECT_QUEUE = &quot;direct.queue&quot;; public static final String ROUTING_KEY = &quot;queue&quot;; /** * 创建队列 * * @return */ @Bean public Queue directQueue() &#123; return new Queue(DIRECT_QUEUE); &#125; /** * 创建交换机 * * @return */ @Bean public DirectExchange directExchange() &#123; return new DirectExchange(DIRECT_EXCHANGE); &#125; /** * 绑定队列 * @param directQueue * @param directExchange * @return */ @Bean public Binding bindDirectQueue(Queue directQueue, DirectExchange directExchange) &#123; return BindingBuilder.bind(directQueue).to(directExchange).with(ROUTING_KEY); &#125;&#125; 添加队列监听 12345678@Slf4j@Componentpublic class DirectListener &#123; @RabbitListener(queues = DirectConfig.DIRECT_QUEUE) public void receive(String msg) &#123; log.info(&quot;队列 &#123;&#125; 接收信息:&#123;&#125;&quot;, DirectConfig.DIRECT_QUEUE, msg); &#125;&#125; 单元测试 1234567@Test public void sendDirect() throws InterruptedException &#123; for (int i = 0; i &lt; 2; i++) &#123; rabbitTemplate.convertAndSend(DirectConfig.DIRECT_EXCHANGE, DirectConfig.ROUTING_KEY, i+ &quot;&quot;); &#125; TimeUnit.SECONDS.sleep(10); &#125; 日志 12INFO 28275 --- [ntContainer#0-1] com.wgf.demo.listener.DirectListener : 队列 direct.queue 接收信息:0INFO 28275 --- [ntContainer#0-1] com.wgf.demo.listener.DirectListener : 队列 direct.queue 接收信息:1 Topic exchange 主题交换机 定义与用途： 主题交换机（Topic Exchange）是RabbitMQ中一种灵活且功能丰富的交换机类型，主要用于实现复杂的消息路由规则，允许一条消息根据路由键模糊匹配，被投递到多个队列。 基本特性： 主题交换机是直连交换机的扩展，允许一个队列绑定多个Routing Key。 它通过路由键的模糊匹配，能够将消息路由到一个或多个绑定队列。 路由键规则： 发送到主题交换机的消息的Routing Key必须是由点号（.）分隔的单词列表，如：“stock.usd.nyse”, “nyse.vmw”, “quick.orange.rabbit”。这些单词没有特定要求，但整个路由键的长度不得超过255个字节。 通配符： 主题交换机在匹配路由键时，支持两种通配符： *（星号）：匹配路由键中的一个单词。 #（井号）：匹配路由键中的零或多个单词。 灵活性： 通过模糊匹配和通配符的使用，主题交换机能够满足多样化和复杂的消息路由需求，展现出极高的灵活性。 quick.orange.rabbit 被队列 Q1Q2 接收到 lazy.orange.elephant 被Q1Q3 接收到 quick.orange.fox 被队列 Q1 接收到 lazy.brown.fox 被队列 Q2 接收到 quick.brown.fox 不匹配任何绑定不会被任何队列接收到会被丢弃 demo 添加队列和交换机配置 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263@Configurationpublic class TopicConfig &#123; public static final String TOPIC_EXCHANGE = &quot;topic_exchange&quot;; public static final String TOPIC_QUEUE_1 = &quot;topic.queue1&quot;; public static final String TOPIC_QUEUE_2 = &quot;topic.queue2&quot;; public static final String ROUTING_KEY = &quot;topic.test&quot;; public static final String ROUTING_KEY2 = &quot;topic.#&quot;; /** * 创建队列 * * @return */ @Bean public Queue topicQueue1() &#123; return new Queue(TOPIC_QUEUE_1); &#125; /** * 创建队列 * * @return */ @Bean public Queue topicQueue2() &#123; return new Queue(TOPIC_QUEUE_2); &#125; /** * 创建交换机 * * @return */ @Bean public TopicExchange topicExchange() &#123; return new TopicExchange(TOPIC_EXCHANGE); &#125; /** * 绑定队列 * * @param topicQueue1 * @param topicExchange * @return */ @Bean public Binding bindTopicQueue1(Queue topicQueue1, TopicExchange topicExchange) &#123; return BindingBuilder.bind(topicQueue1).to(topicExchange).with(ROUTING_KEY); &#125; /** * 绑定队列 * * @param topicQueue1 * @param topicExchange * @return */ @Bean public Binding bindTopicQueue2(Queue topicQueue2, TopicExchange topicExchange) &#123; return BindingBuilder.bind(topicQueue2).to(topicExchange).with(ROUTING_KEY2); &#125;&#125; 添加队列监听 12345678910111213@Slf4j@Componentpublic class TopicListener &#123; @RabbitListener(queues = TopicConfig.TOPIC_QUEUE_1) public void receive(String msg) &#123; log.info(&quot;队列 &#123;&#125; 接收信息:&#123;&#125;&quot;, TopicConfig.TOPIC_QUEUE_1, msg); &#125; @RabbitListener(queues = TopicConfig.TOPIC_QUEUE_2) public void receive2(String msg) &#123; log.info(&quot;队列 &#123;&#125; 接收信息:&#123;&#125;&quot;, TopicConfig.TOPIC_QUEUE_2, msg); &#125;&#125; 单元测试 1234567@Testpublic void sendTopic() throws InterruptedException &#123; rabbitTemplate.convertAndSend(TopicConfig.TOPIC_EXCHANGE, TopicConfig.ROUTING_KEY, TopicConfig.ROUTING_KEY); rabbitTemplate.convertAndSend(TopicConfig.TOPIC_EXCHANGE, &quot;topic.hello&quot;, &quot;topic.hello&quot;); rabbitTemplate.convertAndSend(TopicConfig.TOPIC_EXCHANGE, &quot;topic.hi.hello&quot;, &quot;topic.hi.hello&quot;); TimeUnit.SECONDS.sleep(10);&#125; 日志 1234INFO 10456 --- [ntContainer#0-1] com.wgf.demo.listener.TopicListener : 队列 topic.queue1 接收信息:topic.testINFO 10456 --- [ntContainer#1-1] com.wgf.demo.listener.TopicListener : 队列 topic.queue2 接收信息:topic.testINFO 10456 --- [ntContainer#1-1] com.wgf.demo.listener.TopicListener : 队列 topic.queue2 接收信息:topic.hi.helloINFO 10456 --- [ntContainer#1-1] com.wgf.demo.listener.TopicListener : 队列 topic.queue2 接收信息:topic.hello Fanout exchange 扇出交换机 无需路由键： 扇出交换机（Fanout Exchange）在进行消息路由时，不需要路由键（Routing Key）。即便消息中包含路由键，该键也不会影响消息的路由过程。 广播特性： 当消息发送到扇出交换机时，该交换机会将消息转发给所有绑定到它的队列，实现了一种广播的效果。 多队列绑定： 如果有N个队列绑定到某个扇出交换机上，当有消息发送给该交换机时，交换机会将消息发送给这所有的N个队列，确保每个队列都能收到消息的拷贝。 demo 添加队列交换机配置 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960@Configurationpublic class FanoutConfig &#123; public static final String FANOUT_EXCHANGE = &quot;fanout_exchange&quot;; public static final String FANOUT_QUEUE_1 = &quot;fanout.queue_1&quot;; public static final String FANOUT_QUEUE_2 = &quot;fanout.queue_2&quot;; /** * 创建队列 * * @return */ @Bean public Queue fanoutQueue1() &#123; return new Queue(FANOUT_QUEUE_1); &#125; /** * 创建队列 * * @return */ @Bean public Queue fanoutQueue2() &#123; return new Queue(FANOUT_QUEUE_2); &#125; /** * 创建交换机 * * @return */ @Bean public FanoutExchange fanoutExchange() &#123; return new FanoutExchange(FANOUT_EXCHANGE); &#125; /** * 绑定队列 * @param fanoutQueue1 * @param fanoutExchange * @return */ @Bean public Binding bindFanoutQueue1(Queue fanoutQueue1, FanoutExchange fanoutExchange) &#123; // 不需要 routing_key return BindingBuilder.bind(fanoutQueue1).to(fanoutExchange); &#125; /** * 绑定队列 * @param fanoutQueue2 * @param fanoutExchange * @return */ @Bean public Binding bindFanoutQueue(Queue fanoutQueue2, FanoutExchange fanoutExchange) &#123; // 不需要 routing_key return BindingBuilder.bind(fanoutQueue2).to(fanoutExchange); &#125;&#125; 添加队列监听 12345678910111213@Slf4j@Componentpublic class FanoutListener &#123; @RabbitListener(queues = FanoutConfig.FANOUT_QUEUE_1) public void receive(String msg) &#123; log.info(&quot;队列 &#123;&#125; 接收信息:&#123;&#125;&quot;, TopicConfig.TOPIC_QUEUE_1, msg); &#125; @RabbitListener(queues = FanoutConfig.FANOUT_QUEUE_2) public void receive2(String msg) &#123; log.info(&quot;队列 &#123;&#125; 接收信息:&#123;&#125;&quot;, TopicConfig.TOPIC_QUEUE_2, msg); &#125;&#125; 单元测试 12345@Testpublic void sendFanout() throws InterruptedException &#123; rabbitTemplate.convertAndSend(FanoutConfig.FANOUT_EXCHANGE, null, &quot;test&quot;); TimeUnit.SECONDS.sleep(10);&#125; 日志 12INFO 31660 --- [ntContainer#1-1] com.wgf.demo.listener.FanoutListener : 队列 topic.queue1 接收信息:testINFO 31660 --- [ntContainer#2-1] com.wgf.demo.listener.FanoutListener : 队列 topic.queue2 接收信息:test Headers exchange 头交换机 工作原理： 头交换机（Headers Exchange）与主题交换机类似，但不是使用路由键来路由消息，而是使用消息头中的键值对来建立路由规则。通过判断消息头的键值对是否与绑定的条件相匹配，来确定消息的路由路径。 x-match 参数： 头交换机有一个重要的参数：x-match，它决定了消息头的匹配规则。 当 x-match 设置为 any 时，只要消息头中的任意一个键值对满足绑定条件，就视为匹配成功。 当 x-match 设置为 all（默认设置）时，消息头中的所有键值对都必须满足绑定条件，才视为匹配成功。 性能考虑： 由于头交换机在进行匹配时需要检查消息头中的多个属性，可能会导致性能较差，因此一般不推荐在对性能要求较高的场景中使用。 demo 添加队列交换机配置 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768@Configurationpublic class HeadersConfig &#123; public static final String HEADERS_EXCHANGE = &quot;headers_exchange&quot;; public static final String HEADERS_QUEUE_1 = &quot;headers.queue1&quot;; public static final String HEADERS_QUEUE_2 = &quot;headers.queue2&quot;; /** * 创建队列 * * @return */ @Bean public Queue headersQueue1() &#123; return new Queue(HEADERS_QUEUE_1); &#125; /** * 创建队列 * * @return */ @Bean public Queue headersQueue2() &#123; return new Queue(HEADERS_QUEUE_2); &#125; /** * 创建交换机 * * @return */ @Bean public HeadersExchange headersExchange() &#123; return new HeadersExchange(HEADERS_EXCHANGE); &#125; /** * 绑定队列 * @param directQueue * @param directExchange * @return */ @Bean public Binding bindHeadersQueue1(Queue headersQueue1, HeadersExchange headersExchange) &#123; // 消息头属性 Map&lt;String,Object&gt; headerMap = new HashMap&lt;&gt;(8); headerMap.put(&quot;color&quot;, &quot;black&quot;); headerMap.put(&quot;aging&quot;, &quot;fast&quot;); // whereAny 其中一项匹配即可 return BindingBuilder.bind(headersQueue1).to(headersExchange).whereAny(headerMap).match(); &#125; /** * 绑定队列 * @param directQueue * @param directExchange * @return */ @Bean public Binding bindHeadersQueue2(Queue headersQueue2, HeadersExchange headersExchange) &#123; // 消息头属性 Map&lt;String,Object&gt; headerMap = new HashMap&lt;&gt;(8); headerMap.put(&quot;color&quot;, &quot;black&quot;); headerMap.put(&quot;aging&quot;, &quot;fast&quot;); // whereAll 完全匹配 return BindingBuilder.bind(headersQueue2).to(headersExchange).whereAll(headerMap).match(); &#125;&#125; 添加队列监听 12345678910111213@Slf4j@Componentpublic class HeadersListener &#123; @RabbitListener(queues = HeadersConfig.HEADERS_QUEUE_1) public void receive(String msg) &#123; log.info(&quot;队列 &#123;&#125; 接收信息:&#123;&#125;&quot;, TopicConfig.TOPIC_QUEUE_1, msg); &#125; @RabbitListener(queues = HeadersConfig.HEADERS_QUEUE_2) public void receive2(String msg) &#123; log.info(&quot;队列 &#123;&#125; 接收信息:&#123;&#125;&quot;, TopicConfig.TOPIC_QUEUE_2, msg); &#125;&#125; 单元测试 123456789101112@Test public void sendHeaders() throws InterruptedException &#123; MessageProperties messageProperties = new MessageProperties(); messageProperties.setHeader(&quot;color&quot;, &quot;black&quot;); SimpleMessageConverter messageConverter = new SimpleMessageConverter(); Message message = messageConverter.toMessage(&quot;test1&quot;, messageProperties); rabbitTemplate.convertAndSend(HeadersConfig.HEADERS_EXCHANGE, null, message); messageProperties.setHeader(&quot;aging&quot;, &quot;fast&quot;); message = messageConverter.toMessage(&quot;test2&quot;, messageProperties); rabbitTemplate.convertAndSend(HeadersConfig.HEADERS_EXCHANGE, null, message); &#125; 日志 123INFO 4330 --- [ntContainer#3-1] com.wgf.demo.listener.HeadersListener : 队列 topic.queue1 接收信息:test1INFO 4330 --- [ntContainer#4-1] com.wgf.demo.listener.HeadersListener : 队列 topic.queue2 接收信息:test2INFO 4330 --- [ntContainer#3-1] com.wgf.demo.listener.HeadersListener : 队列 topic.queue1 接收信息:test2 默认交换机 默认交换机（Default Exchange）是RabbitMQ预先声明的一个特殊的直连交换机（Direct Exchange），其名称为空字符串。这个交换机具有以下特点，使其在简单应用中非常实用： 自动绑定： 每个新创建的队列都会自动绑定到默认交换机上。绑定时使用的路由键（Routing Key）与队列的名称相同。 简化的消息发送： 由于每个队列都自动绑定到默认交换机，并使用队列名称作为路由键，因此在发送消息时，可以省略交换机的名称，直接使用队列名称作为routing key来发送消息。 队列 队列是RabbitMQ的核心组件，作为一种内部数据结构，用于存储经过RabbitMQ的消息。尽管消息在RabbitMQ和应用程序之间流动，但它们实际上只能被存储在队列中。队列的容量基本上只受到主机内存和磁盘限制的约束，因此它可以被视为一个庞大的消息缓冲区。多个生产者可以向同一个队列发送消息，同时多个消费者也可以从一个队列中接收数据。 临时队列 没有消费者订阅时，自动删除 当消费者断开连接时，队列将会被删除。自动删除队列允许的消费者没有限制，也就是说当这个队列上最后一个消费者断开连接才会执行删除 消费者 消费者通常是指等待接收并处理消息的程序，这里的“消费”一词与“接收”含义相近。值得注意的是，生产者、消费者和消息中间件常常不会部署在同一台机器上。同时，一个应用程序有可能同时充当生产者和消费者的角色。 获取消息的两种方式 推模式（Push） 推模式是RabbitMQ的 默认 消息获取模式，它以高效率和实时性为特点。 如果消费者处理消息的速率跟不上生产者的生产速度，可能会导致系统宕机。 优点：在推模式下，消息会被提前推送给消费者，消费者需要设置一个 缓冲区 来缓存这些消息。这样，消费者总是有一批待处理的消息存储在内存中，从而保证了处理效率和数据的实时性。 缺点：当大量消息在缓冲区堆积时，会给消费者端的服务带来较大的处理压力。如果消费者的处理能力无法满足大量消息的处理需求，可能会导致服务宕机。 推模式常用的实现方式为spring-boot-starter-amqp的@RabbitListener注解 拉模式（Pull） 拉模式允许消费者以可控的速率获取消息。 消息的实时性在此模式下依赖于消费者的拉取策略。 需要权衡拉取频率和消息的时效性，以避免资源浪费和处理延迟。 优点：与推模式相比，拉模式的一大优势在于消费者可以根据自己的处理能力来拉取消息，这有助于防止因消息堆积导致的服务宕机。 缺点：在拉模式下，消费者端需要不断地从服务器端拉取消息，处理完一定量的消息后再次进行拉取。设定拉取消息的间隔成为了一个挑战：间隔太短会导致消费者处于忙等状态，浪费资源；而间隔太长，则可能导致服务器端的消息未能及时处理。 RabbitMQ支持客户端批量拉取消息，客户端可以连续调用 basicGet 方法拉取多条消息，处理完成之后一次性ACK 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061@Slf4j@Componentpublic class PullMsgService &#123; @Autowired RabbitTemplate rabbitTemplate; /** * 主动拉取消息 * rabbitmq 提供 channel.basicGet API用于主动拉取消息 * * @param queueName 队列名称 * @param count 每一批次获取总数 * @param timeOut 超时时间，超时强制返回 * @param consumer 业务实现 */ public void pull(String queueName, Integer count, int timeOut, Consumer&lt;List&lt;String&gt;&gt; consumer) &#123; rabbitTemplate.execute(new ChannelCallback&lt;Object&gt;() &#123; @Override public Object doInRabbit(Channel channel) throws Exception &#123; List&lt;String&gt; msgList = new ArrayList&lt;&gt;(); long start = System.currentTimeMillis(); long end; GetResponse response; GetResponse lastResponse = null; while (true) &#123; try &#123; end = System.currentTimeMillis(); response = channel.basicGet(queueName, false); if (response == null) &#123; if (msgList.size() == 0) &#123; log.info(&quot;没有消息&quot;); TimeUnit.SECONDS.sleep(5); start = System.currentTimeMillis(); continue; &#125; &#125; else &#123; String msg = new String(response.getBody(), &quot;utf-8&quot;); lastResponse = response; msgList.add(msg); &#125; long step = end - start; if ((step &gt; timeOut &amp;&amp; msgList.size() &gt; 0) || msgList.size() &gt;= count) &#123; consumer.accept(msgList); channel.basicAck(lastResponse.getEnvelope().getDeliveryTag(), true); start = System.currentTimeMillis(); msgList.clear(); &#125; &#125; catch (Exception e) &#123; channel.basicNack(lastResponse.getEnvelope().getDeliveryTag(), true, true); &#125; &#125; &#125; &#125;); &#125;&#125; RabbitMQ 名词解释 Broker：是负责接收、存储和分发消息的应用程序，它构成了消息队列系统的核心组件。RabbitMQ Server便是一个典型的Message Broke Virtual host：Virtual Host（虚拟主机）是一种逻辑分组，它允许你在同一个RabbitMQ实例中隔离不同的环境。每个Virtual Host都有自己独立的Exchange（交换器）、Queue（队列）和Binding（绑定），以及独立的权限管理，是多租户的实现。 Channel：Channel（通道）是建立在Connection（连接）之上的一个轻量级的虚拟连接，它是AMQP协议中的一个重要概念。Channel是客户端与Broker之间进行通信的最常用的接口。主要特点有： 轻量级：Channel是轻量级的，创建和销毁的代价较低 多路复用：在一个TCP连接（Connection）中，可以创建多个Channel，每个Channel都有一个唯一的ID，这样可以实现多路复用 隔离性：每个Channel都是独立的，Channel之间互不影响 Exchange：用于处理消息路由的核心组件。当生产者发送消息到RabbitMQ时，它会首先被发送到一个Exchange。Exchange根据预定的规则和绑定（Bindings）信息，决定如何路由这个消息 Queue：用于存储消息的数据结构。消费者从队列中获取消息进行处理 Binding：用于连接Exchange（交换器）和Queue（队列）的路由规则。通过Binding，RabbitMQ能够知道如何根据路由键（Routing Key）和交换器类型将消息正确地路由到相应的队列 使用RabbitMQ 原生客户端 123456789101112131415public abstract class BasicTest &#123; protected ConnectionFactory connectionFactory; protected Connection connection; protected Channel channel; @BeforeEach public void init() throws IOException, TimeoutException &#123; connectionFactory = new ConnectionFactory(); connectionFactory.setHost(&quot;localhost&quot;); connectionFactory.setUsername(&quot;rabbitmq&quot;); connectionFactory.setPassword(&quot;wgf123&quot;); connection = connectionFactory.newConnection(); channel = connection.createChannel(); &#125;&#125; 生产者 123456789101112131415161718public class ProducerTest extends BasicTest&#123; @Test @SneakyThrows public void send() &#123; /** * 生成一个队列 * 1.队列名称 * 2.队列里面的消息是否持久化 默认消息存储在内存中 * 3.该队列是否只供一个消费者进行消费 是否进行共享 true 可以多个消费者消费 * 4.是否自动删除 最后一个消费者端开连接以后 该队列是否自动删除 true 自动删除 * 5.其他参数 */ String queueName = &quot;test&quot;; String message = &quot;Hello&quot;; channel.queueDeclare(queueName, false, false, false, null); channel.basicPublish(&quot;&quot;, queueName, null, message.getBytes()); &#125;&#125; 消费者 12345678910111213141516171819202122232425262728@Slf4jpublic class ConsumerTest extends BasicTest &#123; @SneakyThrows @Test public void listener() &#123; //推送的消息如何进行消费的接口回调 DeliverCallback deliverCallback = (consumerTag, delivery) -&gt; &#123; String message = new String(delivery.getBody()); log.info(message); &#125;; //取消消费的一个回调接口 如在消费的时候队列被删除掉了 CancelCallback cancelCallback = (consumerTag) -&gt; &#123; System.out.println(&quot;消息消费被中断&quot;); &#125;; String queueName = &quot;test&quot;; /** * 消费者消费消息 * 1.消费哪个队列 * 2.消费成功之后是否要自动应答 true 代表自动应答 false 手动应答 * 3.消费者未成功消费的回调 */ channel.basicConsume(queueName, true, deliverCallback, cancelCallback); &#125;&#125; Spring Boot 添加依赖 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-amqp&lt;/artifactId&gt;&lt;/dependency&gt; 添加配置 123456spring:rabbitmq: port: 5672 host: localhost username: password: 创建队列 1234567891011121314@Configurationpublic class QueueConfig &#123; public static final String QUEUE_NAME = &quot;test_queue&quot;; /** * 简单定义消息队列 * @return */ @Bean public Queue simpleQueue() &#123; // 不指定交换机则使用默认交换机将消息转发到队列 return QueueBuilder.durable(&quot;test_queue&quot;).build(); &#125;&#125; 创建监听器 123456789@Slf4j@Componentpublic class SimpleListener &#123; @RabbitListener(queues = QueueConfig.QUEUE_NAME) public void receive(String msg) &#123; log.info(msg); &#125;&#125; 发送消息 12345678910111213@Slf4j@SpringBootTestpublic class QeueuTest &#123; @Autowired RabbitTemplate rabbitTemplate; @Test public void simpleSend() throws InterruptedException &#123; rabbitTemplate.convertAndSend(QueueConfig.QUEUE_NAME, &quot;test&quot;); // 暂停等待消息接收 TimeUnit.SECONDS.sleep(5); &#125;&#125; 消息分发 多消费者分发 多消费者的消息分发是一种常见的用于提高消息处理能力和实现负载均衡的策略。当多个消费者（Consumers）订阅同一个队列（Queue）时，RabbitMQ默认采用轮询分发策略将队列中的消息分发给各个消费者。多消费者间不保证消息有序，单消费者内消息有序。 多线程分发 为确保每条消息仅被消费一次，在使用直连交换机的场景下，当存在多个消费者或一个消费者启动多个线程进行消息处理时，通常为每个线程分配一个独立的Channel（非线程安全）。这导致消息队列与消费者Channel之间形成一对多的关系，从而使得各个消费者Channel之间产生竞争。 消费者多线程消费 一个消息消费者开启多线程消费，那么每个线程会单独对应一个Channel，多线程消费场景下，不能够保证跨线程间的消息有序性，只能保证本线程消息的有序性 使用@RabbitListener注解的concurrency属性指定线程数 1@RabbitListener(queues = AckQueueConfig.ACK_QUEUE, concurrency = &quot;5&quot;) 轮询分发（默认） 轮询分发是 RabbitMQ 的默认消息分发策略。在这种策略下，RabbitMQ 会按照消息到达的顺序，依次将消息分发给各个消费者。这种方式能够确保所有的消费者能够平均地处理消息，实现负载均衡。 缺点：轮询分发不考虑消费者的实际处理能力和性能差异，可能会导致性能浪费。例如，如果一个消费者处理消息的速度比另一个慢，快速的消费者会在等待新消息时出现空闲，从而导致资源未被充分利用。 验证 每秒发送一条消息 1234567891011121314@Slf4j@SpringBootTestpublic class QeueuTest &#123; @Autowired RabbitTemplate rabbitTemplate; @Test public void simpleSend() throws InterruptedException &#123; for (int i = 0; i &lt; 10; i++) &#123; rabbitTemplate.convertAndSend(QueueConfig.QUEUE_NAME, i + &quot;&quot;); TimeUnit.SECONDS.sleep(1); &#125; &#125;&#125; 定义多个监听器 12345678910111213141516171819@Slf4j@Componentpublic class SimpleListener &#123; @RabbitListener(queues = QueueConfig.QUEUE_NAME) public void receive(String msg) &#123; log.info(&quot;&#123;&#125;:&#123;&#125;&quot;, &quot;receive&quot;, msg); &#125; @RabbitListener(queues = QueueConfig.QUEUE_NAME) public void receive2(String msg) &#123; log.info(&quot;&#123;&#125;:&#123;&#125;&quot;, &quot;receive2&quot;, msg); &#125; @RabbitListener(queues = QueueConfig.QUEUE_NAME) public void receive3(String msg) &#123; log.info(&quot;&#123;&#125;:&#123;&#125;&quot;, &quot;receive3&quot;, msg); &#125;&#125; 执行结果 12345678910INFO 14984 --- [ntContainer#0-1] com.wgf.demo.listener.SimpleListener : receive:0INFO 14984 --- [ntContainer#1-1] com.wgf.demo.listener.SimpleListener : receive2:1INFO 14984 --- [ntContainer#2-1] com.wgf.demo.listener.SimpleListener : receive3:2INFO 14984 --- [ntContainer#0-1] com.wgf.demo.listener.SimpleListener : receive:3INFO 14984 --- [ntContainer#1-1] com.wgf.demo.listener.SimpleListener : receive2:4INFO 14984 --- [ntContainer#2-1] com.wgf.demo.listener.SimpleListener : receive3:5INFO 14984 --- [ntContainer#0-1] com.wgf.demo.listener.SimpleListener : receive:6INFO 14984 --- [ntContainer#1-1] com.wgf.demo.listener.SimpleListener : receive2:7INFO 14984 --- [ntContainer#2-1] com.wgf.demo.listener.SimpleListener : receive3:8INFO 14984 --- [ntContainer#0-1] com.wgf.demo.listener.SimpleListener : receive:9 不公平分发 在某些场景下，RabbitMQ的默认轮询分发方式可能不是最优选择。例如，当存在处理速度不一的两个消费者时，轮询方式可能导致快速消费者有大量空闲时间，而慢速消费者则持续繁忙。尽管RabbitMQ默认采用这种公平的分发策略，但在这种情况下，它可能不是最有效的。 为了解决这个问题，我们可以让消费者在完成当前任务并发送确认回执后，才接收下一个消息。这样，RabbitMQ会将新消息分发给当前不忙的消费者。但是，如果所有消费者都在处理任务，且队列中不断有新消息加入，就有可能导致队列积压。此时，我们可以考虑增加消费者数量或调整消息存储策略来解决这个问题。 prefetch 预取值 prefetch值用于限制Consumer消息缓冲区的大小，这对吞吐量有直接影响。 消息的发送本质上是异步的，因此，channel上通常会有多个消息。同时，消费者的手动确认也是异步的，这就产生了一个未确认消息的缓冲区。为避免缓冲区内未确认消息的无限制增长，建议开发人员限制此缓冲区的大小，这可以通过设置预取计数值来实现。 此值定义了channel上允许的未确认消息的最大数量。当未确认消息数量达到此值时，RabbitMQ将停止向该通道发送新消息，直到有至少一个未处理的消息被确认。例如，若通道上有未确认的消息5、6、7、8，且预取计数设置为4，则RabbitMQ不会再向该通道发送新消息，直到有消息被确认。一旦消息被确认，RabbitMQ会立即发送新消息。 消息确认和QoS预取值对吞吐量有显著影响。通常，增加预取值会提高向消费者传递消息的速度。虽然提高预取值会增加传输速率，但同时也会增加已传递但未处理消息的数量，导致消费者的RAM消耗增加. 因此，使用具有高预取值的自动确认模式或手动确认模式时需谨慎，若消费者消费了大量消息但未进行确认，会导致连接节点的内存消耗增大。 为了避免这种情况，我们可以设置参数 12345678910 port: 8081spring: rabbitmq: port: 5672 host: localhost username: rabbitmq password: wgf123 listener: simple: prefetch: 5 # 动态调整 springBoot默认值是250，当一个队列有多个消费者时，可以根据消费者不同的消费能力灵活配置 demo 启用两个实例，实例1 prefetch = 1，实例2 prefetch = 5，并发送20条消息 结果 实例1 123INFO 28572 --- [ntContainer#5-1] com.wgf.demo.listener.SimpleListener : receive:2INFO 28572 --- [ntContainer#5-1] com.wgf.demo.listener.SimpleListener : receive:8INFO 28572 --- [ntContainer#5-1] com.wgf.demo.listener.SimpleListener : receive:15 实例2 1234567891011121314151617INFO 28901 --- [ntContainer#5-1] com.wgf.demo.listener.SimpleListener : receive:1INFO 28901 --- [ntContainer#5-1] com.wgf.demo.listener.SimpleListener : receive:3INFO 28901 --- [ntContainer#5-1] com.wgf.demo.listener.SimpleListener : receive:4INFO 28901 --- [ntContainer#5-1] com.wgf.demo.listener.SimpleListener : receive:5INFO 28901 --- [ntContainer#5-1] com.wgf.demo.listener.SimpleListener : receive:6INFO 28901 --- [ntContainer#5-1] com.wgf.demo.listener.SimpleListener : receive:7INFO 28901 --- [ntContainer#5-1] com.wgf.demo.listener.SimpleListener : receive:9INFO 28901 --- [ntContainer#5-1] com.wgf.demo.listener.SimpleListener : receive:10INFO 28901 --- [ntContainer#5-1] com.wgf.demo.listener.SimpleListener : receive:11INFO 28901 --- [ntContainer#5-1] com.wgf.demo.listener.SimpleListener : receive:12INFO 28901 --- [ntContainer#5-1] com.wgf.demo.listener.SimpleListener : receive:13INFO 28901 --- [ntContainer#5-1] com.wgf.demo.listener.SimpleListener : receive:14INFO 28901 --- [ntContainer#5-1] com.wgf.demo.listener.SimpleListener : receive:16INFO 28901 --- [ntContainer#5-1] com.wgf.demo.listener.SimpleListener : receive:17INFO 28901 --- [ntContainer#5-1] com.wgf.demo.listener.SimpleListener : receive:18INFO 28901 --- [ntContainer#5-1] com.wgf.demo.listener.SimpleListener : receive:19INFO 28901 --- [ntContainer#5-1] com.wgf.demo.listener.SimpleListener : receive:20 消息队列参数配置 参数名称 说明 x-message-ttl 消息在队列中过期的时间,单位ms x-expires 队列在多长时间没有被使用后删除 x-max-length 限制队列中消息的最大条数。超过设定值后，新消息会替换旧消息，遵循先进先出原则 x-max-length-bytes 设定队列占用内存的最大字节数。达到限制时，将采用LRU算法删除旧消息 x-dead-letter-routing-key 死信队列路由key x-dead-letter-exchange 死信队列交换机，创建queue时参数arguments设置了x-dead-letter-routing-key和x-dead-letter-exchange，会在x-message-ttl时间到期后把消息放到x-dead-letter-routing-key和x-dead-letter-exchange指定的队列中达到延迟队列的目的 x-max-priority 具有更高优先级的队列具有较高的优先权，优先级高的消息具备优先被消费的特权[1,255] x-queue-mode lazt。指定惰性队列 惰性队列会尽可能的将消息存入磁盘中，而在消费者消费到相应的消息时才会被加载到内存中，它的一个重要设计目标是能够支持更长的队列，即支持更多的消息存储。当消费者由于各种各样的原因（比如消费者下线、跌机、或者由于维护而关闭等）致使长时间不能消费消息而造成堆积时，惰性队列就很必要了 x-queue-master-locator 决定队列master节点的选择策略（集群）min-masters：选择master queue 数最少的那个服务节点hostclient-local：选择与client相连接的那个服务节点hostrandom：随机分配 创建消息队列 12345678910/** * @param name 队列名称 * @param durable 是否持久化，如果设置为true，服务器重启了队列仍然存在（默认为true） * @param exclusive 是否为独享队列（排他性队列），只有自己可见的队列，即不允许其它用户访问(默认false) * @param autoDelete 当没有任何消费者使用时，自动删除该队列(默认为false,设置为true即为临时队列) * @param arguments 其他参数 */public Queue(String name, boolean durable, boolean exclusive, boolean autoDelete, @Nullable Map&lt;String, Object&gt; arguments) &#123;&#125; 消息异步发布确认 生产者发送消息，是先发送消息到Exchange，然后Exchange再路由到Queue。这中间就需要确认两个事情，第一，消息是否成功发送到Exchange；第二，消息是否正确的通过Exchange路由到Queue spring提供了两个回调函数来处理这两种消息发送确认 ConfirmCallback 发布确认 ConfirmCallback 是一个用于处理消息确认的回调接口。它用于确保消息是否成功到达交换机。 消息成功到达交换机： ​ 如果消息成功发送到交换机，confirm 方法将被调用，ack 参数为 true，表示消息已经被确认。 消息未能到达交换机： ​ 如果消息未能到达交换机，confirm 方法同样会被调用，但此时 ack 参数为 false。 回调参数： CorrelationData correlationData： 唯一标识发送的消息，与确认或拒绝的消息关联。 boolean ack： true 表示消息成功到达交换机，false 表示发送失败。 String cause： 拒绝时包含原因，成功时为 null。 源码实现 1234567891011121314@Slf4j@Componentpublic class MsgConfirmCallback implements RabbitTemplate.ConfirmCallback &#123; @Override public void confirm(CorrelationData correlationData, boolean ack, String cause) &#123; // 如果发送到交换器都没有成功（比如说删除了交换器），ack 返回值为 false // 如果发送到交换器成功，返回值为还是 true // 获取消息id //String messageId = correlationData.getId(); log.info(&quot;ConfirmCallback , correlationData = &#123;&#125; , ack = &#123;&#125; , cause = &#123;&#125; &quot;, correlationData, ack, cause); &#125;&#125; ReturnCallback 消息回退 ReturnCallback 是 RabbitMQ 中的一个回调机制，用于处理未能路由到合适队列的消息。只有消息没有被正确路由，才会回调此接口。 回调参数： message：未能路由到队列的消息。 replyCode：RabbitMQ 返回的错误代码。 replyText：RabbitMQ 返回的错误描述。 exchange：发送消息时使用的交换机。 routingKey：发送消息时使用的路由 实现源码 12345678910@Slf4j@Componentpublic class MsgReturnCallback implements RabbitTemplate.ReturnCallback &#123; @Override public void returnedMessage(Message message, int replyCode, String replyText, String exchange, String routingKey) &#123; // 获取消息id // String messageId = message.getMessageProperties().getMessageId(); log.info(&quot;ReturnCallback unroutable messages, message = &#123;&#125; , replyCode = &#123;&#125; , replyText = &#123;&#125; , exchange = &#123;&#125; , routingKey = &#123;&#125; &quot;, message, replyCode, replyText, exchange, routingKey); &#125;&#125; 整合 添加两个Callback 12345678910111213141516171819202122232425@Configurationpublic class RabbitmqConfig &#123; @Autowired MsgConfirmCallback msgConfirmCallback; @Autowired MsgReturnCallback msgReturnCallback; @Bean public RabbitTemplate rabbitTemplate(ConnectionFactory connectionFactory) &#123; // connectionFactory包含yml配置文件配置信息 RabbitTemplate template = new RabbitTemplate(connectionFactory); //mandatory参数说明: 交换器无法根据自身类型和路由键找到一个符合条件的队列时的处理方式; true表示会调用Basic.Return命令返回给生产者; false表示丢弃消息 template.setMandatory(true); // 消息从 producer 到 exchange 正常则会返回一个 confirmCallback template.setConfirmCallback(msgConfirmCallback); // 消息从 exchange–&gt;queue 投递失败则会返回一个 returnCallback template.setReturnCallback(msgReturnCallback); return template; &#125;&#125; 开启回调配置 123456789server: port: 8081spring: rabbitmq: port: 5672 host: 121.37.23.172 username: rabbitmq password: wgf123 publisher-confirm-type: correlated # 发布消息成功到交换器后会触发回调方法 发送消息 12345678910111213@Testpublic void testCallback() &#123; //correlationDataId相当于消息的唯一表示 UUID correlationDataId = UUID.randomUUID(); // 这里设置的id是给ConfirmCallback使用的 CorrelationData correlationData = new CorrelationData(correlationDataId.toString()); //发送MQ rabbitTemplate.convertAndSend(DirectConfig.DIRECT_EXCHANGE, &quot;test-queue&quot;, &quot;test&quot;, (message) -&gt; &#123; // 这里设置的id是给 ReturnCallback 使用的 message.getMessageProperties().setMessageId(correlationData.getId()); return message; &#125;, correlationData);&#125; ACK消息确认机制 当消费者从 RabbitMQ 接收到一条消息并成功处理后，消费者需要向 RabbitMQ 发送 ACK 反馈。只有在 RabbitMQ 收到这个 ACK 反馈后，它才会将该消息从队列中删除。 处理网络不稳定或服务器异常 如果消费者在处理消息时遇到网络不稳定或服务器异常等问题，可能导致无法发送 ACK 反馈。 RabbitMQ 将认为这条消息没有被正常消费，于是将其重新放回队列中等待处理。 集群环境下的处理 在 RabbitMQ 集群中，如果一个消费者未能发送 ACK 反馈，RabbitMQ 不会无限期等待。 相反，它会立即将该消息推送给在线的其他消费者，以确保消息和任务不会丢失。 消息的删除条件 消费者成功处理消息并发送 ACK 反馈。 RabbitMQ 确认接收到了这个 ACK 反馈。 消息的ACK确认机制默认是打开的(自动ACK) 消息自动重新入列 Channel断开，未ACK的消息重新入列 如果消费者因某些原因失去与 RabbitMQ 的连接（例如通道关闭、连接关闭或TCP连接丢失），导致消息未能发送 ACK 确认，RabbitMQ 将察觉到消息未被完全处理，并会将其重新放回队列。如果此时有其他可用的消费者可以处理这条消息，RabbitMQ 将尽快将消息重新分发给另一个消费者。这样，即使某个消费者偶尔故障，也可以确保不会丢失任何消息。这个机制保障了消息的可靠传递和处理，即使在不稳定的网络环境或消费者故障的情况下也能保持消息的完整性。 自动ack 优点 简单易用：不需要显式的确认消息，减少了开发的复杂性。 性能较高：由于不涉及额外的确认步骤，处理速度较快，适用于一些不需要严格消息确认的场景。 缺点 消息丢失风险：最大的缺点是消息一旦被投递给消费者就被认为已处理，如果消费者在处理消息时发生故障，消息将会丢失。 无法保证可靠性：无法确保每条消息都被成功处理，适用于不太关键的场景。 手动ack 优点 消息可靠性：消费者可以在处理消息后显式确认，确保消息已经被成功处理，减少了消息丢失的风险。 灵活性：可以自定义确认的时机，例如，只有在下游业务成功完成后才确认消息。 缺点 开发复杂性：需要显式调用API来确认消息，增加了开发的复杂性。 性能开销：相比自动ACK，手动ACK可能会引入一些性能开销，因为需要额外的确认步骤。 使用手动ack 添加配置 1234spring:rabbitmq: listener: acknowledge-mode: manual # ack应答设置为手动模式 监听器实现 12345678910111213141516171819202122232425262728293031323334353637383940@Slf4j@Componentpublic class AckListener &#123; @RabbitListener(queues = AckQueueConfig.ACK_QUEUE) public void listener(String message, Channel channel, Message messageEntity) &#123; int num = Integer.valueOf(message); try &#123; // 让值大于5抛异常进行Nack, 消息就会重回队列头部进行下次消费，这里的代码num&gt;5会是一个死循环消费 if (num &gt;= 5) &#123; throw new RuntimeException(); &#125; log.info(&quot;队列：&#123;&#125; 接收到消息：&#123;&#125;&quot;, AckQueueConfig.ACK_QUEUE, message); /** * 消息消费确认 * 如果客户端在线没有签收这条Message，则此消息进入Unacked状态，此时监听器阻塞等待消息确认，不推送新Message * 如果待消息确认并且客户端下线，下次客户端上线重新推送上次Unacked状态Message */ channel.basicAck(messageEntity.getMessageProperties().getDeliveryTag(), false); &#125; catch (Exception e) &#123; log.error(&quot;消费消息异常&quot;, e); /** * 第一个参数deliveryTag：发布的每一条消息都会获得一个唯一的deliveryTag，deliveryTag在channel范围内是唯一的 * 第二个参数multiple：批量确认标志。如果值为true，包含本条消息在内的、所有比该消息deliveryTag值小的消息都会被处理 * 第三个参数requeue：表示如何处理这条消息，如果值为true，则重新放入RabbitMQ的发送队列，如果值为false，则通知RabbitMQ销毁这条消息 */ try &#123; // nack 消息回到队列头部后会被重新消费，如果多次nack的会无限循环，卡在这里 channel.basicNack(messageEntity.getMessageProperties().getDeliveryTag(), false,true); &#125; catch (IOException ex) &#123; // TODO 重试逻辑或业务补偿（发往死信队列或Redis等） log.error(&quot;nack失败&quot;, ex); &#125; &#125; &#125;&#125; 发送消息 1234567891011121314151617private void send(String msgId, String exchange, String routingKey, Object msg) &#123; CorrelationData correlationData = new CorrelationData(msgId); //发送MQ rabbitTemplate.convertAndSend(exchange, routingKey, msg, (message) -&gt; &#123; message.getMessageProperties().setMessageId(correlationData.getId()); return message; &#125;, correlationData); &#125; @Test public void ackTest() &#123; IntStream.range(0, 6).forEach(line -&gt; &#123; //correlationDataId相当于消息的唯一表示 String msgId = UUID.randomUUID().toString(); this.send(msgId, DirectConfig.DIRECT_EXCHANGE, AckQueueConfig.ROUTING_KEY, line); &#125;); &#125; ack重试配置 retry只能在自动ack模式下使用 重试并不是RabbitMQ重新发送了消息，仅仅是spring封装消费者内部进行了重试，换句话说就是重试跟mq没有任何关系；配置重试次数，当超过重试次数消息投递到异常队列 实现MessageRecoverer接口的recover方法，当重试次数上限则调用该方法处理 添加配置 1234567891011121314151617181920server:port: 8081spring:rabbitmq: port: 5672 host: localhost username: rabbitmq password: wgf123 publisher-confirm-type: correlated # 发布消息成功到交换器后会触发回调方法 listener: simple: prefetch: 5 #acknowledge-mode: manual # 手动ack模式（关闭手动ack） retry: # 重试配置 enabled: true max-attempts: 5 # 最大重试次数 max-interval: 10000 # 重试最大间隔时间 initial-interval: 2000 # 重试初始间隔时间 multiplier: 2 # 间隔时间乘子，间隔时间*乘子=下一次的间隔时间，最大不能超过设置的最大间隔时间 default-requeue-rejected: true # 重试次数超过上面的设置之后是否丢弃 队列和重试策略配置 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162@Configurationpublic class RetryConfig &#123; public static final String RETRY_EXCHANGE = &quot;retry.queue&quot;; public static final String RETRY_QUEUE = &quot;retry.queue&quot;; public static final String ROUTING_KEY = &quot;retry&quot;; public static final String ERROR_QUEUE = &quot;retry.error&quot;; public static final String ERROR_ROUTING_KEY = &quot;error&quot;; @Autowired RabbitTemplate rabbitTemplate; @Bean public DirectExchange retryExchange() &#123; return new DirectExchange(RETRY_EXCHANGE); &#125; @Bean public Queue retryQueue() &#123; return new Queue(RETRY_QUEUE); &#125; @Bean public Binding bindRetryQueue(Queue retryQueue, DirectExchange retryExchange) &#123; return BindingBuilder.bind(retryQueue).to(retryExchange).with(ROUTING_KEY); &#125; /** * 异常队列 * * @return */ @Bean public Queue errorQueue() &#123; return new Queue(ERROR_QUEUE); &#125; /** * 交换机绑定异常队列 * * @param errorQueue * @param directExchange * @return */ @Bean public Binding bindErrorQueue(Queue errorQueue, DirectExchange retryExchange) &#123; return BindingBuilder.bind(errorQueue).to(retryExchange).with(ERROR_ROUTING_KEY); &#125; /** * 调试时，配置打开 * MessageRecoverer 的实现类。达到重试次数后将消息丢第到异常队列 * * @return */ @Bean public MessageRecoverer republishMessageRecoverer() &#123; // 重试次数上限将消息丢到异常队列 return new RepublishMessageRecoverer(rabbitTemplate, RETRY_EXCHANGE, ERROR_ROUTING_KEY); &#125;&#125; 消息监听器 123456789101112@Slf4j@Componentpublic class RetryListener &#123; private AtomicInteger count = new AtomicInteger(0); @RabbitListener(queues = RetryConfig.RETRY_QUEUE) public void listener(String message, Channel channel, Message messageEntity) throws IOException &#123; log.info(&quot;消费次数：&#123;&#125;&quot;, count.incrementAndGet()); // 必须抛出异常才能触发重试次数 throw new RuntimeException(); &#125;&#125; 发送消息 12345@Testpublic void retryTest() throws InterruptedException &#123; this.rabbitTemplate.convertAndSend(RetryConfig.RETRY_EXCHANGE, RetryConfig.ROUTING_KEY, &quot;test&quot;); TimeUnit.SECONDS.sleep(60);&#125; 日志 1234562022-02-04 16:01:32.156 INFO 13932 --- [tContainer#13-1] com.wgf.demo.listener.RetryListener : 消费次数：12022-02-04 16:01:34.157 INFO 13932 --- [tContainer#13-1] com.wgf.demo.listener.RetryListener : 消费次数：22022-02-04 16:01:38.164 INFO 13932 --- [tContainer#13-1] com.wgf.demo.listener.RetryListener : 消费次数：32022-02-04 16:01:46.167 INFO 13932 --- [tContainer#13-1] com.wgf.demo.listener.RetryListener : 消费次数：42022-02-04 16:01:56.168 INFO 13932 --- [tContainer#13-1] com.wgf.demo.listener.RetryListener : 消费次数：52022-02-04 16:01:56.169 WARN 13932 --- [tContainer#13-1] o.s.a.r.retry.RepublishMessageRecoverer : Republishing failed message to exchange &#x27;retry.queue&#x27; with routing key error 消费次数上限丢到error队列 RabbitMQ持久化 默认情况下，RabbitMQ将exchange、queue、message等数据存储在内存中，这意味着如果RabbitMQ在重启、关闭或宕机时，所有信息都会丢失。 为了解决这个问题，RabbitMQ提供了持久化功能。通过启用持久化，当RabbitMQ在发生重启、关闭或宕机后重新启动时，它可以从硬盘中恢复exchange、queue、message等数据。 持久化的配置方式如下： exchange 持久化，在声明时指定 durable 为 true queue 持久化，在声明时指定 durable 为 true message 持久化，在投递时通过参数指定消息持久化，持久化到磁盘 需要注意的是，exchange和queue的持久化确保了它们自身的元数据不会因异常而丢失，但并不保证其中的message不会丢失。要确保message不会丢失，必须同时将message也标记为持久化。 注意 如果 exchange 和 queue 两者之间有一个持久化，一个非持久化，就不允许建立绑定 如果 exchange 和 queue 都是持久化的，那么它们之间的 binding 也是持久化的 exchange 和 queue 持久化的是它们的元数据，保证RabbitMQ重启后 exchange 和 queue 能从磁盘重新加载 exchange 持久化 代码持久化 123456789101112@Beanpublic DirectExchange directExchange() &#123; // 默认持久化 // return new DirectExchange(DIRECT_EXCHANGE); /** * 第一个参数：交换机名称 * 第二个参数：是否持久化 * 第三个参数：是否自动删除，条件：有队列或者交换器绑定了本交换器，然后所有队列或交换器都与本交换器解除绑定，autoDelete=true时，此交换器就会被自动删除 */ return new DirectExchange(DIRECT_EXCHANGE, true, false);&#125; 控制台持久化 queue 持久化 代码持久化 12345678910/** * @param name 队列名称 * @param durable 是否持久化，如果设置为true，服务器重启了队列仍然存在（默认为true） * @param exclusive 是否为独享队列（排他性队列），只有自己可见的队列，即不允许其它用户访问(默认false) * @param autoDelete 当没有任何消费者使用时，自动删除该队列(默认为false,设置为true即为临时队列) * @param arguments 其他参数 */public Queue(String name, boolean durable, boolean exclusive, boolean autoDelete, @Nullable Map&lt;String, Object&gt; arguments) &#123;&#125; 控制台持久化 消息持久化 12345678910111213@Testpublic void persistenceTest() &#123; // 消息持久化 MessagePostProcessor messagePostProcessor = new MessagePostProcessor() &#123; @Override public Message postProcessMessage(Message message) throws AmqpException &#123; message.getMessageProperties().setDeliveryMode(MessageDeliveryMode.PERSISTENT); return message; &#125; &#125;; rabbitTemplate.convertAndSend(QueueConfig.QUEUE_NAME, &quot;test&quot;.getBytes(), messagePostProcessor);&#125; 死信队列 DLX 死信：无法被消费的消息 死信队列：专门存放无法被消费的消息 先从概念解释上搞清楚这个定义，死信，顾名思义就是无法被消费的消息，字面意思可以这样理解，一般来说，producer 将消息投递到 broker 或者直接到 queue 里了，consumer 从 queue 取出消息进行消费，但某些时候由于特定的原因导致 queue 中的某些消息无法被消费，这样的消息如果没有后续的处理，就变成了死信，有死信自然就有了死信队列 应用场景：为了保证订单业务的消息数据不丢失，需要使用到 RabbitMQ 的死信队列机制，当消息消费发生异常时，将消息投入死信队列中。还有比如说: 用户在商城下单成功并点击去支付后在指定时间未支付时自动失效 死信的来源 消息 TTL 过期 队列达到最大限制淘汰旧消息 消息被拒绝(basic.reject 或 basic.nack)并且 requeue=false 死信队列Example 交换机和队列配置 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160@Configurationpublic class DeadConfig &#123; /** * 死信配置 */ public static final String DEAD_EXCHANGE = &quot;dead_exchange&quot;; public static final String DEAD_QUEUE = &quot;dead.queue&quot;; public static final String DEAD_ROUTING_KEY = &quot;dead&quot;; /** * 正常交换机与队列配置 */ public static final String NORMAL_EXCHANGE = &quot;normal_exchange&quot;; /** * 有TTL过期的队列 */ public static final String TTL_QUEUE = &quot;dead.ttl.queue&quot;; public static final String TTL_ROUTING_KEY = &quot;ttl&quot;; /** * 有长度的队列 */ public static final String LENGTH_QUEUE = &quot;dead.length.queue&quot;; public static final String LENGTH_ROUTING_KEY = &quot;length&quot;; /** * 拒绝队列 */ public static final String NACK_QUEUE = &quot;dead.nack.queue&quot;; public static final String NACK_ROUTING_KEY = &quot;nack&quot;; /** * 创建死信交换机 * * @return */ @Bean public DirectExchange deadExchange() &#123; return new DirectExchange(DEAD_EXCHANGE); &#125; /** * 创建死信队列 * * @return */ @Bean public Queue deadQueue() &#123; return new Queue(DEAD_QUEUE); &#125; /** * 绑定死信队列 * * @param directQueue * @param directExchange * @return */ @Bean public Binding bindDeadtQueue(Queue deadQueue, DirectExchange deadExchange) &#123; return BindingBuilder.bind(deadQueue).to(deadExchange).with(DEAD_ROUTING_KEY); &#125; /** * 正常交换机 * * @return */ @Bean public DirectExchange normalExchange() &#123; return new DirectExchange(NORMAL_EXCHANGE); &#125; /** * TTL过期队列 */ @Bean public Queue deadTtlQueue() &#123; // 正常队列绑定死信队列 Map&lt;String, Object&gt; args = new HashMap&lt;&gt;(8); // 设置TTL过期 args.put(&quot;x-message-ttl&quot;, 10000); // 声明死信队列路由键 args.put(&quot;x-dead-letter-routing-key&quot;, DEAD_ROUTING_KEY); // 声明死信队列交换机，消息异常投递到此交换机 args.put(&quot;x-dead-letter-exchange&quot;, DEAD_EXCHANGE); return new Queue(TTL_QUEUE, true, false, false, args); &#125; /** * 有长度限制的队列 * @return */ @Bean public Queue deadLengthQueue() &#123; // 正常队列绑定死信队列 Map&lt;String, Object&gt; args = new HashMap&lt;&gt;(8); // 设置队列最大长度 args.put(&quot;x-max-length&quot;, 4); // 声明死信队列路由键 args.put(&quot;x-dead-letter-routing-key&quot;, DEAD_ROUTING_KEY); // 声明死信队列交换机，消息异常投递到此交换机 args.put(&quot;x-dead-letter-exchange&quot;, DEAD_EXCHANGE); return new Queue(LENGTH_QUEUE, true, false, false, args); &#125; /** * 拒绝队列，监听时NACK * @return */ @Bean public Queue deadNackQueue() &#123; // 正常队列绑定死信队列 Map&lt;String, Object&gt; args = new HashMap&lt;&gt;(8); // 声明死信队列路由键 args.put(&quot;x-dead-letter-routing-key&quot;, DEAD_ROUTING_KEY); // 声明死信队列交换机，消息异常投递到此交换机 args.put(&quot;x-dead-letter-exchange&quot;, DEAD_EXCHANGE); return new Queue(NACK_QUEUE, true, false, false, args); &#125; /** * TTL队列绑定交换机 * * @param deadTtlQueue * @param normalExchange * @return */ @Bean public Binding bingDeadTtlQueue(Queue deadTtlQueue, DirectExchange normalExchange) &#123; return BindingBuilder.bind(deadTtlQueue).to(normalExchange).with(TTL_ROUTING_KEY); &#125; /** * LENGTH队列绑定交换机 * @param deadLengthQueue * @param normalExchange * @return */ @Bean public Binding bingDeadLengthQueue(Queue deadLengthQueue, DirectExchange normalExchange) &#123; return BindingBuilder.bind(deadLengthQueue).to(normalExchange).with(LENGTH_ROUTING_KEY); &#125; /** * NACK队列绑定交换机 * @param deadNackQueue * @param normalExchange * @return */ @Bean public Binding bingDeadNackQueue(Queue deadNackQueue, DirectExchange normalExchange) &#123; return BindingBuilder.bind(deadNackQueue).to(normalExchange).with(NACK_ROUTING_KEY); &#125;&#125; 监听器配置 12345678910111213141516171819202122232425@Slf4j@Componentpublic class DeadListener &#123; /** * 监听死信队列 */ @RabbitListener(queues = DeadConfig.DEAD_QUEUE) public void deadListener(String message, Channel channel, Message messageEntity) throws IOException &#123; log.info(&quot;死信队列接受到死信：&#123;&#125;&quot;, message); channel.basicAck(messageEntity.getMessageProperties().getDeliveryTag(), false); &#125; /** * NACK 监听 */ @SneakyThrows @RabbitListener(queues = DeadConfig.NACK_QUEUE) public void listener(String message, Channel channel, Message messageEntity) throws IOException &#123; // 拒收消息 log.info(&quot;队列：&#123;&#125; 拒收消息：&#123;&#125;&quot;, DeadConfig.TTL_QUEUE, message); // 两种决绝方式都能重新投递死信队列 channel.basicNack(messageEntity.getMessageProperties().getDeliveryTag(), false, false); // channel.basicReject(messageEntity.getMessageProperties().getDeliveryTag(), false); &#125;&#125; TTL过期 配置正常队列与死信队列绑定 发送消息 123456789101112@Test@SneakyThrowspublic void deadTtlTest() &#123; IntStream.range(0, 10).forEach(line -&gt; &#123; int ttl = 5000; String msg = &quot;delay:&quot; + line; rabbitTemplate.convertAndSend(DeadConfig.NORMAL_EXCHANGE, DeadConfig.TTL_ROUTING_KEY, msg); log.info(&quot;发送一条时长: &#123;&#125; 毫秒信息给队列: &#123;&#125; 内容: &#123;&#125;&quot;, ttl, DeadConfig.TTL_QUEUE, msg); &#125;); TimeUnit.SECONDS.sleep(20);&#125; 图中可以看dead.ttl.queue指定了DLX DLK,当消息TTL过期，则会将消息转发到dead_exchange交换机然后根据配置的死信路由键路由到dead.queue队列 12345678910INFO 18352 --- [ntContainer#0-1] com.wgf.demo.listener.DeadListener : 死信队列接受到死信：delay:0INFO 18352 --- [ntContainer#0-1] com.wgf.demo.listener.DeadListener : 死信队列接受到死信：delay:1INFO 18352 --- [ntContainer#0-1] com.wgf.demo.listener.DeadListener : 死信队列接受到死信：delay:2INFO 18352 --- [ntContainer#0-1] com.wgf.demo.listener.DeadListener : 死信队列接受到死信：delay:3INFO 18352 --- [ntContainer#0-1] com.wgf.demo.listener.DeadListener : 死信队列接受到死信：delay:4INFO 18352 --- [ntContainer#0-1] com.wgf.demo.listener.DeadListener : 死信队列接受到死信：delay:5INFO 18352 --- [ntContainer#0-1] com.wgf.demo.listener.DeadListener : 死信队列接受到死信：delay:6INFO 18352 --- [ntContainer#0-1] com.wgf.demo.listener.DeadListener : 死信队列接受到死信：delay:7INFO 18352 --- [ntContainer#0-1] com.wgf.demo.listener.DeadListener : 死信队列接受到死信：delay:8INFO 18352 --- [ntContainer#0-1] com.wgf.demo.listener.DeadListener : 死信队列接受到死信：delay:9 队列达到最大长度 12345678@Test@SneakyThrowspublic void deadLengthTest() &#123; for (int i = 0; i &lt; 10; i++) &#123; rabbitTemplate.convertAndSend(DeadConfig.NORMAL_EXCHANGE, DeadConfig.LENGTH_ROUTING_KEY, i + &quot;&quot;); &#125; TimeUnit.SECONDS.sleep(10);&#125; 先看队列dead.length.queue消息，剩余4条，和配置的容量符合，编号0-5的消息淘汰后被转发到死信队列 123456INFO 16096 --- [ntContainer#1-1] com.wgf.demo.listener.DeadListener : 死信队列接受到死信：0INFO 16096 --- [ntContainer#1-1] com.wgf.demo.listener.DeadListener : 死信队列接受到死信：2INFO 16096 --- [ntContainer#1-1] com.wgf.demo.listener.DeadListener : 死信队列接受到死信：1INFO 16096 --- [ntContainer#1-1] com.wgf.demo.listener.DeadListener : 死信队列接受到死信：4INFO 16096 --- [ntContainer#1-1] com.wgf.demo.listener.DeadListener : 死信队列接受到死信：5INFO 16096 --- [ntContainer#1-1] com.wgf.demo.listener.DeadListener : 死信队列接受到死信：3 队列溢出的默认行为 当队列的消息超过设置的最大长度或大小时，RabbitMQ默认的做法是将处于队列头部的信息（队列中最老的消息）丢弃或变成死信 消息被拒 监听器Nack，拒绝消息 12345678@Test@SneakyThrowspublic void deadNackTest() &#123; for (int i = 0; i &lt; 3; i++) &#123; rabbitTemplate.convertAndSend(DeadConfig.NORMAL_EXCHANGE, DeadConfig.NACK_ROUTING_KEY, i + &quot;&quot;); &#125; TimeUnit.SECONDS.sleep(10);&#125; 最终所有拒收的消息都被转发到死信队列中 123INFO 7464 --- [ntContainer#1-1] com.wgf.demo.listener.DeadListener : 死信队列接受到死信：0INFO 7464 --- [ntContainer#1-1] com.wgf.demo.listener.DeadListener : 死信队列接受到死信：1INFO 7464 --- [ntContainer#1-1] com.wgf.demo.listener.DeadListener : 死信队列接受到死信：2 basicReject / basicNack / basicRecover区别 Reject channel.basicReject(deliveryTag, true); basic.reject方法拒绝deliveryTag对应的消息，第二个参数是否requeue，true则重新入队列，否则丢弃或者进入死信队列，一次只能拒绝一条 Nack channel.basicNack(deliveryTag, false, true); basic.nack方法为不确认deliveryTag对应的消息，第二个参数是否应用于多消息，第三个参数是否requeue，与basic.reject区别就是同时支持多个消息，nack后消息重新回到队列头部重新消费，消息还是会被自己重新消费 Recover channel.basicRecover(true); basic.recover是否恢复消息到队列，参数是是否requeue，true则重新入队列，并且尽可能的将之前recover的消息投递给其他消费者消费，而不是自己再次消费。false则消息会重新被投递给自己。 延迟队列 延迟队列是一种用于实现消息延迟传递的特殊队列类型。它允许你将消息发送到队列，并在一定的延迟时间之后才将消息投递给消费者。这对于需要执行计划任务、调度任务或实现消息重试机制等情况非常有用。 延迟队列使用场景 订单超时取消：如果用户下单后十分钟内未支付，系统会自动取消订单，而不需要等待用户手动取消，提供更好的用户体验。 店铺提醒：新创建的店铺如果在十天内没有上传商品，系统会自动发送消息提醒店主，确保店铺及时添加商品。 用户登录提醒：用户注册成功后，如果三天内没有登录，系统将通过短信提醒用户，激发用户活跃度。 退款处理通知：用户发起退款申请后，如果三天内没有得到处理，系统会通知相关运营人员，保障退款流程的顺利进行。 会议提醒：预定的会议开始前十分钟，系统会自动通知所有与会人员，确保大家按时参加会议，提高会议的准时性。 在这些情况下，延迟队列的作用是在特定的时间点触发相应的任务，而不需要持续不断地查询数据库，从而提高了系统的效率和性能，尤其在处理大规模数据和有时限要求的情况下，延迟队列显得尤为重要和高效。 代码架构图 用TTL和死信队列实现延迟队列功能 原生延迟队列的核心思想是利用消息的 TTL 和 死信队列 来实现。 代码结构可以参考 ttl过期 ，不过需要新增一个队列并将其绑定到死信队列上。重要的是，不监听正常队列，而是监听死信队列。这样，RabbitMQ会在消息的TTL过期后自动将消息转发到死信队列，从而实现延迟效果。 缺点 需要注意的是，这种方法的缺点是每增加一个新的时间需求就需要新增一个队列。例如，如果需要一个小时后处理，就需要创建一个TTL为一个小时的队列。对于一些需要不同延迟时间的场景，可能需要创建大量队列，这会导致管理复杂性增加。 原生延迟队列缺点 当队列中的消息按照它们的TTL（生存时间）排列时，如果队列头部的消息的TTL比后面的消息要长，那么头部的消息会阻塞后面的消息。 缺点 简单来说，如果你使用消息属性上设置TTL的方式，RabbitMQ只会检查队列中的第一个消息是否过期，如果过期，它会被移到死信队列中。问题在于，如果第一个消息的延迟时间很长，而第二个消息的延迟时间很短，第二个消息并不会优先执行，因为RabbitMQ只检查队列头部的消息是否过期。这可能导致消息执行的顺序不符合你的预期。 添加配置 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970@Configurationpublic class TtlQueueConfig &#123; /** * 死信队列 */ public static final String DEAD_EXCHANGE = &quot;dead_ttl_exchange&quot;; public static final String DEAD_QUEUE = &quot;dead.ttl.overdue.queue&quot;; public static final String DEAD_ROUTING_KEY = &quot;overdue&quot;; /** * 正常队列，发送消息时，在消息设置TTL过期 */ public static final String TTL_EXCHANGE = &quot;ttl_exchange&quot;; public static final String TTL_QUEUE = &quot;ttl.queue&quot;; public static final String TTL_ROUTING_KEY = &quot;ttl&quot;; /** * 创建死信交换机 */ @Bean public DirectExchange deadTtlExchange() &#123; return new DirectExchange(DEAD_EXCHANGE); &#125; /** * 创建死信队列，正常队列TTL过期将死信转发到死信队列 */ @Bean public Queue deadTtlOverdueQueue() &#123; return new Queue(DEAD_QUEUE); &#125; /** * 绑定死信队列 */ @Bean public Binding bindDeadTtlOverdueQueue(Queue deadTtlOverdueQueue, DirectExchange deadTtlExchange) &#123; return BindingBuilder.bind(deadTtlOverdueQueue).to(deadTtlExchange).with(DEAD_ROUTING_KEY); &#125; /** * 创建正常交换机 */ @Bean public DirectExchange ttlExchange() &#123; return new DirectExchange(TTL_EXCHANGE); &#125; /** * 创建正常队列 */ @Bean public Queue ttlQueue() &#123; // 正常队列绑定死信队列 Map&lt;String, Object&gt; args = new HashMap&lt;&gt;(8); // 声明死信队列交换机，消息异常投递到此交换机 args.put(&quot;x-dead-letter-exchange&quot;, DEAD_EXCHANGE); // 声明死信队列路由键 args.put(&quot;x-dead-letter-routing-key&quot;, DEAD_ROUTING_KEY); return new Queue(TTL_QUEUE, true, false, false, args); &#125; /** * 正常队列绑定交换机 */ @Bean public Binding bindTtlQueue(Queue ttlQueue, DirectExchange ttlExchange) &#123; return BindingBuilder.bind(ttlQueue).to(ttlExchange).with(TTL_ROUTING_KEY); &#125;&#125; 添加死信监听器 12345678@Slf4j@Componentpublic class TtlListener &#123; @RabbitListener(queues = TtlQueueConfig.DEAD_QUEUE) public void receive(String msg) &#123; log.info(&quot;死信队列 &#123;&#125; 接收信息:&#123;&#125;&quot;, TtlQueueConfig.DEAD_QUEUE, msg); &#125;&#125; 发送动态TTL消息 1234567891011121314151617@Testpublic void ttlMsgTest() throws InterruptedException &#123; IntStream.range(0, 4).forEach(line -&gt; &#123; int ttl = 60000 - (line * 5000); String msg = &quot;delay:&quot; + line; rabbitTemplate.convertAndSend(TtlQueueConfig.TTL_EXCHANGE, TtlQueueConfig.TTL_ROUTING_KEY, msg, correlationData -&gt; &#123; // 动态设置消息ttl correlationData.getMessageProperties().setExpiration(String.valueOf(ttl)); return correlationData; &#125;); log.info(&quot;发送一条时长: &#123;&#125; 毫秒信息给延迟队列: &#123;&#125; 内容: &#123;&#125;&quot;, ttl, TtlQueueConfig.TTL_QUEUE, msg); &#125;); TimeUnit.SECONDS.sleep(70);&#125; 生产者log 123419:09:32.706 INFO 7708 --- [ main] com.wgf.demo.QeueuTest : 发送一条时长: 60000 毫秒信息给延迟队列: ttl.queue 内容: delay:019:09:32.733 INFO 7708 --- [ main] com.wgf.demo.QeueuTest : 发送一条时长: 55000 毫秒信息给延迟队列: ttl.queue 内容: delay:119:09:32.733 INFO 7708 --- [ main] com.wgf.demo.QeueuTest : 发送一条时长: 50000 毫秒信息给延迟队列: ttl.queue 内容: delay:219:09:32.756 INFO 7708 --- [ main] com.wgf.demo.QeueuTest : 发送一条时长: 45000 毫秒信息给延迟队列: ttl.queue 内容: delay:3 消费者log 123419:10:32.736 INFO 7708 --- [ntContainer#0-1] com.wgf.demo.listener.TtlListener : 死信队列 dead.ttl.overdue.queue 接收信息:delay:019:10:32.736 INFO 7708 --- [ntContainer#0-1] com.wgf.demo.listener.TtlListener : 死信队列 dead.ttl.overdue.queue 接收信息:delay:119:10:32.736 INFO 7708 --- [ntContainer#0-1] com.wgf.demo.listener.TtlListener : 死信队列 dead.ttl.overdue.queue 接收信息:delay:219:10:32.737 INFO 7708 --- [ntContainer#0-1] com.wgf.demo.listener.TtlListener : 死信队列 dead.ttl.overdue.queue 接收信息:delay:3 对比生产者和消费者的日志时间不难看出，TTL时间比第一条时间短的消息全部被第一条阻塞 插件实现延迟队列 上文中提到的问题，确实是一个问题，如果不能实现在消息粒度上的TTL，并使其在设置的TTL时间及时死亡，就无法设计成一个通用的延时队列。那如何解决呢，接下来我们就去解决该问题 安装延时队列插件（docker） rabbitmq在容器中的插件目录：/opt/rabbitmq/plugins 在Tags · rabbitmq/rabbitmq-delayed-message-exchange · GitHub，下载 rabbitmq_delayed_message_exchange 对应版本插件，笔者是在页面获取下载链接使用wget命令下载 3.8.9下载地址 修改文件的权限: chmod 755 rabbitmq_delayed_message_exchange-3.8.9-0199d11c.ez 执行命令rabbitmq-plugins enable rabbitmq_delayed_message_exchange 注意：不需要带插件的版本和文件后缀.ez 安装成功日志 查看已安装的插件：rabbitmq-plugins list 重启RabbitMQ docker容器， 查看插件是否生效 代码实现 定义队列和交换机 123456789101112131415161718192021222324252627282930313233343536373839@Configurationpublic class DelayedConfig &#123; public static final String DELAYED_EXCHANGE = &quot;delayed_exchange&quot;; public static final String DELAYED_QUEUE = &quot;delayed.queue&quot;; public static final String DELAYED_ROUTING_KEY = &quot;Delayed&quot;; @Bean public Queue delayedQueue() &#123; return new Queue(DELAYED_QUEUE); &#125; @Bean public CustomExchange delayedExchange() &#123; // 延迟交换机类型 Map&lt;String, Object&gt; args = new HashMap&lt;&gt;(8); args.put(&quot;x-delayed-type&quot;, &quot;direct&quot;); /** * 1.交换机名称 * 2.交换机类型 * 3.是否需要持久化 * 4.是否自动删除 * 5.其他参数 */ return new CustomExchange(DELAYED_EXCHANGE, &quot;x-delayed-message&quot;,true, false, args); &#125; /** * 延迟交换机绑定队列 * @param delayedQueue * @param delayedExchange * @return */ @Bean public Binding bindDelayedQueue(Queue delayedQueue, CustomExchange delayedExchange) &#123; return BindingBuilder.bind(delayedQueue).to(delayedExchange).with(DELAYED_ROUTING_KEY).noargs(); &#125;&#125; 监听器 123456789@Slf4j@Componentpublic class DelayedListener &#123; // 注释手动ack模式 @RabbitListener(queues = DelayedConfig.DELAYED_QUEUE) public void listener(String msg) &#123; log.info(&quot;延迟队列 &#123;&#125; 接收信息:&#123;&#125;&quot;, DelayedConfig.DELAYED_QUEUE, msg); &#125;&#125; 发送消息 12345678910111213141516@Test public void delayedTest() throws InterruptedException &#123; IntStream.range(0, 10).forEach(line -&gt; &#123; int delay = 60000 - (line * 5000); String msg = &quot;delay:&quot; + line; rabbitTemplate.convertAndSend(DelayedConfig.DELAYED_EXCHANGE, DelayedConfig.DELAYED_ROUTING_KEY, msg, correlationData -&gt; &#123; // 设置消息的延迟时间 单位ms correlationData.getMessageProperties().setDelay(delay); return correlationData; &#125;); log.info(&quot;发送一条时长: &#123;&#125; 毫秒信息给延迟队列: &#123;&#125; 内容: &#123;&#125;&quot;, delay, DelayedConfig.DELAYED_QUEUE, msg); &#125;); TimeUnit.SECONDS.sleep(70); &#125; 生产者log 1234567891020:05:29.763 INFO 14100 --- [ main] com.wgf.demo.QeueuTest : 发送一条时长: 60000 毫秒信息给延迟队列: delayed.queue 内容: delay:020:05:29.785 INFO 14100 --- [ main] com.wgf.demo.QeueuTest : 发送一条时长: 55000 毫秒信息给延迟队列: delayed.queue 内容: delay:120:05:29.786 INFO 14100 --- [ main] com.wgf.demo.QeueuTest : 发送一条时长: 50000 毫秒信息给延迟队列: delayed.queue 内容: delay:220:05:29.808 INFO 14100 --- [ main] com.wgf.demo.QeueuTest : 发送一条时长: 45000 毫秒信息给延迟队列: delayed.queue 内容: delay:320:05:29.808 INFO 14100 --- [ main] com.wgf.demo.QeueuTest : 发送一条时长: 40000 毫秒信息给延迟队列: delayed.queue 内容: delay:420:05:29.808 INFO 14100 --- [ main] com.wgf.demo.QeueuTest : 发送一条时长: 35000 毫秒信息给延迟队列: delayed.queue 内容: delay:520:05:29.831 INFO 14100 --- [ main] com.wgf.demo.QeueuTest : 发送一条时长: 30000 毫秒信息给延迟队列: delayed.queue 内容: delay:620:05:29.832 INFO 14100 --- [ main] com.wgf.demo.QeueuTest : 发送一条时长: 25000 毫秒信息给延迟队列: delayed.queue 内容: delay:720:05:29.832 INFO 14100 --- [ main] com.wgf.demo.QeueuTest : 发送一条时长: 20000 毫秒信息给延迟队列: delayed.queue 内容: delay:820:05:29.832 INFO 14100 --- [ main] com.wgf.demo.QeueuTest : 发送一条时长: 15000 毫秒信息给延迟队列: delayed.queue 内容: delay:9 消费者log 1234567891020:05:44.852 INFO 14100 --- [ntContainer#3-1] com.wgf.demo.listener.DelayedListener : 延迟队列 delayed.queue 接收信息:delay:920:05:49.845 INFO 14100 --- [ntContainer#3-1] com.wgf.demo.listener.DelayedListener : 延迟队列 delayed.queue 接收信息:delay:820:05:54.844 INFO 14100 --- [ntContainer#3-1] com.wgf.demo.listener.DelayedListener : 延迟队列 delayed.queue 接收信息:delay:720:05:59.844 INFO 14100 --- [ntContainer#3-1] com.wgf.demo.listener.DelayedListener : 延迟队列 delayed.queue 接收信息:delay:620:06:04.821 INFO 14100 --- [ntContainer#3-1] com.wgf.demo.listener.DelayedListener : 延迟队列 delayed.queue 接收信息:delay:520:06:09.821 INFO 14100 --- [ntContainer#3-1] com.wgf.demo.listener.DelayedListener : 延迟队列 delayed.queue 接收信息:delay:420:06:14.821 INFO 14100 --- [ntContainer#3-1] com.wgf.demo.listener.DelayedListener : 延迟队列 delayed.queue 接收信息:delay:320:06:19.799 INFO 14100 --- [ntContainer#3-1] com.wgf.demo.listener.DelayedListener : 延迟队列 delayed.queue 接收信息:delay:220:06:24.798 INFO 14100 --- [ntContainer#3-1] com.wgf.demo.listener.DelayedListener : 延迟队列 delayed.queue 接收信息:delay:120:06:29.776 INFO 14100 --- [ntContainer#3-1] com.wgf.demo.listener.DelayedListener : 延迟队列 delayed.queue 接收信息:delay:0 从log上看，延迟队列实现了消息粒度的TTL 延迟交换机可以看正在被延迟的消息条数 x-delayed-type 取值 direct fanout topic headers 总结 延时队列在需要延时处理的场景下非常有用，使用 RabbitMQ 来实现延时队列可以很好的利用 RabbitMQ 的特性，如：消息可靠发送、消息可靠投递、死信队列来保障消息至少被消费一次以及未被正确处理的消息不会被丢弃。另外，通过 RabbitMQ集群的特性，可以很好的解决单点故障问题，不会因为单个节点挂掉导致延时队列不可用或者消息丢失。 当然，延时队列还有很多其它选择，比如利用 Java 的 DelayQueue，利用 Redis 的 zset，利用 Quartz 或者利用 kafka 的时间轮，这些方式各有特点,看需要适用的场景 优先级队列 优先级队列是一种队列类型，它允许你为消息设置不同的优先级，以便按照这些优先级来处理消息，越高优先级的消息可能越早被处理。 消费者需要等待所有消息都进入队列后才能开始消费。这是因为要进行消息的优先级排序，队列必须拥有足够的消息来进行比较。因此，只有当队列中积累了足够的消息后，系统才能按照消息的优先级有序地处理它们。 使用场景 有消息堆积则按优先级排序，优先级高的先执行 订单处理：在电子商务平台上，处理订单可能需要不同的速度，以应对大客户和普通客户的需求。通过为订单设置不同的优先级，可以确保及时处理高价值订单。 任务调度：在任务调度系统中，某些任务可能比其他任务更重要或更紧急。使用优先级队列可以确保高优先级任务优先执行，以满足业务需求。 通知和提醒：在需要发送通知或提醒的应用中，例如发送短信或电子邮件提醒，您可以使用优先级队列来处理即时通知和紧急提醒，确保它们在时间上得到优先处理。 资源分配：在资源有限的情况下，可以使用优先级队列来分配资源，确保高优先级任务或消息得到优先满足。 代码实现 队列配置 1234567891011121314151617181920212223242526272829303132333435363738@Configurationpublic class PriorityConfig &#123; public static final String PRIORITY_QUEUE = &quot;priority.queue&quot;; public static final String PRIORITY_EXCHANGE = &quot;priority.queue&quot;; public static final String ROUTING_KEY = &quot;priority&quot;; /** * 创建队列设置优先级 */ @Bean public Queue priorityQueue() &#123; Map&lt;String, Object&gt; args = new HashMap&lt;&gt;(8); // 设置队列的优先级，取值[0~255] 建议[0~10], 如果有消息堆积则会按照优先级排序，越高的越快被消费 args.put(&quot;x-max-priority&quot;, 10); return new Queue(PRIORITY_QUEUE, true, false, false, args); &#125; /** * 创建交换机 */ @Bean public DirectExchange priorityExchange() &#123; // 默认持久化 return new DirectExchange(PRIORITY_EXCHANGE, true, false); &#125; /** * 绑定队列 * * @param directQueue * @param directExchange * @return */ @Bean public Binding bindPriorityQueue(Queue priorityQueue, DirectExchange priorityExchange) &#123; return BindingBuilder.bind(priorityQueue).to(priorityExchange).with(ROUTING_KEY); &#125;&#125; 消息监听器 12345678910111213@Slf4j@Componentpublic class PriorityListener &#123; /** * 要看到效果，必选让消息全部发送到队列后再进行监听消费，让消息有排序的时间，实时消费排序效果不明显 * * @param msg */ @RabbitListener(queues = PriorityConfig.PRIORITY_QUEUE) public void receive(String msg) &#123; log.info(&quot;队列 &#123;&#125; 接收信息:&#123;&#125;&quot;, PriorityConfig.PRIORITY_QUEUE, msg); &#125;&#125; 发送消息 12345678910111213141516171819202122@Testpublic void priorityTest() &#123; IntStream.range(1, 10).forEach(line -&gt; &#123; int temp = line; // 能被2整除的优先级最高 if (line % 2 == 0) &#123; temp = 10; &#125; int priority = temp; String msg = &quot;优先级:&quot; + priority; rabbitTemplate.convertAndSend(PriorityConfig.PRIORITY_EXCHANGE, PriorityConfig.ROUTING_KEY, msg, correlationData -&gt; &#123; // 设置队列优先级 correlationData.getMessageProperties().setPriority(priority); return correlationData; &#125;); log.info(&quot;发送一条优先级: &#123;&#125; 信息给队列: &#123;&#125; 内容: &#123;&#125;&quot;, priority, PriorityConfig.PRIORITY_QUEUE, msg); &#125;);&#125; 生产者log 12345678922:29:31.274 INFO 14480 --- [ main] com.wgf.demo.QeueuTest : 发送一条优先级: 1 信息给队列: priority.queue 内容: 优先级:122:29:31.297 INFO 14480 --- [ main] com.wgf.demo.QeueuTest : 发送一条优先级: 10 信息给队列: priority.queue 内容: 优先级:1022:29:31.297 INFO 14480 --- [ main] com.wgf.demo.QeueuTest : 发送一条优先级: 3 信息给队列: priority.queue 内容: 优先级:322:29:31.321 INFO 14480 --- [ main] com.wgf.demo.QeueuTest : 发送一条优先级: 10 信息给队列: priority.queue 内容: 优先级:1022:29:31.321 INFO 14480 --- [ main] com.wgf.demo.QeueuTest : 发送一条优先级: 5 信息给队列: priority.queue 内容: 优先级:522:29:31.321 INFO 14480 --- [ main] com.wgf.demo.QeueuTest : 发送一条优先级: 10 信息给队列: priority.queue 内容: 优先级:1022:29:31.345 INFO 14480 --- [ main] com.wgf.demo.QeueuTest : 发送一条优先级: 7 信息给队列: priority.queue 内容: 优先级:722:29:31.345 INFO 14480 --- [ main] com.wgf.demo.QeueuTest : 发送一条优先级: 10 信息给队列: priority.queue 内容: 优先级:1022:29:31.345 INFO 14480 --- [ main] com.wgf.demo.QeueuTest : 发送一条优先级: 9 信息给队列: priority.queue 内容: 优先级:9 消费者log 12345678922:30:29.740 INFO 21004 --- [ntContainer#0-1] com.wgf.demo.listener.PriorityListener : 队列 priority.queue 接收信息:优先级:1022:30:29.742 INFO 21004 --- [ntContainer#0-1] com.wgf.demo.listener.PriorityListener : 队列 priority.queue 接收信息:优先级:1022:30:29.742 INFO 21004 --- [ntContainer#0-1] com.wgf.demo.listener.PriorityListener : 队列 priority.queue 接收信息:优先级:1022:30:29.742 INFO 21004 --- [ntContainer#0-1] com.wgf.demo.listener.PriorityListener : 队列 priority.queue 接收信息:优先级:1022:30:29.743 INFO 21004 --- [ntContainer#0-1] com.wgf.demo.listener.PriorityListener : 队列 priority.queue 接收信息:优先级:922:30:29.754 INFO 21004 --- [ntContainer#0-1] com.wgf.demo.listener.PriorityListener : 队列 priority.queue 接收信息:优先级:722:30:29.754 INFO 21004 --- [ntContainer#0-1] com.wgf.demo.listener.PriorityListener : 队列 priority.queue 接收信息:优先级:522:30:29.755 INFO 21004 --- [ntContainer#0-1] com.wgf.demo.listener.PriorityListener : 队列 priority.queue 接收信息:优先级:322:30:29.755 INFO 21004 --- [ntContainer#0-1] com.wgf.demo.listener.PriorityListener : 队列 priority.queue 接收信息:优先级:1 优先级队列标志 惰性队列 消息存入磁盘，减少内存占用以便存储更多消息 RabbitMQ 从 3.6.0 版本开始引入了惰性队列的概念。惰性队列（Lazy Queues）是一种特殊类型的队列，旨在提供对大量消息的高效存储和检索。这些队列的设计目标是最小化内存占用，尤其在处理大型消息或大量消息时。以下是有关RabbitMQ惰性队列的详细说明： 消息的持久性存储：惰性队列将消息持久地存储在磁盘上而不是内存中，这意味着即使RabbitMQ服务器重新启动，消息也不会丢失。这对于关键数据的可靠性非常重要。 内存优化：通常，RabbitMQ队列会将消息保留在内存中以加快消息传递速度。然而，在某些情况下，这可能导致内存耗尽，特别是当处理大型消息或大量消息时。惰性队列通过将消息写入磁盘，减少了内存的使用，因此更适合处理大容量数据。 延迟加载：与普通队列不同，惰性队列在消息入队时不会立即将消息写入磁盘。相反，它们采用延迟加载的策略，只有在消息确实需要时才将其加载到内存中。这有助于降低消息入队的延迟，并减少了磁盘I/O的负担。 适用场景：惰性队列特别适用于需要处理大型文件、大型数据集或需要长时间存储消息的情况。它们可以减少RabbitMQ服务器的内存占用，从而提高系统的稳定性和性能。 注意 惰性队列的消息存储在磁盘上，当消息被消费时，需要从磁盘加载到内存，然后才会被推送给消费者。因此，惰性队列的消费速度相对较慢。它适合用于消息堆积较多且消息的实时性要求不高的场景。 两种模式 队列具备两种模式：default 和 lazy。默认的为 default 模式，在 3.6.0 之前的版本无需做任何变更。lazy 模式即为惰性队列的模式，可以通过调用 channel.queueDeclare 方法的时候在参数中设置，也可以通过 Policy 的方式设置，如果一个队列同时使用这两种方式设置的话，那么 Policy 的方式具备更高的优先级。 如果要通过声明的方式改变已有队列的模式的话，那么只能先删除队列，然后再重新声明一个新的 在队列声明的时候可以通过x-queue-mode参数来设置队列的模式，取值为default和lazy 代码实现 队列配置 123456789101112131415161718192021222324252627282930313233343536373839404142@Configurationpublic class LazyConfig &#123; public static final String LAZY_QUEUE = &quot;lazy.queue&quot;; public static final String LAZY_EXCHANGE = &quot;lazy_exchange&quot;; public static final String ROUTING_KEY = &quot;lazy&quot;; /** * 创建惰性队列 * * @return */ @Bean public Queue lazyQueue() &#123; // 延迟交换机类型 Map&lt;String, Object&gt; args = new HashMap&lt;&gt;(8); args.put(&quot;x-queue-mode&quot;, &quot;lazy&quot;); return new Queue(LAZY_QUEUE, false, false, false, args); &#125; /** * 创建交换机 * * @return */ @Bean public DirectExchange lazyExchange() &#123; return new DirectExchange(LAZY_EXCHANGE); &#125; /** * 队列绑定 * @param directQueue * @param directExchange * @return */ @Bean public Binding bindLazyQueue(Queue lazyQueue, DirectExchange lazyExchange) &#123; return BindingBuilder.bind(lazyQueue).to(lazyExchange).with(ROUTING_KEY); &#125;&#125; 发送消息 1234567@Testpublic void lazyTest() &#123; String msg = UUID.randomUUID().toString(); for (int i = 0; i &lt; 1000000; i++) &#123; rabbitTemplate.convertAndSend(LazyConfig.LAZY_EXCHANGE, LazyConfig.ROUTING_KEY, msg); &#125;&#125; 惰性队列与普通队列比较 内存占用情况 由此可见惰性队列将数据存储在硬盘中，内存占用较小，适合大量消息堆积场景使用 命令：rabbitmqctl list_queues name messages memory 备份交换机 处理无法被交换机路由的消息 消息不丢失又不增加生产者复杂性 在RabbitMQ中，备份交换机可以理解为交换机的“备胎”。当一个交换机接收到一条无法路由的消息时，它会把这条消息转发给备份交换机，由备份交换机来进行转发和处理。备份交换机的类型通常为“fanout”型，这样就能把所有消息都投递到与其绑定的队列中。 备份交换机的优点在于，它提供了一种有效的处理不可路由消息的机制，避免了这些消息被丢失或未被处理的情况。 代码实现 备份交换机配置 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107@Componentpublic class BackUpConfig &#123; /** * 备份交换机配置 */ public static final String BACKUP_EXCHANGE = &quot;backup&quot;; public static final String BACKUP_QUEUE = &quot;backup.queue&quot;; public static final String WARING_QUEUE = &quot;waring.queue&quot;; /** * 备份队列 * * @return */ @Bean public Queue backupQueue() &#123; return new Queue(BACKUP_QUEUE); &#125; /** * 警告队列 * * @return */ @Bean public Queue waringQueue() &#123; return new Queue(WARING_QUEUE); &#125; /** * 备份交换机 * * @return */ @Bean public FanoutExchange backupExchange() &#123; return new FanoutExchange(BACKUP_EXCHANGE); &#125; /** * 备份交换机绑定报警队列 * * @param fanoutQueue1 * @param fanoutExchange * @return */ @Bean public Binding bindWaringQueue(Queue waringQueue, FanoutExchange backupExchange) &#123; // 不需要 routing_key return BindingBuilder.bind(waringQueue).to(backupExchange); &#125; /** * 备份交换机绑定备份队列 * * @param fanoutQueue1 * @param fanoutExchange * @return */ @Bean public Binding bindBackupQueue(Queue backupQueue, FanoutExchange backupExchange) &#123; // 不需要 routing_key return BindingBuilder.bind(backupQueue).to(backupExchange); &#125; /** * 正常交换机队列配置 */ public static final String EXCHANGE = &quot;product_exchange&quot;; public static final String QUEUE = &quot;product.queue&quot;; public static final String ROUTING_KEY = &quot;product&quot;; /** * 创建队列 * * @return */ @Bean public Queue productQueue() &#123; return new Queue(QUEUE); &#125; /** * 创建交换机 * * @return */ @Bean public DirectExchange productExchange() &#123; ExchangeBuilder exchangeBuilder = ExchangeBuilder.directExchange(EXCHANGE) .durable(true) // 指定交换机的备份交换机 .withArgument(&quot;alternate-exchange&quot;, BACKUP_EXCHANGE); return exchangeBuilder.build(); &#125; /** * 正常交换机绑定商品队列 * @param productQueue * @param productExchange * @return */ @Bean public Binding bindProductQueue(Queue productQueue, DirectExchange productExchange) &#123; return BindingBuilder.bind(productQueue).to(productExchange).with(ROUTING_KEY); &#125;&#125; 消息监听器 123456789101112131415161718@Slf4j@Componentpublic class BackUpListener &#123; @RabbitListener(queues = BackUpConfig.QUEUE) public void listenerProduct(String msg) &#123; log.info(&quot;商品队列接收消息：&#123;&#125;&quot;, msg); &#125; @RabbitListener(queues = BackUpConfig.BACKUP_QUEUE) public void listenerBackup(String msg) &#123; log.info(&quot;备份队列接收消息：&#123;&#125;&quot;, msg); &#125; @RabbitListener(queues = BackUpConfig.WARING_QUEUE) public void listenerWaring(String msg) &#123; log.info(&quot;报警队列接收消息：&#123;&#125;&quot;, msg); &#125;&#125; 发送消息 12345@Test public void backUpTest() &#123; rabbitTemplate.convertAndSend(BackUpConfig.EXCHANGE, BackUpConfig.ROUTING_KEY, &quot;正常商品消息&quot;); rabbitTemplate.convertAndSend(BackUpConfig.EXCHANGE, &quot;test&quot;, &quot;没有路由键的商品消息&quot;); &#125; 消费者log 123INFO 860 --- [ntContainer#1-1] com.wgf.demo.listener.BackUpListener : 商品队列接收消息：正常商品消息INFO 860 --- [ntContainer#3-1] com.wgf.demo.listener.BackUpListener : 备份队列接收消息：没有路由键的商品消息INFO 860 --- [ntContainer#2-1] com.wgf.demo.listener.BackUpListener : 报警队列接收消息：没有路由键的商品消息 由log体现，路由键为test的消息被转发到备份交换机处理 备份交换机标识 控制台指定备份交换机 备份交换机和ReturnCallback 当交换机绑定了备份交换机并且生产者代码中开启了 mandatory 参数和声明了ReturnCallback时，备份交换机的优先级更高，会将消息投递到备份交换机，而 ReturnCallback 不执行 RPC模式 TODO https://zq99299.github.io/mq-tutorial/rabbitmq-ac/04/06.html RabbitMQ集群 集群搭建 仲裁队列创建 最开始我们介绍了如何安装及运行 RabbitMQ 服务，不过这些是单机版的，无法满足目前真实应用的 要求。如果 RabbitMQ 服务器遇到内存崩溃、机器掉电或者主板故障等情况，该怎么办？单台 RabbitMQ 服务器可以满足每秒 1000 条消息的吞吐量，那么如果应用需要 RabbitMQ 服务满足每秒 10 万条消息的吞吐量呢？购买昂贵的服务器来增强单机 RabbitMQ 务的性能显得捉襟见肘，搭建一个 RabbitMQ 集群才是解决实际问题的关键 集群节点类型 内存节点（ram）：就是将元数据都放在内存里，内存节点的话，只要服务重启，该节点的所有数据将会丢失 硬盘节点（disk）：就是将元数据都放在硬盘里，所以服务重启的话，数据也还是会存在 注意：硬盘节点如果宕机了，交换机，队列，绑定关系将不可创建，但是原有队列还可以正常使用 仲裁队列 在 RabbitMQ 中，仲裁队列（Quorum Queue）是一种高可用性的队列类型，基于 raft 共识算法的变种实现，它提供了一种可靠的消息存储和传递机制，适用于分布式系统中对消息丢失非常敏感的应用场景。仲裁队列是 RabbitMQ 3.8 版本引入的新功能，它与传统的镜像队列（Mirrored Queue）相比具有数据一致性优势。 描述： 基于 Raft：仲裁队列基于 Raft 协议实现，确保在分布式环境中的一致性和容错能力。 同步复制：消息在被认为已提交之前，必须被复制到集群中的大多数节点。 优点： 强一致性：由于使用了 Raft 协议，仲裁队列提供了强一致性保证。 读取性能：读取操作可以在任何节点上进行，提供较好的读取性能。 缺点： 写入性能：由于同步复制的要求，写入性能可能不如镜像队列。 使用场景： 当数据一致性是首要考虑的因素时。 仲裁队列的消息会优先保存在内存中，使用仲裁队列时，建议定义队列最大长度和最大内存占用，在消息堆积超过阈值时从内存转移到磁盘，以免造成内存高水位 镜像队列 描述： 异步复制：消息被首先写入主节点，然后异步复制到镜像节点。 主-备架构：主节点负责处理写入，镜像节点用于故障恢复。 优点： 写入性能：由于使用异步复制，写入性能通常优于仲裁队列。 灵活配置：允许灵活配置哪些节点应该存储队列的副本。 缺点： 一致性问题：在网络分区或节点故障的情况下，可能会面临一致性问题。 配置复杂性：配置比仲裁队列复杂。 使用场景： 当写入性能是首要考虑的因素时 Haproxy 负载均衡：HAProxy 可以将来自客户端的连接请求分发到 RabbitMQ 集群中的多个节点。 高可用性：在 RabbitMQ 节点发生故障时，HAProxy 可以自动将流量重定向到健康的节点。 Keepalived 故障转移：RabbitMQ 客户端使用虚拟 IP 地址连接到 RabbitMQ 集群。这样，即使当前的主 RabbitMQ 节点失败，Keepalived 会将虚拟 IP 地址切换到另一个健康的节点，客户端可以在不更改 IP 地址的情况下继续发送和接收消息。 RabbitMQ 可靠性知识 消息幂等 在使用 RabbitMQ 时，有时候我们会遇到这样一个问题：同一条消息被消费多次，也就是 重复消费。这通常发生在以下几种情况： 网络中断：消费者成功处理了消息，但在发送确认（ack）给 RabbitMQ 时网络中断，导致 RabbitMQ 没有收到确认。 消费异常：消费者在处理消息时发生异常，虽然消息实际上已被处理，但消费者没有发送确认给 RabbitMQ。 如何保证消息幂等性，不被重复消费 使用唯一消息标识符： 为每条消息分配一个全局唯一的 ID（MessageID）。 在消费消息之前，检查该消息ID是否已被处理过。 如果已处理过，跳过该消息；如果未处理过，正常消费并记录该消息ID。 利用业务逻辑： 使用与业务相关的唯一标识符（例如，订单号）来检查消息是否已被处理。 在处理消息之前，验证该业务标识符是否已在系统中被处理或记录过。 如果已处理，忽略该消息；如果未处理，正常消费并记录该业务标识符 全局MessageID 方案（Redis） 使用 Redis BitMap判断是否幂等消费 生成全局唯一ID，然后由BitMap判断是否消费 利用 redis 执行 setnx 命令，天然具有幂等性。 利用 SADD queueName 消息id 命令存储待消费消息id 利用 SISMEMBER queueName 消息id 命令判断消息是否已消费 利用SREM queueName 消息id 删除已消费的消息id 架构图 优点：消息id不和业务绑定，通用性较强 缺点：架构引入了Redis，需要保证Redis的可用性 使用业务ID方案 使用业务编号作为消息id 每次消费时根据消息id查询数据状态，满足则消费消费，否则丢弃消息保证幂等 场景：订单异步退款 架构图 优点：架构实现简单，相对可靠，贴合业务 缺点：与业务严重偶合，带有一定的业务入侵，无法通用所有场景 消息可靠投递 消息丢失场景 Producer发送消息到 Broker 的过程中，如果Producer或Broker由于网络或者磁盘问题，未能成功接收消息，可能导致消息丢失 Exchange路由Queue过程中，没有匹配到相应的Routing Key导致消息丢失 Comsumer在消费异常时，自动ACK，导致消息没有重回队列头部，消息丢失 消息投递到Broker可靠保证 ​ 使用 ConfirmCallback 异步回调确认机制确认消息是否发送到Exchange 消息Exchange路由到Queue可靠保证 使用 ReturnCallback 感知消息是否无法路由被退回 或使用 备用交换机 转发无法路由的消息做警报和业务补偿 保证消息最终被正确消费 启用 手动ack 机制，消费异常进行Nack，让消息重回队列 或使用 死信队列 ，消息消费异常拒绝消息签收，转发到死信队列，由其他业务进行补偿 消息重发机制（投递失败业务补偿） 使用Redis持久化消息，提供失败消息重新投递数据支撑，DB层也可选择Mysql等其他产品，具体看业务场景 定时任务实现消息重新投递，进行业务补偿 消息幂等 消息幂等，基于Redis的自增Id和BitMap的通用方案或使用业务数据状态 消息最终消费保证 ​ 采用手动ack + 定时任务 + 人工处理方式保证 人工补偿，防止消息无限消费 当重试超过阈值消息永久存储在DB中，这时需要人工介入进行业务补偿，补偿完毕删除DB数据 具体实现 根据以上的理论对方案进行代码实现，解决点有 消息幂等 消息可靠投递保证 消息最终消费保证 技术栈 Mysql，针对11.2章节方案，DB层改为Mysql。原因：底层更通用，便于人工补偿数据检索和扩展。 MybatisPlus RabbitMQ Redis，用于生成消息ID (可不用)，可用雪花算法或美团的 Leaf 等方式生成；保证消息幂等 Scheduled，为了方便实现使用Spring定时任务，推荐使用xxl-job等第三方框架进行补偿解耦 源码地址：rabbitmq-example，模拟用户下单异步生成订单扣减库存流程 关键字段说明 图中的状态代表消息日志状态的流转 logId 消息日志主键，用于变更状态 msgId 消息全局唯一id messageId logId_msgId 拼接，消息体实际使用的id 服务说明 producer_server 消息发送服务，提供消息发送和消息重新投递能力 consumer_server 消息消费者 reparation-server 补偿服务，提供定期补偿消息重新投递，堆积消息日志异步删除能力 核心代码讲解 producer-server 集成swagger 项目启动访问 http://localhost:9000/doc.html MsgController 对外提供消息投递与消息重新投递能力 RabbitMqUtils 消息投递与持久化具体实现 SequenceUtils 全局唯一id具体实现 MsgConfirmCallback 消息投递Broker失败异步回调，重新实现消息投递核心 MsgReturnCallback 消息路由失败异步回调，重新实现消息投递核心 consumer-server @IdempotentMessage 消息幂等与消息重新消费自定义注解，作用于监听方法（可选择是否使用） RabbitListenerAop 消息幂等于重新消费限制具体实现 reparation-server ReparationTask 消息重新投递具体实现 CleanMsgTask 消息日志异步清理实现 消息状态 投递中 投递成功 投递失败 人工修复 消费成功 重复消费 额外扩展 路由失败: 也可以使用 备份交换机 方案，由reparation-server 监听备份队列进行业务补偿 拒绝消息 重试多次消费上限丢弃消息可以使用 死信队列 方案解决 项目下demo.sql为mysql DDL 脚本","categories":[],"tags":[]}],"categories":[],"tags":[{"name":"文章已过时","slug":"文章已过时","permalink":"https://wugengfeng.cn/tags/%E6%96%87%E7%AB%A0%E5%B7%B2%E8%BF%87%E6%97%B6/"}]}